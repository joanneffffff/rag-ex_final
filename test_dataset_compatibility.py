#!/usr/bin/env python3
"""
æµ‹è¯•æ•°æ®é›†å˜åŒ–å…¼å®¹æ€§
éªŒè¯ç³»ç»Ÿèƒ½å¦è‡ªåŠ¨æ£€æµ‹æ•°æ®å˜åŒ–å¹¶é‡æ–°ç”Ÿæˆç´¢å¼•
"""

import os
import sys
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from xlm.components.retriever.bilingual_retriever import BilingualRetriever
from xlm.components.encoder.finbert_encoder import FinbertEncoder
from xlm.utils.document_loader import DocumentLoader

def test_dataset_compatibility():
    """æµ‹è¯•æ•°æ®é›†å˜åŒ–å…¼å®¹æ€§"""
    
    print("ğŸ§ª æµ‹è¯•æ•°æ®é›†å˜åŒ–å…¼å®¹æ€§")
    print("=" * 50)
    
    # 1. åˆå§‹åŒ–ç¼–ç å™¨
    print("1. åˆå§‹åŒ–ç¼–ç å™¨...")
    encoder_en = FinbertEncoder("sentence-transformers/all-MiniLM-L6-v2")
    encoder_ch = FinbertEncoder("shibing624/text2vec-base-chinese")
    
    # 2. åŠ è½½æ–‡æ¡£
    print("2. åŠ è½½æ–‡æ¡£...")
    loader = DocumentLoader()
    
    # åŠ è½½è‹±æ–‡æ–‡æ¡£ï¼ˆä¿®å¤åçš„æ•°æ®ï¼‰
    english_docs = loader.load_documents_from_jsonl(
        "data/unified/tatqa_knowledge_base_combined.jsonl",
        language="english"
    )
    print(f"   è‹±æ–‡æ–‡æ¡£æ•°é‡: {len(english_docs)}")
    
    # åŠ è½½ä¸­æ–‡æ–‡æ¡£
    chinese_docs = loader.load_documents_from_jsonl(
        "data/alphafin/alphafin_summarized_and_structured_qa_0628_colab_missing.json",
        language="chinese"
    )
    print(f"   ä¸­æ–‡æ–‡æ¡£æ•°é‡: {len(chinese_docs)}")
    
    # 3. åˆå§‹åŒ–æ£€ç´¢å™¨
    print("3. åˆå§‹åŒ–æ£€ç´¢å™¨...")
    retriever = BilingualRetriever(
        encoder_en=encoder_en,
        encoder_ch=encoder_ch,
        corpus_documents_en=english_docs,
        corpus_documents_ch=chinese_docs,
        use_faiss=True,
        use_existing_embedding_index=True
    )
    
    # 4. æµ‹è¯•æ£€ç´¢åŠŸèƒ½
    print("4. æµ‹è¯•æ£€ç´¢åŠŸèƒ½...")
    
    # æµ‹è¯•è‹±æ–‡æŸ¥è¯¢
    print("\nğŸ“ æµ‹è¯•è‹±æ–‡æŸ¥è¯¢:")
    test_query_en = "How was internally developed software capitalised?"
    results_en = retriever.retrieve(test_query_en, top_k=3)
    print(f"   æŸ¥è¯¢: {test_query_en}")
    print(f"   ç»“æœæ•°é‡: {len(results_en)}")
    if results_en:
        print(f"   ç¬¬ä¸€ä¸ªç»“æœ: {results_en[0].content[:100]}...")
    
    # æµ‹è¯•ä¸­æ–‡æŸ¥è¯¢
    print("\nğŸ“ æµ‹è¯•ä¸­æ–‡æŸ¥è¯¢:")
    test_query_ch = "ä¸­å…´é€šè®¯åœ¨AIæ—¶ä»£å¦‚ä½•å¸ƒå±€é€šä¿¡èƒ½åŠ›æå‡ï¼Ÿ"
    results_ch = retriever.retrieve(test_query_ch, top_k=3)
    print(f"   æŸ¥è¯¢: {test_query_ch}")
    print(f"   ç»“æœæ•°é‡: {len(results_ch)}")
    if results_ch:
        print(f"   ç¬¬ä¸€ä¸ªç»“æœ: {results_ch[0].content[:100]}...")
    
    # 5. æ£€æŸ¥ç´¢å¼•çŠ¶æ€
    print("\n5. æ£€æŸ¥ç´¢å¼•çŠ¶æ€:")
    print(f"   è‹±æ–‡FAISSç´¢å¼•: {'å·²åˆå§‹åŒ–' if retriever.index_en else 'æœªåˆå§‹åŒ–'}")
    print(f"   ä¸­æ–‡FAISSç´¢å¼•: {'å·²åˆå§‹åŒ–' if retriever.index_ch else 'æœªåˆå§‹åŒ–'}")
    print(f"   è‹±æ–‡åµŒå…¥å‘é‡: {retriever.corpus_embeddings_en.shape if retriever.corpus_embeddings_en is not None else 'None'}")
    print(f"   ä¸­æ–‡åµŒå…¥å‘é‡: {retriever.corpus_embeddings_ch.shape if retriever.corpus_embeddings_ch is not None else 'None'}")
    
    print("\nâœ… æ•°æ®é›†å˜åŒ–å…¼å®¹æ€§æµ‹è¯•å®Œæˆï¼")
    
    return True

if __name__ == "__main__":
    test_dataset_compatibility() 