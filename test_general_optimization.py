#!/usr/bin/env python3
"""
æµ‹è¯•ä¸Šä¸‹æ–‡ä¼˜åŒ–åœ¨ä¸åŒç±»å‹æŸ¥è¯¢ä¸­çš„é€šç”¨æ€§
"""

import sys
import os
import re
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def extract_keywords_general(query: str, domain: str = "general") -> list:
    """
    é€šç”¨å…³é”®è¯æå–å‡½æ•°ï¼Œæ”¯æŒä¸åŒé¢†åŸŸ
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        domain: é¢†åŸŸç±»å‹ ("financial", "technical", "general")
    
    Returns:
        å…³é”®è¯åˆ—è¡¨
    """
    keywords = []
    
    if domain == "financial":
        # é‡‘èé¢†åŸŸå…³é”®è¯
        # æå–è‚¡ç¥¨ä»£ç 
        stock_pattern = r'[A-Z]{2}\d{4}|[A-Z]{2}\d{6}|\d{6}'
        stock_matches = re.findall(stock_pattern, query)
        keywords.extend(stock_matches)
        
        # æå–å…¬å¸åç§°
        company_pattern = r'([A-Za-z\u4e00-\u9fff]+)(?:å…¬å¸|é›†å›¢|è‚¡ä»½|æœ‰é™)'
        company_matches = re.findall(company_pattern, query)
        keywords.extend(company_matches)
        
        # æå–å¹´ä»½
        year_pattern = r'20\d{2}å¹´'
        year_matches = re.findall(year_pattern, query)
        keywords.extend(year_matches)
        
        # é‡‘èå…³é”®æ¦‚å¿µ
        key_concepts = ['åˆ©æ¶¦', 'è¥æ”¶', 'å¢é•¿', 'ä¸šç»©', 'é¢„æµ‹', 'åŸå› ', 'ä¸»è¦', 'æŒç»­', 'è‚¡ä»·', 'å¸‚å€¼', 'è´¢åŠ¡', 'æŠ¥å‘Š']
        
    elif domain == "technical":
        # æŠ€æœ¯é¢†åŸŸå…³é”®è¯
        # æå–æŠ€æœ¯æœ¯è¯­
        tech_pattern = r'[A-Z][a-z]+(?:[A-Z][a-z]+)*'  # é©¼å³°å‘½å
        tech_matches = re.findall(tech_pattern, query)
        keywords.extend(tech_matches)
        
        # æå–ç‰ˆæœ¬å·
        version_pattern = r'\d+\.\d+(?:\.\d+)?'
        version_matches = re.findall(version_pattern, query)
        keywords.extend(version_matches)
        
        # æŠ€æœ¯å…³é”®æ¦‚å¿µ
        key_concepts = ['æ€§èƒ½', 'ä¼˜åŒ–', 'ç®—æ³•', 'æ¶æ„', 'ç³»ç»Ÿ', 'å¼€å‘', 'æµ‹è¯•', 'éƒ¨ç½²', 'å®‰å…¨', 'æ•ˆç‡']
        
    else:
        # é€šç”¨é¢†åŸŸå…³é”®è¯
        # æå–æ•°å­—
        number_pattern = r'\d+'
        number_matches = re.findall(number_pattern, query)
        keywords.extend(number_matches)
        
        # æå–è‹±æ–‡å•è¯
        english_pattern = r'[A-Za-z]+'
        english_matches = re.findall(english_pattern, query)
        keywords.extend(english_matches)
        
        # é€šç”¨å…³é”®æ¦‚å¿µ
        key_concepts = ['å¦‚ä½•', 'ä»€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'æ–¹æ³•', 'æ­¥éª¤', 'åŸå› ', 'ç»“æœ', 'å½±å“', 'å»ºè®®']
    
    # æ·»åŠ é¢†åŸŸç‰¹å®šçš„å…³é”®æ¦‚å¿µ
    for concept in key_concepts:
        if concept in query:
            keywords.append(concept)
    
    return list(set(keywords))

def extract_relevant_sentences_general(content: str, keywords: list, max_chars_per_doc: int = 800) -> list:
    """
    é€šç”¨å¥å­æå–å‡½æ•°
    
    Args:
        content: æ–‡æ¡£å†…å®¹
        keywords: å…³é”®è¯åˆ—è¡¨
        max_chars_per_doc: æ¯ä¸ªæ–‡æ¡£æœ€å¤§å­—ç¬¦æ•°
    
    Returns:
        ç›¸å…³å¥å­åˆ—è¡¨
    """
    if not content or not keywords:
        return []
    
    # æŒ‰å¥å­åˆ†å‰²ï¼ˆæ”¯æŒä¸­è‹±æ–‡ï¼‰
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n\.\!\?]+', content)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    # è®¡ç®—æ¯ä¸ªå¥å­çš„ç›¸å…³æ€§åˆ†æ•°
    sentence_scores = []
    for sentence in sentences:
        score = 0
        for keyword in keywords:
            if keyword.lower() in sentence.lower():  # ä¸åŒºåˆ†å¤§å°å†™
                score += 1
        # è€ƒè™‘å¥å­é•¿åº¦ï¼Œé¿å…è¿‡é•¿çš„å¥å­
        if len(sentence) > 200:
            score *= 0.5
        sentence_scores.append((sentence, score))
    
    # æŒ‰åˆ†æ•°æ’åº
    sentence_scores.sort(key=lambda x: x[1], reverse=True)
    
    # é€‰æ‹©æœ€ç›¸å…³çš„å¥å­
    selected_sentences = []
    total_chars = 0
    
    for sentence, score in sentence_scores:
        if score > 0 and total_chars + len(sentence) <= max_chars_per_doc:
            selected_sentences.append(sentence)
            total_chars += len(sentence)
    
    return selected_sentences

def test_different_domains():
    """æµ‹è¯•ä¸åŒé¢†åŸŸçš„æŸ¥è¯¢ä¼˜åŒ–æ•ˆæœ"""
    
    print("ğŸ§ª æµ‹è¯•ä¸Šä¸‹æ–‡ä¼˜åŒ–åœ¨ä¸åŒé¢†åŸŸçš„é€šç”¨æ€§")
    print("=" * 60)
    
    # æµ‹è¯•æŸ¥è¯¢é›†åˆ
    test_queries = {
        "financial": [
            "å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´åˆ©æ¶¦æŒç»­å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ",
            "000049çš„ä¸šç»©è¡¨ç°å¦‚ä½•ï¼Ÿ",
            "å¾·èµ›ç”µæ± çš„è´¢åŠ¡æ•°æ®æ€ä¹ˆæ ·ï¼Ÿ"
        ],
        "technical": [
            "å¦‚ä½•ä¼˜åŒ–Pythonä»£ç çš„æ€§èƒ½ï¼Ÿ",
            "Dockerå®¹å™¨åŒ–éƒ¨ç½²çš„æœ€ä½³å®è·µæ˜¯ä»€ä¹ˆï¼Ÿ",
            "æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬å“ªäº›æ­¥éª¤ï¼Ÿ"
        ],
        "general": [
            "å¦‚ä½•å­¦ä¹ ä¸€é—¨æ–°çš„ç¼–ç¨‹è¯­è¨€ï¼Ÿ",
            "æé«˜å·¥ä½œæ•ˆç‡çš„æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ",
            "å¥åº·çš„ç”Ÿæ´»æ–¹å¼åŒ…æ‹¬å“ªäº›æ–¹é¢ï¼Ÿ"
        ]
    }
    
    # æ¨¡æ‹Ÿæ–‡æ¡£å†…å®¹
    sample_documents = {
        "financial": [
            "å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰çš„ä¸šç»©é¢„å‘Šè¶…å‡ºé¢„æœŸï¼Œä¸»è¦å¾—ç›ŠäºiPhone 12 Pro Maxéœ€æ±‚å¼ºåŠ²å’Œæ–°å“ç›ˆåˆ©èƒ½åŠ›æå‡ã€‚é¢„è®¡2021å¹´åˆ©æ¶¦å°†æŒç»­å¢é•¿ï¼ŒæºäºAå®¢æˆ·çš„ä¸šåŠ¡æˆé•¿ã€éæ‰‹æœºä¸šåŠ¡çš„å¢é•¿ä»¥åŠå¹¶è¡¨æ¯”ä¾‹çš„å¢åŠ ã€‚",
            "å…¬å¸2021å¹´è¥æ”¶193.9äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿5%ï¼›å½’æ¯å‡€åˆ©æ¶¦63.69äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿25.5%ã€‚äº§å“ç»“æ„ä¼˜åŒ–ï¼Œç›ˆåˆ©èƒ½åŠ›æå‡ã€‚",
            "å¾·èµ›ç”µæ± åœ¨å‚¨èƒ½ä¸šåŠ¡æ–¹é¢è¿›å±•é¡ºåˆ©ï¼Œæ­£åœ¨åŠ å¤§SIPé¢†åŸŸçš„æŠ•å…¥ï¼Œé¢„è®¡å…¬å¸æˆé•¿é€Ÿåº¦åŠ å¿«ã€‚"
        ],
        "technical": [
            "Pythonæ€§èƒ½ä¼˜åŒ–çš„å…³é”®æ–¹æ³•åŒ…æ‹¬ä½¿ç”¨é€‚å½“çš„æ•°æ®ç»“æ„ã€é¿å…ä¸å¿…è¦çš„å¾ªç¯ã€åˆ©ç”¨å†…ç½®å‡½æ•°å’Œåº“ã€‚ä»£ç ä¼˜åŒ–åº”è¯¥ä»ç®—æ³•å±‚é¢å¼€å§‹ï¼Œç„¶åè€ƒè™‘è¯­è¨€ç‰¹å®šçš„ä¼˜åŒ–æŠ€å·§ã€‚",
            "Dockerå®¹å™¨åŒ–éƒ¨ç½²çš„æœ€ä½³å®è·µåŒ…æ‹¬ä½¿ç”¨å¤šé˜¶æ®µæ„å»ºã€ä¼˜åŒ–é•œåƒå¤§å°ã€åˆç†è®¾ç½®èµ„æºé™åˆ¶ã€ä½¿ç”¨å¥åº·æ£€æŸ¥ã€å®ç°è‡ªåŠ¨åŒ–éƒ¨ç½²æµç¨‹ã€‚",
            "æœºå™¨å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹åŒ…æ‹¬æ•°æ®é¢„å¤„ç†ã€ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹é€‰æ‹©ã€è¶…å‚æ•°è°ƒä¼˜ã€äº¤å‰éªŒè¯ã€æ¨¡å‹è¯„ä¼°å’Œéƒ¨ç½²ç­‰æ­¥éª¤ã€‚"
        ],
        "general": [
            "å­¦ä¹ æ–°ç¼–ç¨‹è¯­è¨€çš„æœ‰æ•ˆæ–¹æ³•åŒ…æ‹¬ç†è§£åŸºç¡€æ¦‚å¿µã€åŠ¨æ‰‹å®è·µé¡¹ç›®ã€é˜…è¯»ä¼˜ç§€ä»£ç ã€å‚ä¸å¼€æºé¡¹ç›®ã€æŒç»­å­¦ä¹ å’Œå®è·µã€‚",
            "æé«˜å·¥ä½œæ•ˆç‡çš„æ–¹æ³•åŒ…æ‹¬æ—¶é—´ç®¡ç†ã€ä»»åŠ¡ä¼˜å…ˆçº§æ’åºã€ä½¿ç”¨å·¥å…·è‡ªåŠ¨åŒ–ã€å‡å°‘å¹²æ‰°ã€ä¿æŒä¸“æ³¨ã€å®šæœŸä¼‘æ¯å’Œåæ€ã€‚",
            "å¥åº·çš„ç”Ÿæ´»æ–¹å¼åŒ…æ‹¬å‡è¡¡é¥®é£Ÿã€è§„å¾‹è¿åŠ¨ã€å……è¶³ç¡çœ ã€å¿ƒç†å¥åº·ã€ç¤¾äº¤æ´»åŠ¨ã€é¿å…ä¸è‰¯ä¹ æƒ¯ç­‰å¤šä¸ªæ–¹é¢ã€‚"
        ]
    }
    
    for domain, queries in test_queries.items():
        print(f"\nğŸ“Š æµ‹è¯•é¢†åŸŸ: {domain.upper()}")
        print("-" * 40)
        
        for i, query in enumerate(queries, 1):
            print(f"\nğŸ” æŸ¥è¯¢ {i}: {query}")
            
            # æå–å…³é”®è¯
            keywords = extract_keywords_general(query, domain)
            print(f"   å…³é”®è¯: {keywords}")
            
            # æ¨¡æ‹Ÿæ£€ç´¢åˆ°çš„æ–‡æ¡£
            docs = sample_documents[domain]
            
            # æå–ç›¸å…³å¥å­
            all_relevant_sentences = []
            total_chars = 0
            max_chars = 2000
            
            for doc in docs[:3]:  # åªå¤„ç†å‰3ä¸ªæ–‡æ¡£
                relevant_sentences = extract_relevant_sentences_general(doc, keywords, max_chars_per_doc=800)
                
                for sentence in relevant_sentences:
                    if total_chars + len(sentence) <= max_chars:
                        all_relevant_sentences.append(sentence)
                        total_chars += len(sentence)
                    else:
                        break
                
                if total_chars >= max_chars:
                    break
            
            # æ‹¼æ¥ä¸Šä¸‹æ–‡
            context = "\n\n".join(all_relevant_sentences)
            
            print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
            print(f"   å¥å­æ•°é‡: {len(all_relevant_sentences)}")
            print(f"   å‰100å­—ç¬¦: {context[:100]}...")
            
            # è®¡ç®—ä¼˜åŒ–æ•ˆæœ
            original_length = sum(len(doc) for doc in docs)
            compression_ratio = (1 - len(context) / original_length) * 100
            print(f"   å‹ç¼©æ¯”ä¾‹: {compression_ratio:.1f}%")

def test_metadata_extraction_general():
    """æµ‹è¯•é€šç”¨å…ƒæ•°æ®æå–"""
    
    print(f"\nğŸ”§ æµ‹è¯•é€šç”¨å…ƒæ•°æ®æå–")
    print("=" * 60)
    
    test_queries = [
        # é‡‘èæŸ¥è¯¢
        "å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´åˆ©æ¶¦å¢é•¿åŸå› ",
        "000049çš„ä¸šç»©è¡¨ç°",
        "å¾·èµ›ç”µæ± è´¢åŠ¡æ•°æ®",
        
        # æŠ€æœ¯æŸ¥è¯¢
        "Python 3.9æ€§èƒ½ä¼˜åŒ–æ–¹æ³•",
        "Dockerå®¹å™¨éƒ¨ç½²æœ€ä½³å®è·µ",
        "æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒæ­¥éª¤",
        
        # é€šç”¨æŸ¥è¯¢
        "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹è¯­è¨€",
        "æé«˜å·¥ä½œæ•ˆç‡çš„æ–¹æ³•",
        "å¥åº·ç”Ÿæ´»æ–¹å¼å»ºè®®"
    ]
    
    for query in test_queries:
        print(f"\nğŸ“‹ æŸ¥è¯¢: {query}")
        
        # å°è¯•æå–ä¸åŒç±»å‹çš„å…ƒæ•°æ®
        metadata = {}
        
        # æå–æ•°å­—ï¼ˆå¯èƒ½æ˜¯ç‰ˆæœ¬å·ã€è‚¡ç¥¨ä»£ç ç­‰ï¼‰
        numbers = re.findall(r'\d+', query)
        if numbers:
            metadata['numbers'] = numbers
        
        # æå–è‹±æ–‡å•è¯ï¼ˆå¯èƒ½æ˜¯æŠ€æœ¯æœ¯è¯­ã€å…¬å¸åç­‰ï¼‰
        english_words = re.findall(r'[A-Za-z]+', query)
        if english_words:
            metadata['english_words'] = english_words
        
        # æå–ä¸­æ–‡å®ä½“
        chinese_entities = re.findall(r'[\u4e00-\u9fff]+', query)
        if chinese_entities:
            metadata['chinese_entities'] = chinese_entities
        
        print(f"   æå–çš„å…ƒæ•°æ®: {metadata}")

if __name__ == "__main__":
    test_different_domains()
    test_metadata_extraction_general() 