#!/usr/bin/env python3
"""
Linux GPUç¯å¢ƒæµ‹è¯•è„šæœ¬ - æµ‹è¯•åŒç©ºé—´åŒç´¢å¼•RAGç³»ç»Ÿ
"""

import os
import sys
import traceback
from pathlib import Path

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_gpu_availability():
    """æµ‹è¯•GPUå¯ç”¨æ€§"""
    print("=== GPUç¯å¢ƒæµ‹è¯• ===")
    
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        print(f"CUDAå¯ç”¨: {cuda_available}")
        
        if cuda_available:
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
            print(f"å½“å‰GPU: {torch.cuda.current_device()}")
            print(f"GPUåç§°: {torch.cuda.get_device_name()}")
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
            device = "cuda"
        else:
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
            device = "cpu"
            
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        return device
        
    except ImportError:
        print("âš ï¸  PyTorchæœªå®‰è£…ï¼Œå°†ä½¿ç”¨CPU")
        return "cpu"
    except Exception as e:
        print(f"âš ï¸  GPUæ£€æµ‹å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨CPU")
        return "cpu"

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    print("\n=== æ¨¡å‹åŠ è½½æµ‹è¯• ===")
    
    try:
        from config.parameters import Config
        config = Config()
        
        # è®¾ç½®è®¾å¤‡
        device = test_gpu_availability()
        config.encoder.device = device
        config.reranker.device = device
        # æ³¨æ„ï¼šGeneratorConfigæ²¡æœ‰deviceå±æ€§ï¼Œéœ€è¦åœ¨ç”Ÿæˆå™¨åŠ è½½æ—¶è®¾ç½®
        
        print(f"ä¸­æ–‡ç¼–ç å™¨è·¯å¾„: {config.encoder.chinese_model_path}")
        print(f"è‹±æ–‡ç¼–ç å™¨è·¯å¾„: {config.encoder.english_model_path}")
        print(f"é‡æ’åºå™¨è·¯å¾„: {config.reranker.model_name}")
        print(f"ç”Ÿæˆå™¨è·¯å¾„: {config.generator.model_name}")
        print(f"è®¾å¤‡è®¾ç½®: {device}")
        
        # æµ‹è¯•ä¸­æ–‡ç¼–ç å™¨
        print("\n1. æµ‹è¯•ä¸­æ–‡ç¼–ç å™¨åŠ è½½...")
        if os.path.exists(config.encoder.chinese_model_path):
            print("âœ… ä¸­æ–‡ç¼–ç å™¨è·¯å¾„å­˜åœ¨")
        else:
            print("âŒ ä¸­æ–‡ç¼–ç å™¨è·¯å¾„ä¸å­˜åœ¨")
        
        # æµ‹è¯•è‹±æ–‡ç¼–ç å™¨
        print("\n2. æµ‹è¯•è‹±æ–‡ç¼–ç å™¨åŠ è½½...")
        if os.path.exists(config.encoder.english_model_path):
            print("âœ… è‹±æ–‡ç¼–ç å™¨è·¯å¾„å­˜åœ¨")
        else:
            print("âŒ è‹±æ–‡ç¼–ç å™¨è·¯å¾„ä¸å­˜åœ¨")
        
        return config
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return None

def test_data_loading():
    """æµ‹è¯•æ•°æ®åŠ è½½"""
    print("\n=== æ•°æ®åŠ è½½æµ‹è¯• ===")
    
    # æ£€æŸ¥ä¸­æ–‡æ•°æ®
    chinese_data_paths = [
        "data/alphafin/alphafin_rag_ready_generated_cleaned.json",
        "evaluate_mrr/alphafin_train_qc.jsonl"
    ]
    
    print("ä¸­æ–‡æ•°æ®æ–‡ä»¶:")
    for path in chinese_data_paths:
        if os.path.exists(path):
            size = os.path.getsize(path) / (1024**2)  # MB
            print(f"  âœ… {path} ({size:.1f} MB)")
        else:
            print(f"  âŒ {path} (ä¸å­˜åœ¨)")
    
    # æ£€æŸ¥è‹±æ–‡æ•°æ®
    english_data_paths = [
        "data/tatqa_dataset_raw/tatqa_dataset_train.json",
        "evaluate_mrr/tatqa_train_qc.jsonl"
    ]
    
    print("\nè‹±æ–‡æ•°æ®æ–‡ä»¶:")
    for path in english_data_paths:
        if os.path.exists(path):
            size = os.path.getsize(path) / (1024**2)  # MB
            print(f"  âœ… {path} ({size:.1f} MB)")
        else:
            print(f"  âŒ {path} (ä¸å­˜åœ¨)")
    
    return chinese_data_paths, english_data_paths

def test_basic_encoder():
    """æµ‹è¯•åŸºç¡€ç¼–ç å™¨åŠŸèƒ½"""
    print("\n=== åŸºç¡€ç¼–ç å™¨æµ‹è¯• ===")
    
    try:
        # æµ‹è¯•sentence-transformers
        from sentence_transformers import SentenceTransformer
        print("æµ‹è¯•sentence-transformers...")
        
        # ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å¤šè¯­è¨€æ¨¡å‹
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # æµ‹è¯•ç¼–ç 
        texts = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•", "This is a test"]
        embeddings = model.encode(texts)
        print(f"âœ… ç¼–ç æˆåŠŸï¼ŒåµŒå…¥ç»´åº¦: {embeddings.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŸºç¡€ç¼–ç å™¨æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_simple_retriever():
    """æµ‹è¯•ç®€å•æ£€ç´¢å™¨"""
    print("\n=== ç®€å•æ£€ç´¢å™¨æµ‹è¯• ===")
    
    try:
        # åˆ›å»ºç®€å•çš„æµ‹è¯•æ•°æ®
        test_docs = [
            "å‡€åˆ©æ¶¦æ˜¯å…¬å¸åœ¨ä¸€å®šæœŸé—´å†…çš„æ€»æ”¶å…¥å‡å»æ€»æˆæœ¬åçš„ä½™é¢ã€‚",
            "Net income is the total revenue minus total costs of a company over a period.",
            "è¥ä¸šæ”¶å…¥æ˜¯æŒ‡ä¼ä¸šåœ¨æ­£å¸¸ç»è¥æ´»åŠ¨ä¸­äº§ç”Ÿçš„æ”¶å…¥ã€‚",
            "Revenue refers to income generated from normal business activities."
        ]
        
        # ä½¿ç”¨sentence-transformersè¿›è¡Œç®€å•æ£€ç´¢
        from sentence_transformers import SentenceTransformer
        from sentence_transformers.util import semantic_search
        import torch
        import numpy as np
        
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        
        # ç¼–ç æ–‡æ¡£
        doc_embeddings = model.encode(test_docs)
        
        # æµ‹è¯•æŸ¥è¯¢
        queries = ["ä»€ä¹ˆæ˜¯å‡€åˆ©æ¶¦ï¼Ÿ", "What is net income?"]
        
        print("æµ‹è¯•æ£€ç´¢åŠŸèƒ½:")
        for query in queries:
            query_embedding = model.encode([query])
            # è½¬æ¢ä¸ºtorch tensor
            query_tensor = torch.tensor(query_embedding)
            doc_tensor = torch.tensor(doc_embeddings)
            results = semantic_search(query_tensor, doc_tensor, top_k=2)
            
            print(f"  âœ… '{query}' -> æ£€ç´¢åˆ° {len(results[0])} ä¸ªæ–‡æ¡£")
            for i, result in enumerate(results[0]):
                doc_id = int(result['corpus_id'])
                print(f"      æ–‡æ¡£ {i+1}: {test_docs[doc_id][:50]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€å•æ£€ç´¢å™¨æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_simple_generator():
    """æµ‹è¯•ç®€å•ç”Ÿæˆå™¨"""
    print("\n=== ç®€å•ç”Ÿæˆå™¨æµ‹è¯• ===")
    
    try:
        # ä½¿ç”¨transformersçš„åŸºç¡€åŠŸèƒ½
        from transformers.pipelines import pipeline
        
        # å°è¯•åŠ è½½ä¸€ä¸ªç®€å•çš„æ–‡æœ¬ç”Ÿæˆæ¨¡å‹
        try:
            generator = pipeline("text-generation", model="distilgpt2", device="cpu")
            print("âœ… ä½¿ç”¨distilgpt2æ¨¡å‹")
        except:
            try:
                generator = pipeline("text-generation", model="gpt2", device="cpu")
                print("âœ… ä½¿ç”¨gpt2æ¨¡å‹")
            except:
                print("âš ï¸  æ— æ³•åŠ è½½ç”Ÿæˆæ¨¡å‹ï¼Œè·³è¿‡ç”Ÿæˆå™¨æµ‹è¯•")
                return False
        
        # æµ‹è¯•ç”Ÿæˆ
        test_prompts = [
            "Context: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ä¸Šä¸‹æ–‡ã€‚\nQuestion: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•é—®é¢˜å—ï¼Ÿ\nAnswer:",
            "Context: This is a test context.\nQuestion: Is this a test question?\nAnswer:"
        ]
        
        print("\næµ‹è¯•ç”ŸæˆåŠŸèƒ½:")
        for prompt in test_prompts:
            try:
                response = generator(prompt, max_length=50, do_sample=True)
                # å¤„ç†pipelineè¿”å›çš„ç»“æœ
                if isinstance(response, list) and len(response) > 0:
                    first_result = response[0]
                    if isinstance(first_result, dict) and 'generated_text' in first_result:
                        generated_text = first_result['generated_text']
                        print(f"  âœ… ç”ŸæˆæˆåŠŸ: {generated_text[:100]}...")
                    else:
                        print("  âš ï¸  ç”Ÿæˆç»“æœæ ¼å¼å¼‚å¸¸")
                else:
                    print("  âš ï¸  ç”Ÿæˆç»“æœä¸ºç©º")
            except Exception as e:
                print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€å•ç”Ÿæˆå™¨æµ‹è¯•å¤±è´¥: {e}")
        traceback.print_exc()
        return False

def test_integration_simple():
    """æµ‹è¯•ç®€å•é›†æˆåŠŸèƒ½"""
    print("\n=== ç®€å•é›†æˆæµ‹è¯• ===")
    
    try:
        from sentence_transformers import SentenceTransformer
        from sentence_transformers.util import semantic_search
        from transformers.pipelines import pipeline
        import torch
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_docs = [
            "å‡€åˆ©æ¶¦æ˜¯å…¬å¸åœ¨ä¸€å®šæœŸé—´å†…çš„æ€»æ”¶å…¥å‡å»æ€»æˆæœ¬åçš„ä½™é¢ã€‚",
            "Net income is the total revenue minus total costs of a company over a period.",
            "è¥ä¸šæ”¶å…¥æ˜¯æŒ‡ä¼ä¸šåœ¨æ­£å¸¸ç»è¥æ´»åŠ¨ä¸­äº§ç”Ÿçš„æ”¶å…¥ã€‚",
            "Revenue refers to income generated from normal business activities."
        ]
        
        # 1. æ£€ç´¢
        model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
        doc_embeddings = model.encode(test_docs)
        
        query = "ä»€ä¹ˆæ˜¯å‡€åˆ©æ¶¦ï¼Ÿ"
        query_embedding = model.encode([query])
        # è½¬æ¢ä¸ºtorch tensor
        query_tensor = torch.tensor(query_embedding)
        doc_tensor = torch.tensor(doc_embeddings)
        results = semantic_search(query_tensor, doc_tensor, top_k=2)
        
        print(f"æŸ¥è¯¢: {query}")
        print(f"æ£€ç´¢åˆ° {len(results[0])} ä¸ªæ–‡æ¡£")
        
        if results[0]:
            # 2. ç”Ÿæˆç­”æ¡ˆ
            doc_id = int(results[0][0]['corpus_id'])
            context = test_docs[doc_id]
            prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"
            
            try:
                generator = pipeline("text-generation", model="distilgpt2", device="cpu")
                response = generator(prompt, max_length=50, do_sample=True)
                # å¤„ç†pipelineè¿”å›çš„ç»“æœ
                if isinstance(response, list) and len(response) > 0:
                    first_result = response[0]
                    if isinstance(first_result, dict) and 'generated_text' in first_result:
                        generated_text = first_result['generated_text']
                        print(f"ç”Ÿæˆç­”æ¡ˆ: {generated_text[:200]}...")
                    else:
                        print("ç”Ÿæˆç»“æœæ ¼å¼å¼‚å¸¸")
                else:
                    print("ç”Ÿæˆç»“æœä¸ºç©º")
            except:
                print("ç”Ÿæˆå™¨ä¸å¯ç”¨ï¼Œè·³è¿‡ç”Ÿæˆæ­¥éª¤")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç®€å•é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Linux GPUç¯å¢ƒRAGç³»ç»Ÿæµ‹è¯•")
    print("=" * 60)
    
    # 1. æµ‹è¯•GPUç¯å¢ƒ
    device = test_gpu_availability()
    
    # 2. æµ‹è¯•æ¨¡å‹åŠ è½½
    config = test_model_loading()
    if not config:
        print("âŒ é…ç½®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return
    
    # 3. æµ‹è¯•æ•°æ®åŠ è½½
    chinese_paths, english_paths = test_data_loading()
    
    # 4. æµ‹è¯•åŸºç¡€ç¼–ç å™¨
    encoder_ok = test_basic_encoder()
    
    # 5. æµ‹è¯•ç®€å•æ£€ç´¢å™¨
    retriever_ok = test_simple_retriever()
    
    # 6. æµ‹è¯•ç®€å•ç”Ÿæˆå™¨
    generator_ok = test_simple_generator()
    
    # 7. æµ‹è¯•ç®€å•é›†æˆåŠŸèƒ½
    integration_ok = test_integration_simple()
    
    print("\n" + "=" * 60)
    print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
    
    if encoder_ok and retriever_ok:
        print("âœ… æ ¸å¿ƒåŠŸèƒ½å¯ä»¥æ­£å¸¸è¿è¡Œ")
        print(f"âœ… ä½¿ç”¨è®¾å¤‡: {device}")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print("  1. è¿è¡Œ: python run_enhanced_ui_linux.py")
        print("  2. æˆ–è€…è¿è¡Œ: python test_dual_space_retriever.py")
        print("  3. æˆ–è€…è¿è¡Œ: python test_linux_simple.py")
    else:
        print("âŒ ç³»ç»Ÿå­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
    
    print(f"\næµ‹è¯•ç»“æœæ±‡æ€»:")
    print(f"  ç¼–ç å™¨: {'âœ…' if encoder_ok else 'âŒ'}")
    print(f"  æ£€ç´¢å™¨: {'âœ…' if retriever_ok else 'âŒ'}")
    print(f"  ç”Ÿæˆå™¨: {'âœ…' if generator_ok else 'âŒ'}")
    print(f"  é›†æˆæµ‹è¯•: {'âœ…' if integration_ok else 'âŒ'}")

if __name__ == "__main__":
    main() 