#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬
é›†æˆäº†æ··åˆå†³ç­–ç®—æ³•ã€åŠ¨æ€promptè·¯ç”±ã€æ™ºèƒ½ç­”æ¡ˆæå–å’Œå¤šç»´åº¦è¯„ä¼°ã€‚
"""

# 1. å¯¼å…¥å¿…è¦çš„åº“
import warnings
import logging
import os
import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
import time
import argparse
from collections import Counter
from difflib import SequenceMatcher
import sys

# 2. ç¯å¢ƒè®¾ç½®
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
try:
    from tqdm import tqdm
except ImportError:
    print("âŒ tqdmæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tqdm")
    sys.exit(1)
sys.path.append(str(Path(__file__).parent))
try:
    # ç¡®ä¿ä½ çš„RAGç”Ÿæˆå™¨å¯ä»¥è¢«æ­£ç¡®å¯¼å…¥
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
    print("âœ… ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator")
except ImportError:
    USE_RAG_GENERATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGeneratorï¼Œè„šæœ¬å°†æ— æ³•è¿è¡Œã€‚")
    sys.exit(1)


# ===================================================================
# 3. æ ¸å¿ƒè¾…åŠ©å‡½æ•°
# ===================================================================

def extract_final_answer_with_rescue(raw_output: str) -> str:
    """
    ä»æ¨¡å‹çš„åŸå§‹è¾“å‡ºä¸­æ™ºèƒ½æå–æœ€ç»ˆç­”æ¡ˆã€‚
    å®ƒé¦–å…ˆå°è¯•å¯»æ‰¾<answer>æ ‡ç­¾ï¼Œå¦‚æœå¤±è´¥æˆ–ä¸ºç©ºï¼Œåˆ™å¯åŠ¨æ•‘æ´é€»è¾‘ä»<think>æ ‡ç­¾ä¸­æå–ã€‚
    """
    def _clean_extracted_text(text: str) -> str:
        """å¯¹æå–å‡ºçš„æ–‡æœ¬è¿›è¡Œé€šç”¨æ¸…ç†"""
        text = text.strip()
        # ç§»é™¤æ¨¡å‹å¯èƒ½é”™è¯¯å¤åˆ¶è¿›æ¥çš„ Prompt æŒ‡ä»¤ (å‡è®¾è¿™äº›æ–‡æœ¬ä¸ä¼šå‡ºç°åœ¨æ­£ç¡®ç­”æ¡ˆä¸­)
        text = text.replace("[é‡è¦ï¼šåªåœ¨è¿™é‡Œæä¾›æœ€ç»ˆç­”æ¡ˆã€‚æ— è§£é‡Šï¼Œæ— å•ä½ï¼Œæ— å¤šä½™æ–‡æœ¬ã€‚]", "").strip()
        
        # ç§»é™¤å¸¸è§çš„å¼•å¯¼è¯å¥ï¼Œå¹¶å¤„ç†å¤§å°å†™ä¸æ•æ„Ÿ
        text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
        
        # ç§»é™¤æœ«å°¾å¯èƒ½çš„å¤šä½™æ ‡ç‚¹ç¬¦å·ï¼Œå¦‚å¥å·ã€é€—å·ã€åˆ†å· (ä½†ä¿ç•™ç™¾åˆ†å·)
        text = re.sub(r'[\.ã€‚;,]$', '', text).strip()

        # æ ‡å‡†åŒ–ç™¾åˆ†å· (ä¾‹å¦‚ "percent" -> "%")
        text = re.sub(r'\s*percent\s*', '%', text, flags=re.IGNORECASE).strip()
        
        # ç§»é™¤å¸¸è§çš„è´§å¸ç¬¦å·å’Œå•ä½è¯ (å¦‚æœä½ çš„ expected_answer ä¸åŒ…å«è¿™äº›)
        text = re.sub(r'(\$|million|billion|usd|eur|pounds|Â£)', '', text, flags=re.IGNORECASE).strip()
        
        # ç§»é™¤æ•°å­—ä¸­çš„é€—å· (å¦‚æœä½ çš„ expected_answer ä¸åŒ…å«é€—å·)
        text = text.replace(',', '')
        
        # ç§»é™¤è´Ÿæ•°æ‹¬å· (ä¾‹å¦‚ "(33)" -> "-33")
        if text.startswith('(') and text.endswith(')'):
            text = '-' + text[1:-1] # è½¬æ¢ä¸ºè´Ÿæ•°
            
        return text

    # 1. å°è¯•ä» <answer> æ ‡ç­¾ä¸­æå–
    answer_match = re.search(r'<answer>(.*?)</answer>', raw_output, re.DOTALL)
    if answer_match:
        content = answer_match.group(1).strip()
        if content:
            return _clean_extracted_text(content)

    # 2. å¦‚æœ <answer> æ ‡ç­¾å¤±è´¥æˆ–ä¸ºç©ºï¼Œå°è¯•ä» <think> æ ‡ç­¾ä¸­æå–
    think_match = re.search(r'<think>(.*?)</think>', raw_output, re.DOTALL)
    if not think_match:
        # å¦‚æœè¿ <think> æ ‡ç­¾éƒ½æ²¡æœ‰ï¼Œå°è¯•æå–åŸå§‹è¾“å‡ºçš„æœ€åä¸€è¡Œä½œä¸ºç­”æ¡ˆ
        lines = raw_output.strip().split('\n')
        return _clean_extracted_text(lines[-1]) if lines else ""

    think_content = think_match.group(1)
    
    # --- 2.1. å°è¯•å¯»æ‰¾ç»“è®ºæ€§çŸ­è¯­ ---
    conclusion_phrases = [
        r'the\s*final\s*answer\s*is[:\s]*',
        r'the\s*answer\s*is[:\s]*', 
        r'therefore,\s*the\s*answer\s*is[:\s]*', 
        r'the\s*result\s*is[:\s]*', 
        r'equals\s*to[:\s]*', 
        r'is\s*equal\s*to[:\s]*', 
        r'the\s*value\s*is[:\s]*', 
        r'the\s*change\s*is[:\s]*', 
        r'the\s*amount\s*is[:\s]*',
        r'conclusion[:\s]*', 
        r'final\s*extracted\s*value/calculated\s*result[:\s]*',
        r'final\s*number[:\s]*',
        r'adjusted\s*net\s*income\s*is[:\s]*',
        r'percentage\s*change\s*is[:\s]*', 
        r'decreased\s*by[:\s]*', 
        r'increased\s*by[:\s]*',
        r'net\s*change\s*is[:\s]*', # å¢åŠ æ›´å¤šé€šç”¨æ¨¡å¼
        r'total\s*is[:\s]*',
        r'resulting\s*in[:\s]*', # æ•è· "resulting in X"
        r'is[:\s]*([-+]?[\d,\.]+%?)' # æ•è·"is:"åé¢ç›´æ¥è·Ÿçš„æ•°å­—æˆ–ç™¾åˆ†æ¯”
    ]
    
    for phrase_pattern in conclusion_phrases:
        # æ•è·çŸ­è¯­ååˆ°ä¸‹ä¸€ä¸ªæ ‡ç­¾ã€åŒæ¢è¡Œç¬¦æˆ–å­—ç¬¦ä¸²ç»“æŸçš„å†…å®¹ (éè´ªå©ª)
        conclusion_match = re.search(
            f'{phrase_pattern}(.*?)(?:$|<answer>|<think>|\\n\\n|\\Z)', 
            think_content, 
            re.IGNORECASE | re.DOTALL 
        )
        if conclusion_match:
            conclusion = conclusion_match.group(1).strip()
            # ç¡®ä¿æå–çš„å†…å®¹ä¸åŒ…å«æ€è€ƒè¿‡ç¨‹ä¸­çš„æ­¥éª¤ç¼–å·
            if re.fullmatch(r'\d+\.', conclusion.split('\n')[0].strip()):
                continue # å¦‚æœç¬¬ä¸€è¡Œæ˜¯æ­¥éª¤ç¼–å·ï¼Œè·³è¿‡
            
            return _clean_extracted_text(conclusion)
    
    # --- 2.2. å¦‚æœç»“è®ºæ€§çŸ­è¯­ä¸åŒ¹é…ï¼Œå°è¯•å¯»æ‰¾æœ€åä¸€ä¸ªç¬¦åˆæ•°å€¼/ç™¾åˆ†æ¯”/å¸¸è§æ ¼å¼çš„å­—ç¬¦ä¸² ---
    # ä¼˜å…ˆåŒ¹é…è¡Œå°¾çš„æ•°å­—æˆ–ç™¾åˆ†æ¯”ï¼Œå› ä¸ºå®ƒä»¬æ›´å¯èƒ½æ˜¯æœ€ç»ˆç­”æ¡ˆ
    potential_answers_raw = re.findall(r'[-+]?\s*\(?[\d,\.]+\)?%?\s*$', think_content, re.MULTILINE)
    if not potential_answers_raw:
        # å¦‚æœè¡Œå°¾æ²¡æœ‰ï¼Œåœ¨æ•´ä¸ªæ–‡æœ¬ä¸­ä»åå¾€å‰æ‰¾æ‰€æœ‰å¯èƒ½çš„æ•°å­—/ç™¾åˆ†æ¯”
        potential_answers_raw = re.findall(r'[-+]?\s*\(?[\d,\.]+\)?%?', think_content)
    
    if potential_answers_raw:
        # é€†åºéå†ï¼Œæ‰¾åˆ°æœ€æ¥è¿‘æœ«å°¾ä¸”æœ€å¯èƒ½æ˜¯ç­”æ¡ˆçš„æœ‰æ•ˆé¡¹
        for item_raw in reversed(potential_answers_raw):
            item = item_raw.strip()
            if not item: continue
            
            # æ’é™¤æ˜æ˜¾çš„æ­¥éª¤ç¼–å·æˆ–çŸ­è¯­ (å¦‚"1.", "2.", "Step 1:")
            if re.fullmatch(r'(\d+\.|\bstep\s*\d+\b)[:\s]*', item, re.IGNORECASE):
                continue

            cleaned_item = _clean_extracted_text(item)
            
            # ç®€å•çš„éªŒè¯ï¼Œç¡®ä¿ä¸æ˜¯ç©ºçš„æˆ–çº¯ç²¹çš„æ ‡ç‚¹
            if cleaned_item and len(cleaned_item) > 0 and not re.fullmatch(r'[^\w\s\d%.-]*', cleaned_item):
                return cleaned_item
                
    # --- 2.3. æœ€åå›é€€ï¼šå¦‚æœä»¥ä¸Šéƒ½å¤±è´¥ï¼Œå– <think> å†…å®¹çš„æœ€åä¸€è¡Œ ---
    lines = [line for line in think_content.strip().split('\n') if line.strip()]
    if lines:
        return _clean_extracted_text(lines[-1])
    return "" # å¦‚æœ think ä¹Ÿæ˜¯ç©ºçš„ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²


def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—F1åˆ†æ•°ï¼ŒåŒ…å«æ›´é²æ£’çš„å½’ä¸€åŒ–ï¼Œä¸ç­”æ¡ˆæå–é€»è¾‘ä¿æŒé«˜åº¦ä¸€è‡´"""
    def normalize_for_f1(text):
        # 1. æ ‡å‡†åŒ–ç™¾åˆ†å· (ä¾‹å¦‚ "percent" -> "%")
        text = text.replace(' percent', '%').replace(' Percent', '%').replace(' PERCENT', '%')
        
        # 2. ç§»é™¤å¸¸è§çš„è´§å¸ç¬¦å·ã€å•ä½è¯ã€é€—å·å’Œæ‹¬å·
        # ğŸš¨ å†æ¬¡å¼ºè°ƒï¼šè¿™é‡Œæ˜¯å¦ç§»é™¤å–å†³äºä½ çš„ expected_answer æ ¼å¼ã€‚
        # å¦‚æœ expected_answer æ˜¯ "123,456.78"ï¼Œåˆ™ä¸è¦ç§»é™¤é€—å·ã€‚
        # å¦‚æœ expected_answer æ˜¯ "$123.45"ï¼Œåˆ™ä¸è¦ç§»é™¤ $ã€‚
        # å»ºè®®ä½ çš„ expected_answer å°½é‡æ ‡å‡†åŒ–ä¸ºä¸å«è¿™äº›ç¬¦å·çš„çº¯æ•°å­—æˆ–çº¦å®šæ ¼å¼ï¼Œ
        # è¿™æ ·è¿™é‡Œå¯ä»¥ç»Ÿä¸€ç§»é™¤ï¼Œç®€åŒ–åŒ¹é…ã€‚
        text = re.sub(r'(\$|million|billion|usd|eur|pounds|Â£|\(|\))', '', text, flags=re.IGNORECASE)
        text = text.replace(',', '') # å‡è®¾ expected_answer å’Œ extracted_answer éƒ½ä¸å«æ•°å­—é€—å·

        # 3. ç§»é™¤é™¤äº†å­—æ¯æ•°å­—ã€ç©ºæ ¼ã€å°æ•°ç‚¹ã€è´Ÿå·å’Œç™¾åˆ†å·ä¹‹å¤–çš„æ‰€æœ‰å­—ç¬¦
        text = re.sub(r'[^\w\s%\.-]', '', text) 

        # 4. ç§»é™¤å¸¸è§çš„å¼•å¯¼è¯å¥ (å¯¹ prediction å’Œ ground_truth éƒ½è¿›è¡ŒåŒæ ·æ¸…ç†)
        text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
        
        # 5. ç§»é™¤æœ«å°¾å¯èƒ½çš„å¤šä½™æ ‡ç‚¹ (ä¾‹å¦‚å¥å·)
        text = text.rstrip('.')
        
        # 6. å°å†™å¹¶åˆ†å‰²
        return text.lower().split()

    prediction_tokens = normalize_for_f1(prediction)
    ground_truth_tokens = normalize_for_f1(ground_truth)

    if not ground_truth_tokens: 
        return 1.0 if not prediction_tokens else 0.0
    if not prediction_tokens: 
        return 0.0

    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0: 
        return 0.0

    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

# ===================================================================
# 4. æ™ºèƒ½è·¯ç”±ç®—æ³•
# ===================================================================

def determine_context_type(context: str) -> str:
    """æ ¹æ®contextå†…å®¹åˆ¤æ–­ç»“æ„ç±»å‹"""
    has_table = "Table ID:" in context
    text_content = re.sub(r'Table ID:.*?\n(Headers:.*?\n)?', '', context, flags=re.DOTALL)
    text_content = re.sub(r'Row \d+:.*?\n', '', text_content)
    text_content = re.sub(r'Category:.*?\n', '', text_content)
    has_meaningful_text = any(len(line.strip()) > 20 for line in text_content.split('\n'))

    if has_table and has_meaningful_text: return "table-text"
    elif has_table: return "table"
    else: return "text"

def analyze_query_features(query: str) -> Dict[str, Any]:
    """åˆ†æqueryç‰¹å¾ï¼Œæ›´ç»†è‡´åœ°è¯†åˆ«é—®é¢˜æ„å›¾"""
    query_lower = query.lower()
    
    # è¯†åˆ«è®¡ç®—æ€§å…³é”®è¯
    calculation_keywords = [
        'sum', 'total', 'average', 'mean', 'percentage', 'ratio', 'difference', 
        'increase', 'decrease', 'growth', 'change', 'compare', 'calculate', 
        'how much', 'how many', 'what is the', 'value of', 'amount of' # å¢åŠ æ›´é€šç”¨çš„æ•°å€¼é—®é¢˜è¯
    ]
    
    # è¯†åˆ«æ–‡æœ¬æ€§å…³é”®è¯ (å®šä¹‰ã€è§£é‡Šã€æè¿°)
    text_keywords = [
        'describe', 'explain', 'what is', 'what was the effect', 'how is', 'why', 
        'when', 'where', 'who', 'what does', 'consist of', 'what led to', 
        'define', 'meaning of', 'included in', 'comprised of' # å¢åŠ æ›´å¤šæè¿°æ€§è¯
    ]
    
    # è¯†åˆ«åˆ—è¡¨/æšä¸¾æ€§å…³é”®è¯
    list_keywords = ['list', 'name', 'assumptions', 'factors', 'items', 'components', 'types of', 'categories of'] 
    
    is_calc = any(keyword in query_lower for keyword in calculation_keywords)
    is_textual = any(keyword in query_lower for keyword in text_keywords)
    is_list = any(keyword in query_lower for keyword in list_keywords) 
    
    return {'is_calc': is_calc, 'is_textual': is_textual, 'is_list': is_list}

def hybrid_decision(context: str, query: str) -> str:
    """æ··åˆå†³ç­–ç®—æ³•ï¼Œé¢„æµ‹ç­”æ¡ˆæ¥æºï¼Œä¼˜åŒ–ä¼˜å…ˆçº§"""
    context_type = determine_context_type(context)
    query_features = analyze_query_features(query)

    # ä¼˜å…ˆçº§æœ€é«˜ï¼šå¦‚æœé—®é¢˜æ˜ç¡®æ˜¯åˆ—è¡¨/æšä¸¾ï¼Œé€šå¸¸ç›´æ¥ä»è¡¨æ ¼è¡Œåæˆ–æ–‡æœ¬æšä¸¾ä¸­æå–
    if query_features['is_list']:
        if context_type == "table":
            return "table" 
        elif context_type == "text":
            return "text"
        else: # "table-text"
            # åˆ—è¡¨é—®é¢˜åœ¨æ··åˆä¸Šä¸‹æ–‡ä¸­ï¼Œæ›´å¯èƒ½åå‘è¡¨æ ¼çš„è¡Œ/åˆ—åï¼Œæˆ–è€…æ–‡æœ¬ä¸­çš„æšä¸¾
            # è¿™é‡Œçš„å†³ç­–å¯ä»¥æ ¹æ®å®é™…æ•°æ®é›†ä¸­ "list" ç­”æ¡ˆçš„æ¥æºè¿›è¡Œè°ƒæ•´
            return "table-text" # æˆ–è€… 'table' å¦‚æœåˆ—è¡¨ä¸»è¦æ¥è‡ªè¡¨æ ¼

    # ç¬¬äºŒä¼˜å…ˆçº§ï¼šè®¡ç®—æ€§é—®é¢˜ï¼Œå¼ºçƒˆä¾èµ–æ•°å€¼æ•°æ®
    if query_features['is_calc']:
        if context_type == "table" or context_type == "table-text":
            return "table-text" # å³ä½¿æ˜¯çº¯è¡¨æ ¼ï¼Œè®¡ç®—ä¹Ÿé€šå¸¸éœ€è¦æ›´é€šç”¨çš„è¡¨æ ¼å¤„ç†é€»è¾‘
        else: # çº¯æ–‡æœ¬ï¼Œä½†é—®è®¡ç®—ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªå¤æ‚é—®é¢˜æˆ–éœ€è¦ä»æ–‡æœ¬ä¸­è§£ææ•°å€¼è¿›è¡Œè®¡ç®—
            return "text" # å›é€€åˆ°æ–‡æœ¬å¤„ç†ï¼Œè®©æ¨¡å‹å°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—å¹¶è®¡ç®—

    # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼šè§£é‡Šæ€§/äº‹å®æ€§é—®é¢˜
    if query_features['is_textual']:
        if context_type == "text":
            return "text"
        elif context_type == "table-text":
            # è§£é‡Šæ€§é—®é¢˜åœ¨æ··åˆä¸Šä¸‹æ–‡ä¸­ï¼Œä¼˜å…ˆä»æ–‡æœ¬è·å–è¯¦ç»†æè¿°
            return "text" 
        else: # çº¯è¡¨æ ¼ï¼Œä½†é—®è§£é‡Šï¼Œå¯èƒ½æ¥è‡ªè¡¨æ ¼çš„æè¿°æ€§è¡Œ/åˆ—æˆ–è¡¨æ ¼æ ‡é¢˜/å¤‡æ³¨
            return "table" # è¿™ç§æƒ…å†µéœ€è¦ä½ çš„ 'table' æ¨¡æ¿èƒ½å¤„ç†æè¿°æ€§é—®é¢˜

    # é»˜è®¤å›é€€ï¼šå¦‚æœä»¥ä¸Šè§„åˆ™éƒ½ä¸åŒ¹é…
    # æ ¹æ®ä¸Šä¸‹æ–‡ç±»å‹è¿›è¡Œé»˜è®¤è·¯ç”±
    return context_type # ç›´æ¥è¿”å›è¯†åˆ«åˆ°çš„ä¸Šä¸‹æ–‡ç±»å‹ï¼Œè®©ç›¸åº”æ¨¡æ¿å¤„ç†

# ===================================================================
# 5. åŠ¨æ€PromptåŠ è½½ä¸è·¯ç”±
# ===================================================================

def load_and_format_template(template_name: str, context: str, query: str) -> List[Dict[str, str]]:
    """åŠ è½½å¹¶æ ¼å¼åŒ–æŒ‡å®šçš„promptæ¨¡æ¿"""
    # æ¨¡æ¿æ”¾åœ¨ 'data/prompt_templates' æ–‡ä»¶å¤¹ä¸‹
    template_path = Path("data/prompt_templates") / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template_content = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}ï¼Œæ— æ³•ç»§ç»­ã€‚")
        sys.exit(1)
    
    system_match = re.search(r'<system>(.*?)</system>', template_content, re.DOTALL)
    system_content = system_match.group(1).strip() if system_match else ""
    user_match = re.search(r'<user>(.*?)</user>', template_content, re.DOTALL)
    user_template = user_match.group(1).strip() if user_match else "Context:\n{context}\n\nQuestion:\n{question}"
    user_content = user_template.replace('{context}', context).replace('{question}', query)
    return [{"role": "system", "content": system_content}, {"role": "user", "content": user_content}]

def get_final_prompt(context: str, query: str) -> List[Dict[str, str]]:
    """åŸºäºæ··åˆå†³ç­–ç®—æ³•å®ç°çš„æœ€ç»ˆPromptè·¯ç”±"""
    predicted_answer_source = hybrid_decision(context, query)
    
    if predicted_answer_source == "table":
        template_file = 'template_for_table_answer.txt'
    elif predicted_answer_source == "text":
        template_file = 'template_for_text_answer.txt'
    else: # "table-text"
        template_file = 'template_for_hybrid_answer.txt'
    
    # print(f"  [è·¯ç”±å†³ç­–] Context: {determine_context_type(context)}, Query: '{query[:30]}...', ä½¿ç”¨æ¨¡æ¿: {template_file}")
    return load_and_format_template(template_file, context, query)

# ===================================================================
# 6. æ ¸å¿ƒè¯„ä¼°ç±»
# ===================================================================

class ComprehensiveEvaluator:
    def __init__(self, model_name: str, device: str):
        self.model_name = model_name
        self.device = device
        self.max_new_tokens = 4096
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        self.generator = LocalLLMGenerator(model_name=self.model_name, device=self.device)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def run_evaluation(self, eval_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        results = []
        start_time = time.time()
        pbar = tqdm(eval_data, desc="ğŸ” è¯„ä¼°æ ·æœ¬", unit="ä¸ª")

        for sample in pbar:
            result = self._evaluate_single_sample(sample)
            results.append(result)
        
        total_time = time.time() - start_time
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š å¤„ç†äº† {len(results)} ä¸ªç»“æœ")
        
        analysis = self.analyze_results(results)
        analysis['performance'] = {'total_time': total_time, 'avg_time_per_sample': total_time / len(results) if results else 0}
        return {"results": results, "analysis": analysis}

    def _evaluate_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        try:
            messages = get_final_prompt(sample["context"], sample["query"])
            prompt_text = self._convert_messages_to_text(messages)

            gen_start_time = time.time()
            generation_result = self.generator.generate([prompt_text])[0]
            gen_time = time.time() - gen_start_time
            
            final_answer_to_evaluate = extract_final_answer_with_rescue(generation_result)
            evaluation = self._evaluate_quality(final_answer_to_evaluate, sample["answer"])
            
            # è®°å½•è·¯ç”±å†³ç­–å’Œå®é™…ç­”æ¡ˆæ¥æºï¼Œä¾¿äºåˆ†æ
            predicted_source = hybrid_decision(sample["context"], sample["query"])
            actual_source = sample.get("answer_from", "unknown") # ç¡®ä¿è¿™ä¸ªå­—æ®µå­˜åœ¨äºä½ çš„æ•°æ®é›†ä¸­

            return {
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "generated_answer": generation_result,       # åŸå§‹æ¨¡å‹è¾“å‡º
                "extracted_answer": final_answer_to_evaluate, # ç»è¿‡ extract_final_answer_with_rescue å¤„ç†åçš„ç­”æ¡ˆ
                "evaluation": evaluation,
                "answer_from": actual_source, # æ•°æ®é›†æ ‡æ³¨çš„ç­”æ¡ˆæ¥æº
                "predicted_answer_from": predicted_source, # è·¯ç”±ç®—æ³•é¢„æµ‹çš„ç­”æ¡ˆæ¥æº
                "generation_time": gen_time
            }
        except Exception as e:
            # è¯¦ç»†æ‰“å°é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‘ç”Ÿé”™è¯¯çš„æ ·æœ¬IDæˆ–æŸ¥è¯¢
            sample_id = sample.get("id", "N/A") # å¦‚æœä½ çš„æ ·æœ¬æœ‰ID
            print(f"\nâŒ å¤„ç†æ ·æœ¬å¤±è´¥ (ID: {sample_id}, Query: '{sample.get('query', 'N/A')[:50]}...', Error: {e})", file=sys.stderr)
            return {
                "query": sample["query"], 
                "expected_answer": sample["answer"], 
                "error": str(e),
                "context": sample.get("context", "N/A"), # åŒ…å«ä¸Šä¸‹æ–‡ä»¥ä¾›è°ƒè¯•
                "evaluation": {"exact_match": False, "f1_score": 0.0},
                "generated_answer": "", # ç¡®ä¿æœ‰æ­¤å­—æ®µï¼Œå³ä½¿æ˜¯é”™è¯¯æ ·æœ¬
                "extracted_answer": ""  # ç¡®ä¿æœ‰æ­¤å­—æ®µï¼Œå³ä½¿æ˜¯é”™è¯¯æ ·æœ¬
            }

    def _evaluate_quality(self, generated: str, expected: str) -> Dict[str, Any]:
        exact_match = generated.strip().lower() == expected.strip().lower()
        f1 = calculate_f1_score(generated, expected)
        return {"exact_match": exact_match, "f1_score": f1}

    def _convert_messages_to_text(self, messages: List[Dict[str, str]]) -> str:
        text = ""
        for message in messages:
            text += f'<{message["role"]}>\n{message["content"]}\n'
        return text

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        print(f"ğŸ” å¼€å§‹åˆ†æ {len(results)} ä¸ªç»“æœ...")
        
        if not results: 
            print("âŒ æ²¡æœ‰ç»“æœå¯åˆ†æ")
            return {}
        
        # æ£€æŸ¥ç»“æœç»“æ„
        valid_results = [r for r in results if 'evaluation' in r]
        error_results = [r for r in results if 'error' in r]
        print(f"âœ… æœ‰æ•ˆç»“æœ: {len(valid_results)}, âŒ é”™è¯¯ç»“æœ: {len(error_results)}")
        
        all_f1 = [r['evaluation']['f1_score'] for r in valid_results]
        all_em = [r['evaluation']['exact_match'] for r in valid_results]

        analysis = {
            "overall_metrics": {
                "total_samples": len(results),
                "valid_samples": len(valid_results),
                "error_samples": len(error_results),
                "exact_match_rate": (sum(all_em) / len(all_em) * 100) if all_em else 0,
                "avg_f1_score": np.mean(all_f1) if all_f1 else 0
            },
            "by_answer_type": {}
        }

        types = set(r.get("answer_from", "unknown") for r in results)
        print(f"ğŸ“Š å‘ç°ç­”æ¡ˆç±»å‹: {list(types)}")
        
        for t in types:
            subset = [r for r in results if r.get("answer_from", "unknown") == t]
            subset_valid = [r for r in subset if 'evaluation' in r]
            subset_f1 = [r['evaluation']['f1_score'] for r in subset_valid]
            subset_em = [r['evaluation']['exact_match'] for r in subset_valid]
            analysis["by_answer_type"][t] = {
                "count": len(subset),
                "valid_count": len(subset_valid),
                "exact_match_rate": (sum(subset_em) / len(subset_em) * 100) if subset_em else 0,
                "avg_f1_score": np.mean(subset_f1) if subset_f1 else 0
            }
        return analysis

    def print_summary(self, analysis: Dict[str, Any]):
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*60)
        overall = analysis.get("overall_metrics", {})
        print(f"ğŸ“ˆ æ€»ä½“æŒ‡æ ‡:")
        print(f"  - æ€»æ ·æœ¬æ•°: {overall.get('total_samples', 0)}")
        print(f"  - æœ‰æ•ˆæ ·æœ¬æ•°: {overall.get('valid_samples', 0)}")
        print(f"  - é”™è¯¯æ ·æœ¬æ•°: {overall.get('error_samples', 0)}")
        print(f"  - ç²¾ç¡®åŒ¹é…ç‡: {overall.get('exact_match_rate', 0):.2f}%")
        print(f"  - å¹³å‡F1åˆ†æ•°: {overall.get('avg_f1_score', 0):.4f}")

        by_type = analysis.get("by_answer_type", {})
        print("\nğŸ“Š æŒ‰ç­”æ¡ˆæ¥æºç±»å‹åˆ†æ:")
        for type_name, metrics in by_type.items():
            print(f"  - {type_name.upper()} ç±»å‹:")
            print(f"    - æ€»æ ·æœ¬æ•°: {metrics.get('count', 0)}")
            print(f"    - æœ‰æ•ˆæ ·æœ¬æ•°: {metrics.get('valid_count', 0)}")
            print(f"    - ç²¾ç¡®åŒ¹é…ç‡: {metrics.get('exact_match_rate', 0):.2f}%")
            print(f"    - å¹³å‡F1åˆ†æ•°: {metrics.get('avg_f1_score', 0):.4f}")
        print("="*60)

# ===================================================================
# 7. ä¸»å‡½æ•°
# ===================================================================
def main():
    parser = argparse.ArgumentParser(description="æœ€ç»ˆç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model", type=str, default="SUFE-AIFLM-Lab/Fin-R1", help="è¦è¯„ä¼°çš„LLMåç§°")
    parser.add_argument("--data_path", type=str, required=True, help="è¯„ä¼°æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (jsonlæˆ–jsonæ ¼å¼)")
    parser.add_argument("--sample_size", type=int, default=None, help="éšæœºé‡‡æ ·çš„æ ·æœ¬æ•°é‡ï¼Œä¸æä¾›åˆ™è¯„ä¼°å…¨éƒ¨")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡ (cuda/cpu/auto)")
    args = parser.parse_args()

    # è®¾å¤‡é€‰æ‹©é€»è¾‘
    if args.device == "auto":
        device = "cuda:1" if torch.cuda.is_available() else "cpu"  # é»˜è®¤ä½¿ç”¨cuda:1
    elif args.device == "cuda":
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            device = "cpu"
        else:
            device = "cuda:1"  # é»˜è®¤ä½¿ç”¨cuda:1
            print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(1) if torch.cuda.device_count() > 1 else torch.cuda.get_device_name(0)}")
            gpu_id = 1 if torch.cuda.device_count() > 1 else 0
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3:.1f} GB")
    else:
        device = args.device

    # 1. åŠ è½½æ•°æ®
    print(f"ğŸ“– æ­£åœ¨ä» {args.data_path} åŠ è½½æ•°æ®...")
    eval_data = []
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.data_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.data_path}")
        return
    
    # é¦–å…ˆå°è¯•ä½œä¸ºJSONLæ ¼å¼åŠ è½½ï¼ˆé€è¡Œè§£æï¼‰
    jsonl_success = False
    with open(args.data_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if line:  # è·³è¿‡ç©ºè¡Œ
                try:
                    eval_data.append(json.loads(line))
                except json.JSONDecodeError as e:
                    print(f"âŒ ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                    continue
    
    # å¦‚æœJSONLè§£ææˆåŠŸï¼Œä½¿ç”¨ç»“æœ
    if eval_data:
        print(f"âœ… æˆåŠŸåŠ è½½ä¸ºJSONLæ ¼å¼ï¼Œæ ·æœ¬æ•°: {len(eval_data)}")
        jsonl_success = True
    else:
        print("JSONLè§£æå¤±è´¥ï¼Œå°è¯•ä½œä¸ºå•ä¸ªJSONæ–‡ä»¶è§£æ...")
    
    # å¦‚æœJSONLè§£æå¤±è´¥ï¼Œå°è¯•ä½œä¸ºå•ä¸ªJSONæ–‡ä»¶è§£æ
    if not jsonl_success:
        eval_data = []  # é‡ç½®æ•°æ®
        with open(args.data_path, 'r', encoding='utf-8') as f:
            content = f.read()
            try:
                data = json.loads(content)
                if isinstance(data, list):
                    eval_data = data
                    print(f"âœ… æˆåŠŸåŠ è½½ä¸ºJSONæ•°ç»„ï¼Œæ ·æœ¬æ•°: {len(eval_data)}")
                elif isinstance(data, dict) and 'results' in data:
                    eval_data = data['results']
                    print(f"âœ… æˆåŠŸåŠ è½½ä¸ºJSONå¯¹è±¡ï¼Œæ ·æœ¬æ•°: {len(eval_data)}")
                else:
                    print(f"âŒ ä¸æ”¯æŒçš„JSONæ ¼å¼")
                    return
            except json.JSONDecodeError as e:
                print(f"âŒ JSONè§£æå¤±è´¥: {e}")
                return
    
    if args.sample_size and args.sample_size < len(eval_data):
        np.random.seed(42)
        indices = np.random.choice(len(eval_data), args.sample_size, replace=False)
        eval_data = [eval_data[i] for i in indices]
        print(f"âœ… éšæœºé‡‡æ · {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
    else:
        print(f"âœ… åŠ è½½äº†å…¨éƒ¨ {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")

    # 2. åˆå§‹åŒ–å¹¶è¿è¡Œè¯„ä¼°å™¨
    evaluator = ComprehensiveEvaluator(model_name=args.model, device=device)
    analysis_results = evaluator.run_evaluation(eval_data)
    
    # 3. æ‰“å°å’Œä¿å­˜ç»“æœ
    evaluator.print_summary(analysis_results['analysis'])
    output_filename = f"final_evaluation_results_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")


if __name__ == "__main__":
    main()