#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬ - ç¡®ä¿tqdmè¿›åº¦æ¡æ­£å¸¸æ˜¾ç¤º
ä½¿ç”¨Minimalæ¨¡æ¿è¿›è¡Œ100æ ·æœ¬è¯„ä¼°
"""

# ä¸´æ—¶å…³é—­warningsï¼Œé¿å…transformerså‚æ•°è­¦å‘Š
import warnings
warnings.filterwarnings("ignore")

# æ›´ç²¾ç¡®åœ°è¿‡æ»¤transformersç”Ÿæˆå‚æ•°è­¦å‘Š
import logging
logging.getLogger("transformers.generation.utils").setLevel(logging.ERROR)
logging.getLogger("transformers").setLevel(logging.WARNING)
logging.getLogger("torch").setLevel(logging.WARNING)
logging.getLogger("xlm").setLevel(logging.WARNING)

# è®¾ç½®ç¯å¢ƒå˜é‡å‡å°‘transformersçš„è¯¦ç»†è¾“å‡º
import os
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
import random
import time
from difflib import SequenceMatcher
import sys
import argparse
from collections import Counter

# ç¡®ä¿tqdmæ­£ç¡®å¯¼å…¥å’Œé…ç½®
try:
    from tqdm import tqdm
    # å¼ºåˆ¶å¯ç”¨tqdmè¿›åº¦æ¡
    tqdm.monitor_interval = 0
except ImportError:
    print("âŒ tqdmæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tqdm")
    sys.exit(1)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGenerator
try:
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
    print("âœ… ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator")
except ImportError:
    USE_RAG_GENERATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGeneratorï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from transformers.utils.quantization_config import BitsAndBytesConfig


import re # ç¡®ä¿ä½ çš„è„šæœ¬é¡¶éƒ¨æœ‰ import re

def extract_final_answer(raw_output: str) -> str:
    """ä»æ¨¡å‹çš„åŸå§‹è¾“å‡ºä¸­æå–<answer>æ ‡ç­¾å†…çš„å†…å®¹"""
    match = re.search(r'<answer>(.*?)</answer>', raw_output, re.DOTALL)
    if match:
        # å¦‚æœæ‰¾åˆ°æ ‡ç­¾ï¼Œè¿”å›æ ‡ç­¾å†…çš„å¹²å‡€å†…å®¹
        return match.group(1).strip()
    
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆï¼Œè¿”å›æ•´ä¸ªè¾“å‡ºçš„æœ€åä¸€è¡Œï¼Œè¿™å¯èƒ½åŒ…å«ç­”æ¡ˆ
    lines = raw_output.strip().split('\n')
    if lines:
        return lines[-1].strip()
    return "" # å¦‚æœè¿æœ€åä¸€è¡Œéƒ½æ²¡æœ‰ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²


def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²ä¹‹é—´åŸºäºè¯è¯­é‡å çš„F1åˆ†æ•°ã€‚
    """
    # æ–‡æœ¬è§„èŒƒåŒ–ï¼šè½¬å°å†™ï¼Œç§»é™¤æ ‡ç‚¹ï¼ŒæŒ‰ç©ºæ ¼åˆ†è¯
    def normalize(text):
        return re.sub(r'[^\w\s]', '', text.lower()).split()

    prediction_tokens = normalize(prediction)
    ground_truth_tokens = normalize(ground_truth)

    if not ground_truth_tokens:
        return 1.0 if not prediction_tokens else 0.0
    if not prediction_tokens:
        return 0.0

    # ä½¿ç”¨Counteræ¥å¤„ç†è¯é¢‘
    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())

    if num_same == 0:
        return 0.0

    # è®¡ç®—ç²¾ç¡®ç‡ã€å¬å›ç‡å’ŒF1
    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    
    return f1


class LLMTemplateTester:
    """LLMæ¨¡æ¿æµ‹è¯•å™¨"""
    
    def __init__(self, model_name: str = "SUFE-AIFLM-Lab/Fin-R1", device: str = "auto"):
        self.model_name = model_name
        self.device = self._setup_device(device)
        self.llm_generator = None  # ä½¿ç”¨RAGçš„LocalLLMGenerator
        self.max_length = 2048 # Increased context window for complex prompts
        self.max_new_tokens = 150  # é»˜è®¤tokené™åˆ¶
        
    def _setup_device(self, device: str) -> str:
        """è®¾ç½®è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        if USE_RAG_GENERATOR:
            # ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator
            self.llm_generator = LocalLLMGenerator(
                model_name=self.model_name,
                device=self.device
            )
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä½¿ç”¨transformers
            print("âš ï¸ ä½¿ç”¨å¤‡ç”¨transformersæ–¹æ¡ˆ")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map=self.device
            )
    
    def _convert_messages_to_text(self, messages: List[Dict[str, str]]) -> str:
        """å°†messagesè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"""
        text = ""
        for message in messages:
            role = message["role"]
            content = message["content"]
            if role == "system":
                text += f"System: {content}\n\n"
            elif role == "user":
                text += f"User: {content}\n\n"
            elif role == "assistant":
                text += f"Assistant: {content}\n\n"
        return text.strip()
    
    def generate_response(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­”"""
        start_time = time.time()
        
        if USE_RAG_GENERATOR and self.llm_generator:
            # ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator
            try:
                # å°†messagesè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
                prompt_text = self._convert_messages_to_text(messages)
                
                # è®¾ç½®ç”Ÿæˆå‚æ•°
                generation_params = {
                    "max_new_tokens": self.max_new_tokens,
                    "do_sample": False,  # Fin-R1ä½¿ç”¨ç¡®å®šæ€§ç”Ÿæˆ
                    "repetition_penalty": 1.1
                }
                
                # ç”Ÿæˆå›ç­”
                generated_text = self.llm_generator.generate([prompt_text])[0]
                
                generation_time = time.time() - start_time
                
                # æ¸…ç†å›ç­”
                cleaned_answer = self._clean_answer(generated_text)
                
                return {
                    "generated_answer": generated_text,
                    "cleaned_answer": cleaned_answer,
                    "generation_time": generation_time
                }
                
            except Exception as e:
                print(f"âš ï¸ RAGç”Ÿæˆå™¨é”™è¯¯: {e}")
                return {
                    "generated_answer": f"Error: {e}",
                    "cleaned_answer": f"Error: {e}",
                    "generation_time": time.time() - start_time
                }
        else:
            # å¤‡ç”¨æ–¹æ¡ˆ
            return {
                "generated_answer": "RAG generator not available",
                "cleaned_answer": "RAG generator not available",
                "generation_time": time.time() - start_time
            }
    
    def _clean_answer(self, answer: str) -> str:
        """æ¸…ç†å›ç­”"""
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        cleaned = answer.strip()
        
        # å¦‚æœå›ç­”å¤ªé•¿ï¼Œæˆªæ–­åˆ°åˆç†é•¿åº¦
        if len(cleaned) > 1000:
            cleaned = cleaned[:1000] + "..."
        
        return cleaned
    
    def evaluate_answer_quality(self, generated_answer: str, expected_answer: str, 
                              context: str, question: str, raw_answer: str = "") -> Dict[str, Any]:
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
        # åŸºç¡€è¯„ä¼°
        exact_match = generated_answer.strip().lower() == expected_answer.strip().lower()
        
        # åŒ…å«æ£€æŸ¥
        contains_expected = expected_answer.strip().lower() in generated_answer.strip().lower()
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        similarity = SequenceMatcher(None, generated_answer.lower(), expected_answer.lower()).ratio()
        
        # è´¨é‡åˆ†æ•°è®¡ç®—
        quality_score = 0.0
        
        if exact_match:
            quality_score = 1.0
        elif contains_expected:
            quality_score = 0.8
        elif similarity > 0.7:
            quality_score = 0.6
        elif similarity > 0.5:
            quality_score = 0.4
        elif similarity > 0.3:
            quality_score = 0.2
        else:
            quality_score = 0.0
        
        # æ ¼å¼è¿è§„æ£€æŸ¥
        format_violations = []
        if len(generated_answer) > 500:
            format_violations.append("å›ç­”è¿‡é•¿")
        if not generated_answer.strip():
            format_violations.append("ç©ºå›ç­”")
        
        f1_score = calculate_f1_score(generated_answer, expected_answer)
        return {
            "quality_score": quality_score,
            "exact_match": exact_match,
            "contains_expected": contains_expected,
            "semantic_similarity": similarity,
            "format_violations": format_violations,
            "f1_score": f1_score
        }

def get_detailed_english_prompt_messages(context_content: str, question_text: str, summary_content: Optional[str] = None) -> List[Dict[str, str]]:
    """
    ç”Ÿæˆ LLM æœŸæœ›çš„ messages åˆ—è¡¨ã€‚
    ä½¿ç”¨è¯¦ç»†çš„Chain-of-Thoughtæ¨¡æ¿ï¼Œè§£æsystemå’Œuseræ ‡ç­¾ã€‚
    """
    
    # è¯»å–æ¨¡æ¿æ–‡ä»¶
    try:
        with open('rag_english_template.txt', 'r', encoding='utf-8') as f:
            template_content = f.read().strip()
    except FileNotFoundError:
        print("âš ï¸ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
        return [
            {"role": "system", "content": "You are a world-class quantitative financial analyst AI."},
            {"role": "user", "content": f"Context:\n{context_content}\n\nQuestion:\n{question_text}\n\nA:"}
        ]
    
    # è§£æsystemå’Œuseræ ‡ç­¾
    import re
    
    # æå–systemå†…å®¹
    system_match = re.search(r'<system>(.*?)</system>', template_content, re.DOTALL)
    if system_match:
        system_content = system_match.group(1).strip()
    else:
        system_content = "You are a world-class quantitative financial analyst AI."
    
    # æå–useræ¨¡æ¿
    user_match = re.search(r'<user>(.*?)</user>', template_content, re.DOTALL)
    if user_match:
        user_template = user_match.group(1).strip()
        # æ›¿æ¢å ä½ç¬¦
        user_content = user_template.replace('{context}', context_content).replace('{question}', question_text)
    else:
        user_content = f"Context:\n{context_content}\n\nQuestion:\n{question_text}\n\nA:"

    return [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_content}
    ]

class EnhancedComprehensiveEvaluator:
    """å¢å¼ºç‰ˆå…¨é¢è¯„ä¼°å™¨"""
    
    def __init__(self, model_name: str = "SUFE-AIFLM-Lab/Fin-R1", device: str = "auto"):
        self.tester = LLMTemplateTester(model_name, device)
        # å¢åŠ max_lengthä»¥æ”¯æŒæ›´é•¿çš„æ¨¡æ¿
        self.tester.max_length = 4096
        # å¢åŠ max_new_tokensä»¥æ”¯æŒå®Œæ•´çš„Chain-of-Thoughtæ¨ç†
        self.tester.max_new_tokens = 1024
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        self.tester.load_model()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
    def load_evaluation_data(self, sample_size: int = 100) -> List[Dict[str, Any]]:
        """åŠ è½½è¯„ä¼°æ•°æ®"""
        print(f"ğŸ“– åŠ è½½è¯„ä¼°æ•°æ®ï¼Œç›®æ ‡æ ·æœ¬æ•°: {sample_size}")
        
        # åŠ è½½å¢å¼ºç‰ˆè¯„ä¼°æ•°æ®
        eval_data = []
        data_file = 'evaluate_mrr/tatqa_eval_enhanced.jsonl'
        
        if not os.path.exists(data_file):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return []
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºæ–‡ä»¶è¯»å–è¿›åº¦
        with open(data_file, 'r') as f:
            lines = f.readlines()
            for line in tqdm(lines, desc="ğŸ“– è¯»å–æ•°æ®æ–‡ä»¶", unit="è¡Œ", 
                           bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"):
                eval_data.append(json.loads(line))
        
        print(f"âœ… è¯»å–äº† {len(eval_data)} è¡Œæ•°æ®")
        
        # éšæœºé‡‡æ ·æŒ‡å®šæ•°é‡çš„æ ·æœ¬
        if len(eval_data) > sample_size:
            np.random.seed(42)  # ç¡®ä¿å¯é‡ç°æ€§
            eval_data = np.random.choice(eval_data, sample_size, replace=False).tolist()
            print(f"âœ… éšæœºé‡‡æ ·äº† {len(eval_data)} ä¸ªæ ·æœ¬")
        
        return eval_data
    
    # åœ¨ EnhancedComprehensiveEvaluator ç±»ä¸­
    def evaluate_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬ï¼ŒåŒ…å«æå–æœ€ç»ˆç­”æ¡ˆçš„é€»è¾‘ã€‚
        """
        try:
            # 1. æ„å»ºPrompt (è¿™éƒ¨åˆ†é€»è¾‘ä¸å˜)
            messages = get_detailed_english_prompt_messages(
                context_content=sample["context"],
                question_text=sample["query"],
                summary_content=sample["context"]
            )
            
            # 2. ç”Ÿæˆå®Œæ•´å›ç­” (æ¨¡å‹ä¼šè¾“å‡ºæ€è€ƒè¿‡ç¨‹å’Œ<answer>æ ‡ç­¾) (è¿™éƒ¨åˆ†é€»è¾‘ä¸å˜)
            generation_result = self.tester.generate_response(messages)
            
            # 3. ä»åŸå§‹è¾“å‡ºä¸­æå–<answer>æ ‡ç­¾å†…çš„æœ€ç»ˆç­”æ¡ˆ
            final_answer_to_evaluate = extract_final_answer(generation_result["generated_answer"])
            
            # 4. ä½¿ç”¨æå–å‡ºçš„å¹²å‡€ç­”æ¡ˆè¿›è¡Œè´¨é‡è¯„ä¼°
            evaluation = self.tester.evaluate_answer_quality(
                generated_answer=final_answer_to_evaluate,
                expected_answer=sample["answer"],
                context=sample["context"],
                question=sample["query"],
                raw_answer=generation_result["generated_answer"]
            )
            
            # 5. ç»„è£…å¹¶è¿”å›ç»“æœ
            return {
                "sample_id": sample.get("id", "unknown"),
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "generation": generation_result,
                "evaluation": evaluation,
                "context_type": "table" if "Table ID:" in sample["context"] else "paragraph",
                "success": evaluation["exact_match"] or evaluation["contains_expected"]
            }
            
        except Exception as e:
            print(f"âš ï¸ æ ·æœ¬è¯„ä¼°å¤±è´¥: {str(e)[:100]}...")
            return {
                "sample_id": sample.get("id", "unknown"),
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "error": str(e),
                "success": False
            }
    
    def run_comprehensive_evaluation(self, sample_size: int = 100) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢è¯„ä¼°"""
        print(f"\nğŸš€ å¼€å§‹å…¨é¢è¯„ä¼°ï¼Œæ ·æœ¬æ•°: {sample_size}")
        print("="*60)
        
        # åŠ è½½æ•°æ®
        eval_data = self.load_evaluation_data(sample_size)
        
        if not eval_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è¯„ä¼°æ•°æ®")
            return {"results": [], "analysis": {}, "timestamp": time.time()}
        
        # è¯„ä¼°æ‰€æœ‰æ ·æœ¬
        results = []
        start_time = time.time()
        
        print(f"ğŸ” å¼€å§‹è¯„ä¼° {len(eval_data)} ä¸ªæ ·æœ¬...")
        
        # ä½¿ç”¨tqdmè¿›åº¦æ¡ï¼Œç¡®ä¿æ˜¾ç¤º
        pbar = tqdm(eval_data, desc="ğŸ” è¯„ä¼°æ ·æœ¬", unit="æ ·æœ¬", 
                   bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
                   ncols=100, leave=True)
        
        for i, sample in enumerate(pbar):
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.set_description(f"ğŸ” è¯„ä¼°æ ·æœ¬ {i+1}/{len(eval_data)}")
            
            result = self.evaluate_single_sample(sample)
            results.append(result)
            
            # æ¯10ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 10 == 0:
                success_count = sum(1 for r in results if r.get("success", False))
                pbar.set_postfix({
                    "æˆåŠŸ": f"{success_count}/{i+1}",
                    "æˆåŠŸç‡": f"{success_count/(i+1)*100:.1f}%"
                })
        
        pbar.close()
        
        total_time = time.time() - start_time
        print(f"âœ… è¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’")
        
        # åˆ†æç»“æœ
        analysis = self.analyze_results(results, total_time)
        
        return {
            "results": results,
            "analysis": analysis,
            "timestamp": time.time()
        }
    
    def analyze_results(self, results: List[Dict[str, Any]], total_time: float) -> Dict[str, Any]:
        """åˆ†æè¯„ä¼°ç»“æœ"""
        # åŸºç¡€ç»Ÿè®¡
        total_samples = len(results)
        successful_samples = sum(1 for r in results if r.get("success", False))
        failed_samples = total_samples - successful_samples
        
        # è´¨é‡æŒ‡æ ‡
        quality_scores = [r.get("evaluation", {}).get("quality_score", 0) for r in results]
        exact_matches = sum(1 for r in results if r.get("evaluation", {}).get("exact_match", False))
        contains_expected = sum(1 for r in results if r.get("evaluation", {}).get("contains_expected", False))
        semantic_similarities = [r.get("evaluation", {}).get("semantic_similarity", 0) for r in results]
        
        # æ–°å¢ï¼šæ”¶é›†æ‰€æœ‰çš„f1_score
        f1_scores = [r.get("evaluation", {}).get("f1_score", 0) for r in results]

       

        # ç”Ÿæˆæ—¶é—´ç»Ÿè®¡
        generation_times = [r.get("generation", {}).get("generation_time", 0) for r in results]
        
        # æŒ‰ä¸Šä¸‹æ–‡ç±»å‹åˆ†ç»„
        table_results = [r for r in results if r.get("context_type") == "table"]
        paragraph_results = [r for r in results if r.get("context_type") == "paragraph"]
        
        # è®¡ç®—MRR (Mean Reciprocal Rank) - è¿™é‡Œç®€åŒ–ä¸ºç²¾ç¡®åŒ¹é…ç‡
        mrr = exact_matches / total_samples if total_samples > 0 else 0
        
        analysis = {
            "overall_metrics": {
                "total_samples": total_samples,
                "successful_samples": successful_samples,
                "failed_samples": failed_samples,
                "success_rate": successful_samples / total_samples if total_samples > 0 else 0,
                "exact_match_rate": exact_matches / total_samples if total_samples > 0 else 0,
                "contains_expected_rate": contains_expected / total_samples if total_samples > 0 else 0,
                "mrr": mrr,
                "avg_quality_score": np.mean(quality_scores) if quality_scores else 0,
                "avg_semantic_similarity": np.mean(semantic_similarities) if semantic_similarities else 0,
                "avg_generation_time": np.mean(generation_times) if generation_times else 0,
                "total_evaluation_time": total_time
            },
            "context_type_analysis": {
                "table_samples": {
                    "count": len(table_results),
                    "success_rate": sum(1 for r in table_results if r.get("success", False)) / len(table_results) if table_results else 0,
                    "exact_match_rate": sum(1 for r in table_results if r.get("evaluation", {}).get("exact_match", False)) / len(table_results) if table_results else 0,
                    "avg_quality_score": np.mean([r.get("evaluation", {}).get("quality_score", 0) for r in table_results]) if table_results else 0
                },
                "paragraph_samples": {
                    "count": len(paragraph_results),
                    "success_rate": sum(1 for r in paragraph_results if r.get("success", False)) / len(paragraph_results) if paragraph_results else 0,
                    "exact_match_rate": sum(1 for r in paragraph_results if r.get("evaluation", {}).get("exact_match", False)) / len(paragraph_results) if paragraph_results else 0,
                    "avg_quality_score": np.mean([r.get("evaluation", {}).get("quality_score", 0) for r in paragraph_results]) if paragraph_results else 0
                }
            },
            "quality_distribution": {
                "excellent_quality": sum(1 for score in quality_scores if score >= 0.8),
                "good_quality": sum(1 for score in quality_scores if 0.6 <= score < 0.8),
                "fair_quality": sum(1 for score in quality_scores if 0.4 <= score < 0.6),
                "poor_quality": sum(1 for score in quality_scores if score < 0.4)
            },
            "performance_insights": []
        }
        
        # ... ä½ ç°æœ‰çš„ analysis å­—å…¸ ...
        
        # åœ¨ analysis["overall_metrics"] ä¸­æ·»åŠ  avg_f1_score
        analysis["overall_metrics"]["avg_f1_score"] = np.mean(f1_scores) if f1_scores else 0

        # ä½ ä¹Ÿå¯ä»¥ä¸ºè¡¨æ ¼å’Œæ®µè½æ•°æ®åˆ†åˆ«è®¡ç®—å¹³å‡F1
        table_f1_scores = [r.get("evaluation", {}).get("f1_score", 0) for r in table_results]
        paragraph_f1_scores = [r.get("evaluation", {}).get("f1_score", 0) for r in paragraph_results]
        analysis["context_type_analysis"]["table_samples"]["avg_f1_score"] = np.mean(table_f1_scores) if table_f1_scores else 0
        analysis["context_type_analysis"]["paragraph_samples"]["avg_f1_score"] = np.mean(paragraph_f1_scores) if paragraph_f1_scores else 0


        # ç”Ÿæˆæ€§èƒ½æ´å¯Ÿ
        if analysis["overall_metrics"]["success_rate"] >= 0.8:
            analysis["performance_insights"].append("ğŸ‰ æ•´ä½“è¡¨ç°ä¼˜ç§€ï¼ŒæˆåŠŸç‡è¾¾åˆ°80%ä»¥ä¸Š")
        elif analysis["overall_metrics"]["success_rate"] >= 0.6:
            analysis["performance_insights"].append("âœ… æ•´ä½“è¡¨ç°è‰¯å¥½ï¼ŒæˆåŠŸç‡è¾¾åˆ°60%ä»¥ä¸Š")
        else:
            analysis["performance_insights"].append("âš ï¸ æ•´ä½“è¡¨ç°éœ€è¦æ”¹è¿›")
        
        if analysis["overall_metrics"]["exact_match_rate"] >= 0.7:
            analysis["performance_insights"].append("ğŸ¯ ç²¾ç¡®åŒ¹é…ç‡å¾ˆé«˜ï¼Œæ¨¡å‹è¾“å‡ºè´¨é‡ä¼˜ç§€")
        
        if analysis["context_type_analysis"]["table_samples"]["success_rate"] > analysis["context_type_analysis"]["paragraph_samples"]["success_rate"]:
            analysis["performance_insights"].append("ğŸ“Š è¡¨æ ¼æ•°æ®è¡¨ç°ä¼˜äºæ®µè½æ•°æ®")
        else:
            analysis["performance_insights"].append("ğŸ“ æ®µè½æ•°æ®è¡¨ç°ä¼˜äºè¡¨æ ¼æ•°æ®")
        
        return analysis
    
    def print_analysis_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š å…¨é¢è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*80)
        
        metrics = analysis["overall_metrics"]
        print(f"ğŸ“ˆ æ•´ä½“æŒ‡æ ‡:")
        print(f"   æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
        print(f"   æˆåŠŸæ ·æœ¬æ•°: {metrics['successful_samples']}")
        print(f"   æˆåŠŸç‡: {metrics['success_rate']:.3f} ({metrics['success_rate']*100:.1f}%)")
        print(f"   ç²¾ç¡®åŒ¹é…ç‡: {metrics['exact_match_rate']:.3f} ({metrics['exact_match_rate']*100:.1f}%)")
        print(f"   F1 Score (è¯è¯­é‡å ): {metrics['avg_f1_score']:.3f}")
        print(f"   åŒ…å«æœŸæœ›ç­”æ¡ˆç‡: {metrics['contains_expected_rate']:.3f} ({metrics['contains_expected_rate']*100:.1f}%)")
        print(f"   MRR: {metrics['mrr']:.3f}")
        print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {metrics['avg_quality_score']:.3f}")
        print(f"   å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦: {metrics['avg_semantic_similarity']:.3f}")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {metrics['avg_generation_time']:.2f}s")
        print(f"   æ€»è¯„ä¼°æ—¶é—´: {metrics['total_evaluation_time']:.2f}s")
        
        print(f"\nğŸ“Š ä¸Šä¸‹æ–‡ç±»å‹åˆ†æ:")
        table_analysis = analysis["context_type_analysis"]["table_samples"]
        paragraph_analysis = analysis["context_type_analysis"]["paragraph_samples"]
        print(f"   è¡¨æ ¼æ•°æ® ({table_analysis['count']} æ ·æœ¬):")
        print(f"     æˆåŠŸç‡: {table_analysis['success_rate']:.3f} ({table_analysis['success_rate']*100:.1f}%)")
        print(f"     ç²¾ç¡®åŒ¹é…ç‡: {table_analysis['exact_match_rate']:.3f} ({table_analysis['exact_match_rate']*100:.1f}%)")
        print(f"     å¹³å‡F1 Score: {table_analysis['avg_f1_score']:.3f}")
        print(f"     å¹³å‡è´¨é‡åˆ†æ•°: {table_analysis['avg_quality_score']:.3f}")
        print(f"   æ®µè½æ•°æ® ({paragraph_analysis['count']} æ ·æœ¬):")
        print(f"     æˆåŠŸç‡: {paragraph_analysis['success_rate']:.3f} ({paragraph_analysis['success_rate']*100:.1f}%)")
        print(f"     ç²¾ç¡®åŒ¹é…ç‡: {paragraph_analysis['exact_match_rate']:.3f} ({paragraph_analysis['exact_match_rate']*100:.1f}%)")
        print(f"     å¹³å‡F1 Score: {paragraph_analysis['avg_f1_score']:.3f}")
        print(f"     å¹³å‡è´¨é‡åˆ†æ•°: {paragraph_analysis['avg_quality_score']:.3f}")


        print(f"\nğŸ“ˆ è´¨é‡åˆ†å¸ƒ:")
        quality_dist = analysis["quality_distribution"]
        total = sum(quality_dist.values())
        print(f"   ä¼˜ç§€è´¨é‡ (â‰¥0.8): {quality_dist['excellent_quality']} ({quality_dist['excellent_quality']/total*100:.1f}%)")
        print(f"   è‰¯å¥½è´¨é‡ (0.6-0.8): {quality_dist['good_quality']} ({quality_dist['good_quality']/total*100:.1f}%)")
        print(f"   ä¸€èˆ¬è´¨é‡ (0.4-0.6): {quality_dist['fair_quality']} ({quality_dist['fair_quality']/total*100:.1f}%)")
        print(f"   è¾ƒå·®è´¨é‡ (<0.4): {quality_dist['poor_quality']} ({quality_dist['poor_quality']/total*100:.1f}%)")
        
        print(f"\nğŸ’¡ æ€§èƒ½æ´å¯Ÿ:")
        for insight in analysis["performance_insights"]:
            print(f"   {insight}")
        
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='å¢å¼ºç‰ˆå…¨é¢è¯„ä¼°')
    parser.add_argument('--n', type=int, default=100, help='è¯„ä¼°æ ·æœ¬æ•°é‡ (é»˜è®¤: 100)')
    args = parser.parse_args()
    
    print("ğŸš€ å¢å¼ºç‰ˆå…¨é¢è¯„ä¼°å¼€å§‹")
    print(f"ä½¿ç”¨è¯¦ç»†Chain-of-Thoughtæ¨¡æ¿è¿›è¡Œ{args.n}æ ·æœ¬è¯„ä¼°")
    print("="*60)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = EnhancedComprehensiveEvaluator()
    
    # è¿è¡ŒæŒ‡å®šæ ·æœ¬æ•°é‡çš„è¯„ä¼°
    sample_size = args.n
    print(f"\nğŸ“Š è¯„ä¼°æ ·æœ¬æ•°: {sample_size}")
    
    # è¿è¡Œè¯„ä¼°
    evaluation_results = evaluator.run_comprehensive_evaluation(sample_size)
    
    # æ‰“å°æ‘˜è¦
    evaluator.print_analysis_summary(evaluation_results["analysis"])
    
    # ä¿å­˜ç»“æœ
    output_file = f"comprehensive_evaluation_{sample_size}_samples_enhanced.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    print("\nğŸ‰ å¢å¼ºç‰ˆå…¨é¢è¯„ä¼°å®Œæˆï¼")

if __name__ == "__main__":
    main() 