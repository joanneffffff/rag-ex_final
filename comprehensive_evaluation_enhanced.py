#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬
é›†æˆäº†æ··åˆå†³ç­–ç®—æ³•ã€åŠ¨æ€promptè·¯ç”±ã€æ™ºèƒ½ç­”æ¡ˆæå–å’Œå¤šç»´åº¦è¯„ä¼°ã€‚
"""

# 1. å¯¼å…¥å¿…è¦çš„åº“
import warnings
import logging
import os
import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
import time
import argparse
from collections import Counter
from difflib import SequenceMatcher
import sys

# 2. ç¯å¢ƒè®¾ç½®
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
try:
    from tqdm import tqdm
except ImportError:
    print("âŒ tqdmæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tqdm")
    sys.exit(1)
sys.path.append(str(Path(__file__).parent))
try:
    # ç¡®ä¿ä½ çš„RAGç”Ÿæˆå™¨å¯ä»¥è¢«æ­£ç¡®å¯¼å…¥
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
    print("âœ… ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator")
except ImportError:
    USE_RAG_GENERATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGeneratorï¼Œè„šæœ¬å°†æ— æ³•è¿è¡Œã€‚")
    sys.exit(1)


# ===================================================================
# 3. æ ¸å¿ƒè¾…åŠ©å‡½æ•°
# ===================================================================

def extract_final_answer_with_rescue(raw_output: str) -> str:
    """
    ä»æ¨¡å‹çš„åŸå§‹è¾“å‡ºä¸­æ™ºèƒ½æå–æœ€ç»ˆç­”æ¡ˆã€‚
    å®ƒé¦–å…ˆå°è¯•å¯»æ‰¾<answer>æ ‡ç­¾ï¼Œå¦‚æœå¤±è´¥æˆ–ä¸ºç©ºï¼Œåˆ™å¯åŠ¨æ•‘æ´é€»è¾‘ä»<think>æ ‡ç­¾ä¸­æå–ã€‚
    """
    answer_match = re.search(r'<answer>(.*?)</answer>', raw_output, re.DOTALL)
    if answer_match:
        content = answer_match.group(1).strip()
        if content:
            return content

    think_match = re.search(r'<think>(.*?)</think>', raw_output, re.DOTALL)
    if not think_match:
        lines = raw_output.strip().split('\n')
        return lines[-1].strip() if lines else ""

    think_content = think_match.group(1)
    
    conclusion_phrases = [
        'the answer is', 'the final answer is', 'therefore, the answer is', 
        'the result is', 'equals to', 'is equal to', 'the value is', 
        'the change is', 'the amount is'
    ]
    for phrase in conclusion_phrases:
        # å¯»æ‰¾ç»“è®ºæ€§çŸ­è¯­ï¼Œå¹¶æ•è·åé¢çš„å†…å®¹
        conclusion_match = re.search(
            f'{re.escape(phrase)}\\s*:?\\s*([$()\\d,.;\\w\\s-]+)($|\\.|\\n)', 
            think_content, 
            re.IGNORECASE
        )
        if conclusion_match:
            conclusion = conclusion_match.group(1).strip()
            return re.sub(r'[\.ã€‚,]$', '', conclusion).strip()

    numbers = re.findall(r'[-+]?\$?\(?[\d,]+\.?\d*\)?\%?', think_content)
    if numbers:
        last_number = numbers[-1].replace('$', '').replace(',', '').replace('(', '').replace(')', '').strip()
        return last_number
        
    lines = [line for line in think_content.strip().split('\n') if line.strip()]
    return lines[-1].strip() if lines else ""

def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—F1åˆ†æ•°"""
    def normalize(text):
        return re.sub(r'[^\w\s]', '', text.lower()).split()
    prediction_tokens = normalize(prediction)
    ground_truth_tokens = normalize(ground_truth)
    if not ground_truth_tokens: return 1.0 if not prediction_tokens else 0.0
    if not prediction_tokens: return 0.0
    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0: return 0.0
    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

# ===================================================================
# 4. æ™ºèƒ½è·¯ç”±ç®—æ³•
# ===================================================================

def determine_context_type(context: str) -> str:
    """æ ¹æ®contextå†…å®¹åˆ¤æ–­ç»“æ„ç±»å‹"""
    has_table = "Table ID:" in context
    text_content = re.sub(r'Table ID:.*?\n(Headers:.*?\n)?', '', context, flags=re.DOTALL)
    text_content = re.sub(r'Row \d+:.*?\n', '', text_content)
    text_content = re.sub(r'Category:.*?\n', '', text_content)
    has_meaningful_text = any(len(line.strip()) > 20 for line in text_content.split('\n'))

    if has_table and has_meaningful_text: return "table-text"
    elif has_table: return "table"
    else: return "text"

def analyze_query_features(query: str) -> Dict[str, Any]:
    """åˆ†æqueryç‰¹å¾"""
    query_lower = query.lower()
    calculation_keywords = ['sum', 'total', 'average', 'mean', 'percentage', 'ratio', 'difference', 'increase', 'decrease', 'growth', 'change', 'compare', 'calculate']
    text_keywords = ['describe', 'explain', 'what is', 'what was the effect', 'how', 'why', 'when', 'where', 'who', 'what does', 'consist of', 'what led to']
    
    is_calc = any(keyword in query_lower for keyword in calculation_keywords)
    is_textual = any(keyword in query_lower for keyword in text_keywords)
    
    return {'is_calc': is_calc, 'is_textual': is_textual}

def hybrid_decision(context: str, query: str) -> str:
    """æ··åˆå†³ç­–ç®—æ³•ï¼Œé¢„æµ‹ç­”æ¡ˆæ¥æº"""
    context_type = determine_context_type(context)
    query_features = analyze_query_features(query)

    if context_type == "text":
        return "text"
    
    # å¯¹äºåŒ…å«è¡¨æ ¼çš„context
    if query_features['is_textual'] and not query_features['is_calc']:
        # å¦‚æœé—®é¢˜æ˜æ˜¾æ˜¯è§£é‡Šæ€§çš„ï¼Œç­”æ¡ˆå¾ˆå¯èƒ½åœ¨æ–‡æœ¬ä¸­ï¼Œå³ä½¿è¡¨æ ¼å­˜åœ¨
        return "text" if context_type == "table-text" else "table-text"
    
    if query_features['is_calc']:
         # å¦‚æœé—®é¢˜æ˜¯è®¡ç®—æ€§çš„ï¼Œç­”æ¡ˆå¾ˆå¯èƒ½éœ€è¦ç»“åˆè¡¨æ ¼å’Œæ–‡æœ¬
        return "table-text"
    
    # é»˜è®¤æƒ…å†µï¼Œç­”æ¡ˆæ›´å¯èƒ½ç›´æ¥æ¥è‡ªè¡¨æ ¼
    return "table"


# ===================================================================
# 5. åŠ¨æ€PromptåŠ è½½ä¸è·¯ç”±
# ===================================================================

def load_and_format_template(template_name: str, context: str, query: str) -> List[Dict[str, str]]:
    """åŠ è½½å¹¶æ ¼å¼åŒ–æŒ‡å®šçš„promptæ¨¡æ¿"""
    # æ¨¡æ¿æ”¾åœ¨ 'data/prompt_templates' æ–‡ä»¶å¤¹ä¸‹
    template_path = Path("data/prompt_templates") / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template_content = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}ï¼Œæ— æ³•ç»§ç»­ã€‚")
        sys.exit(1)
    
    system_match = re.search(r'<system>(.*?)</system>', template_content, re.DOTALL)
    system_content = system_match.group(1).strip() if system_match else ""
    user_match = re.search(r'<user>(.*?)</user>', template_content, re.DOTALL)
    user_template = user_match.group(1).strip() if user_match else "Context:\n{context}\n\nQuestion:\n{question}"
    user_content = user_template.replace('{context}', context).replace('{question}', query)
    return [{"role": "system", "content": system_content}, {"role": "user", "content": user_content}]

def get_final_prompt(context: str, query: str) -> List[Dict[str, str]]:
    """åŸºäºæ··åˆå†³ç­–ç®—æ³•å®ç°çš„æœ€ç»ˆPromptè·¯ç”±"""
    predicted_answer_source = hybrid_decision(context, query)
    
    if predicted_answer_source == "table":
        template_file = 'template_for_table_answer.txt'
    elif predicted_answer_source == "text":
        template_file = 'template_for_text_answer.txt'
    else: # "table-text"
        template_file = 'template_for_hybrid_answer.txt'
    
    # print(f"  [è·¯ç”±å†³ç­–] Context: {determine_context_type(context)}, Query: '{query[:30]}...', ä½¿ç”¨æ¨¡æ¿: {template_file}")
    return load_and_format_template(template_file, context, query)

# ===================================================================
# 6. æ ¸å¿ƒè¯„ä¼°ç±»
# ===================================================================

class ComprehensiveEvaluator:
    def __init__(self, model_name: str, device: str):
        self.model_name = model_name
        self.device = device
        self.max_new_tokens = 2048
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        self.generator = LocalLLMGenerator(model_name=self.model_name, device=self.device)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def run_evaluation(self, eval_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        results = []
        start_time = time.time()
        pbar = tqdm(eval_data, desc="ğŸ” è¯„ä¼°æ ·æœ¬", unit="ä¸ª")

        for sample in pbar:
            results.append(self._evaluate_single_sample(sample))
        
        total_time = time.time() - start_time
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        
        analysis = self.analyze_results(results)
        analysis['performance'] = {'total_time': total_time, 'avg_time_per_sample': total_time / len(results) if results else 0}
        return {"results": results, "analysis": analysis}

    def _evaluate_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        try:
            messages = get_final_prompt(sample["context"], sample["query"])
            prompt_text = self._convert_messages_to_text(messages)

            gen_start_time = time.time()
            generation_result = self.generator.generate([prompt_text])[0]
            gen_time = time.time() - gen_start_time
            
            final_answer_to_evaluate = extract_final_answer_with_rescue(generation_result)
            evaluation = self._evaluate_quality(final_answer_to_evaluate, sample["answer"])
            
            return {
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "generated_answer": generation_result,
                "extracted_answer": final_answer_to_evaluate,
                "evaluation": evaluation,
                "answer_from": sample.get("answer_from", "unknown"),
                "predicted_answer_from": hybrid_decision(sample["context"], sample["query"]),
                "generation_time": gen_time
            }
        except Exception as e:
            return {"query": sample["query"], "expected_answer": sample["answer"], "error": str(e)}

    def _evaluate_quality(self, generated: str, expected: str) -> Dict[str, Any]:
        exact_match = generated.strip().lower() == expected.strip().lower()
        f1 = calculate_f1_score(generated, expected)
        return {"exact_match": exact_match, "f1_score": f1}

    def _convert_messages_to_text(self, messages: List[Dict[str, str]]) -> str:
        text = ""
        for message in messages:
            text += f'<{message["role"]}>\n{message["content"]}\n'
        return text

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        # ä½ çš„è¯¦ç»†åˆ†æé€»è¾‘ï¼Œå¯ä»¥å¤ç”¨ä¹‹å‰è„šæœ¬é‡Œçš„ç‰ˆæœ¬
        # è¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–çš„ç‰ˆæœ¬
        if not results: return {}
        
        all_f1 = [r['evaluation']['f1_score'] for r in results if 'evaluation' in r]
        all_em = [r['evaluation']['exact_match'] for r in results if 'evaluation' in r]

        analysis = {
            "overall_metrics": {
                "total_samples": len(results),
                "exact_match_rate": (sum(all_em) / len(all_em) * 100) if all_em else 0,
                "avg_f1_score": np.mean(all_f1) if all_f1 else 0
            },
            "by_answer_type": {}
        }

        types = set(r.get("answer_from") for r in results)
        for t in types:
            subset = [r for r in results if r.get("answer_from") == t]
            subset_f1 = [r['evaluation']['f1_score'] for r in subset if 'evaluation' in r]
            subset_em = [r['evaluation']['exact_match'] for r in subset if 'evaluation' in r]
            analysis["by_answer_type"][t] = {
                "count": len(subset),
                "exact_match_rate": (sum(subset_em) / len(subset_em) * 100) if subset_em else 0,
                "avg_f1_score": np.mean(subset_f1) if subset_f1 else 0
            }
        return analysis

    def print_summary(self, analysis: Dict[str, Any]):
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*60)
        overall = analysis.get("overall_metrics", {})
        print(f"ğŸ“ˆ æ€»ä½“æŒ‡æ ‡:")
        print(f"  - æ€»æ ·æœ¬æ•°: {overall.get('total_samples', 0)}")
        print(f"  - ç²¾ç¡®åŒ¹é…ç‡: {overall.get('exact_match_rate', 0):.2f}%")
        print(f"  - å¹³å‡F1åˆ†æ•°: {overall.get('avg_f1_score', 0):.4f}")

        by_type = analysis.get("by_answer_type", {})
        print("\nğŸ“Š æŒ‰ç­”æ¡ˆæ¥æºç±»å‹åˆ†æ:")
        for type_name, metrics in by_type.items():
            print(f"  - {type_name.upper()} ç±»å‹ ({metrics.get('count', 0)} æ ·æœ¬):")
            print(f"    - ç²¾ç¡®åŒ¹é…ç‡: {metrics.get('exact_match_rate', 0):.2f}%")
            print(f"    - å¹³å‡F1åˆ†æ•°: {metrics.get('avg_f1_score', 0):.4f}")
        print("="*60)

# ===================================================================
# 7. ä¸»å‡½æ•°
# ===================================================================
def main():
    parser = argparse.ArgumentParser(description="æœ€ç»ˆç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model", type=str, default="SUFE-AIFLM-Lab/Fin-R1", help="è¦è¯„ä¼°çš„LLMåç§°")
    parser.add_argument("--data_path", type=str, required=True, help="è¯„ä¼°æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (jsonlæˆ–jsonæ ¼å¼)")
    parser.add_argument("--sample_size", type=int, default=None, help="éšæœºé‡‡æ ·çš„æ ·æœ¬æ•°é‡ï¼Œä¸æä¾›åˆ™è¯„ä¼°å…¨éƒ¨")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡ (cuda/cpu/auto)")
    args = parser.parse_args()

    # è®¾å¤‡é€‰æ‹©é€»è¾‘
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    elif args.device == "cuda":
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            device = "cpu"
        else:
            device = "cuda"
            print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name()}")
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        device = args.device

    # 1. åŠ è½½æ•°æ®
    print(f"ğŸ“– æ­£åœ¨ä» {args.data_path} åŠ è½½æ•°æ®...")
    eval_data = []
    with open(args.data_path, 'r', encoding='utf-8') as f:
        # å…¼å®¹ .json å’Œ .jsonl
        content = f.read()
        try:
            # å°è¯•è§£æä¸ºå•ä¸ªJSONæ•°ç»„
            data = json.loads(content)
            if isinstance(data, list):
                eval_data = data
            # å¦‚æœæ˜¯JSONå¯¹è±¡ï¼Œå¹¶ä¸”æœ‰ 'results' é”®
            elif isinstance(data, dict) and 'results' in data:
                eval_data = data['results']
        except json.JSONDecodeError:
            # å¦‚æœå¤±è´¥ï¼ŒæŒ‰jsonlæ ¼å¼é€è¡Œè§£æ
            f.seek(0)
            for line in f:
                eval_data.append(json.loads(line))
    
    if args.sample_size and args.sample_size < len(eval_data):
        np.random.seed(42)
        indices = np.random.choice(len(eval_data), args.sample_size, replace=False)
        eval_data = [eval_data[i] for i in indices]
        print(f"âœ… éšæœºé‡‡æ · {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
    else:
        print(f"âœ… åŠ è½½äº†å…¨éƒ¨ {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")

    # 2. åˆå§‹åŒ–å¹¶è¿è¡Œè¯„ä¼°å™¨
    evaluator = ComprehensiveEvaluator(model_name=args.model, device=args.device)
    analysis_results = evaluator.run_evaluation(eval_data)
    
    # 3. æ‰“å°å’Œä¿å­˜ç»“æœ
    evaluator.print_summary(analysis_results)
    output_filename = f"final_evaluation_results_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")


if __name__ == "__main__":
    main()