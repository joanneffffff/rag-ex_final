#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬
é›†æˆäº†æ··åˆå†³ç­–ç®—æ³•ã€åŠ¨æ€promptè·¯ç”±ã€æ™ºèƒ½ç­”æ¡ˆæå–å’Œå¤šç»´åº¦è¯„ä¼°ã€‚
"""

# 1. å¯¼å…¥å¿…è¦çš„åº“
import warnings
import logging
import os
import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import time
import argparse
from collections import Counter
from difflib import SequenceMatcher
import sys

# 2. ç¯å¢ƒè®¾ç½®
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'
try:
    from tqdm import tqdm
except ImportError:
    print("âŒ tqdmæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tqdm")
    sys.exit(1)
sys.path.append(str(Path(__file__).parent))
try:
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
    print("âœ… ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator")
except ImportError:
    USE_RAG_GENERATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGeneratorï¼Œè„šæœ¬å°†æ— æ³•è¿è¡Œã€‚")
    sys.exit(1)

try:
    from xlm.utils.context_separator import context_separator
    USE_CONTEXT_SEPARATOR = True
    print("âœ… ä½¿ç”¨ä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½")
except ImportError:
    USE_CONTEXT_SEPARATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥ä¸Šä¸‹æ–‡åˆ†ç¦»å™¨ï¼Œå°†ä½¿ç”¨åŸå§‹ä¸Šä¸‹æ–‡å¤„ç†æ–¹å¼")


# ===================================================================
# 3. æ ¸å¿ƒè¾…åŠ©å‡½æ•°
# ===================================================================

def extract_final_answer_with_rescue(raw_output: str) -> str:
    """
    ä»æ¨¡å‹çš„åŸå§‹è¾“å‡ºä¸­æ™ºèƒ½æå–æœ€ç»ˆç­”æ¡ˆã€‚
    å®ƒé¦–å…ˆå°è¯•å¯»æ‰¾<answer>æ ‡ç­¾ï¼Œå¦‚æœå¤±è´¥æˆ–ä¸ºç©ºï¼Œåˆ™å¯åŠ¨æ•‘æ´é€»è¾‘ä»<think>æ ‡ç­¾ä¸­æå–ã€‚
    """
    def _clean_extracted_text(text: str) -> str:
        """å¯¹æå–å‡ºçš„æ–‡æœ¬è¿›è¡Œé€šç”¨æ¸…ç†ï¼Œä»¥åŒ¹é…æœŸæœ›çš„ç­”æ¡ˆæ ¼å¼"""
        text = text.strip()
        # ç§»é™¤æ•°å­—ä¸­çš„é€—å· (å¦‚æœä½ çš„ expected_answer ä¸åŒ…å«é€—å·)
        text = text.replace(',', '')
        # ç§»é™¤è´Ÿæ•°æ‹¬å· (ä¾‹å¦‚ "(33)" -> "-33")
        if text.startswith('(') and text.endswith(')'):
            text = '-' + text[1:-1]
        
        # æ ‡å‡†åŒ–ç™¾åˆ†å·ï¼Œç¡®ä¿ "15.2%" å’Œ "15.2 %" åŒ¹é…
        text = text.replace('%', ' %').strip()
        text = text.replace(' %', '%')

        # ç§»é™¤å¸¸è§çš„å¼•å¯¼è¯å¥ï¼Œå¹¶å¤„ç†å¤§å°å†™ä¸æ•æ„Ÿï¼ˆä½œä¸ºæ•‘æ´é€»è¾‘çš„æ¸…ç†ï¼‰
        text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
        
        # ç§»é™¤æœ«å°¾å¯èƒ½çš„å¤šä½™æ ‡ç‚¹ç¬¦å·ï¼Œå¦‚å¥å·ã€é€—å·ã€åˆ†å· (ä½†ä¿ç•™ç™¾åˆ†å·)
        text = re.sub(r'[\.ã€‚;,]$', '', text).strip()
        
        # ç§»é™¤å¸¸è§çš„è´§å¸ç¬¦å·å’Œå•ä½è¯ (å¦‚æœä½ çš„ expected_answer ä¸åŒ…å«è¿™äº›)
        text = re.sub(r'(\$|million|billion|usd|eur|pounds|Â£)', '', text, flags=re.IGNORECASE).strip()

        return text

    # 1. å°è¯•ä» <answer> æ ‡ç­¾ä¸­æå– (è¿™æ˜¯é¦–è¦ä¸”æœ€æœŸæœ›çš„)
    answer_match = re.search(r'<answer>(.*?)</answer>', raw_output, re.DOTALL)
    if answer_match:
        content = answer_match.group(1).strip()
        if content:
            return _clean_extracted_text(content)
        # å¦‚æœ <answer> æ ‡ç­¾å­˜åœ¨ä½†å†…å®¹ä¸ºç©ºï¼Œåˆ™ç»§ç»­æ•‘æ´

    # 2. æ•‘æ´é€»è¾‘ï¼šå¦‚æœ <answer> æ ‡ç­¾ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œå°è¯•ä» <think> æ ‡ç­¾ä¸­æå–
    # æŸ¥æ‰¾ <think> æ ‡ç­¾çš„å†…å®¹
    think_match = re.search(r'<think>(.*?)(?:</think>|$)', raw_output, re.DOTALL)
    if not think_match:
        # å¦‚æœè¿ <think> æ ‡ç­¾éƒ½æ²¡æœ‰ï¼Œå›é€€åˆ°åŸå§‹è¾“å‡ºçš„æœ€åä¸€è¡Œ
        lines = raw_output.strip().split('\n')
        return _clean_extracted_text(lines[-1]) if lines else ""

    think_content = think_match.group(1)
    
    # --- 2.1. å°è¯•å¯»æ‰¾ç»“è®ºæ€§çŸ­è¯­ ---
    conclusion_phrases = [
        r'final\s*answer\s*is[:\s]*', r'the\s*answer\s*is[:\s]*', 
        r'therefore,\s*the\s*answer\s*is[:\s]*', r'the\s*result\s*is[:\s]*', 
        r'equals\s*to[:\s]*', r'is\s*equal\s*to[:\s]*', 
        r'the\s*value\s*is[:\s]*', r'the\s*change\s*is[:\s]*', 
        r'the\s*amount\s*is[:\s]*', r'conclusion[:\s]*', 
        r'final\s*extracted\s*value/calculated\s*result[:\s]*', r'final\s*number[:\s]*',
        r'adjusted\s*net\s*income\s*is[:\s]*', r'percentage\s*change\s*is[:\s]*', 
        r'decreased\s*by[:\s]*', r'increased\s*by[:\s]*',
        r'net\s*change\s*is[:\s]*', r'total\s*is[:\s]*',
        r'resulting\s*in[:\s]*', r'is[:\s]*([-+]?[\d,\.]+%?)' # æ•è·"is:"åé¢ç›´æ¥è·Ÿçš„æ•°å­—æˆ–ç™¾åˆ†æ¯”
    ]
    
    for phrase_pattern in conclusion_phrases:
        # æ•è·çŸ­è¯­ååˆ°ä¸‹ä¸€ä¸ªæ ‡ç­¾ã€åŒæ¢è¡Œç¬¦æˆ–å­—ç¬¦ä¸²ç»“æŸçš„å†…å®¹ (éè´ªå©ª)
        conclusion_match = re.search(
            f'{phrase_pattern}(.*?)(?:$|<answer>|<think>|\\n\\n|\\Z)', 
            think_content, 
            re.IGNORECASE | re.DOTALL 
        )
        if conclusion_match:
            conclusion = conclusion_match.group(1).strip()
            # ç¡®ä¿æå–çš„å†…å®¹ä¸åŒ…å«æ€è€ƒè¿‡ç¨‹ä¸­çš„æ­¥éª¤ç¼–å·
            if conclusion and re.fullmatch(r'\d+\.', conclusion.split('\n')[0].strip()):
                continue # å¦‚æœç¬¬ä¸€è¡Œæ˜¯æ­¥éª¤ç¼–å·ï¼Œè·³è¿‡
            
            return _clean_extracted_text(conclusion)
    
    # --- 2.2. å¦‚æœç»“è®ºæ€§çŸ­è¯­ä¸åŒ¹é…ï¼Œå°è¯•å¯»æ‰¾æœ€åä¸€ä¸ªç¬¦åˆæ•°å€¼/ç™¾åˆ†æ¯”/å¸¸è§æ ¼å¼çš„å­—ç¬¦ä¸² ---
    potential_answers_raw = re.findall(r'([-+]?\s*\(?[\d,\.]+\)?%?)\s*$', think_content, re.MULTILINE) # æ•è·ç»„
    if not potential_answers_raw:
        potential_answers_raw = re.findall(r'([-+]?\s*\(?[\d,\.]+\)?%?)', think_content) # æ•è·ç»„
    
    if potential_answers_raw:
        for item_raw in reversed(potential_answers_raw):
            item = item_raw.strip()
            if not item: continue
            
            # æ’é™¤æ˜æ˜¾çš„æ­¥éª¤ç¼–å·æˆ–çŸ­è¯­ (å¦‚"1.", "2.", "Step 1:")
            if re.fullmatch(r'(\d+\.|\bstep\s*\d+\b)[:\s]*', item, re.IGNORECASE):
                continue

            cleaned_item = _clean_extracted_text(item)
            
            # ç®€å•çš„éªŒè¯ï¼Œç¡®ä¿ä¸æ˜¯ç©ºçš„æˆ–çº¯ç²¹çš„æ ‡ç‚¹
            if cleaned_item and len(cleaned_item) > 0 and not re.fullmatch(r'[^\w\s\d%.-]*', cleaned_item):
                return cleaned_item
                
    # --- 2.3. æœ€åå›é€€ï¼šå¦‚æœä»¥ä¸Šéƒ½å¤±è´¥ï¼Œå– <think> å†…å®¹çš„æœ€åä¸€è¡Œ ---
    lines = [line for line in think_content.strip().split('\n') if line.strip()]
    if lines:
        return _clean_extracted_text(lines[-1])
    return "" # å¦‚æœ think ä¹Ÿæ˜¯ç©ºçš„ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²


def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—F1åˆ†æ•°ï¼ŒåŒ…å«æ›´é²æ£’çš„å½’ä¸€åŒ–ï¼Œä¸ç­”æ¡ˆæå–é€»è¾‘ä¿æŒé«˜åº¦ä¸€è‡´"""
    def normalize_for_f1(text):
        text = text.strip()
        
        # ç§»é™¤æ•°å­—ä¸­çš„é€—å·
        text = text.replace(',', '')
        # ç§»é™¤è´Ÿæ•°æ‹¬å· (ä¾‹å¦‚ "(33)" -> "-33")
        if text.startswith('(') and text.endswith(')'):
            text = '-' + text[1:-1]
        
        # æ ‡å‡†åŒ–ç™¾åˆ†å·ï¼Œç¡®ä¿ "15.2%" å’Œ "15.2%" åŒ¹é…
        text = text.replace('%', ' %').strip()
        text = text.replace(' %', '%')

        # ç§»é™¤å¸¸è§çš„å¼•å¯¼è¯å¥ (åº”ä¸ Prompt ä¼˜åŒ–åå‡å°‘å‡ºç°)
        text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
        
        # ç§»é™¤æœ«å°¾å¯èƒ½çš„å¤šä½™æ ‡ç‚¹ (ä¾‹å¦‚å¥å·)
        text = text.rstrip('.')
        
        # æœ€ç»ˆå…¨éƒ¨å°å†™å¹¶åˆ†å‰²
        return text.lower().split()

    prediction_tokens = normalize_for_f1(prediction)
    ground_truth_tokens = normalize_for_f1(ground_truth)

    if not ground_truth_tokens: 
        return 1.0 if not prediction_tokens else 0.0
    if not prediction_tokens: 
        return 0.0

    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0: 
        return 0.0

    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

# ===================================================================
# 4. æ™ºèƒ½è·¯ç”±ç®—æ³•
# ===================================================================

def determine_context_type(context: str) -> str:
    """æ ¹æ®contextå†…å®¹åˆ¤æ–­ç»“æ„ç±»å‹ï¼ŒåŸºäºTable IDå’ŒParagraph ID"""
    has_table_id = "Table ID:" in context
    has_paragraph_id = "Paragraph ID:" in context
    
    # ç§»é™¤IDæ ‡è¯†è¡Œï¼Œè·å–çº¯å†…å®¹
    content_without_ids = re.sub(r'(Table ID|Paragraph ID):.*?\n', '', context, flags=re.DOTALL)
    # ç§»é™¤è¡¨æ ¼ç»“æ„æ ‡è¯†
    content_without_ids = re.sub(r'Headers:.*?\n', '', content_without_ids, flags=re.DOTALL)
    content_without_ids = re.sub(r'Row \d+:.*?\n', '', content_without_ids)
    content_without_ids = re.sub(r'Category:.*?\n', '', content_without_ids)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ„ä¹‰çš„æ–‡æœ¬å†…å®¹ï¼ˆé•¿åº¦>20çš„è¡Œï¼‰
    has_meaningful_text = any(len(line.strip()) > 20 for line in content_without_ids.split('\n') if line.strip())
    
    # åŸºäºIDå­˜åœ¨æ€§è¿›è¡Œç²¾ç¡®åˆ¤æ–­
    if has_table_id and has_paragraph_id:
        return "table-text"  # åŒæ—¶åŒ…å«è¡¨æ ¼å’Œæ®µè½ID
    elif has_table_id:
        return "table"  # åªæœ‰è¡¨æ ¼ID
    elif has_paragraph_id:
        return "text"   # åªæœ‰æ®µè½ID
    else:
        # æ²¡æœ‰IDæ ‡è¯†çš„æƒ…å†µï¼Œå›é€€åˆ°å†…å®¹åˆ†æ
        if has_meaningful_text:
            return "text"
        else:
            return "unknown"

def analyze_query_features(query: str) -> Dict[str, Any]:
    """åˆ†æqueryç‰¹å¾ï¼Œæ›´ç»†è‡´åœ°è¯†åˆ«é—®é¢˜æ„å›¾"""
    query_lower = query.lower()
    
    # è¯†åˆ«è®¡ç®—æ€§å…³é”®è¯
    calculation_keywords = [
        'sum', 'total', 'average', 'mean', 'percentage', 'ratio', 'difference', 
        'increase', 'decrease', 'growth', 'change', 'compare', 'calculate', 
        'how much', 'how many', 'what is the', 'value of', 'amount of' # å¢åŠ æ›´é€šç”¨çš„æ•°å€¼é—®é¢˜è¯
    ]
    
    # è¯†åˆ«æ–‡æœ¬æ€§å…³é”®è¯ (å®šä¹‰ã€è§£é‡Šã€æè¿°)
    text_keywords = [
        'describe', 'explain', 'what is', 'what was the effect', 'how is', 'why', 
        'when', 'where', 'who', 'what does', 'consist of', 'what led to', 
        'define', 'meaning of', 'included in', 'comprised of' # å¢åŠ æ›´å¤šæè¿°æ€§è¯
    ]
    
    # è¯†åˆ«åˆ—è¡¨/æšä¸¾æ€§å…³é”®è¯
    list_keywords = ['list', 'name', 'assumptions', 'factors', 'items', 'components', 'types of', 'categories of'] 
    
    is_calc = any(keyword in query_lower for keyword in calculation_keywords)
    is_textual = any(keyword in query_lower for keyword in text_keywords)
    is_list = any(keyword in query_lower for keyword in list_keywords) 
    
    return {'is_calc': is_calc, 'is_textual': is_textual, 'is_list': is_list}

def calculate_content_ratio(context: str) -> Dict[str, float]:
    """è®¡ç®—è¡¨æ ¼å’Œæ–‡æœ¬å†…å®¹çš„æ¯”ä¾‹"""
    lines = [line.strip() for line in context.split('\n') if line.strip()]
    if not lines:
        return {'table_ratio': 0.0, 'text_ratio': 0.0, 'mixed_ratio': 0.0}
    
    # è¡¨æ ¼ç›¸å…³è¡Œ
    table_lines = 0
    text_lines = 0
    
    for line in lines:
        # è¡¨æ ¼æ ‡è¯† - ä¸determine_context_typeä¿æŒä¸€è‡´
        if (any(keyword in line.lower() for keyword in ['headers:', 'row', 'column']) or 
            line.lower().startswith('table id:') or
            '|' in line and len(line.split('|')) > 2):  # åŒ…å«åˆ†éš”ç¬¦çš„è¡¨æ ¼è¡Œ
            table_lines += 1
        # æ–‡æœ¬æ ‡è¯† - æ’é™¤IDè¡Œï¼Œåªè®¡ç®—å®é™…æ–‡æœ¬å†…å®¹
        elif (len(line) > 15 and  # é™ä½é•¿åº¦è¦æ±‚
              not line.lower().startswith('table id:') and 
              not line.lower().startswith('paragraph id:') and
              not any(keyword in line.lower() for keyword in ['headers:', 'row', 'column', 'category:'])):
            text_lines += 1
    
    total_lines = len(lines)
    table_ratio = table_lines / total_lines if total_lines > 0 else 0.0
    text_ratio = text_lines / total_lines if total_lines > 0 else 0.0
    mixed_ratio = 1.0 - abs(table_ratio - text_ratio)  # æ··åˆç¨‹åº¦
    
    return {
        'table_ratio': table_ratio,
        'text_ratio': text_ratio, 
        'mixed_ratio': mixed_ratio
    }

def calculate_decision_confidence(context_type: str, query_features: Dict[str, Any], content_ratio: Dict[str, float]) -> Dict[str, float]:
    """è®¡ç®—å†³ç­–çš„ç½®ä¿¡åº¦"""
    confidence_scores = {
        'table': 0.0,
        'text': 0.0,
        'hybrid': 0.0
    }
    
    # åŸºäºä¸Šä¸‹æ–‡ç±»å‹çš„ç½®ä¿¡åº¦ï¼ˆæƒé‡ï¼š0.4ï¼‰
    if context_type == "table":
        confidence_scores['table'] += 0.4
    elif context_type == "text":
        confidence_scores['text'] += 0.4
    elif context_type == "table-text":
        confidence_scores['hybrid'] += 0.4
    
    # åŸºäºæŸ¥è¯¢ç‰¹å¾çš„ç½®ä¿¡åº¦ï¼ˆæƒé‡ï¼š0.3ï¼‰
    if query_features['is_list']:
        confidence_scores['table'] += 0.3
        confidence_scores['hybrid'] += 0.1
    elif query_features['is_calc']:
        confidence_scores['table'] += 0.2
        confidence_scores['hybrid'] += 0.2
    elif query_features['is_textual']:
        confidence_scores['text'] += 0.3
        confidence_scores['hybrid'] += 0.1
    
    # åŸºäºå†…å®¹æ¯”ä¾‹çš„ç½®ä¿¡åº¦ï¼ˆæƒé‡ï¼š0.3ï¼‰
    if content_ratio['table_ratio'] > 0.5:  # é™ä½é˜ˆå€¼
        confidence_scores['table'] += 0.3
    elif content_ratio['text_ratio'] > 0.5:  # é™ä½é˜ˆå€¼
        confidence_scores['text'] += 0.3
    else:
        # å¦‚æœå†…å®¹æ¯”ä¾‹ä¸æ˜ç¡®ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡ç±»å‹ç»™äºˆæ”¯æŒ
        if context_type == "text" and content_ratio['text_ratio'] > 0:
            confidence_scores['text'] += 0.3
        elif context_type == "table" and content_ratio['table_ratio'] > 0:
            confidence_scores['table'] += 0.3
        else:
            confidence_scores['hybrid'] += 0.3
    
    # å½’ä¸€åŒ–ç½®ä¿¡åº¦
    total = sum(confidence_scores.values())
    if total > 0:
        for key in confidence_scores:
            confidence_scores[key] /= total
    
    return confidence_scores



def hybrid_decision_enhanced(context: str, query: str) -> Dict[str, Any]:
    """å¢å¼ºç‰ˆæ··åˆå†³ç­–ç®—æ³•ï¼Œä»…ç”¨äºè‹±æ–‡å†…å®¹ï¼ˆä¸­æ–‡å†…å®¹åº”ä½¿ç”¨multi_stage_chinese_templateï¼‰"""
    # è¿™ä¸ªå‡½æ•°åªå¤„ç†è‹±æ–‡å†…å®¹ï¼Œä¸­æ–‡å†…å®¹åº”è¯¥ç›´æ¥ä½¿ç”¨multi_stage_chinese_template
    context_type = determine_context_type(context)
    query_features = analyze_query_features(query)
    content_ratio = calculate_content_ratio(context)
    
    # è®¡ç®—ç½®ä¿¡åº¦
    confidence_scores = calculate_decision_confidence(context_type, query_features, content_ratio)
    
    # è·å–æœ€é«˜ç½®ä¿¡åº¦çš„å†³ç­–
    best_decision = max(confidence_scores.items(), key=lambda x: x[1])
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªé«˜ç½®ä¿¡åº¦å€™é€‰
    high_confidence_threshold = 0.3
    candidates = [(decision, score) for decision, score in confidence_scores.items() if score >= high_confidence_threshold]
    candidates.sort(key=lambda x: x[1], reverse=True)
    
    # å¦‚æœæœ€é«˜ç½®ä¿¡åº¦ä¸å¤Ÿé«˜ï¼Œä¸”æœ‰å¤šä¸ªå€™é€‰ï¼Œæ ‡è®°ä¸ºå›°éš¾å†³ç­–
    is_difficult = best_decision[1] < 0.5 and len(candidates) > 1
    
    # æ„å»ºå†³ç­–ç»“æœ
    decision_result = {
        'primary_decision': best_decision[0],
        'confidence': best_decision[1],
        'is_difficult': is_difficult,
        'candidates': candidates,
        'context_type': context_type,
        'query_features': query_features,
        'content_ratio': content_ratio,
        'confidence_scores': confidence_scores
    }
    
    return decision_result

def hybrid_decision(context: str, query: str) -> str:
    """æ··åˆå†³ç­–ç®—æ³•ï¼ŒåŸºäºTable IDå’ŒParagraph IDè¿›è¡Œç²¾ç¡®è·¯ç”±"""
    # ä½¿ç”¨å¢å¼ºç‰ˆå†³ç­–ç®—æ³•
    decision_result = hybrid_decision_enhanced(context, query)
    
    # å¦‚æœæ˜¯å›°éš¾å†³ç­–ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„ç­–ç•¥
    if decision_result['is_difficult']:
        # å¯¹äºå›°éš¾å†³ç­–ï¼Œä¼˜å…ˆé€‰æ‹©hybridæ¨¡æ¿
        if decision_result['primary_decision'] in ['table', 'text'] and decision_result['confidence'] < 0.6:
            return "hybrid"
    
    return decision_result['primary_decision']

# ===================================================================
# 5. åŠ¨æ€PromptåŠ è½½ä¸è·¯ç”±
# ===================================================================

def _parse_template_string_to_messages(template_full_string: str, query: str, context: str = "", table_context: str = "", text_context: str = "") -> List[Dict[str, str]]:
    """
    è§£æåŒ…å« ===TAG=== åˆ†éš”ç¬¦çš„æ¨¡æ¿å­—ç¬¦ä¸²ï¼Œå¹¶æ„å»ºæ¶ˆæ¯åˆ—è¡¨ã€‚
    æ ¹æ®ä¼ å…¥çš„ context ç±»å‹æ›¿æ¢å ä½ç¬¦ã€‚
    """
    messages = []
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ‰€æœ‰éƒ¨åˆ†ï¼Œå¹¶ä¿ç•™åˆ†éš”ç¬¦å†…å®¹
    parts = re.split(r'(===SYSTEM===|===USER===|===ASSISTANT===)', template_full_string, flags=re.DOTALL)
    
    # ç§»é™¤ç¬¬ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰å’Œå¤šä½™çš„ç©ºç™½
    parts = [p.strip() for p in parts if p.strip()]

    # éå† parts åˆ—è¡¨ï¼Œé‡æ–°ç»„åˆ role å’Œ content
    for i in range(0, len(parts), 2):
        if i + 1 < len(parts):
            role_tag_raw = parts[i].strip() # ä¾‹å¦‚ "===SYSTEM==="
            content = parts[i+1].strip() # æ ‡ç­¾åçš„å†…å®¹
            
            # æå–å®é™…çš„è§’è‰²åç§°
            role = None
            if role_tag_raw == "===SYSTEM===":
                role = "system"
            elif role_tag_raw == "===USER===":
                role = "user"
            elif role_tag_raw == "===ASSISTANT===":
                role = "assistant"
            
            if role and content:
                # æ›¿æ¢å ä½ç¬¦ (åªé’ˆå¯¹ 'user' è§’è‰²æ¶ˆæ¯è¿›è¡Œæ›¿æ¢)
                if role == "user":
                    content = content.replace('{query}', query)
                    
                    # å¤„ç†ä¸­æ–‡æ¨¡æ¿çš„ç‰¹æ®Šå ä½ç¬¦
                    if '{summary}' in content and '{context}' in content:
                        # ä¸­æ–‡æ¨¡æ¿ï¼šä½¿ç”¨æ‘˜è¦å’Œå®Œæ•´ä¸Šä¸‹æ–‡
                        combined_context = f"{table_context}\n{text_context}".strip()
                        summary = combined_context[:500] + "..." if len(combined_context) > 500 else combined_context
                        content = content.replace('{summary}', summary)
                        content = content.replace('{context}', combined_context)
                    else:
                        # è‹±æ–‡æ¨¡æ¿ï¼šå¤„ç†åˆ†ç¦»çš„ä¸Šä¸‹æ–‡å ä½ç¬¦
                        content = content.replace('{question}', query)
                        
                        if '{table_context}' in content and '{text_context}' in content:
                            content = content.replace('{table_context}', table_context)
                            content = content.replace('{text_context}', text_context)
                        elif '{context}' in content: # å…¼å®¹åªæœ‰ {context} çš„æ¨¡æ¿
                            # å¦‚æœæ˜¯é€šç”¨ {context} å ä½ç¬¦ï¼Œä¸”æœ‰åˆ†ç¦»çš„ä¸Šä¸‹æ–‡ï¼Œåˆ™æ‹¼æ¥
                            if table_context and text_context:
                                combined_context = f"Table Context:\n{table_context}\n\nText Context:\n{text_context}"
                                content = content.replace('{context}', combined_context.strip())
                            elif table_context:
                                content = content.replace('{context}', f"Table Context:\n{table_context}")
                            elif text_context:
                                content = content.replace('{context}', f"Text Context:\n{text_context}")
                            else: # å¦‚æœæ²¡æœ‰åˆ†ç¦»ä¸Šä¸‹æ–‡ï¼Œå°±ç”¨åŸå§‹ context
                                content = content.replace('{context}', context)
                        # ç¡®ä¿æ²¡æœ‰æœªæ›¿æ¢çš„ä¸Šä¸‹æ–‡å ä½ç¬¦ï¼ˆè¿™äº›åº”è¯¥è¢«å‰é¢çš„é€»è¾‘å¤„ç†æ‰ï¼‰
                        content = content.replace('{table_context}', '').replace('{text_context}', '')
                
                messages.append({"role": role, "content": content})
                
    return messages

def load_and_format_template(template_name: str, context: str, query: str) -> List[Dict[str, str]]:
    """
    åŠ è½½å¹¶æ ¼å¼åŒ–æŒ‡å®šçš„promptæ¨¡æ¿ï¼ˆåŒ…å« ===TAG=== åˆ†éš”ç¬¦ï¼‰ã€‚
    è¯¥å‡½æ•°ä¸å¤„ç†ä¸Šä¸‹æ–‡åˆ†ç¦»ï¼Œé€šç”¨ {context} å ä½ç¬¦ã€‚
    """
    template_path = Path("data/prompt_templates") / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template_full_string = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}ï¼Œæ— æ³•ç»§ç»­ã€‚")
        sys.exit(1)
    
    return _parse_template_string_to_messages(template_full_string, query, context=context)

def load_and_format_template_with_separated_context(template_name: str, table_context: str, text_context: str, query: str) -> List[Dict[str, str]]:
    """
    åŠ è½½å¹¶æ ¼å¼åŒ–æŒ‡å®šçš„promptæ¨¡æ¿ï¼ˆåŒ…å« ===TAG=== åˆ†éš”ç¬¦ï¼‰ï¼Œä½¿ç”¨åˆ†ç¦»çš„ä¸Šä¸‹æ–‡ã€‚
    ä¸“é—¨å¤„ç† {table_context} å’Œ {text_context} å ä½ç¬¦ã€‚
    """
    template_path = Path("data/prompt_templates") / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template_full_string = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}ï¼Œæ— æ³•ç»§ç»­ã€‚")
        sys.exit(1)
    
    return _parse_template_string_to_messages(template_full_string, query, table_context=table_context, text_context=text_context)

def get_final_prompt(context: str, query: str) -> List[Dict[str, str]]:
    """åŸºäºå¢å¼ºæ··åˆå†³ç­–ç®—æ³•å®ç°çš„æœ€ç»ˆPromptè·¯ç”±ï¼Œé›†æˆä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½ï¼ˆä»…å¤„ç†è‹±æ–‡å†…å®¹ï¼‰"""
    # è‹±æ–‡å†…å®¹ä½¿ç”¨æ··åˆå†³ç­–ç®—æ³•
    decision_result = hybrid_decision_enhanced(context, query)
    predicted_answer_source = decision_result['primary_decision']
    
    # è®°å½•å†³ç­–ä¿¡æ¯ç”¨äºè°ƒè¯•
    if decision_result['is_difficult']:
        print(f"âš ï¸  å›°éš¾å†³ç­–æ£€æµ‹: {predicted_answer_source} (ç½®ä¿¡åº¦: {decision_result['confidence']:.3f})")
        print(f"   å€™é€‰å†³ç­–: {decision_result['candidates']}")
    
    if predicted_answer_source == "table":
        template_file = 'template_for_table_answer.txt'
    elif predicted_answer_source == "text":
        template_file = 'template_for_text_answer.txt'
    elif predicted_answer_source == "hybrid":
        template_file = 'template_for_hybrid_answer.txt'
    else: # "unknown" å›é€€
        template_file = 'template_for_hybrid_answer.txt'
    
    # è‹±æ–‡å†…å®¹ä½¿ç”¨ä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½
    if USE_CONTEXT_SEPARATOR:
        try:
            # åˆ†ç¦»ä¸Šä¸‹æ–‡
            separated = context_separator.separate_context(context)
            
            # æ ¼å¼åŒ– prompt å‚æ•°
            prompt_params = context_separator.format_for_prompt(separated, query)
            
            # ä½¿ç”¨åˆ†ç¦»åçš„ä¸Šä¸‹æ–‡æ ¼å¼åŒ–æ¨¡æ¿
            # load_and_format_template_with_separated_context ä¼šè°ƒç”¨ _parse_template_string_to_messages
            return load_and_format_template_with_separated_context(
                template_file, 
                prompt_params["table_context"], 
                prompt_params["text_context"], 
                query
            )
        except Exception as e:
            print(f"âš ï¸ ä¸Šä¸‹æ–‡åˆ†ç¦»å¤±è´¥: {e}ï¼Œå›é€€åˆ°åŸå§‹æ–¹å¼")
            return load_and_format_template(template_file, context, query)
    else:
        # å›é€€åˆ°åŸå§‹æ–¹å¼
        return load_and_format_template(template_file, context, query)

# ===================================================================
# 6. æ ¸å¿ƒè¯„ä¼°ç±»
# ===================================================================

class ComprehensiveEvaluator:
    def __init__(self, model_name: str, device: str):
        self.model_name = model_name
        self.device = device
        self.max_new_tokens = 4096 # åˆç†çš„ max_new_tokens
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        # LocalLLMGenerator çš„åˆå§‹åŒ–ä¿æŒä¸å˜ï¼Œå®ƒä¼šåœ¨å†…éƒ¨åŠ è½½æ¨¡å‹å’Œ Tokenizer
        self.generator = LocalLLMGenerator(model_name=self.model_name, device=self.device)
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def run_evaluation(self, eval_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        results = []
        start_time = time.time()
        pbar = tqdm(eval_data, desc="ğŸ” è¯„ä¼°æ ·æœ¬", unit="ä¸ª")

        for sample in pbar:
            result = self._evaluate_single_sample(sample)
            results.append(result)
        
        total_time = time.time() - start_time
        print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“Š å¤„ç†äº† {len(results)} ä¸ªç»“æœ")
        
        analysis = self.analyze_results(results)
        analysis['performance'] = {'total_time': total_time, 'avg_time_per_sample': total_time / len(results) if results else 0}
        return {"results": results, "analysis": analysis}

    def _evaluate_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        try:
            messages = get_final_prompt(sample["context"], sample["query"])
            
            # ### æ ¸å¿ƒï¼šå°† messages åˆ—è¡¨è½¬æ¢ä¸º Fin-R1 (Qwen2.5) æœŸæœ›çš„ChatMLæ ¼å¼å­—ç¬¦ä¸²
            prompt_text = self._convert_messages_to_text(messages) 

            gen_start_time = time.time()
            # generator.generate æœŸæœ› List[str]ï¼Œæ‰€ä»¥ç”¨ [prompt_text] åŒ…è£¹
            generation_result = self.generator.generate([prompt_text])[0]
            gen_time = time.time() - gen_start_time
            
            final_answer_to_evaluate = extract_final_answer_with_rescue(generation_result)
            evaluation = self._evaluate_quality(final_answer_to_evaluate, sample["answer"])
            
            # è®°å½•è·¯ç”±å†³ç­–å’Œå®é™…ç­”æ¡ˆæ¥æºï¼Œä¾¿äºåˆ†æ
            decision_result = hybrid_decision_enhanced(sample["context"], sample["query"])
            predicted_source = decision_result['primary_decision']
            actual_source = sample.get("answer_from", "unknown") 

            return {
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "generated_answer": generation_result,      # åŸå§‹æ¨¡å‹è¾“å‡º
                "extracted_answer": final_answer_to_evaluate, # ç»è¿‡ extract_final_answer_with_rescue å¤„ç†åçš„ç­”æ¡ˆ
                "evaluation": evaluation,
                "answer_from": actual_source, 
                "predicted_answer_from": predicted_source,
                "decision_confidence": decision_result['confidence'],
                "is_difficult_decision": decision_result['is_difficult'],
                "context_type": decision_result['context_type'],
                "content_ratio": decision_result['content_ratio'],
                "generation_time": gen_time
            }
        except Exception as e:
            # è¯¦ç»†æ‰“å°é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‘ç”Ÿé”™è¯¯çš„æ ·æœ¬IDæˆ–æŸ¥è¯¢
            sample_id = sample.get("id", "N/A") # å¦‚æœä½ çš„æ ·æœ¬æœ‰ID
            print(f"\nâŒ å¤„ç†æ ·æœ¬å¤±è´¥ (ID: {sample_id}, Query: '{sample.get('query', 'N/A')[:50]}...', Error: {e})", file=sys.stderr)
            return {
                "query": sample["query"], 
                "expected_answer": sample["answer"], 
                "error": str(e),
                "context": sample.get("context", "N/A"), # åŒ…å«ä¸Šä¸‹æ–‡ä»¥ä¾›è°ƒè¯•
                "evaluation": {"exact_match": False, "f1_score": 0.0},
                "generated_answer": "", # ç¡®ä¿æœ‰æ­¤å­—æ®µï¼Œå³ä½¿æ˜¯é”™è¯¯æ ·æœ¬
                "extracted_answer": ""  # ç¡®ä¿æœ‰æ­¤å­—æ®µï¼Œå³ä½¿æ˜¯é”™è¯¯æ ·æœ¬
            }

    def _evaluate_quality(self, generated: str, expected: str) -> Dict[str, Any]:
        exact_match = generated.strip().lower() == expected.strip().lower()
        f1 = calculate_f1_score(generated, expected)
        return {"exact_match": exact_match, "f1_score": f1}

    def _convert_messages_to_text(self, messages: List[Dict[str, str]]) -> str:
        """
        å°† messages åˆ—è¡¨è½¬æ¢ä¸ºFin-R1ï¼ˆQwen2.5 basedï¼‰æœŸæœ›çš„ChatMLæ ¼å¼å­—ç¬¦ä¸²ã€‚
        è¿™æ˜¯æœ€ç»ˆä¼ é€’ç»™ LocalLLMGenerator çš„å­—ç¬¦ä¸²ã€‚
        """
        if not messages:
            return ""
        
        # Qwen2.5 ä½¿ç”¨ ChatML æ ¼å¼
        formatted_prompt = ""
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")
            
            if role == "system":
                formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n"
            elif role == "user":
                formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n"
            elif role == "assistant":
                formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n"
        
        # <<< å…³é”®ä¿®å¤ï¼šç§»é™¤æˆ–æ³¨é‡Šæ‰è¿™ä¸€è¡Œ >>>
        # å› ä¸ºPromptæ¨¡æ¿çš„æœ«å°¾æ˜¯ç”¨æˆ·æ¶ˆæ¯çš„ä¸€éƒ¨åˆ†ï¼Œæ¨¡å‹ä¼šæ ¹æ®ChatMLçš„è§„åˆ™è‡ªåŠ¨åœ¨ç”¨æˆ·æ¶ˆæ¯åç”ŸæˆåŠ©æ‰‹å›åº”ï¼Œæ— éœ€é¢å¤–æ·»åŠ  <|im_start|>assistantã€‚
        # formatted_prompt += "<|im_start|>assistant\n" 
        
        return formatted_prompt

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        print(f"ğŸ” å¼€å§‹åˆ†æ {len(results)} ä¸ªç»“æœ...")
        
        if not results: 
            print("âŒ æ²¡æœ‰ç»“æœå¯åˆ†æ")
            return {}
        
        # æ£€æŸ¥ç»“æœç»“æ„
        valid_results = [r for r in results if 'evaluation' in r]
        error_results = [r for r in results if 'error' in r]
        print(f"âœ… æœ‰æ•ˆç»“æœ: {len(valid_results)}, âŒ é”™è¯¯ç»“æœ: {len(error_results)}")
        
        all_f1 = [r['evaluation']['f1_score'] for r in valid_results]
        all_em = [r['evaluation']['exact_match'] for r in valid_results]

        # åˆ†æå†³ç­–ç›¸å…³æŒ‡æ ‡
        difficult_decisions = [r for r in valid_results if r.get('is_difficult_decision', False)]
        avg_confidence = np.mean([r.get('decision_confidence', 0) for r in valid_results])
        
        analysis = {
            "overall_metrics": {
                "total_samples": len(results),
                "valid_samples": len(valid_results),
                "error_samples": len(error_results),
                "exact_match_rate": (sum(all_em) / len(all_em) * 100) if all_em else 0,
                "avg_f1_score": np.mean(all_f1) if all_f1 else 0,
                "difficult_decisions": len(difficult_decisions),
                "avg_decision_confidence": avg_confidence
            },
            "by_answer_type": {},
            "decision_analysis": {
                "difficult_decisions_count": len(difficult_decisions),
                "difficult_decisions_ratio": len(difficult_decisions) / len(valid_results) if valid_results else 0,
                "avg_confidence": avg_confidence
            }
        }

        types = set(r.get("answer_from", "unknown") for r in results)
        print(f"ğŸ“Š å‘ç°ç­”æ¡ˆç±»å‹: {list(types)}")
        
        for t in types:
            subset = [r for r in results if r.get("answer_from", "unknown") == t]
            subset_valid = [r for r in subset if 'evaluation' in r]
            subset_f1 = [r['evaluation']['f1_score'] for r in subset_valid]
            subset_em = [r['evaluation']['exact_match'] for r in subset_valid]
            analysis["by_answer_type"][t] = {
                "count": len(subset),
                "valid_count": len(subset_valid),
                "exact_match_rate": (sum(subset_em) / len(subset_em) * 100) if subset_em else 0,
                "avg_f1_score": np.mean(subset_f1) if subset_f1 else 0
            }
        return analysis

    def print_summary(self, analysis: Dict[str, Any]):
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*60)
        overall = analysis.get("overall_metrics", {})
        print(f"ğŸ“ˆ æ€»ä½“æŒ‡æ ‡:")
        print(f"    - æ€»æ ·æœ¬æ•°: {overall.get('total_samples', 0)}")
        print(f"    - æœ‰æ•ˆæ ·æœ¬æ•°: {overall.get('valid_samples', 0)}")
        print(f"    - é”™è¯¯æ ·æœ¬æ•°: {overall.get('error_samples', 0)}")
        print(f"    - ç²¾ç¡®åŒ¹é…ç‡: {overall.get('exact_match_rate', 0):.2f}%")
        print(f"    - å¹³å‡F1åˆ†æ•°: {overall.get('avg_f1_score', 0):.4f}")
        print(f"    - å›°éš¾å†³ç­–æ•°: {overall.get('difficult_decisions', 0)}")
        print(f"    - å¹³å‡å†³ç­–ç½®ä¿¡åº¦: {overall.get('avg_decision_confidence', 0):.3f}")

        # æ˜¾ç¤ºå†³ç­–åˆ†æ
        decision_analysis = analysis.get("decision_analysis", {})
        print(f"\nğŸ§  å†³ç­–åˆ†æ:")
        print(f"    - å›°éš¾å†³ç­–æ¯”ä¾‹: {decision_analysis.get('difficult_decisions_ratio', 0):.2%}")
        print(f"    - å¹³å‡ç½®ä¿¡åº¦: {decision_analysis.get('avg_confidence', 0):.3f}")

        by_type = analysis.get("by_answer_type", {})
        print("\nğŸ“Š æŒ‰ç­”æ¡ˆæ¥æºç±»å‹åˆ†æ:")
        for type_name, metrics in by_type.items():
            print(f"    - {type_name.upper()} ç±»å‹:")
            print(f"      - æ€»æ ·æœ¬æ•°: {metrics.get('count', 0)}")
            print(f"      - æœ‰æ•ˆæ ·æœ¬æ•°: {metrics.get('valid_count', 0)}")
            print(f"      - ç²¾ç¡®åŒ¹é…ç‡: {metrics.get('exact_match_rate', 0):.2f}%")
            print(f"      - å¹³å‡F1åˆ†æ•°: {metrics.get('avg_f1_score', 0):.4f}")
        print("="*60)

# ===================================================================
# 7. ä¸»å‡½æ•°
# ===================================================================
def main():
    parser = argparse.ArgumentParser(description="æœ€ç»ˆç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model", type=str, default="SUFE-AIFLM-Lab/Fin-R1", help="è¦è¯„ä¼°çš„LLMåç§°")
    parser.add_argument("--data_path", type=str, required=True, help="è¯„ä¼°æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (jsonlæˆ–jsonæ ¼å¼)")
    parser.add_argument("--sample_size", type=int, default=None, help="éšæœºé‡‡æ ·çš„æ ·æœ¬æ•°é‡ï¼Œä¸æä¾›åˆ™è¯„ä¼°å…¨éƒ¨")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡ (cuda/cpu/auto)")
    args = parser.parse_args()

    # è®¾å¤‡é€‰æ‹©é€»è¾‘
    if args.device == "auto":
        device = "cuda:0" if torch.cuda.is_available() else "cpu"  # é»˜è®¤ä½¿ç”¨cuda:0
    elif args.device == "cuda":
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            device = "cpu"
        else:
            device = "cuda:0"  # é»˜è®¤ä½¿ç”¨cuda:0
            print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(0)}")
            gpu_id = 0
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3:.1f} GB")
    else:
        device = args.device

    # 1. åŠ è½½æ•°æ®
    try:
        from utils.data_loader import load_json_or_jsonl, sample_data
        eval_data = load_json_or_jsonl(args.data_path)
        
        # é‡‡æ ·
        if args.sample_size and args.sample_size < len(eval_data):
            eval_data = sample_data(eval_data, args.sample_size, 42)
            print(f"âœ… éšæœºé‡‡æ · {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
        else:
            print(f"âœ… åŠ è½½äº†å…¨éƒ¨ {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
            
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return

    # 2. åˆå§‹åŒ–å¹¶è¿è¡Œè¯„ä¼°å™¨
    evaluator = ComprehensiveEvaluator(model_name=args.model, device=device)
    analysis_results = evaluator.run_evaluation(eval_data)
    
    # 3. æ‰“å°å’Œä¿å­˜ç»“æœ
    evaluator.print_summary(analysis_results['analysis'])
    output_filename = f"final_evaluation_results_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(analysis_results, f, indent=2, ensure_ascii=False)
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")


if __name__ == "__main__":
    main()