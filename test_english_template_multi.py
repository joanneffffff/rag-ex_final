#!/usr/bin/env python3
"""
å¤šæ¨¡æ¿è‹±æ–‡æµ‹è¯•è„šæœ¬
å¿«é€Ÿæ¯”è¾ƒä¸åŒRAGæ¨¡æ¿çš„æ•ˆæœ
"""

import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥æµ‹è¯•å™¨
from test_english_template import LLMTemplateTester, load_sample_data

def load_template(template_path: str) -> Optional[str]:
    """åŠ è½½æ¨¡æ¿æ–‡ä»¶"""
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°æ¨¡æ¿æ–‡ä»¶: {template_path}")
        return None

def create_messages_from_template(template_content: str, context: str, question: str) -> List[Dict[str, str]]:
    """ä»æ¨¡æ¿åˆ›å»ºæ¶ˆæ¯åˆ—è¡¨"""
    if "===SYSTEM===" in template_content and "===USER===" in template_content:
        system_part = template_content.split("===SYSTEM===")[1].split("===USER===")[0].strip()
        user_part = template_content.split("===USER===")[1].strip()
        
        # æ›¿æ¢useréƒ¨åˆ†ä¸­çš„å ä½ç¬¦
        user_message = user_part.replace("{context}", context).replace("{question}", question)
        system_message = system_part
    else:
        # å¦‚æœæ¨¡æ¿æ ¼å¼ä¸æ­£ç¡®ï¼Œä½¿ç”¨æ•´ä¸ªå†…å®¹ä½œä¸ºsystemæ¶ˆæ¯
        system_message = template_content
        user_message = f"""Context:
{context}

Question:
{question}

A:"""
    
    return [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ]

def test_single_template(tester: LLMTemplateTester, template_name: str, template_content: str, 
                        sample_data: List[Dict[str, Any]], max_samples: int = 3) -> Dict[str, Any]:
    """æµ‹è¯•å•ä¸ªæ¨¡æ¿"""
    print(f"\nğŸ§ª æµ‹è¯•æ¨¡æ¿: {template_name}")
    print("="*50)
    
    results = []
    
    for i, sample in enumerate(sample_data[:max_samples]):
        print(f"\n--- æ ·æœ¬ {i+1} ---")
        print(f"é—®é¢˜: {sample['question']}")
        print(f"é¢„æœŸç­”æ¡ˆ: {sample['answer']}")
        
        # åˆ›å»ºæ¶ˆæ¯
        messages = create_messages_from_template(template_content, sample["context"], sample["question"])
        
        # ç”Ÿæˆå›ç­”
        generation_result = tester.generate_response(messages)
        
        # è¯„ä¼°
        evaluation = tester.evaluate_answer_quality(
            generated_answer=generation_result["cleaned_answer"],
            expected_answer=sample["answer"],
            context=sample["context"],
            question=sample["question"]
        )
        
        result = {
            "template_name": template_name,
            "sample_id": i + 1,
            "question": sample["question"],
            "expected_answer": sample["answer"],
            "generated_answer": generation_result["cleaned_answer"],
            "raw_answer": generation_result["generated_answer"],
            "quality_score": evaluation["quality_score"],
            "exact_match": evaluation["exact_match"],
            "semantic_similarity": evaluation["semantic_similarity"],
            "format_violations": evaluation["format_violations"],
            "generation_time": generation_result["generation_time"]
        }
        
        results.append(result)
        
        # æ‰“å°ç»“æœ
        print(f"âœ… ç”Ÿæˆç­”æ¡ˆ: {generation_result['cleaned_answer']}")
        print(f"ğŸ“Š è´¨é‡åˆ†æ•°: {evaluation['quality_score']:.3f}")
        print(f"ğŸ“Š ç²¾ç¡®åŒ¹é…: {evaluation['exact_match']}")
        if evaluation['format_violations']:
            print(f"âš ï¸ æ ¼å¼è¿è§„: {evaluation['format_violations']}")
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_quality = sum(r["quality_score"] for r in results) / len(results)
    avg_time = sum(r["generation_time"] for r in results) / len(results)
    exact_match_rate = sum(1 for r in results if r["exact_match"]) / len(results)
    format_violation_rate = sum(1 for r in results if r["format_violations"]) / len(results)
    
    print(f"\nğŸ“Š {template_name} æ€»ç»“:")
    print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality:.3f}")
    print(f"   ç²¾ç¡®åŒ¹é…ç‡: {exact_match_rate:.3f}")
    print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f}s")
    print(f"   æ ¼å¼è¿è§„ç‡: {format_violation_rate:.3f}")
    
    return {
        "template_name": template_name,
        "results": results,
        "summary": {
            "avg_quality": avg_quality,
            "avg_time": avg_time,
            "exact_match_rate": exact_match_rate,
            "format_violation_rate": format_violation_rate
        }
    }

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¤šæ¨¡æ¿è‹±æ–‡æµ‹è¯•å¼€å§‹")
    
    # å®šä¹‰è¦æµ‹è¯•çš„æ¨¡æ¿
    templates = {
        "Original": "data/prompt_templates/rag_english_template.txt",
        "Optimized": "data/prompt_templates/rag_english_template_optimized.txt", 
        "Minimal": "data/prompt_templates/rag_english_template_minimal.txt",
        "Focused": "data/prompt_templates/rag_english_template_focused.txt"
    }
    
    # åˆå§‹åŒ–æµ‹è¯•å™¨
    tester = LLMTemplateTester(
        model_name="SUFE-AIFLM-Lab/Fin-R1",
        device="auto"
    )
    
    try:
        tester.load_model()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åŠ è½½æ ·æœ¬æ•°æ®
    sample_data = load_sample_data()
    print(f"âœ… åŠ è½½äº† {len(sample_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    # æµ‹è¯•æ‰€æœ‰æ¨¡æ¿
    all_results = []
    
    for template_name, template_path in templates.items():
        template_content = load_template(template_path)
        if template_content:
            result = test_single_template(tester, template_name, template_content, sample_data, max_samples=3)
            all_results.append(result)
        else:
            print(f"âš ï¸ è·³è¿‡æ¨¡æ¿: {template_name}")
    
    # æ¯”è¾ƒç»“æœ
    print(f"\nğŸ† æ¨¡æ¿æ•ˆæœå¯¹æ¯”")
    print("="*60)
    
    for result in all_results:
        summary = result["summary"]
        print(f"\n{result['template_name']:12}:")
        print(f"   è´¨é‡åˆ†æ•°: {summary['avg_quality']:.3f}")
        print(f"   ç²¾ç¡®åŒ¹é…: {summary['exact_match_rate']:.3f}")
        print(f"   ç”Ÿæˆæ—¶é—´: {summary['avg_time']:.2f}s")
        print(f"   æ ¼å¼è¿è§„: {summary['format_violation_rate']:.3f}")
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡æ¿
    best_quality = max(all_results, key=lambda x: x["summary"]["avg_quality"])
    best_time = min(all_results, key=lambda x: x["summary"]["avg_time"])
    best_format = min(all_results, key=lambda x: x["summary"]["format_violation_rate"])
    
    print(f"\nğŸ¯ æœ€ä½³æ¨¡æ¿æ¨è:")
    print(f"   æœ€ä½³è´¨é‡: {best_quality['template_name']} ({best_quality['summary']['avg_quality']:.3f})")
    print(f"   æœ€å¿«é€Ÿåº¦: {best_time['template_name']} ({best_time['summary']['avg_time']:.2f}s)")
    print(f"   æœ€å°‘è¿è§„: {best_format['template_name']} ({best_format['summary']['format_violation_rate']:.3f})")
    
    # ä¿å­˜ç»“æœ
    output_file = "multi_template_test_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "results": all_results,
            "timestamp": time.time()
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("ğŸ‰ å¤šæ¨¡æ¿æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 