# LLMç”Ÿæˆå™¨ä¿®å¤æ€»ç»“

## ğŸ” é—®é¢˜è¯Šæ–­

### 1. æ ¸å¿ƒé—®é¢˜
ä»æ—¥å¿—åˆ†æå‘ç°ï¼ŒLLMç”Ÿæˆçš„ç­”æ¡ˆå®Œå…¨åç¦»äº†æŸ¥è¯¢å†…å®¹ï¼Œç”Ÿæˆäº†æ— å…³çš„è¡¨æ ¼å’Œé”™è¯¯çš„é—®é¢˜ã€‚

### 2. æ ¹æœ¬åŸå› åˆ†æ

#### 2.1 è¾“å…¥æˆªæ–­é—®é¢˜
**é—®é¢˜**: `LocalLLMGenerator.generate()`æ–¹æ³•ä¸­è®¾ç½®äº†`max_length=1024`ï¼Œå¯¼è‡´12445å­—ç¬¦çš„Promptè¢«æˆªæ–­
```python
inputs = self.tokenizer(
    text,
    return_tensors="pt",
    truncation=True,
    max_length=1024,  # âŒ è¿™é‡Œé™åˆ¶äº†è¾“å…¥é•¿åº¦ï¼
    padding="max_length"
)
```

**å½±å“**: 
- Promptè¢«æˆªæ–­åˆ°1024ä¸ªtoken
- æ¨¡å‹çœ‹åˆ°çš„ä¸æ˜¯å®Œæ•´çš„æŒ‡ä»¤
- å¯¼è‡´æ¨¡å‹è¯¯è§£ä»»åŠ¡è¦æ±‚

#### 2.2 Fin-R1èŠå¤©æ ¼å¼é—®é¢˜
**é—®é¢˜**: Fin-R1æ˜¯èŠå¤©æ¨¡å‹ï¼ŒæœŸæœ›ç‰¹å®šçš„èŠå¤©æ ¼å¼ï¼Œä½†æˆ‘ä»¬çš„Promptæ˜¯å•ä¸€å­—ç¬¦ä¸²æ ¼å¼

**å½±å“**:
- æ¨¡å‹å¯èƒ½æ— æ³•æ­£ç¡®ç†è§£ç³»ç»ŸæŒ‡ä»¤å’Œç”¨æˆ·è¾“å…¥çš„åŒºåˆ«
- å¯¼è‡´æ¨¡å‹ä½¿ç”¨é»˜è®¤è¡Œä¸ºè€Œä¸æ˜¯æˆ‘ä»¬çš„æŒ‡ä»¤

## ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ

### 1. ä¿®å¤è¾“å…¥æˆªæ–­
```python
# ä¿®å¤å‰
max_length=1024,  # é™åˆ¶è¾“å…¥é•¿åº¦
padding="max_length"

# ä¿®å¤å  
max_length=8192,  # å¢åŠ åˆ°8192ï¼Œé¿å…æˆªæ–­
padding=False,    # æ”¹ä¸ºFalseï¼Œé¿å…ä¸å¿…è¦çš„padding
add_special_tokens=True
```

### 2. æ·»åŠ èŠå¤©æ ¼å¼æ”¯æŒ
```python
# æ£€æŸ¥Fin-R1æ˜¯å¦æ”¯æŒèŠå¤©æ ¼å¼
if "Fin-R1" in self.model_name:
    print("Fin-R1 detected, using chat format...")
    # å°†Promptæ‹†åˆ†ä¸ºsystemå’Œuseréƒ¨åˆ†
    if "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆ" in text and "ã€å…¬å¸è´¢åŠ¡æŠ¥å‘Šç‰‡æ®µã€‘" in text:
        # æå–systeméƒ¨åˆ†ï¼ˆæŒ‡ä»¤éƒ¨åˆ†ï¼‰
        system_start = text.find("ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆ")
        context_start = text.find("ã€å…¬å¸è´¢åŠ¡æŠ¥å‘Šç‰‡æ®µã€‘")
        
        if system_start != -1 and context_start != -1:
            system_content = text[system_start:context_start].strip()
            user_content = text[context_start:].strip()
            
            # æ„é€ èŠå¤©æ ¼å¼
            chat_text = f"<|im_start|>system\n{system_content}<|im_end|>\n<|im_start|>user\n{user_content}<|im_end|>\n<|im_start|>assistant\n"
            text = chat_text
```

### 3. å¢åŠ è°ƒè¯•ä¿¡æ¯
```python
# æ£€æŸ¥è¾“å…¥é•¿åº¦
print(f"Input text length: {len(text)} characters")
print(f"Tokenized input length: {inputs['input_ids'].shape[1]} tokens")
```

## âœ… éªŒè¯ç»“æœ

### æµ‹è¯•1: ç®€å•èŠå¤©æ ¼å¼
```
èŠå¤©Prompt: <|im_start|>system
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆã€‚<|im_end|>
<|im_start|>user
å¾·èµ›ç”µæ± 2021å¹´åˆ©æ¶¦å¢é•¿çš„åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ<|im_end|>
<|im_start|>assistant

ç”Ÿæˆç»“æœ: è¦åˆ†æå¾·èµ›ç”µæ± 2021å¹´çš„åˆ©æ¶¦å¢é•¿åŸå› ï¼Œæˆ‘ä»¬éœ€è¦ä»å¤šä¸ªè§’åº¦è¿›è¡Œè€ƒå¯Ÿ...
```

### æµ‹è¯•2: å®Œæ•´Promptæ ¼å¼
```
å®Œæ•´Prompté•¿åº¦: 611 å­—ç¬¦
Fin-R1 detected, using chat format...
Converted to chat format, length: 690 characters
Tokenized input length: 365 tokens

ç”Ÿæˆç»“æœ: æ ¹æ®ç°æœ‰ä¿¡æ¯ï¼Œæ— æ³•æä¾›æ­¤é¡¹ä¿¡æ¯ã€‚
```

## ğŸ¯ å…³é”®å‘ç°

1. **èŠå¤©æ ¼å¼è½¬æ¢æˆåŠŸ**: ç¬¬äºŒä¸ªæµ‹è¯•æ˜¾ç¤º"Converted to chat format, length: 690 characters"
2. **æ¨¡å‹æ­£ç¡®å“åº”**: ç¬¬äºŒä¸ªæµ‹è¯•è¿”å›äº†"æ ¹æ®ç°æœ‰ä¿¡æ¯ï¼Œæ— æ³•æä¾›æ­¤é¡¹ä¿¡æ¯ã€‚"ï¼Œè¿™ç¬¦åˆæˆ‘ä»¬çš„PromptæŒ‡ä»¤
3. **è¾“å…¥é•¿åº¦æ­£å¸¸**: Tokenized input length: 365 tokensï¼Œè¿œä½äº8192é™åˆ¶

## ğŸ“‹ ä¿®å¤æ–‡ä»¶

1. **`xlm/components/generator/local_llm_generator.py`**
   - ä¿®å¤è¾“å…¥æˆªæ–­é—®é¢˜
   - æ·»åŠ Fin-R1èŠå¤©æ ¼å¼æ”¯æŒ
   - å¢åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯

2. **æµ‹è¯•è„šæœ¬**
   - `test_llm_fix.py`: å®Œæ•´æµ‹è¯•è„šæœ¬
   - `simple_llm_test.py`: ç®€å•æµ‹è¯•è„šæœ¬  
   - `test_chat_format.py`: èŠå¤©æ ¼å¼æµ‹è¯•è„šæœ¬

## ğŸš€ ä¸‹ä¸€æ­¥

1. **é‡å¯UIç³»ç»Ÿ**: åº”ç”¨ä¿®å¤åçš„LLMç”Ÿæˆå™¨
2. **æµ‹è¯•ä¸­æ–‡æŸ¥è¯¢**: éªŒè¯å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿçš„LLMç­”æ¡ˆç”Ÿæˆ
3. **ç›‘æ§æ—¥å¿—**: ç¡®è®¤Promptæ ¼å¼è½¬æ¢å’Œè¾“å…¥é•¿åº¦æ­£å¸¸
4. **ä¼˜åŒ–Prompt**: æ ¹æ®å®é™…æ•ˆæœè¿›ä¸€æ­¥ä¼˜åŒ–Promptæ¨¡æ¿

## ğŸ“Š é¢„æœŸæ•ˆæœ

ä¿®å¤åï¼ŒLLMç”Ÿæˆå™¨åº”è¯¥èƒ½å¤Ÿï¼š
- âœ… æ­£ç¡®å¤„ç†å®Œæ•´çš„Promptï¼ˆä¸è¢«æˆªæ–­ï¼‰
- âœ… ä½¿ç”¨æ­£ç¡®çš„èŠå¤©æ ¼å¼ä¸Fin-R1æ¨¡å‹äº¤äº’
- âœ… ç”Ÿæˆç¬¦åˆæŒ‡ä»¤çš„ç›¸å…³ç­”æ¡ˆ
- âœ… åœ¨ä¿¡æ¯ä¸è¶³æ—¶è¿”å›"æ ¹æ®ç°æœ‰ä¿¡æ¯ï¼Œæ— æ³•æä¾›æ­¤é¡¹ä¿¡æ¯ã€‚" 