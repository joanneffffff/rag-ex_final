#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€‰æ‹©é€‚åˆæ‰°åŠ¨å®éªŒçš„ä¸­æ–‡æ ·æœ¬
ä¸ºtrendã€yearã€termä¸‰ç§æ‰°åŠ¨ç±»å‹é€‰æ‹©æ€»å…±20ä¸ªæ ·æœ¬
"""

import json
import re
from typing import List, Dict, Set, Tuple
from collections import defaultdict

class PerturbationSampleSelector:
    def __init__(self):
        # å®šä¹‰ä¸‰ç§æ‰°åŠ¨ç±»å‹çš„å…³é”®è¯
        self.trend_keywords = {
            'ä¸Šå‡', 'ä¸‹é™', 'ä¸Šæ¶¨', 'ä¸‹è·Œ', 'å¢é•¿', 'å‡å°‘', 'æå‡', 'é™ä½', 'å¢åŠ ', 'å‡å°‘',
            'å¥½è½¬', 'æ¶åŒ–', 'æ”¹å–„', 'ç§¯æ', 'æ¶ˆæ', 'ç›ˆåˆ©', 'äºæŸ', 'æ‰©å¼ ', 'æ”¶ç¼©',
            'æŒç»­å¢é•¿', 'æŒç»­ä¸‹æ»‘', 'ç¨³æ­¥å¢é•¿', 'æ˜¾è‘—ä¸‹é™', 'å¼ºåŠ²', 'ç–²è½¯', 'é«˜äº', 'ä½äº',
            'ä¼˜äº', 'åŠ£äº', 'é¢†å…ˆ', 'è½å', 'å¢åŠ ç‡', 'å‡å°‘ç‡', 'ä¸Šå‡è¶‹åŠ¿', 'ä¸‹é™è¶‹åŠ¿',
            'å¢é•¿è¶‹åŠ¿', 'å‡å°‘è¶‹åŠ¿'
        }
        
        # å¹´ä»½å…³é”®è¯ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŠ¨æ€æ£€æµ‹ï¼Œä¸YearPerturberä¿æŒä¸€è‡´
        self.year_pattern = re.compile(r'\b(20\d{2})(?:å¹´|å¹´åº¦)?\b')
        
        self.term_keywords = {
            'å¸‚ç›ˆç‡', 'å‡€åˆ©æ¶¦', 'å¸‚å‡€ç‡', 'å¸‚é”€ç‡', 'è¥æ”¶', 'æ”¶å…¥', 'è¥ä¸šæ”¶å…¥', 'è¥ä¸šåˆ©æ¶¦',
            'è¥ä¸šåˆ©æ¶¦', 'æ€»èµ„äº§', 'å‡€èµ„äº§', 'è´Ÿå€º', 'èµ„äº§', 'åˆ©æ¶¦', 'æˆæœ¬', 'å¸‚å€¼', 'ä¼°å€¼',
            'è‚¡æ¯', 'åˆ†çº¢', 'é…è‚¡', 'å¢å‘', 'å›è´­', 'äº¤æ˜“é‡', 'æˆäº¤é¢', 'æ¢æ‰‹ç‡'
        }
    
    def load_dataset(self, file_path: str) -> List[Dict]:
        """åŠ è½½è¯„æµ‹æ•°æ®é›† - æ”¯æŒJSONLæ ¼å¼"""
        samples = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        samples.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                        continue
        return samples
    
    def extract_keywords(self, text: str, keyword_set: Set[str]) -> Set[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å…¨è¯åŒ¹é…"""
        found_keywords = set()
        for keyword in keyword_set:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œå…¨è¯åŒ¹é…ï¼Œä¸æ‰°åŠ¨å™¨ä¿æŒä¸€è‡´
            pattern = r'\b' + re.escape(keyword) + r'\b'
            if re.search(pattern, text, re.IGNORECASE):
                found_keywords.add(keyword)
        return found_keywords
    
    def analyze_sample(self, sample: Dict) -> Dict:
        """åˆ†æå•ä¸ªæ ·æœ¬çš„å…³é”®è¯åˆ†å¸ƒ"""
        summary = sample.get('summary', '')
        content = sample.get('content', '')
        generated_question = sample.get('generated_question', '')
        
        # ä¸»è¦å…³æ³¨contextå­—æ®µï¼ˆsummaryå’Œcontentï¼‰ï¼Œå› ä¸ºè¿™æ˜¯æ‰°åŠ¨å™¨ä½œç”¨çš„å¯¹è±¡
        context_text = f"{summary} {content}"
        question_text = generated_question
        
        # åˆ†åˆ«åˆ†æcontextå’Œquestionä¸­çš„å…³é”®è¯
        context_trend_found = self.extract_keywords(context_text, self.trend_keywords)
        context_year_found = set()
        context_year_matches = self.year_pattern.findall(context_text)
        for match in context_year_matches:
            context_year_found.add(match)
        context_term_found = self.extract_keywords(context_text, self.term_keywords)
        
        question_trend_found = self.extract_keywords(question_text, self.trend_keywords)
        question_year_found = set()
        question_year_matches = self.year_pattern.findall(question_text)
        for match in question_year_matches:
            question_year_found.add(match)
        question_term_found = self.extract_keywords(question_text, self.term_keywords)
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯ï¼ˆä½†ä¸»è¦æƒé‡ç»™contextï¼‰
        trend_found = context_trend_found | question_trend_found
        year_found = context_year_found | question_year_found
        term_found = context_term_found | question_term_found
        
        return {
            'sample_id': sample.get('id', 'unknown'),
            'summary': summary,
            'content': content,
            'generated_question': generated_question,
            'trend_keywords': trend_found,
            'year_keywords': year_found,
            'term_keywords': term_found,
            'context_trend_score': len(context_trend_found),
            'context_year_score': len(context_year_found),
            'context_term_score': len(context_term_found),
            'question_trend_score': len(question_trend_found),
            'question_year_score': len(question_year_found),
            'question_term_score': len(question_term_found),
            'trend_score': len(trend_found),
            'year_score': len(year_found),
            'term_score': len(term_found),
            'total_score': len(trend_found) + len(year_found) + len(term_found),
            'context_score': len(context_trend_found) + len(context_year_found) + len(context_term_found)
        }
    
    def select_samples(self, samples: List[Dict], target_count: int = 20) -> Dict[str, List[Dict]]:
        """é€‰æ‹©é€‚åˆçš„æ ·æœ¬ - ä½¿ç”¨å¤šæ ·æ€§é€‰æ‹©ç­–ç•¥"""
        analyzed_samples = [self.analyze_sample(sample) for sample in samples]
        
        # ç¬¬ä¸€è½®ï¼šæŒ‰context_scoreæ’åºï¼Œä¼˜å…ˆé€‰æ‹©contextä¸­æœ‰å…³é”®è¯çš„æ ·æœ¬
        context_samples = [s for s in analyzed_samples if s['context_score'] > 0]
        context_samples.sort(key=lambda x: x['context_score'], reverse=True)
        
        # ç¬¬äºŒè½®ï¼šå¤šæ ·æ€§é€‰æ‹©ï¼Œç¡®ä¿è¦†ç›–ä¸åŒçš„é—®é¢˜ç±»å‹å’Œä¸Šä¸‹æ–‡ç±»å‹
        selected_samples = []
        remaining_samples = context_samples.copy()
        
        # ç¡®ä¿è¦†ç›–ä¸åŒçš„é—®é¢˜ç±»å‹å’Œä¸Šä¸‹æ–‡ç±»å‹
        type_coverage = {
            'question_types': set(),
            'context_types': set()
        }
        
        # é¦–å…ˆé€‰æ‹©context_scoreæœ€é«˜çš„æ ·æœ¬
        for _ in range(min(target_count, len(context_samples))):
            if not remaining_samples:
                break
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¤šæ ·æ€§åˆ†æ•°
            for sample in remaining_samples:
                # ç®€å•çš„å¤šæ ·æ€§è®¡ç®—ï¼šä¸å·²é€‰æ ·æœ¬çš„å·®å¼‚
                diversity_score = 0.0
                for selected in selected_samples:
                    # é—®é¢˜ç±»å‹å·®å¼‚
                    if sample.get('question_type') != selected.get('question_type'):
                        diversity_score += 1.0
                    # ä¸Šä¸‹æ–‡ç±»å‹å·®å¼‚
                    if sample.get('context_type') != selected.get('context_type'):
                        diversity_score += 1.0
                
                sample['diversity_score'] = diversity_score
            
            # é€‰æ‹©æœ€ä½³æ ·æœ¬ï¼ˆå¹³è¡¡context_scoreå’Œå¤šæ ·æ€§ï¼‰
            best_sample = max(remaining_samples, key=lambda s: s['context_score'] + s.get('diversity_score', 0))
            
            selected_samples.append(best_sample)
            remaining_samples.remove(best_sample)
        
        # å¦‚æœè¿˜ä¸å¤Ÿï¼Œä»å‰©ä½™æ ·æœ¬ä¸­è¡¥å……
        if len(selected_samples) < target_count:
            remaining_all = [s for s in analyzed_samples if s not in selected_samples]
            remaining_all.sort(key=lambda x: x['total_score'], reverse=True)
            selected_samples.extend(remaining_all[:target_count - len(selected_samples)])
        
        # æŒ‰æ‰°åŠ¨ç±»å‹åˆ†ç±»
        categorized_samples = {
            'trend': [],
            'year': [],
            'term': []
        }
        
        for sample in selected_samples:
            # æ¯ä¸ªæ ·æœ¬å¯ä»¥ç”¨äºå¤šç§æ‰°åŠ¨ç±»å‹
            if sample['trend_score'] > 0:
                categorized_samples['trend'].append(sample)
            if sample['year_score'] > 0:
                categorized_samples['year'].append(sample)
            if sample['term_score'] > 0:
                categorized_samples['term'].append(sample)
        
        return categorized_samples
    
    def print_analysis(self, categorized_samples: Dict[str, List[Dict]]):
        """æ‰“å°åˆ†æç»“æœ"""
        print("=" * 80)
        print("ğŸ“Š æ‰°åŠ¨æ ·æœ¬é€‰æ‹©åˆ†æç»“æœ")
        print("=" * 80)
        
        for perturber_type, samples in categorized_samples.items():
            print(f"\nğŸ” {perturber_type.upper()} æ‰°åŠ¨å™¨æ ·æœ¬ ({len(samples)}ä¸ª):")
            print("-" * 60)
            
            for i, sample in enumerate(samples, 1):
                print(f"{i:2d}. æ ·æœ¬ID: {sample['sample_id']}")
                print(f"    è¶‹åŠ¿å…³é”®è¯: {sample['trend_keywords']}")
                print(f"    å¹´ä»½å…³é”®è¯: {sample['year_keywords']}")
                print(f"    æœ¯è¯­å…³é”®è¯: {sample['term_keywords']}")
                print(f"    æ€»åˆ†: {sample['total_score']}")
                print(f"    é—®é¢˜: {sample['generated_question'][:100]}...")
                print()
        
        # ç»Ÿè®¡ä¿¡æ¯
        print("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        print("-" * 40)
        total_samples = len(set([s['sample_id'] for samples in categorized_samples.values() for s in samples]))
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        for perturber_type, samples in categorized_samples.items():
            print(f"{perturber_type} æ‰°åŠ¨å™¨å¯ç”¨æ ·æœ¬: {len(samples)}")
    
    def save_selected_samples(self, categorized_samples: Dict[str, List[Dict]], output_file: str):
        """ä¿å­˜é€‰ä¸­çš„æ ·æœ¬"""
        # å»é‡å¹¶ä¿å­˜
        unique_samples = {}
        for samples in categorized_samples.values():
            for sample in samples:
                # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ–
                sample_copy = sample.copy()
                sample_copy['trend_keywords'] = list(sample['trend_keywords'])
                sample_copy['year_keywords'] = list(sample['year_keywords'])
                sample_copy['term_keywords'] = list(sample['term_keywords'])
                unique_samples[sample['sample_id']] = sample_copy
        
        selected_data = {
            'total_samples': len(unique_samples),
            'categorized_samples': {
                k: [s.copy() for s in v] for k, v in categorized_samples.items()
            },
            'unique_samples': list(unique_samples.values())
        }
        
        # è½¬æ¢æ‰€æœ‰setä¸ºlist
        for perturber_type in selected_data['categorized_samples']:
            for sample in selected_data['categorized_samples'][perturber_type]:
                sample['trend_keywords'] = list(sample['trend_keywords'])
                sample['year_keywords'] = list(sample['year_keywords'])
                sample['term_keywords'] = list(sample['term_keywords'])
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(selected_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… é€‰ä¸­æ ·æœ¬å·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    selector = PerturbationSampleSelector()
    
    # åŠ è½½æ•°æ®é›†
    print("ğŸ“‚ åŠ è½½è¯„æµ‹æ•°æ®é›†...")
    samples = selector.load_dataset('data/alphafin/alphafin_eval_samples_updated.jsonl')
    print(f"âœ… åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
    
    # é€‰æ‹©æ ·æœ¬
    print("\nğŸ” åˆ†ææ ·æœ¬å…³é”®è¯åˆ†å¸ƒ...")
    categorized_samples = selector.select_samples(samples, target_count=20)
    
    # æ‰“å°åˆ†æç»“æœ
    selector.print_analysis(categorized_samples)
    
    # ä¿å­˜ç»“æœ
    output_file = 'selected_perturbation_samples.json'
    selector.save_selected_samples(categorized_samples, output_file)
    
    print(f"\nğŸ¯ å®Œæˆï¼å·²ä¸ºä¸‰ç§æ‰°åŠ¨ç±»å‹é€‰æ‹©äº†20ä¸ªæ ·æœ¬")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶: {output_file}")

if __name__ == "__main__":
    main() 