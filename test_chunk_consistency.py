#!/usr/bin/env python3
"""æµ‹è¯•è®­ç»ƒæ•°æ®å’Œè¯„ä¼°æ•°æ®çš„chunkä¸€è‡´æ€§"""

import json
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

def test_chunk_consistency():
    """æµ‹è¯•chunkä¸€è‡´æ€§"""
    print("=== æµ‹è¯•è®­ç»ƒæ•°æ®å’Œè¯„ä¼°æ•°æ®çš„chunkä¸€è‡´æ€§ ===")
    
    try:
        from xlm.utils.optimized_data_loader import OptimizedDataLoader
        
        print("1. åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆä¸åŒ…å«è¯„ä¼°æ•°æ®ï¼‰...")
        train_loader = OptimizedDataLoader(
            data_dir="data",
            max_samples=50,  # åªåŠ è½½50ä¸ªæ ·æœ¬ç”¨äºå¯¹æ¯”
            chinese_document_level=True,
            english_chunk_level=True,
            include_eval_data=False  # ä¸åŒ…å«è¯„ä¼°æ•°æ®
        )
        
        train_chinese = train_loader.chinese_docs
        train_english = train_loader.english_docs
        
        print(f"   âœ… è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"      ä¸­æ–‡chunks: {len(train_chinese)}")
        print(f"      è‹±æ–‡chunks: {len(train_english)}")
        
        print("\n2. åŠ è½½åŒ…å«è¯„ä¼°æ•°æ®çš„çŸ¥è¯†åº“...")
        eval_loader = OptimizedDataLoader(
            data_dir="data",
            max_samples=50,  # åªåŠ è½½50ä¸ªæ ·æœ¬ç”¨äºå¯¹æ¯”
            chinese_document_level=True,
            english_chunk_level=True,
            include_eval_data=True  # åŒ…å«è¯„ä¼°æ•°æ®
        )
        
        eval_chinese = eval_loader.chinese_docs
        eval_english = eval_loader.english_docs
        
        print(f"   âœ… åŒ…å«è¯„ä¼°æ•°æ®çš„çŸ¥è¯†åº“åŠ è½½æˆåŠŸ:")
        print(f"      ä¸­æ–‡chunks: {len(eval_chinese)}")
        print(f"      è‹±æ–‡chunks: {len(eval_english)}")
        
        print("\n3. åˆ†æchunkä¸€è‡´æ€§...")
        
        # åˆ†æä¸­æ–‡chunké•¿åº¦åˆ†å¸ƒ
        print(f"\n--- ä¸­æ–‡chunké•¿åº¦åˆ†æ ---")
        train_chinese_lengths = [len(doc.content) for doc in train_chinese]
        eval_chinese_lengths = [len(doc.content) for doc in eval_chinese]
        
        print(f"è®­ç»ƒæ•°æ®ä¸­æ–‡chunké•¿åº¦:")
        print(f"  å¹³å‡é•¿åº¦: {sum(train_chinese_lengths)/len(train_chinese_lengths):.0f} å­—ç¬¦")
        print(f"  æœ€å°é•¿åº¦: {min(train_chinese_lengths)} å­—ç¬¦")
        print(f"  æœ€å¤§é•¿åº¦: {max(train_chinese_lengths)} å­—ç¬¦")
        print(f"  é•¿åº¦åˆ†å¸ƒ: {len([l for l in train_chinese_lengths if l <= 1000])} çŸ­æ–‡æ¡£, {len([l for l in train_chinese_lengths if 1000 < l <= 5000])} ä¸­æ–‡æ¡£, {len([l for l in train_chinese_lengths if l > 5000])} é•¿æ–‡æ¡£")
        
        print(f"\nè¯„ä¼°æ•°æ®ä¸­æ–‡chunké•¿åº¦:")
        print(f"  å¹³å‡é•¿åº¦: {sum(eval_chinese_lengths)/len(eval_chinese_lengths):.0f} å­—ç¬¦")
        print(f"  æœ€å°é•¿åº¦: {min(eval_chinese_lengths)} å­—ç¬¦")
        print(f"  æœ€å¤§é•¿åº¦: {max(eval_chinese_lengths)} å­—ç¬¦")
        print(f"  é•¿åº¦åˆ†å¸ƒ: {len([l for l in eval_chinese_lengths if l <= 1000])} çŸ­æ–‡æ¡£, {len([l for l in eval_chinese_lengths if 1000 < l <= 5000])} ä¸­æ–‡æ¡£, {len([l for l in eval_chinese_lengths if l > 5000])} é•¿æ–‡æ¡£")
        
        # åˆ†æè‹±æ–‡chunké•¿åº¦åˆ†å¸ƒ
        print(f"\n--- è‹±æ–‡chunké•¿åº¦åˆ†æ ---")
        train_english_lengths = [len(doc.content) for doc in train_english]
        eval_english_lengths = [len(doc.content) for doc in eval_english]
        
        print(f"è®­ç»ƒæ•°æ®è‹±æ–‡chunké•¿åº¦:")
        print(f"  å¹³å‡é•¿åº¦: {sum(train_english_lengths)/len(train_english_lengths):.0f} å­—ç¬¦")
        print(f"  æœ€å°é•¿åº¦: {min(train_english_lengths)} å­—ç¬¦")
        print(f"  æœ€å¤§é•¿åº¦: {max(train_english_lengths)} å­—ç¬¦")
        
        print(f"\nè¯„ä¼°æ•°æ®è‹±æ–‡chunké•¿åº¦:")
        print(f"  å¹³å‡é•¿åº¦: {sum(eval_english_lengths)/len(eval_english_lengths):.0f} å­—ç¬¦")
        print(f"  æœ€å°é•¿åº¦: {min(eval_english_lengths)} å­—ç¬¦")
        print(f"  æœ€å¤§é•¿åº¦: {max(eval_english_lengths)} å­—ç¬¦")
        
        # æ£€æŸ¥è¯„ä¼°æ•°æ®æ˜¯å¦éµå¾ªæ–‡æ¡£çº§åˆ«chunking
        print(f"\n--- è¯„ä¼°æ•°æ®chunkç­–ç•¥éªŒè¯ ---")
        
        # æ£€æŸ¥ä¸­æ–‡è¯„ä¼°æ•°æ®æ˜¯å¦ä½¿ç”¨æ–‡æ¡£çº§åˆ«
        eval_chinese_sources = set()
        for doc in eval_chinese:
            if 'eval' in doc.metadata.source:
                eval_chinese_sources.add(doc.metadata.source)
        
        print(f"ä¸­æ–‡è¯„ä¼°æ•°æ®æ¥æº: {eval_chinese_sources}")
        
        # æ£€æŸ¥è‹±æ–‡è¯„ä¼°æ•°æ®
        eval_english_sources = set()
        for doc in eval_english:
            if 'eval' in doc.metadata.source:
                eval_english_sources.add(doc.metadata.source)
        
        print(f"è‹±æ–‡è¯„ä¼°æ•°æ®æ¥æº: {eval_english_sources}")
        
        # æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹
        print(f"\n--- ç¤ºä¾‹å¯¹æ¯” ---")
        
        print(f"è®­ç»ƒæ•°æ®ä¸­æ–‡ç¤ºä¾‹:")
        if train_chinese:
            sample_doc = train_chinese[0]
            print(f"  æ¥æº: {sample_doc.metadata.source}")
            print(f"  é•¿åº¦: {len(sample_doc.content)} å­—ç¬¦")
            print(f"  å†…å®¹é¢„è§ˆ: {sample_doc.content[:200]}...")
        
        print(f"\nè¯„ä¼°æ•°æ®ä¸­æ–‡ç¤ºä¾‹:")
        eval_chinese_docs = [doc for doc in eval_chinese if 'eval' in doc.metadata.source]
        if eval_chinese_docs:
            sample_eval_doc = eval_chinese_docs[0]
            print(f"  æ¥æº: {sample_eval_doc.metadata.source}")
            print(f"  é•¿åº¦: {len(sample_eval_doc.content)} å­—ç¬¦")
            print(f"  å†…å®¹é¢„è§ˆ: {sample_eval_doc.content[:200]}...")
        
        # ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ
        print(f"\n=== ä¸€è‡´æ€§æ£€æŸ¥ç»“æœ ===")
        
        # æ£€æŸ¥ä¸­æ–‡æ˜¯å¦éƒ½ä½¿ç”¨æ–‡æ¡£çº§åˆ«
        chinese_consistency = True
        if train_chinese_lengths and eval_chinese_lengths:
            train_avg = sum(train_chinese_lengths) / len(train_chinese_lengths)
            eval_avg = sum(eval_chinese_lengths) / len(eval_chinese_lengths)
            # å¹³å‡é•¿åº¦åº”è¯¥ç›¸è¿‘ï¼ˆå…è®¸20%çš„å·®å¼‚ï¼‰
            if abs(train_avg - eval_avg) / train_avg > 0.2:
                chinese_consistency = False
        
        print(f"ä¸­æ–‡chunkä¸€è‡´æ€§: {'âœ… ä¸€è‡´' if chinese_consistency else 'âŒ ä¸ä¸€è‡´'}")
        
        # æ£€æŸ¥è‹±æ–‡æ˜¯å¦éƒ½ä½¿ç”¨chunkçº§åˆ«
        english_consistency = True
        if train_english_lengths and eval_english_lengths:
            train_avg = sum(train_english_lengths) / len(train_english_lengths)
            eval_avg = sum(eval_english_lengths) / len(eval_english_lengths)
            # å¹³å‡é•¿åº¦åº”è¯¥ç›¸è¿‘ï¼ˆå…è®¸20%çš„å·®å¼‚ï¼‰
            if abs(train_avg - eval_avg) / train_avg > 0.2:
                english_consistency = False
        
        print(f"è‹±æ–‡chunkä¸€è‡´æ€§: {'âœ… ä¸€è‡´' if english_consistency else 'âŒ ä¸ä¸€è‡´'}")
        
        overall_consistency = chinese_consistency and english_consistency
        print(f"\næ€»ä½“ä¸€è‡´æ€§: {'âœ… è®­ç»ƒæ•°æ®å’Œè¯„ä¼°æ•°æ®ä½¿ç”¨ç›¸åŒçš„chunké€»è¾‘' if overall_consistency else 'âŒ å­˜åœ¨ä¸ä¸€è‡´'}")
        
        if overall_consistency:
            print(f"\nğŸ‰ éªŒè¯é€šè¿‡ï¼è¯„ä¼°æ•°æ®æ­£ç¡®éµå¾ªäº†ä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„chunkç­–ç•¥:")
            print(f"   âœ… ä¸­æ–‡æ•°æ®ä½¿ç”¨æ–‡æ¡£çº§åˆ«chunking")
            print(f"   âœ… è‹±æ–‡æ•°æ®ä½¿ç”¨chunkçº§åˆ«å¤„ç†")
            print(f"   âœ… è¯„ä¼°æ•°æ®ä¸è®­ç»ƒæ•°æ®é•¿åº¦åˆ†å¸ƒä¸€è‡´")
        
        return overall_consistency
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_chunk_consistency() 