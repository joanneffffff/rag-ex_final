#!/usr/bin/env python3
"""
æµ‹è¯• Prompt ä¼˜åŒ–æ•ˆæœ
å›ºå®šæµ‹è¯•é—®é¢˜ï¼šå¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´åˆ©æ¶¦æŒç»­å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
"""

import sys
import os
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent))

def test_prompt_optimization():
    """æµ‹è¯• Prompt ä¼˜åŒ–æ•ˆæœ"""
    
    print("=== Generator LLM Prompt ä¼˜åŒ–æµ‹è¯• ===")
    print("æµ‹è¯•é—®é¢˜ï¼šå¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´åˆ©æ¶¦æŒç»­å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ")
    print("=" * 60)
    
    try:
        # å¯¼å…¥å¿…è¦çš„æ¨¡å—
        from xlm.components.generator.local_llm_generator import LocalLLMGenerator
        from xlm.components.prompt_templates.template_loader import template_loader
        
        print("1. åˆå§‹åŒ– LLM ç”Ÿæˆå™¨...")
        generator = LocalLLMGenerator()
        print(f"âœ… ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ: {generator.model_name}")
        print(f"âœ… è®¾å¤‡: {generator.device}")
        
        # æ„é€ æµ‹è¯•æ•°æ®
        print("\n2. æ„é€ æµ‹è¯•æ•°æ®...")
        test_context = """
        å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´ä¸šç»©é¢„å‘Šæ˜¾ç¤ºï¼Œå…¬å¸é¢„è®¡å®ç°å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦ä¸º6.5äº¿å…ƒè‡³7.5äº¿å…ƒï¼Œ
        åŒæ¯”å¢é•¿11.02%è‡³28.23%ã€‚ä¸šç»©å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ï¼š
        1. iPhone 12 Pro Maxç­‰é«˜ç«¯äº§å“éœ€æ±‚å¼ºåŠ²ï¼Œå¸¦åŠ¨å…¬å¸ç”µæ± ä¸šåŠ¡å¢é•¿
        2. æ–°äº§å“ç›ˆåˆ©èƒ½åŠ›æå‡ï¼Œæ¯›åˆ©ç‡æ”¹å–„
        3. Aå®¢æˆ·ä¸šåŠ¡æŒç»­æˆé•¿ï¼Œéæ‰‹æœºä¸šåŠ¡ç¨³æ­¥å¢é•¿
        4. å¹¶è¡¨æ¯”ä¾‹å¢åŠ ï¼Œè´¡çŒ®ä¸šç»©å¢é‡
        """
        
        test_summary = "å¾·èµ›ç”µæ± 2021å¹´ä¸šç»©å¢é•¿ä¸»è¦å—ç›ŠäºiPhone 12 Pro Maxéœ€æ±‚å¼ºåŠ²å’Œæ–°å“ç›ˆåˆ©èƒ½åŠ›æå‡ã€‚"
        
        test_query = "å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´åˆ©æ¶¦æŒç»­å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"
        
        print(f"âœ… ä¸Šä¸‹æ–‡é•¿åº¦: {len(test_context)} å­—ç¬¦")
        print(f"âœ… æ‘˜è¦é•¿åº¦: {len(test_summary)} å­—ç¬¦")
        print(f"âœ… é—®é¢˜é•¿åº¦: {len(test_query)} å­—ç¬¦")
        
        # ä½¿ç”¨ä¼˜åŒ–åçš„æ¨¡æ¿ç”Ÿæˆ Prompt
        print("\n3. ç”Ÿæˆä¼˜åŒ–åçš„ Prompt...")
        prompt = template_loader.format_template(
            "multi_stage_chinese_template",
            context=test_context,
            query=test_query,
            summary=test_summary
        )
        
        if prompt is None:
            print("âŒ Prompt æ¨¡æ¿åŠ è½½å¤±è´¥")
            return False
            
        print(f"âœ… Prompt ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {len(prompt)} å­—ç¬¦")
        print(f"âœ… Prompt é¢„è§ˆ:\n{prompt[:300]}...")
        
        # æµ‹è¯•æ ¼å¼è½¬æ¢
        print("\n4. æµ‹è¯•æ ¼å¼è½¬æ¢...")
        if "Fin-R1" in generator.model_name:
            print("ğŸ” æ£€æµ‹åˆ° Fin-R1 æ¨¡å‹ï¼Œæµ‹è¯•æ ¼å¼è½¬æ¢...")
            
            # æµ‹è¯• JSON æ ¼å¼è½¬æ¢
            json_chat = generator.convert_to_json_chat_format(prompt)
            print(f"âœ… JSON æ ¼å¼è½¬æ¢å®Œæˆï¼Œé•¿åº¦: {len(json_chat)} å­—ç¬¦")
            
            # æµ‹è¯• Fin-R1 æ ¼å¼è½¬æ¢
            fin_r1_format = generator.convert_json_to_fin_r1_format(json_chat)
            print(f"âœ… Fin-R1 æ ¼å¼è½¬æ¢å®Œæˆï¼Œé•¿åº¦: {len(fin_r1_format)} å­—ç¬¦")
            
            # æ˜¾ç¤ºè½¬æ¢åçš„æ ¼å¼é¢„è§ˆ
            print("\nğŸ“‹ è½¬æ¢åæ ¼å¼é¢„è§ˆ:")
            print("-" * 50)
            print(fin_r1_format[:500] + "..." if len(fin_r1_format) > 500 else fin_r1_format)
            print("-" * 50)
        
        # ç”Ÿæˆç­”æ¡ˆ
        print("\n5. ç”Ÿæˆç­”æ¡ˆ...")
        print("ğŸš€ å¼€å§‹ç”Ÿæˆï¼Œè¯·ç¨å€™...")
        
        responses = generator.generate([prompt])
        answer = responses[0] if responses else "ç”Ÿæˆå¤±è´¥"
        
        print("\n" + "=" * 60)
        print("ğŸ“ ç”Ÿæˆç»“æœ")
        print("=" * 60)
        print(f"é—®é¢˜: {test_query}")
        print(f"ç­”æ¡ˆ: {answer}")
        print("=" * 60)
        
        # åˆ†æç»“æœ
        print("\n6. ç»“æœåˆ†æ...")
        
        # æ£€æŸ¥ç­”æ¡ˆè´¨é‡
        quality_indicators = {
            "ç®€æ´æ€§": len(answer) <= 200,  # æ§åˆ¶åœ¨200å­—ç¬¦å†…
            "å‡†ç¡®æ€§": "å¾·èµ›ç”µæ± " in answer or "iPhone" in answer or "éœ€æ±‚" in answer,
            "çº¯ç²¹æ€§": not any(marker in answer for marker in ["ã€", "ã€‘", "å›ç­”ï¼š", "Answer:"]),
            "å®Œæ•´æ€§": answer.strip().endswith(("ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"))
        }
        
        print("ğŸ“Š è´¨é‡æŒ‡æ ‡:")
        for indicator, passed in quality_indicators.items():
            status = "âœ…" if passed else "âŒ"
            print(f"   {status} {indicator}: {'é€šè¿‡' if passed else 'éœ€æ”¹è¿›'}")
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        score = sum(quality_indicators.values()) / len(quality_indicators) * 100
        print(f"\nğŸ¯ æ€»ä½“è¯„åˆ†: {score:.1f}%")
        
        if score >= 75:
            print("ğŸ‰ Prompt ä¼˜åŒ–æ•ˆæœè‰¯å¥½ï¼")
        elif score >= 50:
            print("âš ï¸ Prompt ä¼˜åŒ–æ•ˆæœä¸€èˆ¬ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´")
        else:
            print("âŒ Prompt ä¼˜åŒ–æ•ˆæœä¸ä½³ï¼Œéœ€è¦é‡æ–°è®¾è®¡")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_prompt_optimization() 