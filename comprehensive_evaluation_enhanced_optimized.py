#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬ - è§£å†³ç”Ÿæˆè¶…æ—¶é—®é¢˜
"""

import json
import os
import time
import re
import numpy as np
from typing import List, Dict, Any, Optional
from collections import Counter
from difflib import SequenceMatcher
from tqdm import tqdm
import torch
import signal
from contextlib import contextmanager

# å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGenerator
try:
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥LocalLLMGeneratorï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
    USE_RAG_GENERATOR = False

class TimeoutException(Exception):
    pass

@contextmanager
def timeout(seconds):
    """è¶…æ—¶æ§åˆ¶ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    def signal_handler(signum, frame):
        raise TimeoutException(f"æ“ä½œè¶…æ—¶ ({seconds}ç§’)")
    
    # è®¾ç½®ä¿¡å·å¤„ç†å™¨
    old_handler = signal.signal(signal.SIGALRM, signal_handler)
    signal.alarm(seconds)
    
    try:
        yield
    finally:
        signal.alarm(0)
        signal.signal(signal.SIGALRM, old_handler)

# ä½ è„šæœ¬ä¸­çš„è¿™ä¸ªå‡½æ•°ç°åœ¨æ˜¯å®Œç¾çš„ï¼Œå› ä¸ºå®ƒåšçš„å°±æ˜¯è¿™ä»¶äº‹
def extract_final_answer(raw_output: str) -> str:
    """ä»æ¨¡å‹çš„åŸå§‹è¾“å‡ºä¸­æå–<answer>æ ‡ç­¾å†…çš„å†…å®¹"""
    match = re.search(r'<answer>(.*?)</answer>', raw_output, re.DOTALL)
    if match:
        # å®Œæ•´åœ°è¿”å›æ ‡ç­¾å†…çš„æ‰€æœ‰å†…å®¹
        return match.group(1).strip()
    # å¦‚æœæ²¡æ‰¾åˆ°æ ‡ç­¾ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²æˆ–æ•´ä¸ªè¾“å‡ºä½œä¸ºå¤‡ç”¨
    return ""

def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—ä¸¤ä¸ªå­—ç¬¦ä¸²ä¹‹é—´åŸºäºè¯è¯­é‡å çš„F1åˆ†æ•°"""
    def normalize(text):
        return re.sub(r'[^\w\s]', '', text.lower()).split()

    prediction_tokens = normalize(prediction)
    ground_truth_tokens = normalize(ground_truth)

    if not ground_truth_tokens:
        return 1.0 if not prediction_tokens else 0.0
    if not prediction_tokens:
        return 0.0

    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())

    if num_same == 0:
        return 0.0

    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    
    return f1

class OptimizedLLMTemplateTester:
    """ä¼˜åŒ–ç‰ˆLLMæ¨¡æ¿æµ‹è¯•å™¨ - è§£å†³è¶…æ—¶é—®é¢˜"""
    
    def __init__(self, model_name: str = "SUFE-AIFLM-Lab/Fin-R1", device: str = "auto"):
        self.model_name = model_name
        self.device = self._setup_device(device)
        self.llm_generator = None
        self.max_length = 8192  # å¢åŠ æœ€å¤§é•¿åº¦
        self.max_new_tokens = 2048  # å¢åŠ tokenæ•°é‡ä»¥ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ
        self.timeout_seconds = 120   # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°2åˆ†é’Ÿ
        
    def _setup_device(self, device: str) -> str:
        """è®¾ç½®è®¾å¤‡"""
        if device == "auto":
            if torch.cuda.is_available():
                return "cuda"
            else:
                return "cpu"
        return device
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹"""
        global USE_RAG_GENERATOR
        if USE_RAG_GENERATOR:
            try:
                self.llm_generator = LocalLLMGenerator(
                    model_name=self.model_name,
                    device=self.device
                )
                print("âœ… RAGç”Ÿæˆå™¨åŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ RAGç”Ÿæˆå™¨åŠ è½½å¤±è´¥: {e}")
                USE_RAG_GENERATOR = False
        else:
            print("âš ï¸ ä½¿ç”¨å¤‡ç”¨transformersæ–¹æ¡ˆ")
    
    def _convert_messages_to_text(self, messages: List[Dict[str, str]]) -> str:
        """å°†messagesè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"""
        text = ""
        for message in messages:
            role = message["role"]
            content = message["content"]
            if role == "system":
                text += f"System: {content}\n\n"
            elif role == "user":
                text += f"User: {content}\n\n"
            elif role == "assistant":
                text += f"Assistant: {content}\n\n"
        return text.strip()
    
    def generate_response(self, messages: List[Dict[str, str]]) -> Dict[str, Any]:
        """ç”Ÿæˆå›ç­” - å¸¦è¶…æ—¶æ§åˆ¶"""
        start_time = time.time()
        
        if USE_RAG_GENERATOR and self.llm_generator:
            try:
                # å°†messagesè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
                prompt_text = self._convert_messages_to_text(messages)
                
                # ä¼˜åŒ–çš„ç”Ÿæˆå‚æ•°
                generation_params = {
                    "max_new_tokens": self.max_new_tokens,
                    "do_sample": True,  # å¯ç”¨é‡‡æ ·ä»¥è·å¾—æ›´å¥½çš„ç”Ÿæˆ
                    "repetition_penalty": 1.1,
                    "temperature": 0.3,  # é€‚ä¸­çš„æ¸©åº¦ä»¥è·å¾—å¹³è¡¡çš„ç”Ÿæˆ
                    "top_p": 0.9,
                    "top_k": 50,  # æ·»åŠ top_kå‚æ•°
                    "pad_token_id": 0,
                    "eos_token_id": 2,
                    "length_penalty": 1.0,  # ä¸æƒ©ç½šé•¿ç­”æ¡ˆ
                    "no_repeat_ngram_size": 3  # é¿å…é‡å¤
                }
                
                # å¸¦è¶…æ—¶æ§åˆ¶çš„ç”Ÿæˆ
                try:
                    with timeout(self.timeout_seconds):
                        generated_text = self.llm_generator.generate([prompt_text])[0]
                except TimeoutException:
                    print(f"âš ï¸ ç”Ÿæˆè¶…æ—¶ ({self.timeout_seconds}ç§’)ï¼Œè¿”å›éƒ¨åˆ†ç»“æœ...")
                    # è¿”å›ä¸€ä¸ªæç¤ºä¿¡æ¯
                    generated_text = "<think>Generation timeout occurred. Please try with a simpler prompt or increase timeout.</think>"
                
                generation_time = time.time() - start_time
                
                # æ¸…ç†å›ç­”
                cleaned_answer = self._clean_answer(generated_text)
                
                return {
                    "generated_answer": generated_text,
                    "cleaned_answer": cleaned_answer,
                    "generation_time": generation_time,
                    "timeout_occurred": "timeout" in generated_text.lower()
                }
                
            except Exception as e:
                print(f"âš ï¸ RAGç”Ÿæˆå™¨é”™è¯¯: {e}")
                return {
                    "generated_answer": f"Error: {e}",
                    "cleaned_answer": f"Error: {e}",
                    "generation_time": time.time() - start_time,
                    "timeout_occurred": False
                }
        else:
            return {
                "generated_answer": "RAG generator not available",
                "cleaned_answer": "RAG generator not available",
                "generation_time": time.time() - start_time,
                "timeout_occurred": False
            }
    
    def _clean_answer(self, answer: str) -> str:
        """æ¸…ç†å›ç­”"""
        return answer.strip()
    
    def evaluate_answer_quality(self, generated_answer: str, expected_answer: str, 
                              context: str, question: str, raw_answer: str = "") -> Dict[str, Any]:
        """è¯„ä¼°ç­”æ¡ˆè´¨é‡"""
        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        if "timeout" in generated_answer.lower():
            return {
                "quality_score": 0.0,
                "exact_match": False,
                "contains_expected": False,
                "semantic_similarity": 0.0,
                "format_violations": ["ç”Ÿæˆè¶…æ—¶"],
                "f1_score": 0.0,
                "timeout_occurred": True
            }
        
        # åŸºç¡€è¯„ä¼°
        exact_match = generated_answer.strip().lower() == expected_answer.strip().lower()
        contains_expected = expected_answer.strip().lower() in generated_answer.strip().lower()
        similarity = SequenceMatcher(None, generated_answer.lower(), expected_answer.lower()).ratio()
        
        # è´¨é‡åˆ†æ•°è®¡ç®—
        quality_score = 0.0
        if exact_match:
            quality_score = 1.0
        elif contains_expected:
            quality_score = 0.8
        elif similarity > 0.7:
            quality_score = 0.6
        elif similarity > 0.5:
            quality_score = 0.4
        elif similarity > 0.3:
            quality_score = 0.2
        else:
            quality_score = 0.0
        
        format_violations = []
        if not generated_answer.strip():
            format_violations.append("ç©ºå›ç­”")
        
        f1_score = calculate_f1_score(generated_answer, expected_answer)
        
        return {
            "quality_score": quality_score,
            "exact_match": exact_match,
            "contains_expected": contains_expected,
            "semantic_similarity": similarity,
            "format_violations": format_violations,
            "f1_score": f1_score,
            "timeout_occurred": False
        }

def get_detailed_english_prompt_messages(context_content: str, question_text: str, summary_content: Optional[str] = None) -> List[Dict[str, str]]:
    """ç”ŸæˆLLMæœŸæœ›çš„messagesåˆ—è¡¨"""
    try:
        with open('rag_english_template.txt', 'r', encoding='utf-8') as f:
            template_content = f.read().strip()
    except FileNotFoundError:
        print("âš ï¸ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
        return [
            {"role": "system", "content": "You are a world-class quantitative financial analyst AI."},
            {"role": "user", "content": f"Context:\n{context_content}\n\nQuestion:\n{question_text}\n\nA:"}
        ]
    
    # è§£æsystemå’Œuseræ ‡ç­¾
    system_match = re.search(r'<system>(.*?)</system>', template_content, re.DOTALL)
    if system_match:
        system_content = system_match.group(1).strip()
    else:
        system_content = "You are a world-class quantitative financial analyst AI."
    
    user_match = re.search(r'<user>(.*?)</user>', template_content, re.DOTALL)
    if user_match:
        user_template = user_match.group(1).strip()
        user_content = user_template.replace('{context}', context_content).replace('{question}', question_text)
    else:
        user_content = f"Context:\n{context_content}\n\nQuestion:\n{question_text}\n\nA:"

    return [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_content}
    ]

class OptimizedComprehensiveEvaluator:
    """ä¼˜åŒ–ç‰ˆå…¨é¢è¯„ä¼°å™¨ - è§£å†³è¶…æ—¶é—®é¢˜"""
    
    def __init__(self, model_name: str = "SUFE-AIFLM-Lab/Fin-R1", device: str = "auto"):
        self.tester = OptimizedLLMTemplateTester(model_name, device)
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        self.tester.load_model()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
        
    def load_evaluation_data(self, sample_size: int = 20) -> List[Dict[str, Any]]:
        """åŠ è½½è¯„ä¼°æ•°æ®"""
        print(f"ğŸ“– åŠ è½½è¯„ä¼°æ•°æ®ï¼Œç›®æ ‡æ ·æœ¬æ•°: {sample_size}")
        
        eval_data = []
        data_file = 'evaluate_mrr/tatqa_eval_enhanced.jsonl'
        
        if not os.path.exists(data_file):
            print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
            return []
        
        with open(data_file, 'r') as f:
            lines = f.readlines()
            for line in tqdm(lines, desc="ğŸ“– è¯»å–æ•°æ®æ–‡ä»¶", unit="è¡Œ"):
                eval_data.append(json.loads(line))
        
        print(f"âœ… è¯»å–äº† {len(eval_data)} è¡Œæ•°æ®")
        
        # éšæœºé‡‡æ ·æŒ‡å®šæ•°é‡çš„æ ·æœ¬
        if len(eval_data) > sample_size:
            np.random.seed(42)
            eval_data = np.random.choice(eval_data, sample_size, replace=False).tolist()
            print(f"âœ… éšæœºé‡‡æ ·äº† {len(eval_data)} ä¸ªæ ·æœ¬")
        
        return eval_data
    
    def evaluate_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        try:
            # æ„å»ºPrompt
            messages = get_detailed_english_prompt_messages(
                context_content=sample["context"],
                question_text=sample["query"],
                summary_content=sample["context"]
            )
            
            # ç”Ÿæˆå›ç­”
            generation_result = self.tester.generate_response(messages)
            
            # æå–æœ€ç»ˆç­”æ¡ˆ
            final_answer_to_evaluate = extract_final_answer(generation_result["generated_answer"])
            
            # è´¨é‡è¯„ä¼°
            evaluation = self.tester.evaluate_answer_quality(
                generated_answer=final_answer_to_evaluate,
                expected_answer=sample["answer"],
                context=sample["context"],
                question=sample["query"],
                raw_answer=generation_result["generated_answer"]
            )
            
            return {
                "sample_id": sample.get("id", "unknown"),
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "generation": generation_result,
                "evaluation": evaluation,
                "context_type": "table" if "Table ID:" in sample["context"] else "paragraph",
                "success": evaluation["exact_match"] or evaluation["contains_expected"]
            }
            
        except Exception as e:
            print(f"âš ï¸ æ ·æœ¬è¯„ä¼°å¤±è´¥: {str(e)[:100]}...")
            return {
                "sample_id": sample.get("id", "unknown"),
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "error": str(e),
                "success": False
            }
    
    def run_comprehensive_evaluation(self, sample_size: int = 20) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢è¯„ä¼°"""
        print(f"\nğŸš€ å¼€å§‹ä¼˜åŒ–ç‰ˆå…¨é¢è¯„ä¼°ï¼Œæ ·æœ¬æ•°: {sample_size}")
        print("="*60)
        
        # åŠ è½½æ•°æ®
        eval_data = self.load_evaluation_data(sample_size)
        
        if not eval_data:
            print("âŒ æ²¡æœ‰å¯ç”¨çš„è¯„ä¼°æ•°æ®")
            return {"results": [], "analysis": {}, "timestamp": time.time()}
        
        # è¯„ä¼°æ‰€æœ‰æ ·æœ¬
        results = []
        start_time = time.time()
        timeout_count = 0
        
        print(f"ğŸ” å¼€å§‹è¯„ä¼° {len(eval_data)} ä¸ªæ ·æœ¬...")
        
        pbar = tqdm(eval_data, desc="ğŸ” è¯„ä¼°æ ·æœ¬", unit="æ ·æœ¬")
        
        for i, sample in enumerate(pbar):
            pbar.set_description(f"ğŸ” è¯„ä¼°æ ·æœ¬ {i+1}/{len(eval_data)}")
            
            result = self.evaluate_single_sample(sample)
            results.append(result)
            
            # ç»Ÿè®¡è¶…æ—¶æƒ…å†µ
            if result.get("evaluation", {}).get("timeout_occurred", False):
                timeout_count += 1
            
            # æ¯5ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if (i + 1) % 5 == 0:
                success_count = sum(1 for r in results if r.get("success", False))
                pbar.set_postfix({
                    "æˆåŠŸ": f"{success_count}/{i+1}",
                    "è¶…æ—¶": f"{timeout_count}/{i+1}",
                    "æˆåŠŸç‡": f"{success_count/(i+1)*100:.1f}%"
                })
        
        pbar.close()
        
        total_time = time.time() - start_time
        print(f"âœ… è¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {total_time:.2f}ç§’")
        print(f"âš ï¸ è¶…æ—¶æ¬¡æ•°: {timeout_count}/{len(eval_data)}")
        
        # åˆ†æç»“æœ
        analysis = self.analyze_results(results, total_time)
        
        return {
            "results": results,
            "analysis": analysis,
            "timestamp": time.time()
        }
    
    def analyze_results(self, results: List[Dict[str, Any]], total_time: float) -> Dict[str, Any]:
        """åˆ†æè¯„ä¼°ç»“æœ"""
        total_samples = len(results)
        successful_samples = sum(1 for r in results if r.get("success", False))
        timeout_samples = sum(1 for r in results if r.get("evaluation", {}).get("timeout_occurred", False))
        
        quality_scores = [r.get("evaluation", {}).get("quality_score", 0) for r in results]
        exact_matches = sum(1 for r in results if r.get("evaluation", {}).get("exact_match", False))
        contains_expected = sum(1 for r in results if r.get("evaluation", {}).get("contains_expected", False))
        semantic_similarities = [r.get("evaluation", {}).get("semantic_similarity", 0) for r in results]
        f1_scores = [r.get("evaluation", {}).get("f1_score", 0) for r in results]
        generation_times = [r.get("generation", {}).get("generation_time", 0) for r in results]
        
        analysis = {
            "overall_metrics": {
                "total_samples": total_samples,
                "successful_samples": successful_samples,
                "timeout_samples": timeout_samples,
                "success_rate": successful_samples / total_samples if total_samples > 0 else 0,
                "timeout_rate": timeout_samples / total_samples if total_samples > 0 else 0,
                "exact_match_rate": exact_matches / total_samples if total_samples > 0 else 0,
                "contains_expected_rate": contains_expected / total_samples if total_samples > 0 else 0,
                "avg_quality_score": np.mean(quality_scores) if quality_scores else 0,
                "avg_semantic_similarity": np.mean(semantic_similarities) if semantic_similarities else 0,
                "avg_f1_score": np.mean(f1_scores) if f1_scores else 0,
                "avg_generation_time": np.mean(generation_times) if generation_times else 0,
                "total_evaluation_time": total_time
            }
        }
        
        return analysis
    
    def print_analysis_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š ä¼˜åŒ–ç‰ˆå…¨é¢è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*80)
        
        metrics = analysis["overall_metrics"]
        
        print(f"ğŸ“ˆ æ€»ä½“æŒ‡æ ‡:")
        print(f"   â€¢ æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
        print(f"   â€¢ æˆåŠŸæ ·æœ¬: {metrics['successful_samples']}")
        print(f"   â€¢ è¶…æ—¶æ ·æœ¬: {metrics['timeout_samples']}")
        print(f"   â€¢ æˆåŠŸç‡: {metrics['success_rate']:.2%}")
        print(f"   â€¢ è¶…æ—¶ç‡: {metrics['timeout_rate']:.2%}")
        print(f"   â€¢ ç²¾ç¡®åŒ¹é…ç‡: {metrics['exact_match_rate']:.2%}")
        print(f"   â€¢ åŒ…å«æœŸæœ›ç­”æ¡ˆç‡: {metrics['contains_expected_rate']:.2%}")
        print(f"   â€¢ å¹³å‡è´¨é‡åˆ†æ•°: {metrics['avg_quality_score']:.3f}")
        print(f"   â€¢ å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦: {metrics['avg_semantic_similarity']:.3f}")
        print(f"   â€¢ å¹³å‡F1åˆ†æ•°: {metrics['avg_f1_score']:.3f}")
        print(f"   â€¢ å¹³å‡ç”Ÿæˆæ—¶é—´: {metrics['avg_generation_time']:.2f}ç§’")
        print(f"   â€¢ æ€»è¯„ä¼°æ—¶é—´: {metrics['total_evaluation_time']:.2f}ç§’")
        
        print("\n" + "="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¯åŠ¨ä¼˜åŒ–ç‰ˆå…¨é¢è¯„ä¼°ç³»ç»Ÿ")
    print("="*60)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = OptimizedComprehensiveEvaluator(
        model_name="SUFE-AIFLM-Lab/Fin-R1",
        device="auto"
    )
    
    # è¿è¡Œè¯„ä¼°
    sample_size = 20  # å‡å°‘æ ·æœ¬æ•°é‡ä»¥é¿å…é•¿æ—¶é—´è¿è¡Œ
    results = evaluator.run_comprehensive_evaluation(sample_size)
    
    # æ‰“å°ç»“æœ
    if results["analysis"]:
        evaluator.print_analysis_summary(results["analysis"])
    
    # ä¿å­˜ç»“æœ
    output_file = f"comprehensive_evaluation_optimized_{sample_size}_samples.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("âœ… è¯„ä¼°å®Œæˆï¼")

if __name__ == "__main__":
    main() 