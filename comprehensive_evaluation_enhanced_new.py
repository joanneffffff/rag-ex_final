#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬ - ä¿®å¤ç‰ˆæœ¬
ä½¿ç”¨ä¸comprehensive_evaluation_enhanced.pyç›¸åŒçš„é€»è¾‘ï¼Œä½†åªä½¿ç”¨ä¸€ä¸ªç»Ÿä¸€æ¨¡æ¿ï¼ŒåŒ…å«contextåˆ†ç¦»åŠŸèƒ½
"""

import warnings
import logging
import os
import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import time
import argparse
from collections import Counter
from difflib import SequenceMatcher
import sys
import gc
import signal
import atexit

# ç¯å¢ƒè®¾ç½®
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

try:
    from tqdm import tqdm
except ImportError:
    print("âŒ tqdmæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tqdm")
    sys.exit(1)

sys.path.append(str(Path(__file__).parent))

try:
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
    print("âœ… ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator")
except ImportError:
    USE_RAG_GENERATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGeneratorï¼Œè„šæœ¬å°†æ— æ³•è¿è¡Œã€‚")
    sys.exit(1)

try:
    from xlm.utils.context_separator import context_separator
    USE_CONTEXT_SEPARATOR = True
    print("âœ… ä½¿ç”¨ä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½")
except ImportError:
    USE_CONTEXT_SEPARATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥ä¸Šä¸‹æ–‡åˆ†ç¦»å™¨ï¼Œå°†ä½¿ç”¨åŸå§‹ä¸Šä¸‹æ–‡å¤„ç†æ–¹å¼")

# ===================================================================
# èµ„æºæ¸…ç†æœºåˆ¶
# ===================================================================

class ResourceManager:
    """èµ„æºç®¡ç†å™¨ï¼Œç¡®ä¿ç¨‹åºç»“æŸæ—¶æ­£ç¡®æ¸…ç†èµ„æº"""
    
    def __init__(self):
        self.generator = None
        self.cleanup_registered = False
        self._register_cleanup()
    
    def _register_cleanup(self):
        """æ³¨å†Œæ¸…ç†å‡½æ•°"""
        if not self.cleanup_registered:
            atexit.register(self.cleanup_resources)
            signal.signal(signal.SIGINT, self._signal_handler)
            signal.signal(signal.SIGTERM, self._signal_handler)
            self.cleanup_registered = True
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        print(f"\nğŸ›‘ æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
        self.cleanup_resources()
        sys.exit(0)
    
    def set_generator(self, generator):
        """è®¾ç½®ç”Ÿæˆå™¨å¼•ç”¨"""
        self.generator = generator
    
    def cleanup_resources(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        print("ğŸ§¹ å¼€å§‹æ¸…ç†èµ„æº...")
        
        try:
            # 1. æ¸…ç†ç”Ÿæˆå™¨
            if self.generator:
                print("ğŸ—‘ï¸ æ¸…ç†ç”Ÿæˆå™¨...")
                if hasattr(self.generator, 'model'):
                    del self.generator.model
                if hasattr(self.generator, 'tokenizer'):
                    del self.generator.tokenizer
                self.generator = None
            
            # 2. æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                print("ğŸ—‘ï¸ æ¸…ç†GPUå†…å­˜...")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
                # æ˜¾ç¤ºæ¸…ç†åçš„å†…å­˜çŠ¶æ€
                for i in range(torch.cuda.device_count()):
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    cached = torch.cuda.memory_reserved(i) / 1024**3
                    print(f"   GPU {i}: å·²åˆ†é… {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")
            
            # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶
            print("ğŸ—‘ï¸ å¼ºåˆ¶åƒåœ¾å›æ”¶...")
            gc.collect()
            
            print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ èµ„æºæ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

# å…¨å±€èµ„æºç®¡ç†å™¨
resource_manager = ResourceManager()

# ===================================================================
# æ ¸å¿ƒè¾…åŠ©å‡½æ•°
# ===================================================================

def extract_final_answer_with_rescue(raw_output: str) -> str:
    """
    ä»æ¨¡å‹çš„åŸå§‹è¾“å‡ºä¸­æ™ºèƒ½æå–æœ€ç»ˆç­”æ¡ˆã€‚
    å®ƒé¦–å…ˆå°è¯•å¯»æ‰¾<answer>æ ‡ç­¾ï¼Œå¦‚æœå¤±è´¥æˆ–ä¸ºç©ºï¼Œåˆ™å¯åŠ¨æ•‘æ´é€»è¾‘ä»<think>æ ‡ç­¾ä¸­æå–ã€‚
    """
    def _clean_extracted_text(text: str) -> str:
        """å¯¹æå–å‡ºçš„æ–‡æœ¬è¿›è¡Œé€šç”¨æ¸…ç†ï¼Œä»¥åŒ¹é…æœŸæœ›çš„ç­”æ¡ˆæ ¼å¼"""
        text = text.strip()
        # ç§»é™¤æ•°å­—ä¸­çš„é€—å· (å¦‚æœä½ çš„ expected_answer ä¸åŒ…å«é€—å·)
        text = text.replace(',', '')
        # ç§»é™¤è´Ÿæ•°æ‹¬å· (ä¾‹å¦‚ "(33)" -> "-33")
        if text.startswith('(') and text.endswith(')'):
            text = '-' + text[1:-1]
        
        # æ ‡å‡†åŒ–ç™¾åˆ†å·ï¼Œç¡®ä¿ "15.2%" å’Œ "15.2 %" åŒ¹é…
        text = text.replace('%', ' %').strip()
        text = text.replace(' %', '%')

        # ç§»é™¤å¸¸è§çš„å¼•å¯¼è¯å¥ (åº”ä¸ Prompt ä¼˜åŒ–åå‡å°‘å‡ºç°)
        text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
        
        # ç§»é™¤æœ«å°¾å¯èƒ½çš„å¤šä½™æ ‡ç‚¹ç¬¦å·ï¼Œå¦‚å¥å·ã€é€—å·ã€åˆ†å· (ä½†ä¿ç•™ç™¾åˆ†å·)
        text = re.sub(r'[\.ã€‚;,]$', '', text).strip()
        
        # ç§»é™¤å¸¸è§çš„è´§å¸ç¬¦å·å’Œå•ä½è¯ (å¦‚æœä½ çš„ expected_answer ä¸åŒ…å«è¿™äº›)
        text = re.sub(r'(\$|million|billion|usd|eur|pounds|Â£)', '', text, flags=re.IGNORECASE).strip()

        return text

    # 1. å°è¯•ä» <answer> æ ‡ç­¾ä¸­æå– (è¿™æ˜¯é¦–è¦ä¸”æœ€æœŸæœ›çš„)
    answer_match = re.search(r'<answer>(.*?)</answer>', raw_output, re.DOTALL)
    if answer_match:
        content = answer_match.group(1).strip()
        if content:
            return _clean_extracted_text(content)
        # å¦‚æœ <answer> æ ‡ç­¾å­˜åœ¨ä½†å†…å®¹ä¸ºç©ºï¼Œåˆ™ç»§ç»­æ•‘æ´

    # 2. æ•‘æ´é€»è¾‘ï¼šå¦‚æœ <answer> æ ‡ç­¾ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œå°è¯•ä» <think> æ ‡ç­¾ä¸­æå–
    # æŸ¥æ‰¾ <think> æ ‡ç­¾çš„å†…å®¹
    think_match = re.search(r'<think>(.*?)(?:</think>|$)', raw_output, re.DOTALL)
    if not think_match:
        # å¦‚æœè¿ <think> æ ‡ç­¾éƒ½æ²¡æœ‰ï¼Œå›é€€åˆ°åŸå§‹è¾“å‡ºçš„æœ€åä¸€è¡Œ
        lines = raw_output.strip().split('\n')
        return _clean_extracted_text(lines[-1]) if lines else ""

    think_content = think_match.group(1)
    
    # --- 2.1. å°è¯•å¯»æ‰¾ç»“è®ºæ€§çŸ­è¯­ ---
    conclusion_phrases = [
        r'final\s*answer\s*is[:\s]*', r'the\s*answer\s*is[:\s]*', 
        r'therefore,\s*the\s*answer\s*is[:\s]*', r'the\s*result\s*is[:\s]*', 
        r'equals\s*to[:\s]*', r'is\s*equal\s*to[:\s]*', 
        r'the\s*value\s*is[:\s]*', r'the\s*change\s*is[:\s]*', 
        r'the\s*amount\s*is[:\s]*', r'conclusion[:\s]*', 
        r'final\s*extracted\s*value/calculated\s*result[:\s]*', r'final\s*number[:\s]*',
        r'adjusted\s*net\s*income\s*is[:\s]*', r'percentage\s*change\s*is[:\s]*', 
        r'decreased\s*by[:\s]*', r'increased\s*by[:\s]*',
        r'net\s*change\s*is[:\s]*', r'total\s*is[:\s]*',
        r'resulting\s*in[:\s]*', r'is[:\s]*([-+]?[\d,\.]+%?)' # æ•è·"is:"åé¢ç›´æ¥è·Ÿçš„æ•°å­—æˆ–ç™¾åˆ†æ¯”
    ]
    
    for phrase_pattern in conclusion_phrases:
        # æ•è·çŸ­è¯­ååˆ°ä¸‹ä¸€ä¸ªæ ‡ç­¾ã€åŒæ¢è¡Œç¬¦æˆ–å­—ç¬¦ä¸²ç»“æŸçš„å†…å®¹ (éè´ªå©ª)
        conclusion_match = re.search(
            f'{phrase_pattern}(.*?)(?:$|<answer>|<think>|\\n\\n|\\Z)', 
            think_content, 
            re.IGNORECASE | re.DOTALL 
        )
        if conclusion_match:
            conclusion = conclusion_match.group(1).strip()
            # ç¡®ä¿æå–çš„å†…å®¹ä¸åŒ…å«æ€è€ƒè¿‡ç¨‹ä¸­çš„æ­¥éª¤ç¼–å·
            if conclusion and re.fullmatch(r'\d+\.', conclusion.split('\n')[0].strip()):
                continue # å¦‚æœç¬¬ä¸€è¡Œæ˜¯æ­¥éª¤ç¼–å·ï¼Œè·³è¿‡
            
            return _clean_extracted_text(conclusion)
    
    # --- 2.2. å¦‚æœç»“è®ºæ€§çŸ­è¯­ä¸åŒ¹é…ï¼Œå°è¯•å¯»æ‰¾æœ€åä¸€ä¸ªç¬¦åˆæ•°å€¼/ç™¾åˆ†æ¯”/å¸¸è§æ ¼å¼çš„å­—ç¬¦ä¸² ---
    potential_answers_raw = re.findall(r'([-+]?\s*\(?[\d,\.]+\)?%?)\s*$', think_content, re.MULTILINE) # æ•è·ç»„
    if not potential_answers_raw:
        potential_answers_raw = re.findall(r'([-+]?\s*\(?[\d,\.]+\)?%?)', think_content) # æ•è·ç»„
    
    if potential_answers_raw:
        for item_raw in reversed(potential_answers_raw):
            item = item_raw.strip()
            if not item: continue
            
            # æ’é™¤æ˜æ˜¾çš„æ­¥éª¤ç¼–å·æˆ–çŸ­è¯­ (å¦‚"1.", "2.", "Step 1:")
            if re.fullmatch(r'(\d+\.|\bstep\s*\d+\b)[:\s]*', item, re.IGNORECASE):
                continue

            cleaned_item = _clean_extracted_text(item)
            
            # ç®€å•çš„éªŒè¯ï¼Œç¡®ä¿ä¸æ˜¯ç©ºçš„æˆ–çº¯ç²¹çš„æ ‡ç‚¹
            if cleaned_item and len(cleaned_item) > 0 and not re.fullmatch(r'[^\w\s\d%.-]*', cleaned_item):
                return cleaned_item
                
    # --- 2.3. æœ€åå›é€€ï¼šå¦‚æœä»¥ä¸Šéƒ½å¤±è´¥ï¼Œå– <think> å†…å®¹çš„æœ€åä¸€è¡Œ ---
    lines = [line for line in think_content.strip().split('\n') if line.strip()]
    if lines:
        return _clean_extracted_text(lines[-1])
    return "" # å¦‚æœ think ä¹Ÿæ˜¯ç©ºçš„ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²

def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—F1åˆ†æ•°ï¼ŒåŒ…å«æ›´é²æ£’çš„å½’ä¸€åŒ–ï¼Œä¸ç­”æ¡ˆæå–é€»è¾‘ä¿æŒé«˜åº¦ä¸€è‡´"""
    def normalize_for_f1(text):
        text = text.strip()
        
        # ç§»é™¤æ•°å­—ä¸­çš„é€—å·
        text = text.replace(',', '')
        # ç§»é™¤è´Ÿæ•°æ‹¬å· (ä¾‹å¦‚ "(33)" -> "-33")
        if text.startswith('(') and text.endswith(')'):
            text = '-' + text[1:-1]
        
        # æ ‡å‡†åŒ–ç™¾åˆ†å·ï¼Œç¡®ä¿ "15.2%" å’Œ "15.2%" åŒ¹é…
        text = text.replace('%', ' %').strip()
        text = text.replace(' %', '%')

        # ç§»é™¤å¸¸è§çš„å¼•å¯¼è¯å¥ (åº”ä¸ Prompt ä¼˜åŒ–åå‡å°‘å‡ºç°)
        text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
        
        # ç§»é™¤æœ«å°¾å¯èƒ½çš„å¤šä½™æ ‡ç‚¹ (ä¾‹å¦‚å¥å·)
        text = text.rstrip('.')
        
        # æœ€ç»ˆå…¨éƒ¨å°å†™å¹¶åˆ†å‰²
        return text.lower().split()

    prediction_tokens = normalize_for_f1(prediction)
    ground_truth_tokens = normalize_for_f1(ground_truth)

    if not ground_truth_tokens: 
        return 1.0 if not prediction_tokens else 0.0
    if not prediction_tokens: 
        return 0.0

    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0: 
        return 0.0

    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def _parse_template_string_to_messages(template_full_string: str, query: str, context: str = "", table_context: str = "", text_context: str = "") -> List[Dict[str, str]]:
    """
    è§£ææ¨¡æ¿å­—ç¬¦ä¸²ä¸ºæ¶ˆæ¯åˆ—è¡¨ï¼Œæ”¯æŒåˆ†ç¦»çš„ä¸Šä¸‹æ–‡
    """
    # æ›¿æ¢å ä½ç¬¦
    formatted_template = template_full_string.replace("{query}", query)
    
    if table_context and text_context:
        # ä½¿ç”¨åˆ†ç¦»çš„ä¸Šä¸‹æ–‡
        formatted_template = formatted_template.replace("{table_context}", table_context)
        formatted_template = formatted_template.replace("{text_context}", text_context)
    else:
        # ä½¿ç”¨ç»Ÿä¸€ä¸Šä¸‹æ–‡
        formatted_template = formatted_template.replace("{context_content}", context)
    
    # è§£æ ===SYSTEM=== å’Œ ===USER=== æ ‡ç­¾
    system_match = re.search(r'===SYSTEM===(.*?)(?:===USER===|$)', formatted_template, re.DOTALL)
    user_match = re.search(r'===USER===(.*?)(?:===|$)', formatted_template, re.DOTALL)
    
    messages = []
    
    if system_match:
        system_content = system_match.group(1).strip()
        if system_content:
            messages.append({"role": "system", "content": system_content})
    
    if user_match:
        user_content = user_match.group(1).strip()
        if user_content:
            messages.append({"role": "user", "content": user_content})
    
    return messages

def load_and_format_template(template_name: str, context: str, query: str) -> List[Dict[str, str]]:
    """
    åŠ è½½å¹¶æ ¼å¼åŒ–æŒ‡å®šçš„promptæ¨¡æ¿ï¼ˆç»Ÿä¸€ä¸Šä¸‹æ–‡ï¼‰
    """
    template_path = Path("data/prompt_templates") / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template_full_string = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}")
        # ä½¿ç”¨é»˜è®¤æ¨¡æ¿
        template_full_string = """===SYSTEM===
You are a helpful assistant that answers questions based on the provided context.

===USER===
Context: {context_content}

Question: {query}

Answer:"""
    
    return _parse_template_string_to_messages(template_full_string, query, context=context)

def load_and_format_template_with_separated_context(template_name: str, table_context: str, text_context: str, query: str) -> List[Dict[str, str]]:
    """
    åŠ è½½å¹¶æ ¼å¼åŒ–æŒ‡å®šçš„promptæ¨¡æ¿ï¼ˆåˆ†ç¦»çš„ä¸Šä¸‹æ–‡ï¼‰
    """
    template_path = Path("data/prompt_templates") / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template_full_string = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}")
        # ä½¿ç”¨é»˜è®¤æ¨¡æ¿
        template_full_string = """===SYSTEM===
You are a helpful assistant that answers questions based on the provided context.

===USER===
Table Context: {table_context}

Text Context: {text_context}

Question: {query}

Answer:"""
    
    return _parse_template_string_to_messages(template_full_string, query, table_context=table_context, text_context=text_context)

def get_final_prompt(context: str, query: str) -> List[Dict[str, str]]:
    """ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡æ¿ï¼ŒåŒ…å«contextåˆ†ç¦»åŠŸèƒ½"""
    # å§‹ç»ˆä½¿ç”¨ç»Ÿä¸€çš„æ¨¡æ¿
    template_file = 'unified_english_template.txt'
    
    # ä½¿ç”¨ä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½
    if USE_CONTEXT_SEPARATOR:
        try:
            # åˆ†ç¦»ä¸Šä¸‹æ–‡
            separated = context_separator.separate_context(context)
            
            # æ ¼å¼åŒ– prompt å‚æ•°
            prompt_params = context_separator.format_for_prompt(separated, query)
            
            # ä½¿ç”¨åˆ†ç¦»åçš„ä¸Šä¸‹æ–‡æ ¼å¼åŒ–æ¨¡æ¿
            return load_and_format_template_with_separated_context(
                template_file, 
                prompt_params["table_context"], 
                prompt_params["text_context"], 
                query
            )
        except Exception as e:
            print(f"âš ï¸ ä¸Šä¸‹æ–‡åˆ†ç¦»å¤±è´¥: {e}ï¼Œå›é€€åˆ°åŸå§‹æ–¹å¼")
            return load_and_format_template(template_file, context, query)
    else:
        # å›é€€åˆ°åŸå§‹æ–¹å¼
        return load_and_format_template(template_file, context, query)

# ===================================================================
# æ ¸å¿ƒè¯„ä¼°ç±»
# ===================================================================

class ComprehensiveEvaluator:
    def __init__(self, model_name: str, device: str):
        self.model_name = model_name
        self.device = device
        self.max_new_tokens = 4096
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        self.generator = LocalLLMGenerator(model_name=self.model_name, device=self.device)
        
        # æ³¨å†Œç”Ÿæˆå™¨åˆ°èµ„æºç®¡ç†å™¨
        resource_manager.set_generator(self.generator)
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def run_evaluation(self, eval_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        results = []
        start_time = time.time()
        pbar = tqdm(eval_data, desc="ğŸ” è¯„ä¼°æ ·æœ¬", unit="ä¸ª")

        try:
            for sample in pbar:
                result = self._evaluate_single_sample(sample)
                results.append(result)
                
                # å®šæœŸæ¸…ç†GPUå†…å­˜
                if len(results) % 5 == 0:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
            resource_manager.cleanup_resources()
            raise
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            resource_manager.cleanup_resources()
            raise
        finally:
            total_time = time.time() - start_time
            print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"ğŸ“Š å¤„ç†äº† {len(results)} ä¸ªç»“æœ")
        
        analysis = self.analyze_results(results)
        analysis['performance'] = {'total_time': total_time, 'avg_time_per_sample': total_time / len(results) if results else 0}
        return {"results": results, "analysis": analysis}

    def _evaluate_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        try:
            # ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡æ¿ï¼ŒåŒ…å«contextåˆ†ç¦»
            messages = get_final_prompt(sample["context"], sample["query"])
            
            # è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
            prompt_text = self._convert_messages_to_text(messages)

            gen_start_time = time.time()
            # generator.generate æœŸæœ› List[str]ï¼Œæ‰€ä»¥ç”¨ [prompt_text] åŒ…è£¹
            generation_result = self.generator.generate([prompt_text])[0]
            gen_time = time.time() - gen_start_time
            
            final_answer_to_evaluate = extract_final_answer_with_rescue(generation_result)
            evaluation = self._evaluate_quality(final_answer_to_evaluate, sample["answer"])
            
            return {
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "generated_answer": generation_result,      # åŸå§‹æ¨¡å‹è¾“å‡º
                "extracted_answer": final_answer_to_evaluate, # ç»è¿‡ extract_final_answer_with_rescue å¤„ç†åçš„ç­”æ¡ˆ
                "evaluation": evaluation,
                "answer_from": sample.get("answer_from", "unknown"), 
                "predicted_answer_from": "unified",
                "decision_confidence": 1.0,
                "is_difficult_decision": False,
                "context_type": "unified",
                "content_ratio": {"table_ratio":0.0, "text_ratio":0.0, "mixed_ratio":0.0},
                "generation_time": gen_time
            }
        except Exception as e:
            # è¯¦ç»†æ‰“å°é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‘ç”Ÿé”™è¯¯çš„æ ·æœ¬IDæˆ–æŸ¥è¯¢
            sample_id = sample.get("id", "N/A") # å¦‚æœä½ çš„æ ·æœ¬æœ‰ID
            print(f"\nâŒ å¤„ç†æ ·æœ¬å¤±è´¥ (ID: {sample_id}, Query: '{sample.get('query', 'N/A')[:50]}...', Error: {e})", file=sys.stderr)
            return {
                "query": sample["query"], 
                "expected_answer": sample["answer"], 
                "error": str(e),
                "context": sample.get("context", "N/A"), # åŒ…å«ä¸Šä¸‹æ–‡ä»¥ä¾›è°ƒè¯•
                "evaluation": {"exact_match": False, "f1_score": 0.0},
                "generated_answer": "", # ç¡®ä¿æœ‰æ­¤å­—æ®µï¼Œå³ä½¿æ˜¯é”™è¯¯æ ·æœ¬
                "extracted_answer": ""  # ç¡®ä¿æœ‰æ­¤å­—æ®µï¼Œå³ä½¿æ˜¯é”™è¯¯æ ·æœ¬
            }

    def _evaluate_quality(self, generated: str, expected: str) -> Dict[str, Any]:
        exact_match = generated.strip().lower() == expected.strip().lower()
        f1 = calculate_f1_score(generated, expected)
        return {"exact_match": exact_match, "f1_score": f1}

    def _convert_messages_to_text(self, messages: List[Dict[str, str]]) -> str:
        """
        å°† messages åˆ—è¡¨è½¬æ¢ä¸ºFin-R1ï¼ˆQwen2.5 basedï¼‰æœŸæœ›çš„ChatMLæ ¼å¼å­—ç¬¦ä¸²ã€‚
        è¿™æ˜¯æœ€ç»ˆä¼ é€’ç»™ LocalLLMGenerator çš„å­—ç¬¦ä¸²ã€‚
        """
        if not messages:
            return ""
        
        # Qwen2.5 ä½¿ç”¨ ChatML æ ¼å¼
        formatted_prompt = ""
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")
            
            if role == "system":
                formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n"
            elif role == "user":
                formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n"
            elif role == "assistant":
                formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n"
        
        # æç¤ºæ¨¡å‹å¼€å§‹ç”Ÿæˆ
        formatted_prompt += "<|im_start|>assistant\n" 
        
        return formatted_prompt

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        print(f"ğŸ” å¼€å§‹åˆ†æ {len(results)} ä¸ªç»“æœ...")
        
        if not results: 
            print("âŒ æ²¡æœ‰ç»“æœå¯åˆ†æ")
            return {}
        
        # æ£€æŸ¥ç»“æœç»“æ„
        valid_results = [r for r in results if 'evaluation' in r]
        error_results = [r for r in results if 'error' in r]
        print(f"âœ… æœ‰æ•ˆç»“æœ: {len(valid_results)}, âŒ é”™è¯¯ç»“æœ: {len(error_results)}")
        
        all_f1 = [r['evaluation']['f1_score'] for r in valid_results]
        all_em = [r['evaluation']['exact_match'] for r in valid_results]

        analysis = {
            'total_samples': len(results),
            'valid_samples': len(valid_results),
            'error_samples': len(error_results),
            'exact_match_rate': np.mean(all_em) if all_em else 0.0,
            'average_f1_score': np.mean(all_f1) if all_f1 else 0.0,
            'average_generation_time': np.mean([r.get('generation_time', 0) for r in valid_results]) if valid_results else 0.0
        }
        
        return analysis

    def print_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š å…¨é¢è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*80)
        print("ğŸ“ˆ æ•´ä½“æŒ‡æ ‡:")
        print(f"   - æ€»æ ·æœ¬æ•°: {analysis['total_samples']}")
        print(f"   - æœ‰æ•ˆæ ·æœ¬æ•°: {analysis['valid_samples']}")
        print(f"   - é”™è¯¯æ ·æœ¬æ•°: {analysis['error_samples']}")
        print(f"   - ç²¾ç¡®åŒ¹é…ç‡: {analysis['exact_match_rate']:.2%}")
        print(f"   - å¹³å‡F1åˆ†æ•°: {analysis['average_f1_score']:.4f}")
        print(f"   - å¹³å‡ç”Ÿæˆæ—¶é—´: {analysis['average_generation_time']:.2f}ç§’")
        
        print("\nğŸ’¡ æ€§èƒ½æ´å¯Ÿ:")
        if analysis['average_f1_score'] < 0.3:
            print("   - âš ï¸ æ•´ä½“F1åˆ†æ•°è¾ƒä½ï¼Œéœ€è¦æ˜¾è‘—æ”¹è¿›ã€‚")
        elif analysis['average_f1_score'] < 0.6:
            print("   - ğŸ”¶ F1åˆ†æ•°ä¸­ç­‰ï¼Œæœ‰æ”¹è¿›ç©ºé—´ã€‚")
        else:
            print("   - âœ… F1åˆ†æ•°è‰¯å¥½ã€‚")
        
        print("="*80)

def load_evaluation_data(data_path: str, sample_size: Optional[int] = None) -> List[Dict[str, Any]]:
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    try:
        from utils.data_loader import load_json_or_jsonl, sample_data
        eval_data = load_json_or_jsonl(data_path)
        
        if sample_size and sample_size < len(eval_data):
            eval_data = sample_data(eval_data, sample_size, 42)
            print(f"âœ… éšæœºé‡‡æ · {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
        else:
            print(f"âœ… åŠ è½½æ‰€æœ‰ {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
        return eval_data
            
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥ {data_path}: {e}")
        raise

def main():
    parser = argparse.ArgumentParser(description="æœ€ç»ˆå…¨é¢è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model", type=str, default="SUFE-AIFLM-Lab/Fin-R1", help="è¦è¯„ä¼°çš„LLMåç§°")
    parser.add_argument("--data_path", type=str, required=True, help="è¯„ä¼°æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (jsonl æˆ– json)")
    parser.add_argument("--sample_size", type=int, default=None, help="è¦è¯„ä¼°çš„éšæœºæ ·æœ¬æ•°é‡ (Noneè¡¨ç¤ºå…¨éƒ¨)")
    parser.add_argument("--device", type=str, default="cuda", help="è®¾å¤‡ (cuda/cpu/auto)")
    args = parser.parse_args()

    # è®¾å¤‡è®¾ç½®
    device = args.device
    if device == "auto":
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
    elif device == "cuda":
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
            device = "cpu"
        else:
            device = "cuda:1"
            print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(1)}")
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(1).total_memory / 1024**3:.1f} GB")
    
    try:
        # 1. åŠ è½½è¯„ä¼°æ•°æ®
        eval_data = load_evaluation_data(args.data_path, args.sample_size)
        
        # 2. åˆå§‹åŒ–å’Œè¿è¡Œè¯„ä¼°å™¨
        evaluator = ComprehensiveEvaluator(model_name=args.model, device=device)
        analysis_results = evaluator.run_evaluation(eval_data)
        
        # 3. æ‰“å°å’Œä¿å­˜ç»“æœ
        evaluator.print_summary(analysis_results['analysis'])
        output_filename = f"comprehensive_evaluation_results_{time.strftime('%Y%m%d_%H%M%S')}.json"
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ ç»“æœä¿å­˜åˆ°: {output_filename}")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        resource_manager.cleanup_resources()
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        resource_manager.cleanup_resources()
        raise
    finally:
        # ç¡®ä¿ç¨‹åºç»“æŸæ—¶æ¸…ç†èµ„æº
        resource_manager.cleanup_resources()

if __name__ == "__main__":
    main()