#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬ - ä¿®å¤ç‰ˆæœ¬
ä½¿ç”¨ä¸comprehensive_evaluation_enhanced.pyç›¸åŒçš„é€»è¾‘ï¼Œä½†åªä½¿ç”¨ä¸€ä¸ªç»Ÿä¸€æ¨¡æ¿ï¼ŒåŒ…å«contextåˆ†ç¦»åŠŸèƒ½
"""

import warnings
import logging
import os
import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import time
import argparse
from collections import Counter
from difflib import SequenceMatcher
import sys
import gc
import signal
import atexit

# ç¯å¢ƒè®¾ç½®
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

try:
    from tqdm import tqdm
except ImportError:
    print("âŒ tqdmæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tqdm")
    sys.exit(1)

sys.path.append(str(Path(__file__).parent))

try:
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
    print("âœ… ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator")
except ImportError:
    USE_RAG_GENERATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGeneratorï¼Œè„šæœ¬å°†æ— æ³•è¿è¡Œã€‚")
    sys.exit(1)

try:
    from xlm.utils.context_separator import context_separator
    USE_CONTEXT_SEPARATOR = True
    print("âœ… ä½¿ç”¨ä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½")
except ImportError:
    USE_CONTEXT_SEPARATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥ä¸Šä¸‹æ–‡åˆ†ç¦»å™¨ï¼Œå°†ä½¿ç”¨åŸå§‹ä¸Šä¸‹æ–‡å¤„ç†æ–¹å¼")

# ===================================================================
# èµ„æºæ¸…ç†æœºåˆ¶
# ===================================================================

class ResourceManager:
    """èµ„æºç®¡ç†å™¨ï¼Œç¡®ä¿ç¨‹åºç»“æŸæ—¶æ­£ç¡®æ¸…ç†èµ„æº"""
    
    def __init__(self):
        self.generator = None
        self.cleanup_registered = False
        self._register_cleanup()
    
    def _register_cleanup(self):
        """æ³¨å†Œæ¸…ç†å‡½æ•°"""
        if not self.cleanup_registered:
            atexit.register(self.cleanup_resources)
            signal.signal(signal.SIGINT, self._signal_handler)
            signal.signal(signal.SIGTERM, self._signal_handler)
            self.cleanup_registered = True
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        print(f"\nğŸ›‘ æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
        self.cleanup_resources()
        sys.exit(0)
    
    def set_generator(self, generator):
        """è®¾ç½®ç”Ÿæˆå™¨å¼•ç”¨"""
        self.generator = generator
    
    def cleanup_resources(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        print("ğŸ§¹ å¼€å§‹æ¸…ç†èµ„æº...")
        
        try:
            # 1. æ¸…ç†ç”Ÿæˆå™¨
            if self.generator:
                print("ğŸ—‘ï¸ æ¸…ç†ç”Ÿæˆå™¨...")
                if hasattr(self.generator, 'model'):
                    del self.generator.model
                if hasattr(self.generator, 'tokenizer'):
                    del self.generator.tokenizer
                self.generator = None
            
            # 2. æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                print("ğŸ—‘ï¸ æ¸…ç†GPUå†…å­˜...")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
                # æ˜¾ç¤ºæ¸…ç†åçš„å†…å­˜çŠ¶æ€
                for i in range(torch.cuda.device_count()):
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    cached = torch.cuda.memory_reserved(i) / 1024**3
                    print(f"   GPU {i}: å·²åˆ†é… {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")
            
            # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶
            print("ğŸ—‘ï¸ å¼ºåˆ¶åƒåœ¾å›æ”¶...")
            gc.collect()
            
            print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ èµ„æºæ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

# å…¨å±€èµ„æºç®¡ç†å™¨
resource_manager = ResourceManager()

# ===================================================================
# æ ¸å¿ƒè¾…åŠ©å‡½æ•°
# ===================================================================

def _shared_text_standardizer(text: str) -> str:
    """
    è¾…åŠ©å‡½æ•°ï¼Œç”¨äºæ ‡å‡†åŒ–æ–‡æœ¬ï¼Œä¾›ç­”æ¡ˆæå–å’ŒF1åˆ†æ•°è®¡ç®—ä½¿ç”¨ã€‚
    ç¡®ä¿ç§»é™¤é€—å·ã€å¤„ç†è´Ÿæ•°æ‹¬å·ã€æ ‡å‡†åŒ–ç™¾åˆ†å·ã€ç§»é™¤å¼•å¯¼è¯å¥ã€ç§»é™¤æœ«å°¾æ ‡ç‚¹å’Œè´§å¸å•ä½ã€‚
    """
    text = text.strip()
    # ç§»é™¤æ•°å­—ä¸­çš„é€—å·
    text = text.replace(',', '')
    # ç§»é™¤è´Ÿæ•°æ‹¬å· (ä¾‹å¦‚ "(33)" -> "-33")
    if text.startswith('(') and text.endswith(')'):
        text = '-' + text[1:-1]
    
    # æ ‡å‡†åŒ–ç™¾åˆ†å·ï¼Œç¡®ä¿ "15.2%" å’Œ "15.2 %" åŒ¹é…
    text = text.replace('%', ' %').strip()
    text = text.replace(' %', '%')

    # ç§»é™¤å¸¸è§çš„å¼•å¯¼è¯å¥ (åº”ä¸ Prompt ä¼˜åŒ–åå‡å°‘å‡ºç°)
    text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
    
    # ç§»é™¤æœ«å°¾å¯èƒ½çš„å¤šä½™æ ‡ç‚¹ (ä¾‹å¦‚å¥å·ã€é€—å·ã€åˆ†å·ï¼Œä½†ä¿ç•™ç™¾åˆ†å·)
    text = re.sub(r'[\.ã€‚;,]$', '', text).strip()
    
    # ç§»é™¤å¸¸è§çš„è´§å¸ç¬¦å·å’Œå•ä½è¯
    text = re.sub(r'(\$|million|billion|usd|eur|pounds|Â£)', '', text, flags=re.IGNORECASE).strip()

    return text

def extract_final_answer_with_rescue(raw_output: str) -> str:
    """
    ä»æ¨¡å‹çš„åŸå§‹è¾“å‡ºä¸­æå–æœ€ç»ˆç­”æ¡ˆã€‚
    ä¼˜å…ˆå¯»æ‰¾ <final-answer> æ ‡ç­¾ã€‚å…¶æ¬¡å°è¯•å¯»æ‰¾ "FINAL ANSWER: " å‰ç¼€ã€‚
    å¦‚æœå¤±è´¥æˆ–æœªæ‰¾åˆ°ï¼Œåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²ã€‚
    """
    cleaned_output = raw_output.strip()

    # --- 1. é¦–è¦å°è¯•ï¼šå¯»æ‰¾ <final-answer> æ ‡ç­¾ (æ¨¡å‹å®é™…è¾“å‡ºçš„æ ‡ç­¾) ---
    # ä½¿ç”¨éè´ªå©ªåŒ¹é… .*? æ¥æ•è· <final-answer> æ ‡ç­¾å†…éƒ¨çš„å†…å®¹
    final_answer_tag_match = re.search(r'<final-answer>(.*?)</final-answer>', cleaned_output, re.DOTALL)
    if final_answer_tag_match:
        content = final_answer_tag_match.group(1).strip()
        # ç¡®è®¤æå–çš„å†…å®¹éç©ºï¼Œä¸”ä¸åªæ˜¯å¦ä¸€ä¸ªç©ºæ ‡ç­¾
        if content and content.lower() not in ['<final></final>', '<answer></answer>']:
            return _shared_text_standardizer(content)

    # --- 2. å…¶æ¬¡å°è¯•ï¼šå¯»æ‰¾ "FINAL ANSWER: " å‰ç¼€ (Prompt ä¸­æŒ‡å®šçš„æ ¼å¼) ---
    # è¿™é€šå¸¸ä¼šåœ¨ <think> æ ‡ç­¾å†…éƒ¨æˆ–å…¶é™„è¿‘ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åœ¨æ•´ä¸ª cleaned_output ä¸­å¯»æ‰¾
    final_answer_prefix_match = re.search(r'FINAL ANSWER:\s*(.*?)(?:\n|$)', cleaned_output, re.IGNORECASE | re.DOTALL)
    if final_answer_prefix_match:
        content = final_answer_prefix_match.group(1).strip()
        # æ’é™¤æ¨¡å‹å¶å°”ä¼šåœ¨ FINAL ANSWER: åè·Ÿä¸€ä¸ªç©ºçš„æ ‡ç­¾å¯¹
        if content and content.lower() not in ['<final-answer></final-answer>', '<answer></answer>', '<final></final>']:
            return _shared_text_standardizer(content)

    # --- 3. æœ€åæ•‘æ´ï¼šä» <think> æ ‡ç­¾å†…éƒ¨çš„æœ€åéƒ¨åˆ†æå– ---
    # å¦‚æœä»¥ä¸Šéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œä½†æ¨¡å‹è¾“å‡ºäº† <think>ï¼Œåˆ™å°è¯•ä»å…¶å†…éƒ¨å†…å®¹ä¸­æå–ç­”æ¡ˆã€‚
    # è¿™æ•è·äº†æ¨¡å‹ä»ç„¶åœ¨ <think> é‡Œ"æ€è€ƒ"å¹¶ç»™å‡ºç­”æ¡ˆä½†æ²¡åŠ æ˜ç¡®å‰ç¼€æˆ–æ ‡ç­¾çš„æƒ…å†µã€‚
    think_match = re.search(r'<think>(.*?)</think>', cleaned_output, re.DOTALL)
    if think_match:
        think_content = think_match.group(1).strip()
        lines = [line.strip() for line in think_content.split('\n') if line.strip()]
        if lines:
            last_line_content = lines[-1]
            # ç®€å•åˆ¤æ–­æœ€åä¸€è¡Œæ˜¯å¦å¯èƒ½æ˜¯ç­”æ¡ˆï¼ˆä¾‹å¦‚ï¼ŒåŒ…å«æ•°å­—æˆ–å­—æ¯ï¼Œä¸”ä¸å¤ªé•¿ï¼‰
            if re.search(r'[-+]?\s*\(?[\d,\.]+\)?%?|[a-zA-Z]', last_line_content) and \
               not re.search(r'^(okay|let\'s|wait|but|hmm|alternatively|given|so|thus|first|to|from)', last_line_content, re.IGNORECASE) and \
               len(last_line_content.split()) < 25: # å¢åŠ é•¿åº¦é™åˆ¶ï¼Œæ’é™¤è¿‡é•¿çš„æ€è€ƒç‰‡æ®µ
                return _shared_text_standardizer(last_line_content)
    
    # å¦‚æœä»¥ä¸Šæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
    return ""

def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—F1åˆ†æ•°ï¼ŒåŒ…å«æ›´é²æ£’çš„å½’ä¸€åŒ–ï¼Œä¸ç­”æ¡ˆæå–é€»è¾‘ä¿æŒé«˜åº¦ä¸€è‡´"""
    def normalize_for_f1(text):
        return _shared_text_standardizer(text).lower().split() # è°ƒç”¨å…±äº«å‡½æ•°
    
    prediction_tokens = normalize_for_f1(prediction)
    ground_truth_tokens = normalize_for_f1(ground_truth)

    if not ground_truth_tokens: 
        return 1.0 if not prediction_tokens else 0.0
    if not prediction_tokens: 
        return 0.0

    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0: 
        return 0.0

    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def _parse_template_string_to_messages(template_full_string: str, query: str, context: str = "", table_context: str = "", text_context: str = "") -> List[Dict[str, str]]:
    """
    è§£ææ¨¡æ¿å­—ç¬¦ä¸²å¹¶æ ¼å¼åŒ–ä¸ºæ¶ˆæ¯åˆ—è¡¨ã€‚
    æ ¹æ®å½“å‰æ¨¡æ¿è®¾è®¡ï¼Œå¤„ç†åˆ†ç¦»çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ç²¾ç¡®è§£æ SYSTEM å’Œ USER å—ã€‚
    """
    # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    formatted_template = template_full_string.replace("{query}", query)
    formatted_template = formatted_template.replace("{table_context}", table_context)
    formatted_template = formatted_template.replace("{text_context}", text_context)
    
    # è§£æ ===SYSTEM=== å’Œ ===USER=== æ ‡ç­¾
    # è¿™é‡Œçš„æ­£åˆ™è¡¨è¾¾å¼éœ€è¦ç²¾ç¡®åŒ¹é… SYSTEM/USER å—
    # ä½¿ç”¨éè´ªå©ªåŒ¹é… .*? å’Œæ˜ç¡®çš„ç»“æŸè¾¹ç•Œ (?=...)
    system_match = re.search(r'===SYSTEM===\n(.*?)(?=\n===USER===|$)', formatted_template, re.DOTALL)
    user_match = re.search(r'===USER===\n(.*?)$', formatted_template, re.DOTALL) # åŒ¹é…åˆ°å­—ç¬¦ä¸²æœ«å°¾
    
    messages = []
    
    if system_match:
        system_content = system_match.group(1).strip()
        if system_content:
            messages.append({"role": "system", "content": system_content})
    
    if user_match:
        user_content = user_match.group(1).strip()
        if user_content:
            messages.append({"role": "user", "content": user_content})
    
    return messages

def load_and_format_template(template_name: str, context: str, query: str) -> List[Dict[str, str]]:
    """
    åŠ è½½å¹¶æ ¼å¼åŒ–æŒ‡å®šçš„promptæ¨¡æ¿ï¼ˆç»Ÿä¸€ä¸Šä¸‹æ–‡ï¼‰
    æ³¨æ„ï¼šæ­¤å‡½æ•°åœ¨æ–°æµç¨‹ä¸­å¯èƒ½ä¸ç›´æ¥ä½¿ç”¨ï¼Œä½†å…¶é»˜è®¤æ¨¡æ¿å·²æ›´æ–°ã€‚
    """
    template_path = Path("data/prompt_templates") / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template_full_string = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}")
        # ä½¿ç”¨ä¸æ–°ç­–ç•¥ä¸€è‡´çš„é»˜è®¤æ¨¡æ¿
        template_full_string = """===SYSTEM===
You are a helpful assistant that provides well-reasoned and detailed responses.
You MUST respond with a <think> tag containing your detailed, step-by-step reasoning process. Within the <think> tag, provide the final, direct, and concise answer on a new line prefixed with "FINAL ANSWER: ".

===USER===
Context: {context_content}

Question: {query}
<think>""" # æ›´æ–°ä¸º <think> æ ‡ç­¾å¼€å§‹
    
    return _parse_template_string_to_messages(template_full_string, query, context=context)

def load_and_format_template_with_separated_context(template_name: str, table_context: str, text_context: str, query: str) -> List[Dict[str, str]]:
    """
    åŠ è½½å¹¶æ ¼å¼åŒ–æŒ‡å®šçš„promptæ¨¡æ¿ï¼ˆåˆ†ç¦»çš„ä¸Šä¸‹æ–‡ï¼‰
    """
    template_path = Path("data/prompt_templates") / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template_full_string = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}")
        # ä½¿ç”¨ä¸æ–°ç­–ç•¥ä¸€è‡´çš„é»˜è®¤æ¨¡æ¿
        template_full_string = """===SYSTEM===
You are a helpful assistant that provides well-reasoned and detailed responses.
You MUST respond with a <think> tag containing your detailed, step-by-step reasoning process. Within the <think> tag, provide the final, direct, and concise answer on a new line prefixed with "FINAL ANSWER: ".

===USER===
Table Context: {table_context}

Text Context: {text_context}

Question: {query}
<think>""" # æ›´æ–°ä¸º <think> æ ‡ç­¾å¼€å§‹
    
    return _parse_template_string_to_messages(template_full_string, query, table_context=table_context, text_context=text_context)

def get_final_prompt(context: str, query: str) -> List[Dict[str, str]]:
    """ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡æ¿ï¼ŒåŒ…å«contextåˆ†ç¦»åŠŸèƒ½"""
    # ä½¿ç”¨åŒ…å«å—æ§æ€è€ƒè¿‡ç¨‹çš„æ–°æ¨¡æ¿
    template_file = 'unified_english_template_with_controlled_think.txt' # **ä¿®æ”¹è¿™é‡Œ**
    
    # å¼ºåˆ¶ä½¿ç”¨ä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½
    if not USE_CONTEXT_SEPARATOR:
        print("âŒ æœªå¯ç”¨ä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½ï¼Œä½†å½“å‰Promptæ¨¡æ¿è¦æ±‚åˆ†ç¦»ä¸Šä¸‹æ–‡ã€‚è„šæœ¬å°†æ— æ³•ç»§ç»­ã€‚")
        raise NotImplementedError("å½“å‰Promptæ¨¡æ¿è¦æ±‚ä¸Šä¸‹æ–‡åˆ†ç¦»ï¼Œä½†åŠŸèƒ½æœªå¯ç”¨ã€‚è¯·æ£€æŸ¥ USE_CONTEXT_SEPARATOR é…ç½®ã€‚")

    try:
        # åˆ†ç¦»ä¸Šä¸‹æ–‡
        separated = context_separator.separate_context(context)
        
        # æ ¼å¼åŒ– prompt å‚æ•°
        prompt_params = context_separator.format_for_prompt(separated, query)
        
        # ä½¿ç”¨åˆ†ç¦»åçš„ä¸Šä¸‹æ–‡æ ¼å¼åŒ–æ¨¡æ¿
        return load_and_format_template_with_separated_context(
            template_file, 
            prompt_params["table_context"], 
            prompt_params["text_context"], 
            query
        )
    except Exception as e:
        # å¦‚æœä¸Šä¸‹æ–‡åˆ†ç¦»å¤±è´¥ï¼Œæ— æ³•æ„é€ å¸¦æœ‰ table_context/text_context çš„ promptï¼Œ
        # å¹¶ä¸”æ–°æ¨¡æ¿ä¸å…¼å®¹ç»Ÿä¸€ä¸Šä¸‹æ–‡ï¼Œåˆ™ç›´æ¥æŠ›å‡ºé”™è¯¯ã€‚
        print(f"âŒ ä¸Šä¸‹æ–‡åˆ†ç¦»å¤±è´¥ï¼Œä¸”æ— æ³•ä½¿ç”¨å…¼å®¹çš„Promptæ¨¡æ¿ã€‚é”™è¯¯: {e}", file=sys.stderr)
        raise # å¼ºåˆ¶æŠ›å‡ºé”™è¯¯ï¼Œå› ä¸ºæ— æ³•æ­£ç¡®æ„é€  Prompt

# ===================================================================
# æ ¸å¿ƒè¯„ä¼°ç±»
# ===================================================================

class ComprehensiveEvaluator:
    def __init__(self, model_name: str, device: str):
        self.model_name = model_name
        self.device = device
        self.max_new_tokens = 4096
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        self.generator = LocalLLMGenerator(model_name=self.model_name, device=self.device)
        
        # æ³¨å†Œç”Ÿæˆå™¨åˆ°èµ„æºç®¡ç†å™¨
        resource_manager.set_generator(self.generator)
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def run_evaluation(self, eval_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        results = []
        start_time = time.time()
        pbar = tqdm(eval_data, desc="ğŸ” è¯„ä¼°æ ·æœ¬", unit="ä¸ª")

        try:
            for sample in pbar:
                result = self._evaluate_single_sample(sample)
                results.append(result)
                
                # å®šæœŸæ¸…ç†GPUå†…å­˜
                if len(results) % 5 == 0:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
            resource_manager.cleanup_resources()
            raise
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            resource_manager.cleanup_resources()
            raise
        finally:
            total_time = time.time() - start_time
            print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"ğŸ“Š å¤„ç†äº† {len(results)} ä¸ªç»“æœ")
        
        analysis = self.analyze_results(results)
        analysis['performance'] = {'total_time': total_time, 'avg_time_per_sample': total_time / len(results) if results else 0}
        return {"results": results, "analysis": analysis}

    def _evaluate_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        try:
            table_len_ratio, text_len_ratio = 0.0, 0.0 # åˆå§‹åŒ–ä¸º0

            if USE_CONTEXT_SEPARATOR:
                try:
                    # å‡è®¾ context_separator.separate_context èƒ½å¤Ÿè¿”å›åŸå§‹çš„ table/text é•¿åº¦
                    # ä½ éœ€è¦æ ¹æ® context_separator çš„å®é™…è¿”å›æ¥è·å–é•¿åº¦
                    # ç¤ºä¾‹ï¼š
                    # separated_data_raw = context_separator.separate_context_raw(sample["context"]) # å‡è®¾æœ‰è¿”å›åŸå§‹é•¿åº¦çš„å‡½æ•°
                    # total_context_length = len(sample["context"])
                    # if total_context_length > 0:
                    #     table_len_ratio = len(separated_data_raw.get('table_content_raw', '')) / total_context_length
                    #     text_len_ratio = len(separated_data_raw.get('text_content_raw', '')) / total_context_length
                    pass # å¦‚æœ context_separator æ— æ³•æä¾›ï¼Œåˆ™ä¿æŒä¸º0
                except Exception:
                    pass # è¿™é‡Œçš„é”™è¯¯å·²åœ¨ get_final_prompt ä¸­å¤„ç†ï¼Œæ­¤å¤„å¿½ç•¥

            messages = get_final_prompt(sample["context"], sample["query"])
            
            # è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
            prompt_text = self._convert_messages_to_text(messages)

            gen_start_time = time.time()
            # generator.generate æœŸæœ› List[str]ï¼Œæ‰€ä»¥ç”¨ [prompt_text] åŒ…è£¹
            generation_result = self.generator.generate([prompt_text])[0]
            gen_time = time.time() - gen_start_time
            
            final_answer_to_evaluate = extract_final_answer_with_rescue(generation_result)
            evaluation = self._evaluate_quality(final_answer_to_evaluate, sample["answer"])
            
            return {
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "generated_answer": generation_result,      # åŸå§‹æ¨¡å‹è¾“å‡º
                "extracted_answer": final_answer_to_evaluate, # ç»è¿‡ extract_final_answer_with_rescue å¤„ç†åçš„ç­”æ¡ˆ
                "evaluation": evaluation,
                "answer_from": sample.get("answer_from", "unknown"), 
                "predicted_answer_from": "separated_context_controlled_think", # æ›´æ–°æè¿°
                "decision_confidence": 1.0,
                "is_difficult_decision": False,
                "context_type": "separated_context",
                "content_ratio": {"table_ratio": table_len_ratio, "text_ratio": text_len_ratio, "mixed_ratio": table_len_ratio + text_len_ratio}, # mixed_ratio å¯ä»¥æ˜¯ä¸¤è€…ä¹‹å’Œ
                "generation_time": gen_time
            }
        except Exception as e:
            # è¯¦ç»†æ‰“å°é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‘ç”Ÿé”™è¯¯çš„æ ·æœ¬IDæˆ–æŸ¥è¯¢
            sample_id = sample.get("id", "N/A") # å¦‚æœä½ çš„æ ·æœ¬æœ‰ID
            print(f"\nâŒ å¤„ç†æ ·æœ¬å¤±è´¥ (ID: {sample_id}, Query: '{sample.get('query', 'N/A')[:50]}...', Error: {e})", file=sys.stderr)
            return {
                "query": sample["query"], 
                "expected_answer": sample["answer"], 
                "error": str(e),
                "context": sample.get("context", "N/A"), # åŒ…å«ä¸Šä¸‹æ–‡ä»¥ä¾›è°ƒè¯•
                "evaluation": {"exact_match": False, "f1_score": 0.0},
                "generated_answer": "", # ç¡®ä¿æœ‰æ­¤å­—æ®µï¼Œå³ä½¿æ˜¯é”™è¯¯æ ·æœ¬
                "extracted_answer": ""  # ç¡®ä¿æœ‰æ­¤å­—æ®µï¼Œå³ä½¿æ˜¯é”™è¯¯æ ·æœ¬
            }

    def _evaluate_quality(self, generated: str, expected: str) -> Dict[str, Any]:
        exact_match = generated.strip().lower() == expected.strip().lower()
        f1 = calculate_f1_score(generated, expected)
        return {"exact_match": exact_match, "f1_score": f1}

    def _convert_messages_to_text(self, messages: List[Dict[str, str]]) -> str:
        """
        å°† messages åˆ—è¡¨è½¬æ¢ä¸ºFin-R1ï¼ˆQwen2.5 basedï¼‰æœŸæœ›çš„ChatMLæ ¼å¼å­—ç¬¦ä¸²ã€‚
        """
        if not messages:
            return ""
        
        # Qwen2.5 ä½¿ç”¨ ChatML æ ¼å¼
        formatted_prompt = ""
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")
            
            if role == "system":
                formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n"
            elif role == "user":
                formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n"
            elif role == "assistant": 
                formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n"
        
        # ç§»é™¤æˆ–æ³¨é‡Šæ‰è¿™ä¸€è¡Œï¼Œå› ä¸ºPromptæ¨¡æ¿çš„æœ«å°¾å·²ç»æœ‰ <think>ï¼Œ
        # ä¸” <think> ä¼šåœ¨ user æ¶ˆæ¯ä¸­è¢«è§£æï¼Œæ¨¡å‹ä¼šç†è§£è¿™æ˜¯ç”¨æˆ·è¯·æ±‚åå¼€å§‹ç”Ÿæˆã€‚
        # formatted_prompt += "<|im_start|>assistant\n" # <-- ç§»é™¤æˆ–æ³¨é‡Šæ‰æ­¤è¡Œ
        
        return formatted_prompt

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè¯„ä¼°ç»“æœ"""
        if not results:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆç»“æœå¯åˆ†æ"}
        
        # è¿‡æ»¤æ‰æœ‰é”™è¯¯çš„æ ·æœ¬
        valid_results = [r for r in results if "error" not in r]
        error_count = len(results) - len(valid_results)
        
        if not valid_results:
            return {"error": "æ‰€æœ‰æ ·æœ¬éƒ½æœ‰é”™è¯¯", "error_count": error_count}
        
        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        exact_matches = sum(1 for r in valid_results if r["evaluation"]["exact_match"])
        f1_scores = [r["evaluation"]["f1_score"] for r in valid_results]
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_f1 = np.mean(f1_scores)
        std_f1 = np.std(f1_scores)
        median_f1 = np.median(f1_scores)
        
        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        generation_times = [r.get("generation_time", 0) for r in valid_results]
        avg_gen_time = np.mean(generation_times) if generation_times else 0
        
        return {
            "total_samples": len(results),
            "valid_samples": len(valid_results),
            "error_samples": error_count,
            "exact_match_count": exact_matches,
            "exact_match_rate": exact_matches / len(valid_results),
            "avg_f1_score": avg_f1,
            "std_f1_score": std_f1,
            "median_f1_score": median_f1,
            "min_f1_score": min(f1_scores),
            "max_f1_score": max(f1_scores),
            "avg_generation_time": avg_gen_time,
            "performance": {
                "total_time": sum(generation_times),
                "avg_time_per_sample": avg_gen_time
            }
        }

    def print_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*60)
        
        if "error" in analysis:
            print(f"âŒ åˆ†æå¤±è´¥: {analysis['error']}")
            return
        
        print(f"ğŸ“ˆ æ ·æœ¬ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {analysis['total_samples']}")
        print(f"   æœ‰æ•ˆæ ·æœ¬: {analysis['valid_samples']}")
        print(f"   é”™è¯¯æ ·æœ¬: {analysis['error_samples']}")
        
        print(f"\nğŸ¯ å‡†ç¡®ç‡æŒ‡æ ‡:")
        print(f"   ç²¾ç¡®åŒ¹é…æ•°: {analysis['exact_match_count']}")
        print(f"   ç²¾ç¡®åŒ¹é…ç‡: {analysis['exact_match_rate']:.4f} ({analysis['exact_match_rate']*100:.2f}%)")
        
        print(f"\nğŸ“Š F1åˆ†æ•°ç»Ÿè®¡:")
        print(f"   å¹³å‡F1: {analysis['avg_f1_score']:.4f}")
        print(f"   æ ‡å‡†å·®: {analysis['std_f1_score']:.4f}")
        print(f"   ä¸­ä½æ•°: {analysis['median_f1_score']:.4f}")
        print(f"   æœ€å°å€¼: {analysis['min_f1_score']:.4f}")
        print(f"   æœ€å¤§å€¼: {analysis['max_f1_score']:.4f}")
        
        print(f"\nâ±ï¸ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {analysis['avg_generation_time']:.3f}ç§’")
        if 'performance' in analysis:
            print(f"   æ€»å¤„ç†æ—¶é—´: {analysis['performance']['total_time']:.2f}ç§’")
            print(f"   å¹³å‡æ ·æœ¬æ—¶é—´: {analysis['performance']['avg_time_per_sample']:.3f}ç§’")
        
        print("="*60)

def load_evaluation_data(data_path: str, sample_size: Optional[int] = None) -> List[Dict[str, Any]]:
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    print(f"ğŸ“– æ­£åœ¨ä» {data_path} åŠ è½½æ•°æ®...")
    
    try:
        if data_path.endswith('.jsonl'):
            data = []
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip(): # é¿å…ç©ºè¡Œ
                        data.append(json.loads(line))
            print(f"âœ… æˆåŠŸåŠ è½½ä¸ºJSONLï¼Œæ ·æœ¬æ•°: {len(data)}")
        else: # å‡è®¾æ˜¯ .json æ–‡ä»¶
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼ (å…¼å®¹ TATQA å¸¸è§çš„ JSON æ ¼å¼)
            if isinstance(data, dict):
                if "data" in data: # TATQAæ•°æ®é›†é€šå¸¸æ˜¯ {"data": [...]}
                    data = data["data"]
                elif "samples" in data:
                    data = data["samples"]
                else: # å°è¯•æ‰¾åˆ°åŒ…å«åˆ—è¡¨çš„é”®
                    found_list = False
                    for key, value in data.items():
                        if isinstance(value, list):
                            data = value
                            found_list = True
                            break
                    if not found_list:
                        raise ValueError("æ— æ³•åœ¨JSONæ–‡ä»¶ä¸­æ‰¾åˆ°æ ·æœ¬æ•°æ®åˆ—è¡¨")
            if not isinstance(data, list):
                raise ValueError("æ•°æ®å¿…é¡»æ˜¯æ ·æœ¬åˆ—è¡¨æ ¼å¼")
            print(f"âœ… æˆåŠŸåŠ è½½ä¸ºJSONæ•°ç»„ï¼Œæ ·æœ¬æ•°: {len(data)}")
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if sample_size and sample_size < len(data):
            eval_data = data[:sample_size] # ä½¿ç”¨åˆ‡ç‰‡
            print(f"âœ… é™åˆ¶ä¸ºå‰ {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
        else:
            eval_data = data # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…åŸæ•°æ®è¢«ä¿®æ”¹çš„é£é™©
            print(f"âœ… åŠ è½½æ‰€æœ‰ {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
        
        return eval_data
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {data_path}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æé”™è¯¯: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½é”™è¯¯: {e}")
        sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description="å…¨é¢è¯„ä¼°è„šæœ¬")
    parser.add_argument("--model", type=str, default="SUFE-AIFLM-Lab/Fin-R1", help="è¦è¯„ä¼°çš„LLMåç§°")
    parser.add_argument("--data_path", type=str, required=True, help="è¯„ä¼°æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (jsonl æˆ– json)")
    parser.add_argument("--sample_size", type=int, default=None, help="è¦è¯„ä¼°çš„éšæœºæ ·æœ¬æ•°é‡ (Noneè¡¨ç¤ºå…¨éƒ¨)")
    parser.add_argument("--device", type=str, default="cuda:0", help="è®¾å¤‡ (cuda:0/cuda:1/cpu/auto)")
    
    args = parser.parse_args()
    
    # è®¾å¤‡è®¾ç½®
    device = args.device
    if device == "auto":
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
    elif device.startswith("cuda"): # æ£€æŸ¥æ˜¯å¦ä»¥cudaå¼€å¤´ï¼Œå…è®¸ cuda:0, cuda:1 ç­‰
        try:
            if not torch.cuda.is_available():
                print("âŒ CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
                device = "cpu"
            else:
                # å°è¯•è§£æå…·ä½“çš„GPU ID
                device_id = int(device.split(':')[1]) if ':' in device else 0
                if device_id >= torch.cuda.device_count():
                    print(f"âŒ GPU ID {device_id} ä¸å¯ç”¨ï¼Œæœ€å¤§IDä¸º {torch.cuda.device_count() - 1}ã€‚å›é€€åˆ°cuda:0")
                    device = "cuda:0"
                else:
                    device = f"cuda:{device_id}"
                    print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(device_id)}")
                    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(device_id).total_memory / 1024**3:.1f} GB")
        except (ValueError, IndexError): # å¤„ç† cuda:bad_id çš„æƒ…å†µ
            print(f"âŒ æ— æ•ˆçš„CUDAè®¾å¤‡å‚æ•° '{args.device}'ã€‚å›é€€åˆ°cuda:0")
            device = "cuda:0"
    else:
        print(f"âš ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    eval_data = load_evaluation_data(args.data_path, args.sample_size)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ComprehensiveEvaluator(args.model, device)
    
    try:
        # è¿è¡Œè¯„ä¼°
        results = evaluator.run_evaluation(eval_data)
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_summary(results["analysis"])
        
        # ä¿å­˜ç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = f"comprehensive_evaluation_results_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise
    finally:
        # ç¡®ä¿èµ„æºæ¸…ç†
        resource_manager.cleanup_resources()

if __name__ == "__main__":
    main()