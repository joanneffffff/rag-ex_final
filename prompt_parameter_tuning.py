#!/usr/bin/env python3
"""
Generator LLM Prompt å’Œå‚æ•°è°ƒä¼˜æµ‹è¯•
å›ºå®šæµ‹è¯•é—®é¢˜ï¼šå¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´åˆ©æ¶¦æŒç»­å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
"""

import sys
import os
from pathlib import Path
import json
import time

# Add parent directory to path
sys.path.append(str(Path(__file__).parent))

def test_prompt_variations():
    """æµ‹è¯•ä¸åŒçš„ Prompt å˜ä½“"""
    
    print("=== Generator LLM Prompt è°ƒä¼˜æµ‹è¯• ===")
    print("æµ‹è¯•é—®é¢˜ï¼šå¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´åˆ©æ¶¦æŒç»­å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ")
    print("=" * 70)
    
    # æµ‹è¯•æ•°æ®
    test_context = """
    å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´ä¸šç»©é¢„å‘Šæ˜¾ç¤ºï¼Œå…¬å¸é¢„è®¡å®ç°å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦ä¸º6.5äº¿å…ƒè‡³7.5äº¿å…ƒï¼Œ
    åŒæ¯”å¢é•¿11.02%è‡³28.23%ã€‚ä¸šç»©å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ï¼š
    1. iPhone 12 Pro Maxç­‰é«˜ç«¯äº§å“éœ€æ±‚å¼ºåŠ²ï¼Œå¸¦åŠ¨å…¬å¸ç”µæ± ä¸šåŠ¡å¢é•¿
    2. æ–°äº§å“ç›ˆåˆ©èƒ½åŠ›æå‡ï¼Œæ¯›åˆ©ç‡æ”¹å–„
    3. Aå®¢æˆ·ä¸šåŠ¡æŒç»­æˆé•¿ï¼Œéæ‰‹æœºä¸šåŠ¡ç¨³æ­¥å¢é•¿
    4. å¹¶è¡¨æ¯”ä¾‹å¢åŠ ï¼Œè´¡çŒ®ä¸šç»©å¢é‡
    """
    
    test_summary = "å¾·èµ›ç”µæ± 2021å¹´ä¸šç»©å¢é•¿ä¸»è¦å—ç›ŠäºiPhone 12 Pro Maxéœ€æ±‚å¼ºåŠ²å’Œæ–°å“ç›ˆåˆ©èƒ½åŠ›æå‡ã€‚"
    test_query = "å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´åˆ©æ¶¦æŒç»­å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"
    
    # ä¸åŒçš„ Prompt å˜ä½“
    prompt_variations = {
        "ç®€æ´ç‰ˆ": f"""ä½ æ˜¯ä¸€ä½é‡‘èåˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

æ‘˜è¦ï¼š{test_summary}

è¯¦ç»†å†…å®¹ï¼š{test_context}

é—®é¢˜ï¼š{test_query}

å›ç­”ï¼š""",
        
        "è¯¦ç»†ç‰ˆ": f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œæ“…é•¿åˆ†æå…¬å¸è´¢åŠ¡æŠ¥å‘Šã€‚

è¯·åŸºäºä»¥ä¸‹å…¬å¸è´¢åŠ¡æŠ¥å‘Šä¿¡æ¯ï¼Œå‡†ç¡®å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

ã€è´¢åŠ¡æŠ¥å‘Šæ‘˜è¦ã€‘
{test_summary}

ã€è¯¦ç»†è´¢åŠ¡æ•°æ®ã€‘
{test_context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{test_query}

è¯·æä¾›å‡†ç¡®ã€ç®€æ´çš„åˆ†æå›ç­”ï¼š""",
        
        "æŒ‡ä»¤ç‰ˆ": f"""ä½ æ˜¯ä¸€ä½é‡‘èåˆ†æå¸ˆã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š

è¦æ±‚ï¼š
1. åŸºäºæä¾›çš„è´¢åŠ¡ä¿¡æ¯å›ç­”
2. å›ç­”ç®€æ´ï¼Œæ§åˆ¶åœ¨2-3å¥è¯å†…
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œå›ç­”"æ ¹æ®ç°æœ‰ä¿¡æ¯ï¼Œæ— æ³•æä¾›æ­¤é¡¹ä¿¡æ¯ã€‚"
4. ä¸è¦åŒ…å«ä»»ä½•æ ¼å¼æ ‡è®°æˆ–é¢å¤–è¯´æ˜

ä¿¡æ¯ï¼š
{test_summary}

{test_context}

é—®é¢˜ï¼š{test_query}

å›ç­”ï¼š""",
        
        "é—®ç­”ç‰ˆ": f"""åŸºäºä»¥ä¸‹è´¢åŠ¡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

{test_summary}

{test_context}

é—®é¢˜ï¼š{test_query}

ç­”æ¡ˆï¼š""",
        
        "åˆ†æç‰ˆ": f"""ä½œä¸ºé‡‘èåˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹è´¢åŠ¡æ•°æ®å¹¶å›ç­”é—®é¢˜ï¼š

è´¢åŠ¡æ‘˜è¦ï¼š{test_summary}

è¯¦ç»†æ•°æ®ï¼š{test_context}

åˆ†æé—®é¢˜ï¼š{test_query}

åˆ†æç»“æœï¼š"""
    }
    
    return prompt_variations, test_query

def test_parameter_variations():
    """æµ‹è¯•ä¸åŒçš„å‚æ•°ç»„åˆ"""
    
    parameter_sets = {
        "ä¿å®ˆå‹": {
            "temperature": 0.1,
            "top_p": 0.7,
            "max_new_tokens": 200,
            "repetition_penalty": 1.2
        },
        "å¹³è¡¡å‹": {
            "temperature": 0.2,
            "top_p": 0.8,
            "max_new_tokens": 300,
            "repetition_penalty": 1.3
        },
        "åˆ›é€ æ€§": {
            "temperature": 0.3,
            "top_p": 0.9,
            "max_new_tokens": 400,
            "repetition_penalty": 1.1
        },
        "ç²¾ç¡®å‹": {
            "temperature": 0.05,
            "top_p": 0.6,
            "max_new_tokens": 150,
            "repetition_penalty": 1.4
        }
    }
    
    return parameter_sets

def evaluate_response(response, query):
    """è¯„ä¼°å“åº”è´¨é‡"""
    
    # è´¨é‡æŒ‡æ ‡
    indicators = {
        "ç®€æ´æ€§": {
            "score": 0,
            "max": 25,
            "description": "å›ç­”é•¿åº¦é€‚ä¸­ï¼ˆ50-200å­—ç¬¦ï¼‰"
        },
        "å‡†ç¡®æ€§": {
            "score": 0,
            "max": 25,
            "description": "åŒ…å«å…³é”®ä¿¡æ¯ï¼ˆå¾·èµ›ç”µæ± ã€iPhoneã€éœ€æ±‚ç­‰ï¼‰"
        },
        "çº¯ç²¹æ€§": {
            "score": 0,
            "max": 25,
            "description": "æ— æ ¼å¼æ ‡è®°ã€å¼•å¯¼è¯­ç­‰"
        },
        "å®Œæ•´æ€§": {
            "score": 0,
            "max": 25,
            "description": "å¥å­å®Œæ•´ï¼Œæœ‰æ˜ç¡®ç»“è®º"
        }
    }
    
    # ç®€æ´æ€§è¯„åˆ†
    length = len(response.strip())
    if 50 <= length <= 200:
        indicators["ç®€æ´æ€§"]["score"] = 25
    elif 30 <= length <= 300:
        indicators["ç®€æ´æ€§"]["score"] = 15
    else:
        indicators["ç®€æ´æ€§"]["score"] = 5
    
    # å‡†ç¡®æ€§è¯„åˆ†
    key_terms = ["å¾·èµ›ç”µæ± ", "iPhone", "éœ€æ±‚", "å¢é•¿", "åˆ©æ¶¦", "ä¸šç»©"]
    found_terms = sum(1 for term in key_terms if term in response)
    indicators["å‡†ç¡®æ€§"]["score"] = min(25, found_terms * 4)
    
    # çº¯ç²¹æ€§è¯„åˆ†
    unwanted_patterns = ["ã€", "ã€‘", "å›ç­”ï¼š", "Answer:", "---", "===", "___"]
    has_unwanted = any(pattern in response for pattern in unwanted_patterns)
    indicators["çº¯ç²¹æ€§"]["score"] = 0 if has_unwanted else 25
    
    # å®Œæ•´æ€§è¯„åˆ†
    if response.strip().endswith(("ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?")):
        indicators["å®Œæ•´æ€§"]["score"] = 25
    elif len(response.strip()) > 20:
        indicators["å®Œæ•´æ€§"]["score"] = 15
    else:
        indicators["å®Œæ•´æ€§"]["score"] = 5
    
    # è®¡ç®—æ€»åˆ†
    total_score = sum(ind["score"] for ind in indicators.values())
    max_score = sum(ind["max"] for ind in indicators.values())
    
    return indicators, total_score, max_score

def run_single_test(generator, prompt, params, test_name):
    """è¿è¡Œå•æ¬¡æµ‹è¯•"""
    
    print(f"\nğŸ” æµ‹è¯•: {test_name}")
    print("-" * 50)
    
    # ä¸´æ—¶ä¿®æ”¹å‚æ•°
    original_params = {
        "temperature": generator.temperature,
        "top_p": generator.top_p,
        "max_new_tokens": generator.max_new_tokens
    }
    
    try:
        # åº”ç”¨æ–°å‚æ•°
        generator.temperature = params.get("temperature", 0.2)
        generator.top_p = params.get("top_p", 0.8)
        generator.max_new_tokens = params.get("max_new_tokens", 300)
        
        print(f"å‚æ•°: temp={generator.temperature}, top_p={generator.top_p}, max_tokens={generator.max_new_tokens}")
        print(f"Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        # ç”Ÿæˆå“åº”
        start_time = time.time()
        responses = generator.generate([prompt])
        end_time = time.time()
        
        response = responses[0] if responses else "ç”Ÿæˆå¤±è´¥"
        generation_time = end_time - start_time
        
        print(f"ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
        print(f"å“åº”é•¿åº¦: {len(response)} å­—ç¬¦")
        print(f"å“åº”å†…å®¹: {response}")
        
        return response, generation_time
        
    finally:
        # æ¢å¤åŸå§‹å‚æ•°
        generator.temperature = original_params["temperature"]
        generator.top_p = original_params["top_p"]
        generator.max_new_tokens = original_params["max_new_tokens"]

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    
    try:
        # å¯¼å…¥æ¨¡å—
        from xlm.components.generator.local_llm_generator import LocalLLMGenerator
        
        print("1. åˆå§‹åŒ– LLM ç”Ÿæˆå™¨...")
        generator = LocalLLMGenerator()
        print(f"âœ… ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ: {generator.model_name}")
        
        # è·å–æµ‹è¯•æ•°æ®
        prompt_variations, test_query = test_prompt_variations()
        parameter_sets = test_parameter_variations()
        
        # å­˜å‚¨æµ‹è¯•ç»“æœ
        results = []
        
        # æµ‹è¯•æ‰€æœ‰ç»„åˆ
        for prompt_name, prompt in prompt_variations.items():
            for param_name, params in parameter_sets.items():
                test_name = f"{prompt_name} + {param_name}"
                
                try:
                    response, generation_time = run_single_test(generator, prompt, params, test_name)
                    
                    # è¯„ä¼°è´¨é‡
                    indicators, total_score, max_score = evaluate_response(response, test_query)
                    
                    # å­˜å‚¨ç»“æœ
                    result = {
                        "test_name": test_name,
                        "prompt_name": prompt_name,
                        "param_name": param_name,
                        "response": response,
                        "generation_time": generation_time,
                        "total_score": total_score,
                        "max_score": max_score,
                        "score_percentage": (total_score / max_score) * 100,
                        "indicators": indicators
                    }
                    results.append(result)
                    
                    print(f"è´¨é‡è¯„åˆ†: {total_score}/{max_score} ({result['score_percentage']:.1f}%)")
                    
                except Exception as e:
                    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
                    continue
        
        # åˆ†æç»“æœ
        print("\n" + "=" * 70)
        print("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 70)
        
        # æŒ‰è¯„åˆ†æ’åº
        results.sort(key=lambda x: x["score_percentage"], reverse=True)
        
        print("\nğŸ† æœ€ä½³ç»„åˆ:")
        for i, result in enumerate(results[:5]):
            print(f"{i+1}. {result['test_name']}: {result['score_percentage']:.1f}%")
            print(f"   å“åº”: {result['response'][:100]}...")
        
        print("\nğŸ“ˆ è¯¦ç»†åˆ†æ:")
        for result in results:
            print(f"\n{result['test_name']}: {result['score_percentage']:.1f}%")
            print(f"  ç®€æ´æ€§: {result['indicators']['ç®€æ´æ€§']['score']}/25")
            print(f"  å‡†ç¡®æ€§: {result['indicators']['å‡†ç¡®æ€§']['score']}/25")
            print(f"  çº¯ç²¹æ€§: {result['indicators']['çº¯ç²¹æ€§']['score']}/25")
            print(f"  å®Œæ•´æ€§: {result['indicators']['å®Œæ•´æ€§']['score']}/25")
            print(f"  ç”Ÿæˆæ—¶é—´: {result['generation_time']:.2f}ç§’")
        
        # ä¿å­˜ç»“æœ
        with open("prompt_tuning_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: prompt_tuning_results.json")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    main() 