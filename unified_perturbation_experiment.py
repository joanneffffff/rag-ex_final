#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„RAGæ‰°åŠ¨å®éªŒç³»ç»Ÿ
é›†æˆæ‰€æœ‰æ‰°åŠ¨å™¨ã€ç‰¹å¾æå–ã€LLM Judgeè¯„ä¼°
"""

import sys
import os
import json
import time
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from xlm.components.rag_system.rag_system import RagSystem
from xlm.components.retriever.enhanced_retriever import EnhancedRetriever
from xlm.components.generator.local_llm_generator import LocalLLMGenerator
from xlm.modules.perturber.leave_one_out_perturber import LeaveOneOutPerturber
from xlm.modules.perturber.reorder_perturber import ReorderPerturber
from xlm.modules.perturber.trend_perturber import TrendPerturber
from xlm.modules.perturber.year_perturber import YearPerturber
from xlm.modules.perturber.term_perturber import TermPerturber
from xlm.modules.feature_extractor import FeatureExtractor, Granularity
from config.parameters import Config

@dataclass
class PerturbationResult:
    """æ‰°åŠ¨å®éªŒç»“æœæ•°æ®ç»“æ„"""
    sample_id: str
    query: str
    original_context: str
    original_answer: str
    expected_answer: str
    
    perturber_name: str
    perturbation_detail: str
    perturbed_context: str
    perturbed_answer: str
    
    f1_original_vs_expected: float
    f1_perturbed_vs_expected: float
    f1_perturbed_vs_original: float
    
    llm_judge_score_accuracy: Optional[float] = None
    llm_judge_score_completeness: Optional[float] = None
    llm_judge_score_professionalism: Optional[float] = None
    llm_judge_reasoning: Optional[str] = None
    
    timestamp: Optional[str] = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()

class UnifiedPerturbationExperiment:
    """ç»Ÿä¸€çš„æ‰°åŠ¨å®éªŒç³»ç»Ÿ"""
    
    def __init__(self):
        """åˆå§‹åŒ–å®éªŒç³»ç»Ÿ"""
        print("ğŸ”¬ åˆå§‹åŒ–ç»Ÿä¸€æ‰°åŠ¨å®éªŒç³»ç»Ÿ...")
        
        # åŠ è½½é…ç½®
        self.config = Config()
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿç»„ä»¶
        self.generator = LocalLLMGenerator()
        self.retriever = EnhancedRetriever(config=self.config)
        self.rag_system = RagSystem(
            retriever=self.retriever,
            generator=self.generator,
            retriever_top_k=5
        )
        
        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        self.feature_extractor = FeatureExtractor(language="zh")
        
        # åˆå§‹åŒ–æ‰€æœ‰æ‰°åŠ¨å™¨
        self.perturbers = {
            "leave_one_out": LeaveOneOutPerturber(),
            "reorder": ReorderPerturber(),
            "trend": TrendPerturber(),
            "year": YearPerturber(),
            "term": TermPerturber()
        }
        
        print("âœ… å®éªŒç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š å¯ç”¨æ‰°åŠ¨å™¨: {list(self.perturbers.keys())}")
    
    def calculate_f1_score(self, answer1: str, answer2: str) -> float:
        """è®¡ç®—F1åˆ†æ•°ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        if not answer1 or not answer2:
            return 0.0
        
        # ç®€å•çš„è¯æ±‡é‡å è®¡ç®—
        words1 = set(answer1.lower().split())
        words2 = set(answer2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = len(words1.intersection(words2))
        precision = intersection / len(words1) if words1 else 0
        recall = intersection / len(words2) if words2 else 0
        
        if precision + recall == 0:
            return 0.0
        
        return 2 * (precision * recall) / (precision + recall)
    
    def run_llm_judge_evaluation(self, original_answer: str, perturbed_answer: str, 
                                expected_answer: str, query: str) -> Dict[str, Any]:
        """è¿è¡ŒLLM Judgeè¯„ä¼°"""
        try:
            # æ„å»ºè¯„ä¼°prompt
            judge_prompt = f"""
è¯·è¯„ä¼°ä»¥ä¸‹ä¸¤ä¸ªç­”æ¡ˆçš„è´¨é‡ï¼Œé’ˆå¯¹é—®é¢˜ï¼š{query}

æ ‡å‡†ç­”æ¡ˆï¼š{expected_answer}

ç­”æ¡ˆAï¼š{original_answer}
ç­”æ¡ˆBï¼š{perturbed_answer}

è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦è¯„åˆ†ï¼ˆ1-10åˆ†ï¼‰ï¼š
1. å‡†ç¡®æ€§ï¼šç­”æ¡ˆæ˜¯å¦å‡†ç¡®å›ç­”äº†é—®é¢˜
2. å®Œæ•´æ€§ï¼šç­”æ¡ˆæ˜¯å¦åŒ…å«äº†æ‰€æœ‰å¿…è¦ä¿¡æ¯
3. ä¸“ä¸šæ€§ï¼šç­”æ¡ˆæ˜¯å¦ä½¿ç”¨äº†æ­£ç¡®çš„ä¸“ä¸šæœ¯è¯­

è¯·ç»™å‡ºè¯„åˆ†å’Œç†ç”±ï¼š
"""
            
            # ä½¿ç”¨ç”Ÿæˆå™¨è¿›è¡Œè¯„ä¼°
            judge_response = self.generator.generate([judge_prompt])
            
            # è§£æè¯„åˆ†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            scores = {
                'accuracy': 5.0,  # é»˜è®¤åˆ†æ•°
                'completeness': 5.0,
                'professionalism': 5.0,
                'reasoning': judge_response
            }
            
            return scores
            
        except Exception as e:
            print(f"LLM Judgeè¯„ä¼°å¤±è´¥: {e}")
            return {
                'accuracy': 5.0,
                'completeness': 5.0,
                'professionalism': 5.0,
                'reasoning': f"è¯„ä¼°å¤±è´¥: {str(e)}"
            }
    
    def run_single_perturbation_experiment(self, sample: Dict[str, Any], 
                                         perturber_name: str) -> List[PerturbationResult]:
        """è¿è¡Œå•ä¸ªæ‰°åŠ¨å™¨çš„å®éªŒ"""
        results = []
        
        try:
            query = sample['query']
            expected_answer = sample['answer']
            
            # 1. è¿è¡Œæ ‡å‡†RAG
            print(f"ğŸ” è¿è¡Œæ ‡å‡†RAG...")
            rag_result = self.rag_system.run(query)
            original_answer = rag_result.generated_responses[0]
            original_context = "\n\n".join([doc.content for doc in rag_result.retrieved_documents])
            
            # 2. æå–ç‰¹å¾
            features = self.feature_extractor.extract_features(original_context, Granularity.WORD)
            print(f"ğŸ“Š æå–äº† {len(features)} ä¸ªç‰¹å¾")
            
            # 3. åº”ç”¨æ‰°åŠ¨
            perturber = self.perturbers[perturber_name]
            perturbations = perturber.perturb(original_context, features)
            
            print(f"ğŸ”„ {perturber_name} ç”Ÿæˆäº† {len(perturbations)} ä¸ªæ‰°åŠ¨")
            
            # 4. å¯¹æ¯ä¸ªæ‰°åŠ¨è¿è¡ŒRAG
            for i, perturbation in enumerate(perturbations):
                if isinstance(perturbation, dict):
                    # BasePerturberè¿”å›å­—å…¸æ ¼å¼
                    perturbed_context = perturbation['perturbed_text']
                    perturbation_detail = perturbation['perturbation_detail']
                else:
                    # å…¼å®¹å­—ç¬¦ä¸²æ ¼å¼
                    perturbed_context = perturbation
                    perturbation_detail = f"Perturbation {i+1} from {perturber_name}"
                
                # æ„å»ºæ‰°åŠ¨åçš„prompt
                perturbed_prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

ä¸Šä¸‹æ–‡ï¼š{perturbed_context}

é—®é¢˜ï¼š{query}

è¯·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„å›ç­”ï¼š"""
                
                # ç”Ÿæˆæ‰°åŠ¨åçš„ç­”æ¡ˆ
                perturbed_response = self.generator.generate([perturbed_prompt])
                
                # è®¡ç®—F1åˆ†æ•°
                f1_original_vs_expected = self.calculate_f1_score(original_answer, expected_answer)
                f1_perturbed_vs_expected = self.calculate_f1_score(perturbed_response[0], expected_answer)
                f1_perturbed_vs_original = self.calculate_f1_score(perturbed_response[0], original_answer)
                
                # è¿è¡ŒLLM Judgeè¯„ä¼°
                judge_scores = self.run_llm_judge_evaluation(
                    original_answer, perturbed_response[0], expected_answer, query
                )
                
                # åˆ›å»ºç»“æœå¯¹è±¡
                result = PerturbationResult(
                    sample_id=sample.get('id', f'sample_{i}'),
                    query=query,
                    original_context=original_context,
                    original_answer=original_answer,
                    expected_answer=expected_answer,
                    perturber_name=perturber_name,
                    perturbation_detail=perturbation_detail,
                    perturbed_context=perturbed_context,
                    perturbed_answer=perturbed_response[0],
                    f1_original_vs_expected=f1_original_vs_expected,
                    f1_perturbed_vs_expected=f1_perturbed_vs_expected,
                    f1_perturbed_vs_original=f1_perturbed_vs_original,
                    llm_judge_score_accuracy=judge_scores['accuracy'],
                    llm_judge_score_completeness=judge_scores['completeness'],
                    llm_judge_score_professionalism=judge_scores['professionalism'],
                    llm_judge_reasoning=judge_scores['reasoning']
                )
                
                results.append(result)
                
                print(f"âœ… æ‰°åŠ¨ {i+1}/{len(perturbations)} å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ {perturber_name} æ‰°åŠ¨å®éªŒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        return results
    
    def run_comprehensive_experiment(self, samples: List[Dict[str, Any]]) -> List[PerturbationResult]:
        """è¿è¡Œå…¨é¢çš„æ‰°åŠ¨å®éªŒ"""
        all_results = []
        
        print(f"ğŸš€ å¼€å§‹å…¨é¢æ‰°åŠ¨å®éªŒï¼Œå…± {len(samples)} ä¸ªæ ·æœ¬")
        
        for i, sample in enumerate(samples):
            print(f"\n{'='*20} æ ·æœ¬ {i+1}/{len(samples)} {'='*20}")
            print(f"é—®é¢˜: {sample['query']}")
            
            # å¯¹æ¯ä¸ªæ‰°åŠ¨å™¨è¿è¡Œå®éªŒ
            for perturber_name in self.perturbers.keys():
                print(f"\n--- æµ‹è¯• {perturber_name} ---")
                results = self.run_single_perturbation_experiment(sample, perturber_name)
                all_results.extend(results)
                
                print(f"âœ… {perturber_name}: {len(results)} ä¸ªç»“æœ")
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            if (i + 1) % 5 == 0:
                self.save_results(all_results, f"partial_results_{i+1}.json")
        
        return all_results
    
    def save_results(self, results: List[PerturbationResult], filename: str):
        """ä¿å­˜å®éªŒç»“æœ"""
        data = [asdict(result) for result in results]
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")
    
    def analyze_results(self, results: List[PerturbationResult]) -> Dict[str, Any]:
        """åˆ†æå®éªŒç»“æœ"""
        analysis = {
            'total_experiments': len(results),
            'perturber_stats': {},
            'f1_score_analysis': {},
            'llm_judge_analysis': {}
        }
        
        # æŒ‰æ‰°åŠ¨å™¨åˆ†ç»„
        perturber_groups = {}
        for result in results:
            if result.perturber_name not in perturber_groups:
                perturber_groups[result.perturber_name] = []
            perturber_groups[result.perturber_name].append(result)
        
        # åˆ†ææ¯ä¸ªæ‰°åŠ¨å™¨
        for perturber_name, group_results in perturber_groups.items():
            f1_scores = [r.f1_perturbed_vs_expected for r in group_results]
            accuracy_scores = [r.llm_judge_score_accuracy for r in group_results if r.llm_judge_score_accuracy]
            
            analysis['perturber_stats'][perturber_name] = {
                'count': len(group_results),
                'avg_f1_score': sum(f1_scores) / len(f1_scores) if f1_scores else 0,
                'avg_accuracy_score': sum(accuracy_scores) / len(accuracy_scores) if accuracy_scores else 0
            }
        
        return analysis

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª ç»Ÿä¸€RAGæ‰°åŠ¨å®éªŒç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»ºå®éªŒå®ä¾‹
    experiment = UnifiedPerturbationExperiment()
    
    # æµ‹è¯•æ ·æœ¬
    test_samples = [
        {
            'id': 'sample_1',
            'query': 'é¦–é’¢è‚¡ä»½åœ¨2023å¹´ä¸ŠåŠå¹´çš„ä¸šç»©è¡¨ç°å¦‚ä½•ï¼Ÿ',
            'answer': 'é¦–é’¢è‚¡ä»½åœ¨2023å¹´ä¸ŠåŠå¹´ä¸šç»©è¡¨ç°è‰¯å¥½ï¼Œè¥æ”¶å¢é•¿15%ï¼Œå‡€åˆ©æ¶¦å¢é•¿20%'
        },
        {
            'id': 'sample_2', 
            'query': 'ä¸­å›½å¹³å®‰çš„è´¢åŠ¡çŠ¶å†µæ€ä¹ˆæ ·ï¼Ÿ',
            'answer': 'ä¸­å›½å¹³å®‰è´¢åŠ¡çŠ¶å†µç¨³å¥ï¼Œæ€»èµ„äº§è¶…è¿‡10ä¸‡äº¿å…ƒï¼Œå‡€åˆ©æ¶¦æŒç»­å¢é•¿'
        }
    ]
    
    # è¿è¡Œå®éªŒ
    results = experiment.run_comprehensive_experiment(test_samples)
    
    # ä¿å­˜ç»“æœ
    experiment.save_results(results, 'unified_perturbation_results.json')
    
    # åˆ†æç»“æœ
    analysis = experiment.analyze_results(results)
    
    print(f"\nğŸ“Š å®éªŒåˆ†æç»“æœ:")
    print(f"æ€»å®éªŒæ•°: {analysis['total_experiments']}")
    print(f"æ‰°åŠ¨å™¨ç»Ÿè®¡:")
    for perturber_name, stats in analysis['perturber_stats'].items():
        print(f"  {perturber_name}: {stats['count']} ä¸ªå®éªŒ, å¹³å‡F1: {stats['avg_f1_score']:.3f}")
    
    print(f"\nğŸ‰ å®éªŒå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° unified_perturbation_results.json")

if __name__ == "__main__":
    main() 