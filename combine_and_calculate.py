#!/usr/bin/env python3
"""
åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶å¹¶è®¡ç®—æ•´ä½“çš„F1å’ŒEMåˆ†æ•°
"""

import json
import os
import re
import jieba
from pathlib import Path
from typing import Dict, List, Any

def normalize_answer_chinese(s: str) -> str:
    """æ ‡å‡†åŒ–ä¸­æ–‡ç­”æ¡ˆ"""
    if not s:
        return ""
    
    # ç§»é™¤"è§£æ"åŠå…¶åé¢çš„å†…å®¹
    # æŸ¥æ‰¾"è§£æ"çš„ä½ç½®ï¼Œç§»é™¤å®ƒåŠå…¶åé¢çš„æ‰€æœ‰å†…å®¹
    parse_index = s.find("è§£æ")
    if parse_index != -1:
        s = s[:parse_index]
    
    s = ' '.join(s.split())
    s = re.sub(r'[^\u4e00-\u9fff\w\s]', '', s)
    return s.strip()

def normalize_answer_english(s: str) -> str:
    """æ ‡å‡†åŒ–è‹±æ–‡ç­”æ¡ˆ"""
    if not s:
        return ""
    s = ' '.join(s.split())
    s = re.sub(r'[^\w\s]', '', s)
    return s.strip().lower()

def get_tokens_chinese(s: str) -> List[str]:
    """ä½¿ç”¨jiebaåˆ†è¯è·å–ä¸­æ–‡tokenåˆ—è¡¨"""
    return list(jieba.cut(s))

def get_tokens_english(s: str) -> List[str]:
    """è·å–è‹±æ–‡tokenåˆ—è¡¨"""
    return s.split()

def calculate_f1_score(prediction: str, ground_truth: str, language: str = "chinese") -> float:
    """è®¡ç®—F1-scoreï¼Œæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡"""
    if language == "chinese":
        pred_tokens = set(get_tokens_chinese(normalize_answer_chinese(prediction)))
        gt_tokens = set(get_tokens_chinese(normalize_answer_chinese(ground_truth)))
    else:
        pred_tokens = set(get_tokens_english(normalize_answer_english(prediction)))
        gt_tokens = set(get_tokens_english(normalize_answer_english(ground_truth)))
    
    if not gt_tokens:
        return 1.0 if not pred_tokens else 0.0
    
    intersection = pred_tokens & gt_tokens
    precision = len(intersection) / len(pred_tokens) if pred_tokens else 0.0
    recall = len(intersection) / len(gt_tokens)
    
    if precision + recall == 0:
        return 0.0
    
    return 2 * precision * recall / (precision + recall)

def calculate_exact_match(prediction: str, ground_truth: str, language: str = "chinese") -> float:
    """è®¡ç®—Exact Matchï¼Œæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡"""
    if language == "chinese":
        pred_normalized = normalize_answer_chinese(prediction)
        gt_normalized = normalize_answer_chinese(ground_truth)
    else:
        pred_normalized = normalize_answer_english(prediction)
        gt_normalized = normalize_answer_english(ground_truth)
    
    return 1.0 if pred_normalized == gt_normalized else 0.0

def process_answer(answer: str) -> str:
    """
    ç§»é™¤ç­”æ¡ˆä¸­çš„[Reranker: Enabled]æ–‡æœ¬
    """
    if not answer:
        return answer
    
    # ç§»é™¤[Reranker: Enabled]å‰ç¼€
    answer = re.sub(r'^\[Reranker: Enabled\]\s*', '', answer)
    
    return answer.strip()

def load_and_process_batch(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½å¹¶å¤„ç†å•ä¸ªæ‰¹æ¬¡æ–‡ä»¶
    """
    print(f"ğŸ“ åŠ è½½æ–‡ä»¶: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    processed_samples = []
    
    for sample in data["data"]:
        original_answer = sample.get("answer", "")
        expected_answer = sample.get("expected_answer", "")
        language = sample.get("language", "chinese")
        
        # ç§»é™¤[Reranker: Enabled]æ–‡æœ¬
        cleaned_answer = process_answer(original_answer)
        
        # é‡æ–°è®¡ç®—F1å’ŒEMåˆ†æ•°
        f1_score = calculate_f1_score(cleaned_answer, expected_answer, language)
        exact_match = calculate_exact_match(cleaned_answer, expected_answer, language)
        
        # åˆ›å»ºå¤„ç†åçš„æ ·æœ¬
        processed_sample = {
            "sample_id": sample.get("sample_id", 0),
            "query": sample.get("query", ""),
            "summary_context": sample.get("summary_context", ""),
            "answer": cleaned_answer,
            "expected_answer": expected_answer,
            "f1": f1_score,
            "em": exact_match,
            "processing_time": sample.get("processing_time", 0.0),
            "generation_time": sample.get("generation_time", 0.0),
            "token_count": sample.get("token_count", 0),
            "success": sample.get("success", True),
            "language": language,
            "auto_stock_prediction": sample.get("auto_stock_prediction", False)
        }
        
        processed_samples.append(processed_sample)
    
    print(f"âœ… å¤„ç†å®Œæˆ: {len(processed_samples)} ä¸ªæ ·æœ¬")
    return processed_samples

def combine_all_batches(data_dir: str) -> Dict[str, Any]:
    """
    åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶
    """
    print(f"ğŸš€ å¼€å§‹åˆå¹¶ç›®å½•: {data_dir}")
    
    if not os.path.exists(data_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return {}
    
    # è·å–æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶
    batch_files = []
    for file in os.listdir(data_dir):
        if file.startswith('batch_') and file.endswith('.json'):
            batch_files.append(os.path.join(data_dir, file))
    
    batch_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(batch_files)} ä¸ªæ‰¹æ¬¡æ–‡ä»¶")
    
    # åˆå¹¶æ‰€æœ‰æ ·æœ¬
    all_samples = []
    total_processing_time = 0.0
    total_generation_time = 0.0
    total_token_count = 0
    successful_samples = 0
    failed_samples = 0
    stock_prediction_samples = 0
    
    for file_path in batch_files:
        samples = load_and_process_batch(file_path)
        all_samples.extend(samples)
        
        # ç´¯è®¡ç»Ÿè®¡
        for sample in samples:
            total_processing_time += sample.get("processing_time", 0.0)
            total_generation_time += sample.get("generation_time", 0.0)
            total_token_count += sample.get("token_count", 0)
            
            if sample.get("success", True):
                successful_samples += 1
            else:
                failed_samples += 1
            
            if sample.get("auto_stock_prediction", False):
                stock_prediction_samples += 1
    
    # è®¡ç®—æ€»ä½“æŒ‡æ ‡
    total_samples = len(all_samples)
    total_f1 = sum(sample["f1"] for sample in all_samples)
    total_em = sum(sample["em"] for sample in all_samples)
    
    avg_f1 = total_f1 / total_samples if total_samples > 0 else 0.0
    avg_em = total_em / total_samples if total_samples > 0 else 0.0
    avg_processing_time = total_processing_time / total_samples if total_samples > 0 else 0.0
    avg_generation_time = total_generation_time / total_samples if total_samples > 0 else 0.0
    avg_token_count = total_token_count / total_samples if total_samples > 0 else 0.0
    
    # æ„å»ºåˆå¹¶ç»“æœ
    combined_result = {
        "timestamp": "2025-07-14 00:00:00",
        "data_path": "data/alphafin/alphafin_eval_samples_updated.jsonl",
        "total_samples": total_samples,
        "successful_samples": successful_samples,
        "failed_samples": failed_samples,
        "success_rate": (successful_samples / total_samples * 100) if total_samples > 0 else 0.0,
        "avg_f1_score": avg_f1,
        "avg_exact_match": avg_em,
        "avg_processing_time": avg_processing_time,
        "total_processing_time": total_processing_time,
        "avg_generation_time": avg_generation_time,
        "avg_token_count": avg_token_count,
        "total_token_count": total_token_count,
        "stock_prediction_samples": stock_prediction_samples,
        "reranker_enabled": True,
        "stock_prediction_enabled": stock_prediction_samples > 0,
        "auto_detected_stock_prediction": stock_prediction_samples,
        "data": all_samples
    }
    
    return combined_result

def save_combined_result(result: Dict[str, Any], output_file: str):
    """
    ä¿å­˜åˆå¹¶ç»“æœ
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ åˆå¹¶ç»“æœä¿å­˜åˆ°: {output_file}")

def print_summary(result: Dict[str, Any]):
    """
    æ‰“å°ç»“æœæ‘˜è¦
    """
    print("\n" + "=" * 80)
    print("ğŸ“Š æ•°æ®é›†æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 80)
    print(f"ğŸ“ æ•°æ®è·¯å¾„: {result['data_path']}")
    print(f"ğŸŒ è¯­è¨€: chinese")
    print(f"ğŸ“ˆ æ€»æ ·æœ¬æ•°: {result['total_samples']}")
    print(f"âœ… æˆåŠŸæ ·æœ¬æ•°: {result['successful_samples']}")
    print(f"âŒ å¤±è´¥æ ·æœ¬æ•°: {result['failed_samples']}")
    print(f"ğŸ“Š æˆåŠŸç‡: {result['success_rate']:.2f}%")
    print(f"ğŸ¯ å¹³å‡F1-score: {result['avg_f1_score']:.4f}")
    print(f"ğŸ¯ å¹³å‡Exact Match: {result['avg_exact_match']:.4f}")
    print(f"â±ï¸ å¹³å‡å¤„ç†æ—¶é—´: {result['avg_processing_time']:.2f}ç§’")
    print(f"â±ï¸ æ€»å¤„ç†æ—¶é—´: {result['total_processing_time']:.2f}ç§’")
    print(f"â±ï¸ å¹³å‡ç”Ÿæˆæ—¶é—´: {result['avg_generation_time']:.2f}ç§’")
    print(f"ğŸ”¢ å¹³å‡Tokenæ•°: {result['avg_token_count']:.1f}")
    print(f"ğŸ”¢ æ€»Tokenæ•°: {result['total_token_count']}")
    print(f"ğŸ”® é‡æ’åºå™¨: {'å¯ç”¨' if result['reranker_enabled'] else 'ç¦ç”¨'}")
    print(f"ğŸ”® è‚¡ç¥¨é¢„æµ‹: {'å¯ç”¨' if result['stock_prediction_enabled'] else 'ç¦ç”¨'}")
    print(f"ğŸ”® è‡ªåŠ¨æ£€æµ‹è‚¡ç¥¨é¢„æµ‹: {result['auto_detected_stock_prediction']} ä¸ª")
    print("=" * 80)

def main():
    """
    ä¸»å‡½æ•°
    """
    data_dir = "raw_data_alphafin_eval_samples_updated"
    output_file = "combined_alphafin_results.json"
    
    print("ğŸ”§ å¼€å§‹åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶...")
    print("ğŸ“ ä»»åŠ¡: ç§»é™¤[Reranker: Enabled]æ–‡æœ¬å¹¶è®¡ç®—æ•´ä½“F1å’ŒEMåˆ†æ•°")
    print("=" * 60)
    
    # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡
    result = combine_all_batches(data_dir)
    
    if result:
        # ä¿å­˜åˆå¹¶ç»“æœ
        save_combined_result(result, output_file)
        
        # æ‰“å°æ‘˜è¦
        print_summary(result)
        
        print("\nğŸ‰ åˆå¹¶å®Œæˆï¼")
    else:
        print("âŒ åˆå¹¶å¤±è´¥ï¼")

if __name__ == "__main__":
    main() 