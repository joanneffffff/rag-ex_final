#!/usr/bin/env python3
"""
åˆå¹¶TAT-QAè®­ç»ƒå’Œè¯„ä¼°æ•°æ®ä½œä¸ºçŸ¥è¯†åº“
"""

import json
from pathlib import Path
from typing import List, Dict, Any

def combine_tatqa_knowledge_base():
    """åˆå¹¶TAT-QAè®­ç»ƒå’Œè¯„ä¼°æ•°æ®ä½œä¸ºçŸ¥è¯†åº“"""
    
    # è¾“å…¥æ–‡ä»¶è·¯å¾„
    train_file = "evaluate_mrr/tatqa_train_qc_enhanced.jsonl"
    eval_file = "evaluate_mrr/tatqa_eval_enhanced.jsonl"
    
    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = "data/unified/tatqa_knowledge_base_combined.jsonl"
    
    print("ğŸ”„ å¼€å§‹åˆå¹¶TAT-QAçŸ¥è¯†åº“...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰æ–‡æ¡£
    all_documents = []
    doc_id_counter = 0
    
    # å¤„ç†è®­ç»ƒæ–‡ä»¶
    print(f"ğŸ“– å¤„ç†è®­ç»ƒæ–‡ä»¶: {train_file}")
    if Path(train_file).exists():
        with open(train_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        item = json.loads(line)
                        
                        # æå–contextä½œä¸ºçŸ¥è¯†åº“å†…å®¹
                        context = item.get('context', '')
                        if context:
                            doc = {
                                'doc_id': f"train_{doc_id_counter}",
                                'content': context,
                                'source': 'tatqa_train',
                                'language': 'english',
                                'created_at': '',
                                'author': ''
                            }
                            all_documents.append(doc)
                            doc_id_counter += 1
                            
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ è®­ç»ƒæ–‡ä»¶ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
                        continue
    else:
        print(f"âŒ è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
    
    print(f"âœ… ä»è®­ç»ƒæ–‡ä»¶åŠ è½½äº† {doc_id_counter} ä¸ªæ–‡æ¡£")
    
    # å¤„ç†è¯„ä¼°æ–‡ä»¶
    print(f"ğŸ“– å¤„ç†è¯„ä¼°æ–‡ä»¶: {eval_file}")
    eval_start_id = doc_id_counter
    
    if Path(eval_file).exists():
        with open(eval_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        item = json.loads(line)
                        
                        # æå–contextä½œä¸ºçŸ¥è¯†åº“å†…å®¹
                        context = item.get('context', '')
                        if context:
                            doc = {
                                'doc_id': f"eval_{doc_id_counter}",
                                'content': context,
                                'source': 'tatqa_eval',
                                'language': 'english',
                                'created_at': '',
                                'author': ''
                            }
                            all_documents.append(doc)
                            doc_id_counter += 1
                            
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ è¯„ä¼°æ–‡ä»¶ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
                        continue
    else:
        print(f"âŒ è¯„ä¼°æ–‡ä»¶ä¸å­˜åœ¨: {eval_file}")
    
    print(f"âœ… ä»è¯„ä¼°æ–‡ä»¶åŠ è½½äº† {doc_id_counter - eval_start_id} ä¸ªæ–‡æ¡£")
    
    # å»é‡ï¼ˆåŸºäºcontentå†…å®¹ï¼‰
    print("ğŸ”„ å»é‡å¤„ç†...")
    unique_docs = []
    seen_contents = set()
    
    for doc in all_documents:
        content = doc['content'].strip()
        if content and content not in seen_contents:
            unique_docs.append(doc)
            seen_contents.add(content)
    
    print(f"âœ… å»é‡åä¿ç•™ {len(unique_docs)} ä¸ªå”¯ä¸€æ–‡æ¡£")
    
    # å†™å…¥åˆå¹¶åçš„æ–‡ä»¶
    print(f"ğŸ’¾ å†™å…¥åˆå¹¶æ–‡ä»¶: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        for doc in unique_docs:
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')
    
    print(f"ğŸ‰ åˆå¹¶å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ€»æ–‡æ¡£æ•°: {len(unique_docs)}")
    print(f"   - è®­ç»ƒæ–‡æ¡£: {len([d for d in unique_docs if d['source'] == 'tatqa_train'])}")
    print(f"   - è¯„ä¼°æ–‡æ¡£: {len([d for d in unique_docs if d['source'] == 'tatqa_eval'])}")
    print(f"   - è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    return output_file

if __name__ == "__main__":
    combine_tatqa_knowledge_base() 