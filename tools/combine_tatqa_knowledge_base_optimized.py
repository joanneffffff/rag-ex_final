#!/usr/bin/env python3
"""
åˆå¹¶TAT-QAè®­ç»ƒå’Œè¯„ä¼°æ•°æ®ä½œä¸ºçŸ¥è¯†åº“ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
ä½¿ç”¨ä¼˜åŒ–åŽçš„è¡¨æ ¼æ–‡æœ¬åŒ–æ•°æ®
æ ¹æ®æ•°æ®ç±»åž‹é‡‡ç”¨ä¸åŒçš„åŽ»é‡ç­–ç•¥ï¼š
- å•ä¸ªè¡¨æ ¼ï¼šæŒ‰table_idåŽ»é‡
- å•ä¸ªæ®µè½ï¼šæŒ‰paragraph_idåŽ»é‡
- è¡¨æ ¼+æ–‡æœ¬ï¼šæŒ‰(table_id + paragraph_id)ç»„åˆåŽ»é‡ï¼Œåªä¿ç•™ä¸€ä¸ª
æ·»åŠ è¡¨æ ¼å®Œæ•´æ€§æ£€æŸ¥ï¼Œè¿‡æ»¤æŽ‰ä¸å®Œæ•´çš„è¡¨æ ¼æ•°æ®
"""

import json
from pathlib import Path
from typing import List, Dict, Any
import re

def is_complete_table(context: str) -> bool:
    """
    æ£€æŸ¥è¡¨æ ¼æ˜¯å¦å®Œæ•´
    å®Œæ•´è¡¨æ ¼åº”è¯¥åŒ…å«ï¼š
    1. Table ID
    2. Table columns
    3. è‡³å°‘ä¸€è¡Œå…·ä½“çš„æ•°æ®ï¼ˆåŒ…å«"For"å¼€å¤´çš„è¡Œï¼‰
    """
    if not context:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼æ ‡è¯†
    if "Table ID:" not in context:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆ—ä¿¡æ¯
    if "Table columns:" not in context:
        return False
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æ•°æ®è¡Œï¼ˆä»¥"For"å¼€å¤´çš„è¡Œï¼‰
    # è¿™æ˜¯åˆ¤æ–­è¡¨æ ¼æ˜¯å¦å®Œæ•´çš„å…³é”®æŒ‡æ ‡
    for_lines = re.findall(r'For [^:]+:', context)
    if len(for_lines) == 0:
        print(f"âš ï¸ å‘çŽ°ä¸å®Œæ•´è¡¨æ ¼ï¼Œç¼ºå°‘å…·ä½“æ•°æ®è¡Œ: {context[:100]}...")
        return False
    
    return True

def get_data_type(context: str) -> str:
    """
    åˆ¤æ–­æ•°æ®ç±»åž‹ï¼š
    - 'table': åªåŒ…å«è¡¨æ ¼æ•°æ®ï¼ˆæœ‰Table IDä½†æ²¡æœ‰Paragraph IDï¼‰
    - 'paragraph': åªåŒ…å«æ®µè½æ•°æ®ï¼ˆæœ‰Paragraph IDä½†æ²¡æœ‰Table IDï¼‰
    - 'table+text': åŒ…å«è¡¨æ ¼å’Œæ®µè½æ•°æ®ï¼ˆåŒæ—¶æœ‰Table IDå’ŒParagraph IDï¼‰
    """
    has_table = "Table ID:" in context
    has_paragraph = "Paragraph ID:" in context
    
    if has_table and has_paragraph:
        return "table+text"
    elif has_table:
        return "table"
    elif has_paragraph:
        return "paragraph"
    else:
        return "unknown"

def combine_tatqa_knowledge_base_optimized():
    """åˆå¹¶TAT-QAè®­ç»ƒå’Œè¯„ä¼°æ•°æ®ä½œä¸ºçŸ¥è¯†åº“ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
    
    # è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨enhancedç‰ˆæœ¬ï¼‰
    train_file = "evaluate_mrr/tatqa_train_qc_enhanced.jsonl"
    eval_file = "evaluate_mrr/tatqa_eval_enhanced.jsonl"
    
    # è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = "data/unified/tatqa_knowledge_base_combined.jsonl"
    
    print("ðŸ”„ å¼€å§‹åˆå¹¶TAT-QAçŸ¥è¯†åº“ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰...")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰æ–‡æ¡£
    all_documents = []
    doc_id_counter = 0
    incomplete_tables_count = 0
    
    # å¤„ç†è®­ç»ƒæ–‡ä»¶
    print(f"ðŸ“– å¤„ç†è®­ç»ƒæ–‡ä»¶: {train_file}")
    if Path(train_file).exists():
        with open(train_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        item = json.loads(line)
                        
                        # æå–contextä½œä¸ºçŸ¥è¯†åº“å†…å®¹
                        context = item.get('context', '')
                        if context:
                            # æ£€æŸ¥è¡¨æ ¼å®Œæ•´æ€§
                            if not is_complete_table(context):
                                incomplete_tables_count += 1
                                continue
                            
                            # æå–table_idå’Œparagraph_idç”¨äºŽåŽ»é‡åˆ¤æ–­
                            table_id = item.get('table_id', '')
                            paragraph_id = item.get('paragraph_id', '')
                            relevant_doc_ids = item.get('relevant_doc_ids', [])
                            
                            # åˆ¤æ–­æ•°æ®ç±»åž‹
                            data_type = get_data_type(context)
                            
                            doc = {
                                'doc_id': f"train_optimized_{doc_id_counter}",
                                'context': context,
                                'source': 'tatqa_train_optimized',
                                'language': 'english',
                                'created_at': '',
                                'author': '',
                                'table_id': table_id,
                                'paragraph_id': paragraph_id,
                                'relevant_doc_ids': relevant_doc_ids,
                                'data_type': data_type
                            }
                            all_documents.append(doc)
                            doc_id_counter += 1
                            
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ è®­ç»ƒæ–‡ä»¶ç¬¬{line_num}è¡ŒJSONè§£æžé”™è¯¯: {e}")
                        continue
    else:
        print(f"âŒ è®­ç»ƒæ–‡ä»¶ä¸å­˜åœ¨: {train_file}")
        return None
    
    print(f"âœ… ä»Žè®­ç»ƒæ–‡ä»¶åŠ è½½äº† {doc_id_counter} ä¸ªå®Œæ•´æ–‡æ¡£")
    print(f"âš ï¸ è¿‡æ»¤æŽ‰äº† {incomplete_tables_count} ä¸ªä¸å®Œæ•´è¡¨æ ¼")
    
    # å¤„ç†è¯„ä¼°æ–‡ä»¶
    print(f"ðŸ“– å¤„ç†è¯„ä¼°æ–‡ä»¶: {eval_file}")
    eval_start_id = doc_id_counter
    eval_incomplete_count = 0
    
    if Path(eval_file).exists():
        with open(eval_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if line.strip():
                    try:
                        item = json.loads(line)
                        
                        # æå–contextä½œä¸ºçŸ¥è¯†åº“å†…å®¹
                        context = item.get('context', '')
                        if context:
                            # æ£€æŸ¥è¡¨æ ¼å®Œæ•´æ€§
                            if not is_complete_table(context):
                                eval_incomplete_count += 1
                                continue
                            
                            # æå–table_idå’Œparagraph_idç”¨äºŽåŽ»é‡åˆ¤æ–­
                            table_id = item.get('table_id', '')
                            paragraph_id = item.get('paragraph_id', '')
                            relevant_doc_ids = item.get('relevant_doc_ids', [])
                            
                            # åˆ¤æ–­æ•°æ®ç±»åž‹
                            data_type = get_data_type(context)
                            
                            doc = {
                                'doc_id': f"eval_optimized_{doc_id_counter}",
                                'context': context,
                                'source': 'tatqa_eval_optimized',
                                'language': 'english',
                                'created_at': '',
                                'author': '',
                                'table_id': table_id,
                                'paragraph_id': paragraph_id,
                                'relevant_doc_ids': relevant_doc_ids,
                                'data_type': data_type
                            }
                            all_documents.append(doc)
                            doc_id_counter += 1
                            
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ è¯„ä¼°æ–‡ä»¶ç¬¬{line_num}è¡ŒJSONè§£æžé”™è¯¯: {e}")
                        continue
    else:
        print(f"âŒ è¯„ä¼°æ–‡ä»¶ä¸å­˜åœ¨: {eval_file}")
        return None
    
    print(f"âœ… ä»Žè¯„ä¼°æ–‡ä»¶åŠ è½½äº† {doc_id_counter - eval_start_id} ä¸ªå®Œæ•´æ–‡æ¡£")
    print(f"âš ï¸ è¿‡æ»¤æŽ‰äº† {eval_incomplete_count} ä¸ªä¸å®Œæ•´è¡¨æ ¼")
    
    # å…ˆåˆå¹¶æ‰€æœ‰æ•°æ®ï¼Œç„¶åŽæŒ‰æ•°æ®ç±»åž‹åˆ†ç»„åŽ»é‡
    print("ðŸ”„ å…ˆåˆå¹¶æ‰€æœ‰æ•°æ®ï¼Œç„¶åŽæŒ‰æ•°æ®ç±»åž‹åˆ†ç»„åŽ»é‡...")
    
    # æŒ‰æ•°æ®ç±»åž‹åˆ†ç»„
    table_docs = []  # å•ä¸ªè¡¨æ ¼
    paragraph_docs = []  # å•ä¸ªæ®µè½
    table_text_docs = []  # è¡¨æ ¼+æ–‡æœ¬
    
    for doc in all_documents:
        data_type = doc.get('data_type', 'unknown')
        if data_type == 'table':
            table_docs.append(doc)
        elif data_type == 'paragraph':
            paragraph_docs.append(doc)
        elif data_type == 'table+text':
            table_text_docs.append(doc)
        else:
            print(f"âš ï¸ æœªçŸ¥æ•°æ®ç±»åž‹: {data_type}, doc_id: {doc['doc_id']}")
    
    print(f"ðŸ“Š æ•°æ®ç±»åž‹ç»Ÿè®¡:")
    print(f"   - å•ä¸ªè¡¨æ ¼: {len(table_docs)}")
    print(f"   - å•ä¸ªæ®µè½: {len(paragraph_docs)}")
    print(f"   - è¡¨æ ¼+æ–‡æœ¬: {len(table_text_docs)}")
    
    # åŽ»é‡å¤„ç†
    unique_docs = []
    
    # 1. å•ä¸ªè¡¨æ ¼ï¼šæŒ‰relevant_doc_idsåŽ»é‡ï¼Œåªä¿ç•™ä¸€ä¸ªæ–‡æ¡£ä½†ä¿å­˜æ‰€æœ‰ç›¸å…³ID
    print("ðŸ”„ å¤„ç†å•ä¸ªè¡¨æ ¼æ•°æ®ï¼ˆæŒ‰relevant_doc_idsåŽ»é‡ï¼Œåªä¿ç•™ä¸€ä¸ªæ–‡æ¡£ï¼‰...")
    table_seen = {}
    for doc in table_docs:
        relevant_doc_ids = tuple(sorted(doc.get('relevant_doc_ids', [])))
        if relevant_doc_ids not in table_seen:
            table_seen[relevant_doc_ids] = doc
            unique_docs.append(doc)
        else:
            # åªä¿ç•™ç¬¬ä¸€ä¸ªæ–‡æ¡£ï¼Œä½†æ”¶é›†æ‰€æœ‰ç›¸å…³ID
            existing_doc = table_seen[relevant_doc_ids]
            if 'all_doc_ids' not in existing_doc:
                existing_doc['all_doc_ids'] = [existing_doc['doc_id']]
            existing_doc['all_doc_ids'].append(doc['doc_id'])
            print(f"  - è¡¨æ ¼åŽ»é‡: {doc['doc_id']} -> {existing_doc['doc_id']} (relevant_doc_ids: {relevant_doc_ids})")
    
    # 2. å•ä¸ªæ®µè½ï¼šæŒ‰relevant_doc_idsåŽ»é‡ï¼Œåªä¿ç•™ä¸€ä¸ªæ–‡æ¡£ä½†ä¿å­˜æ‰€æœ‰ç›¸å…³ID
    print("ðŸ”„ å¤„ç†å•ä¸ªæ®µè½æ•°æ®ï¼ˆæŒ‰relevant_doc_idsåŽ»é‡ï¼Œåªä¿ç•™ä¸€ä¸ªæ–‡æ¡£ï¼‰...")
    paragraph_seen = {}
    for doc in paragraph_docs:
        relevant_doc_ids = tuple(sorted(doc.get('relevant_doc_ids', [])))
        if relevant_doc_ids not in paragraph_seen:
            paragraph_seen[relevant_doc_ids] = doc
            unique_docs.append(doc)
        else:
            # åªä¿ç•™ç¬¬ä¸€ä¸ªæ–‡æ¡£ï¼Œä½†æ”¶é›†æ‰€æœ‰ç›¸å…³ID
            existing_doc = paragraph_seen[relevant_doc_ids]
            if 'all_doc_ids' not in existing_doc:
                existing_doc['all_doc_ids'] = [existing_doc['doc_id']]
            existing_doc['all_doc_ids'].append(doc['doc_id'])
            print(f"  - æ®µè½åŽ»é‡: {doc['doc_id']} -> {existing_doc['doc_id']} (relevant_doc_ids: {relevant_doc_ids})")
    
    # 3. è¡¨æ ¼+æ–‡æœ¬ï¼šæŒ‰relevant_doc_idsåŽ»é‡ï¼Œåªä¿ç•™ä¸€ä¸ª
    print("ðŸ”„ å¤„ç†è¡¨æ ¼+æ–‡æœ¬æ•°æ®ï¼ˆæŒ‰relevant_doc_idsåŽ»é‡ï¼Œåªä¿ç•™ä¸€ä¸ªï¼‰...")
    table_text_seen = {}
    for doc in table_text_docs:
        relevant_doc_ids = tuple(sorted(doc.get('relevant_doc_ids', [])))
        if relevant_doc_ids not in table_text_seen:
            table_text_seen[relevant_doc_ids] = doc
            unique_docs.append(doc)
        else:
            # åªä¿ç•™ç¬¬ä¸€ä¸ªï¼Œä¸åˆå¹¶IDä¿¡æ¯
            existing_doc = table_text_seen[relevant_doc_ids]
            print(f"  - è¡¨æ ¼+æ–‡æœ¬åŽ»é‡: {doc['doc_id']} -> {existing_doc['doc_id']} (relevant_doc_ids: {relevant_doc_ids})")
    
    print(f"âœ… åŽ»é‡åŽä¿ç•™ {len(unique_docs)} ä¸ªæ–‡æ¡£")
    
    # å†™å…¥åˆå¹¶åŽçš„æ–‡ä»¶
    print(f"ðŸ’¾ å†™å…¥åˆå¹¶æ–‡ä»¶: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        for doc in unique_docs:
            # ä¿ç•™æ ‡å‡†å­—æ®µï¼Œå¹¶æ·»åŠ æ–°çš„IDå­—æ®µ
            output_doc = {
                'doc_id': doc['doc_id'],
                'context': doc['context'],
                'source': doc['source'],
                'language': doc['language'],
                'created_at': doc['created_at'],
                'author': doc['author'],
                'data_type': doc['data_type']
            }
            
            # æ·»åŠ IDç›¸å…³å­—æ®µï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
            if 'all_doc_ids' in doc:
                output_doc['all_doc_ids'] = doc['all_doc_ids']
            
            f.write(json.dumps(output_doc, ensure_ascii=False) + '\n')
    
    print(f"ðŸŽ‰ åˆå¹¶å®Œæˆï¼")
    print(f"ðŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ€»æ–‡æ¡£æ•°: {len(unique_docs)}")
    print(f"   - å•ä¸ªè¡¨æ ¼: {len([d for d in unique_docs if d['data_type'] == 'table'])}")
    print(f"   - å•ä¸ªæ®µè½: {len([d for d in unique_docs if d['data_type'] == 'paragraph'])}")
    print(f"   - è¡¨æ ¼+æ–‡æœ¬: {len([d for d in unique_docs if d['data_type'] == 'table+text'])}")
    print(f"   - è®­ç»ƒæ–‡æ¡£: {len([d for d in unique_docs if d['source'] == 'tatqa_train_optimized'])}")
    print(f"   - è¯„ä¼°æ–‡æ¡£: {len([d for d in unique_docs if d['source'] == 'tatqa_eval_optimized'])}")
    print(f"   - è¿‡æ»¤çš„ä¸å®Œæ•´è¡¨æ ¼: {incomplete_tables_count + eval_incomplete_count}")
    print(f"   - è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    # æ˜¾ç¤ºä¸€äº›æ ·æœ¬å†…å®¹
    print(f"\nðŸ“‹ æ ·æœ¬å†…å®¹é¢„è§ˆ:")
    for i, doc in enumerate(unique_docs[:3]):
        print(f"\næ ·æœ¬ {i+1} (ID: {doc['doc_id']}, ç±»åž‹: {doc['data_type']}):")
        content = doc['context']
        if len(content) > 200:
            print(f"   {content[:200]}...")
        else:
            print(f"   {content}")
    
    return output_file

if __name__ == "__main__":
    combine_tatqa_knowledge_base_optimized() 