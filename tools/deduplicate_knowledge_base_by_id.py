#!/usr/bin/env python3
"""
åŸºäºTable IDå’ŒParagraph IDçš„çŸ¥è¯†åº“å»é‡è„šæœ¬
è§£å†³çŸ¥è¯†åº“ä¸­ç›¸åŒIDä½†ä¸åŒå•ä½è¯´æ˜çš„é‡å¤é—®é¢˜
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Any

def extract_table_id(context: str) -> str:
    """ä»contextä¸­æå–Table ID"""
    if "Table ID:" in context:
        match = re.search(r'Table ID:\s*([a-f0-9-]+)', context)
        if match:
            return match.group(1)
    return ""

def extract_paragraph_id(context: str) -> str:
    """ä»contextä¸­æå–Paragraph ID"""
    if "Paragraph ID:" in context:
        match = re.search(r'Paragraph ID:\s*([a-f0-9-]+)', context)
        if match:
            return match.group(1)
    return ""

def deduplicate_knowledge_base_by_id():
    """åŸºäºTable IDå’ŒParagraph IDå»é‡çŸ¥è¯†åº“"""
    
    # è¾“å…¥å’Œè¾“å‡ºæ–‡ä»¶
    input_file = "data/unified/tatqa_knowledge_base_combined.jsonl"
    output_file = "data/unified/tatqa_knowledge_base_deduplicated.jsonl"
    
    print("ğŸ”„ å¼€å§‹åŸºäºTable IDå’ŒParagraph IDå»é‡çŸ¥è¯†åº“...")
    
    if not Path(input_file).exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return None
    
    # è¯»å–æ‰€æœ‰æ–‡æ¡£
    documents = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                try:
                    doc = json.loads(line)
                    documents.append(doc)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
                    continue
    
    print(f"ğŸ“– è¯»å–äº† {len(documents)} ä¸ªæ–‡æ¡£")
    
    # åˆ†ç±»æ–‡æ¡£
    table_docs = []  # åŒ…å«Table IDçš„æ–‡æ¡£
    paragraph_docs = []  # åªåŒ…å«Paragraph IDçš„æ–‡æ¡£
    other_docs = []  # å…¶ä»–æ–‡æ¡£
    
    for doc in documents:
        context = doc.get('context', '')
        table_id = extract_table_id(context)
        paragraph_id = extract_paragraph_id(context)
        
        if table_id:
            # åŒ…å«Table IDçš„æ–‡æ¡£ï¼ˆåŒ…æ‹¬è¡¨æ ¼+æ–‡æœ¬ï¼‰
            table_docs.append({
                'doc': doc,
                'table_id': table_id,
                'paragraph_id': paragraph_id
            })
        elif paragraph_id:
            # åªåŒ…å«Paragraph IDçš„æ–‡æ¡£
            paragraph_docs.append({
                'doc': doc,
                'paragraph_id': paragraph_id
            })
        else:
            # å…¶ä»–æ–‡æ¡£
            other_docs.append(doc)
    
    print(f"ğŸ“Š æ–‡æ¡£åˆ†ç±»:")
    print(f"   - è¡¨æ ¼æ–‡æ¡£: {len(table_docs)}")
    print(f"   - æ®µè½æ–‡æ¡£: {len(paragraph_docs)}")
    print(f"   - å…¶ä»–æ–‡æ¡£: {len(other_docs)}")
    
    # åŸºäºTable IDå»é‡
    print("\nğŸ”„ åŸºäºTable IDå»é‡...")
    unique_table_docs = []
    seen_table_ids = set()
    table_duplicates = 0
    
    for item in table_docs:
        table_id = item['table_id']
        if table_id in seen_table_ids:
            table_duplicates += 1
            print(f"  âŒ è·³è¿‡é‡å¤Table ID: {table_id}")
            continue
        
        seen_table_ids.add(table_id)
        unique_table_docs.append(item['doc'])
        print(f"  âœ… ä¿ç•™Table ID: {table_id}")
    
    print(f"  ğŸ“Š Table IDå»é‡ç»“æœ: {len(unique_table_docs)} ä¸ªæ–‡æ¡£ï¼Œç§»é™¤ {table_duplicates} ä¸ªé‡å¤")
    
    # åŸºäºParagraph IDå»é‡
    print("\nğŸ”„ åŸºäºParagraph IDå»é‡...")
    unique_paragraph_docs = []
    seen_paragraph_ids = set()
    paragraph_duplicates = 0
    
    for item in paragraph_docs:
        paragraph_id = item['paragraph_id']
        if paragraph_id in seen_paragraph_ids:
            paragraph_duplicates += 1
            print(f"  âŒ è·³è¿‡é‡å¤Paragraph ID: {paragraph_id}")
            continue
        
        seen_paragraph_ids.add(paragraph_id)
        unique_paragraph_docs.append(item['doc'])
        print(f"  âœ… ä¿ç•™Paragraph ID: {paragraph_id}")
    
    print(f"  ğŸ“Š Paragraph IDå»é‡ç»“æœ: {len(unique_paragraph_docs)} ä¸ªæ–‡æ¡£ï¼Œç§»é™¤ {paragraph_duplicates} ä¸ªé‡å¤")
    
    # åˆå¹¶æ‰€æœ‰å”¯ä¸€æ–‡æ¡£
    all_unique_docs = unique_table_docs + unique_paragraph_docs + other_docs
    
    print(f"\nğŸ“Š å»é‡æ€»ç»“:")
    print(f"   - åŸå§‹æ–‡æ¡£æ•°: {len(documents)}")
    print(f"   - å»é‡åæ–‡æ¡£æ•°: {len(all_unique_docs)}")
    print(f"   - ç§»é™¤é‡å¤æ•°: {len(documents) - len(all_unique_docs)}")
    print(f"   - å»é‡ç‡: {(len(documents) - len(all_unique_docs)) / len(documents) * 100:.2f}%")
    
    # å†™å…¥å»é‡åçš„æ–‡ä»¶
    print(f"\nğŸ’¾ å†™å…¥å»é‡æ–‡ä»¶: {output_file}")
    Path(output_file).parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for doc in all_unique_docs:
            f.write(json.dumps(doc, ensure_ascii=False) + '\n')
    
    # æ˜¾ç¤ºä¸€äº›æ ·æœ¬
    print(f"\nğŸ“‹ å»é‡åæ ·æœ¬é¢„è§ˆ:")
    for i, doc in enumerate(all_unique_docs[:3]):
        print(f"\næ ·æœ¬ {i+1} (ID: {doc['doc_id']}):")
        context = doc['context']
        table_id = extract_table_id(context)
        paragraph_id = extract_paragraph_id(context)
        
        if table_id:
            print(f"  Table ID: {table_id}")
        if paragraph_id:
            print(f"  Paragraph ID: {paragraph_id}")
        
        if len(context) > 150:
            print(f"  å†…å®¹: {context[:150]}...")
        else:
            print(f"  å†…å®¹: {context}")
    
    print(f"\nğŸ‰ å»é‡å®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_file}")
    
    return output_file

if __name__ == "__main__":
    deduplicate_knowledge_base_by_id() 