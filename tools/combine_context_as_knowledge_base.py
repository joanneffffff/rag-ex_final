#!/usr/bin/env python3
"""
å°†è®­ç»ƒæ•°æ®å’Œè¯„ä¼°æ•°æ®çš„contextåˆå¹¶æˆçŸ¥è¯†åº“
é¿å…æ•°æ®æ³„éœ²ï¼ŒåŒæ—¶ç¡®ä¿è¯„ä¼°æ•°æ®çš„contextåœ¨çŸ¥è¯†åº“ä¸­
"""

import json
from pathlib import Path

def combine_context_as_knowledge_base():
    """åˆå¹¶contextä½œä¸ºçŸ¥è¯†åº“"""
    
    # è¾“å…¥æ–‡ä»¶
    train_file = "evaluate_mrr/tatqa_train_qc_enhanced.jsonl"
    eval_file = "evaluate_mrr/tatqa_eval_enhanced.jsonl"
    
    # è¾“å‡ºæ–‡ä»¶
    knowledge_base_file = "evaluate_mrr/tatqa_knowledge_base.jsonl"
    
    print("ğŸ”„ åˆå¹¶contextä½œä¸ºçŸ¥è¯†åº“...")
    
    # è¯»å–è®­ç»ƒæ•°æ®
    train_contexts = []
    with open(train_file, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            train_contexts.append({
                "text": item.get("context", ""),
                "doc_id": item.get("doc_id", ""),
                "source_type": "train"
            })
    
    print(f"âœ… è¯»å–è®­ç»ƒæ•°æ®: {len(train_contexts)} ä¸ªcontext")
    
    # è¯»å–è¯„ä¼°æ•°æ®
    eval_contexts = []
    with open(eval_file, "r", encoding="utf-8") as f:
        for line in f:
            item = json.loads(line)
            eval_contexts.append({
                "text": item.get("context", ""),
                "doc_id": item.get("doc_id", ""),
                "source_type": "eval"
            })
    
    print(f"âœ… è¯»å–è¯„ä¼°æ•°æ®: {len(eval_contexts)} ä¸ªcontext")
    
    # åˆå¹¶æ‰€æœ‰context
    all_contexts = train_contexts + eval_contexts
    
    # å»é‡ï¼ˆåŸºäºdoc_idï¼‰
    unique_contexts = {}
    for ctx in all_contexts:
        doc_id = ctx["doc_id"]
        if doc_id not in unique_contexts:
            unique_contexts[doc_id] = ctx
        else:
            # å¦‚æœå·²å­˜åœ¨ï¼Œä¿ç•™è®­ç»ƒæ•°æ®çš„ç‰ˆæœ¬
            if ctx["source_type"] == "train":
                unique_contexts[doc_id] = ctx
    
    print(f"âœ… å»é‡å: {len(unique_contexts)} ä¸ªå”¯ä¸€context")
    
    # å†™å…¥çŸ¥è¯†åº“æ–‡ä»¶
    with open(knowledge_base_file, "w", encoding="utf-8") as f:
        for ctx in unique_contexts.values():
            f.write(json.dumps(ctx, ensure_ascii=False) + "\n")
    
    print(f"âœ… çŸ¥è¯†åº“å·²ç”Ÿæˆ: {knowledge_base_file}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    train_count = sum(1 for ctx in unique_contexts.values() if ctx["source_type"] == "train")
    eval_count = sum(1 for ctx in unique_contexts.values() if ctx["source_type"] == "eval")
    
    print(f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡:")
    print(f"  è®­ç»ƒæ•°æ®context: {train_count}")
    print(f"  è¯„ä¼°æ•°æ®context: {eval_count}")
    print(f"  æ€»è®¡: {len(unique_contexts)}")

if __name__ == "__main__":
    combine_context_as_knowledge_base() 