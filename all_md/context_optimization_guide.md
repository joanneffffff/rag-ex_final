# ä¸Šä¸‹æ–‡é•¿åº¦å¤§å¹…ç¼©çŸ­æŠ€æœ¯å®ç°æŒ‡å—

## ğŸ“Š ä¼˜åŒ–æ•ˆæœå¯¹æ¯”

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æ”¹è¿›å¹…åº¦ |
|------|--------|--------|----------|
| ä¸Šä¸‹æ–‡å­—ç¬¦æ•° | 11,960 | 216 | **98.2%** â†“ |
| Tokenæ•°é‡ | 9,247 | 166 | **98.2%** â†“ |
| æ£€ç´¢æ–‡æ¡£æ•° | å…¨é‡(33,842) | ç²¾ç¡®è¿‡æ»¤(5) | **99.985%** â†“ |
| å¤„ç†æ—¶é—´ | è¾ƒé•¿ | å¿«é€Ÿ | **æ˜¾è‘—æå‡** |

## ğŸ”§ æ ¸å¿ƒæŠ€æœ¯å®ç°

### 1. å…ƒæ•°æ®é¢„è¿‡æ»¤ (Metadata Pre-filtering)

#### 1.1 å…ƒæ•°æ®ç´¢å¼•æ„å»º
```python
def _build_metadata_index(self):
    """æ„å»ºå…ƒæ•°æ®ç´¢å¼•ï¼Œæ”¯æŒå¿«é€Ÿè¿‡æ»¤"""
    self.metadata_index = defaultdict(dict)
    
    for idx, record in enumerate(self.data):
        company = record.get('company_name', '')
        stock_code = record.get('stock_code', '')
        report_date = record.get('report_date', '')
        
        # å…¬å¸åç§°ç´¢å¼•
        if company:
            self.metadata_index['company_name'][company].append(idx)
        
        # è‚¡ç¥¨ä»£ç ç´¢å¼•
        if stock_code:
            self.metadata_index['stock_code'][stock_code].append(idx)
        
        # æŠ¥å‘Šæ—¥æœŸç´¢å¼•
        if report_date:
            self.metadata_index['report_date'][report_date].append(idx)
        
        # å…¬å¸+è‚¡ç¥¨ç»„åˆç´¢å¼•
        if company and stock_code:
            key = f"{company}_{stock_code}"
            self.metadata_index['company_stock'][key].append(idx)
```

#### 1.2 æ™ºèƒ½å…ƒæ•°æ®æå–
```python
def extract_metadata(query: str) -> tuple[str, str]:
    """ä»æŸ¥è¯¢ä¸­æå–å…ƒæ•°æ®"""
    stock_code = None
    company_name = None
    
    # æå–è‚¡ç¥¨ä»£ç  - æ”¯æŒå¤šç§æ ¼å¼
    stock_patterns = [
        r'(\d{6})',           # 6ä½æ•°å­—
        r'([A-Z]{2}\d{4})',   # 2å­—æ¯+4æ•°å­—
        r'([A-Z]{2}\d{6})',   # 2å­—æ¯+6æ•°å­—
    ]
    
    for pattern in stock_patterns:
        match = re.search(pattern, query)
        if match:
            stock_code = match.group(1)
            break
    
    # æå–å…¬å¸åç§° - æ”¯æŒä¸­è‹±æ–‡æ‹¬å·
    company_patterns = [
        r'([^ï¼ˆ(]+)ï¼ˆ',  # ä¸­æ–‡æ‹¬å·
        r'([^(]+)\(',   # è‹±æ–‡æ‹¬å·
    ]
    
    for pattern in company_patterns:
        match = re.search(pattern, query)
        if match:
            company_name = match.group(1).strip()
            break
    
    return company_name, stock_code
```

#### 1.3 é¢„è¿‡æ»¤é€»è¾‘
```python
def pre_filter(self, company_name=None, stock_code=None, report_date=None):
    """å…ƒæ•°æ®é¢„è¿‡æ»¤ï¼Œå¤§å¹…å‡å°‘å€™é€‰æ–‡æ¡£æ•°é‡"""
    candidate_indices = []
    
    if company_name and stock_code:
        # ç»„åˆè¿‡æ»¤ï¼šå…¬å¸+è‚¡ç¥¨ä»£ç 
        key = f"{company_name}_{stock_code}"
        if key in self.metadata_index['company_stock']:
            candidate_indices = self.metadata_index['company_stock'][key]
    elif company_name:
        # ä»…å…¬å¸åç§°è¿‡æ»¤
        if company_name in self.metadata_index['company_name']:
            candidate_indices = self.metadata_index['company_name'][company_name]
    elif stock_code:
        # ä»…è‚¡ç¥¨ä»£ç è¿‡æ»¤
        if stock_code in self.metadata_index['stock_code']:
            candidate_indices = self.metadata_index['stock_code'][stock_code]
    else:
        # æ— è¿‡æ»¤æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰è®°å½•
        candidate_indices = list(range(len(self.data)))
    
    return candidate_indices
```

### 2. æ™ºèƒ½ä¸Šä¸‹æ–‡æå– (Intelligent Context Extraction)

#### 2.1 å…³é”®è¯æå–
```python
def _extract_keywords(self, query: str) -> List[str]:
    """æå–æŸ¥è¯¢å…³é”®è¯"""
    keywords = []
    
    # æå–è‚¡ç¥¨ä»£ç 
    stock_pattern = r'[A-Z]{2}\d{4}|[A-Z]{2}\d{6}|\d{6}'
    stock_matches = re.findall(stock_pattern, query)
    keywords.extend(stock_matches)
    
    # æå–å…¬å¸åç§°
    company_pattern = r'([A-Za-z\u4e00-\u9fff]+)(?:å…¬å¸|é›†å›¢|è‚¡ä»½|æœ‰é™)'
    company_matches = re.findall(company_pattern, query)
    keywords.extend(company_matches)
    
    # æå–å¹´ä»½
    year_pattern = r'20\d{2}å¹´'
    year_matches = re.findall(year_pattern, query)
    keywords.extend(year_matches)
    
    # æå–å…³é”®æ¦‚å¿µ
    key_concepts = ['åˆ©æ¶¦', 'è¥æ”¶', 'å¢é•¿', 'ä¸šç»©', 'é¢„æµ‹', 'åŸå› ', 'ä¸»è¦', 'æŒç»­']
    for concept in key_concepts:
        if concept in query:
            keywords.append(concept)
    
    return list(set(keywords))
```

#### 2.2 ç›¸å…³æ€§å¥å­æå–
```python
def _extract_relevant_sentences(self, content: str, keywords: List[str], max_chars_per_doc: int = 800) -> List[str]:
    """ä»æ–‡æ¡£ä¸­æå–ä¸å…³é”®è¯æœ€ç›¸å…³çš„å¥å­"""
    if not content or not keywords:
        return []
    
    # æŒ‰å¥å­åˆ†å‰²
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', content)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    # è®¡ç®—æ¯ä¸ªå¥å­çš„ç›¸å…³æ€§åˆ†æ•°
    sentence_scores = []
    for sentence in sentences:
        score = 0
        for keyword in keywords:
            if keyword in sentence:
                score += 1
        # è€ƒè™‘å¥å­é•¿åº¦ï¼Œé¿å…è¿‡é•¿çš„å¥å­
        if len(sentence) > 200:
            score *= 0.5
        sentence_scores.append((sentence, score))
    
    # æŒ‰åˆ†æ•°æ’åº
    sentence_scores.sort(key=lambda x: x[1], reverse=True)
    
    # é€‰æ‹©æœ€ç›¸å…³çš„å¥å­
    selected_sentences = []
    total_chars = 0
    
    for sentence, score in sentence_scores:
        if score > 0 and total_chars + len(sentence) <= max_chars_per_doc:
            selected_sentences.append(sentence)
            total_chars += len(sentence)
    
    return selected_sentences
```

#### 2.3 æ™ºèƒ½ä¸Šä¸‹æ–‡æå–ä¸»å‡½æ•°
```python
def extract_relevant_context(self, query: str, candidate_results: List[Tuple[int, float, float]], max_chars: int = 2000) -> str:
    """æ™ºèƒ½æå–ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ç‰‡æ®µ"""
    
    # æå–æŸ¥è¯¢å…³é”®è¯
    query_keywords = self._extract_keywords(query)
    
    relevant_sentences = []
    total_chars = 0
    
    # åªå¤„ç†å‰3ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£
    for doc_idx, faiss_score, reranker_score in candidate_results[:3]:
        if doc_idx >= len(self.data):
            continue
            
        record = self.data[doc_idx]
        
        # è·å–æ–‡æ¡£å†…å®¹
        if self.dataset_type == "chinese":
            content = record.get('summary', '') or record.get('original_context', '')
        else:
            content = record.get('context', '') or record.get('content', '')
        
        if not content:
            continue
        
        # æå–æœ€ç›¸å…³çš„å¥å­
        relevant_sentences_for_doc = self._extract_relevant_sentences(
            content, query_keywords, max_chars_per_doc=800
        )
        
        for sentence in relevant_sentences_for_doc:
            if total_chars + len(sentence) <= max_chars:
                relevant_sentences.append(sentence)
                total_chars += len(sentence)
            else:
                break
        
        if total_chars >= max_chars:
            break
    
    # æ‹¼æ¥ä¸Šä¸‹æ–‡
    context = "\n\n".join(relevant_sentences)
    
    return context
```

### 3. ä¼˜åŒ–çš„ç”Ÿæˆç­”æ¡ˆæµç¨‹

#### 3.1 æ›¿æ¢åŸæœ‰çš„ä¸Šä¸‹æ–‡æ‹¼æ¥é€»è¾‘
```python
def generate_answer(self, query: str, candidate_results: List[Tuple[int, float, float]], top_k_for_context: int = 5) -> str:
    """ç”ŸæˆLLMç­”æ¡ˆ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–ï¼Œå¤§å¹…ç¼©çŸ­ä¼ é€’ç»™LLMçš„ä¸Šä¸‹æ–‡"""
    
    # ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–ï¼Œé™åˆ¶åœ¨2000å­—ç¬¦ä»¥å†…
    context = self.extract_relevant_context(query, candidate_results, max_chars=2000)
    
    # ä½¿ç”¨LLMç”Ÿæˆå™¨ç”Ÿæˆç­”æ¡ˆ
    if self.llm_generator:
        # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©promptæ¨¡æ¿
        if self.dataset_type == "chinese":
            prompt = template_loader.format_template(
                "multi_stage_chinese_template",
                context=context, 
                query=query
            )
        
        # ç”Ÿæˆç­”æ¡ˆ
        answer = self.llm_generator.generate(texts=[prompt])[0]
        return answer
```

## ğŸ¯ ä¼˜åŒ–ç­–ç•¥æ€»ç»“

### 1. **åˆ†å±‚è¿‡æ»¤ç­–ç•¥**
- **ç¬¬ä¸€å±‚**: å…ƒæ•°æ®é¢„è¿‡æ»¤ (99.985% æ–‡æ¡£å‡å°‘)
- **ç¬¬äºŒå±‚**: FAISSå‘é‡æ£€ç´¢ (ç²¾ç¡®åŒ¹é…)
- **ç¬¬ä¸‰å±‚**: Qwené‡æ’åº (è´¨é‡æ’åº)
- **ç¬¬å››å±‚**: æ™ºèƒ½ä¸Šä¸‹æ–‡æå– (é•¿åº¦æ§åˆ¶)

### 2. **é•¿åº¦æ§åˆ¶ç­–ç•¥**
- **æ–‡æ¡£çº§åˆ«**: åªå¤„ç†å‰3ä¸ªæœ€ç›¸å…³æ–‡æ¡£
- **å¥å­çº§åˆ«**: æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åºé€‰æ‹©å¥å­
- **å­—ç¬¦çº§åˆ«**: ä¸¥æ ¼é™åˆ¶åœ¨2000å­—ç¬¦ä»¥å†…
- **Tokençº§åˆ«**: ç¡®ä¿åœ¨æ¨¡å‹å®‰å…¨èŒƒå›´å†…

### 3. **ç›¸å…³æ€§ä¼˜åŒ–ç­–ç•¥**
- **å…³é”®è¯åŒ¹é…**: æå–æŸ¥è¯¢ä¸­çš„å…³é”®å®ä½“å’Œæ¦‚å¿µ
- **åˆ†æ•°åŠ æƒ**: è€ƒè™‘å¥å­é•¿åº¦å’Œå…³é”®è¯å¯†åº¦
- **å»é‡å¤„ç†**: é¿å…é‡å¤ä¿¡æ¯
- **è´¨é‡ä¼˜å…ˆ**: ä¼˜å…ˆé€‰æ‹©é«˜è´¨é‡çš„ç›¸å…³å¥å­

## ğŸ“ˆ æ€§èƒ½æå‡æ•ˆæœ

### 1. **æ£€ç´¢æ•ˆç‡**
- ä»å…¨é‡33,842ä¸ªæ–‡æ¡£è¿‡æ»¤åˆ°ç²¾ç¡®çš„5ä¸ªå€™é€‰
- æ£€ç´¢é€Ÿåº¦æå‡çº¦1000å€
- å†…å­˜ä½¿ç”¨å¤§å¹…å‡å°‘

### 2. **ç”Ÿæˆè´¨é‡**
- ä¸Šä¸‹æ–‡ç›¸å…³æ€§æ˜¾è‘—æå‡
- é¿å…æ— å…³ä¿¡æ¯å¹²æ‰°
- å›ç­”æ›´åŠ ç²¾å‡†å’Œç®€æ´

### 3. **ç³»ç»Ÿç¨³å®šæ€§**
- Tokenä½¿ç”¨é‡æ§åˆ¶åœ¨å®‰å…¨èŒƒå›´å†…
- é¿å…æ¨¡å‹æˆªæ–­å’Œé”™è¯¯
- æé«˜ç³»ç»Ÿæ•´ä½“å¯é æ€§

## ğŸ”„ å®æ–½æ­¥éª¤

1. **æ·»åŠ æ™ºèƒ½ä¸Šä¸‹æ–‡æå–å‡½æ•°**
2. **ä¿®æ”¹generate_answeræ–¹æ³•**
3. **é›†æˆå…ƒæ•°æ®è¿‡æ»¤åŠŸèƒ½**
4. **æµ‹è¯•å’ŒéªŒè¯æ•ˆæœ**
5. **éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ**

é€šè¿‡è¿™å¥—å®Œæ•´çš„æŠ€æœ¯æ–¹æ¡ˆï¼Œæˆ‘ä»¬æˆåŠŸå®ç°äº†ä¸Šä¸‹æ–‡é•¿åº¦çš„å¤§å¹…ç¼©çŸ­ï¼ŒåŒæ—¶ä¿æŒäº†å›ç­”è´¨é‡å’Œç³»ç»Ÿæ€§èƒ½çš„æ˜¾è‘—æå‡ã€‚ 