# æ•°æ®å­—æ®µæ˜ å°„æŒ‡å—

## ğŸ” é—®é¢˜èƒŒæ™¯

åœ¨RAGç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬å‘ç°ä¸åŒæ•°æ®æºä½¿ç”¨äº†ä¸åŒçš„å­—æ®µåæ¥è¡¨ç¤ºç›¸åŒçš„å†…å®¹ï¼Œè¿™å¯¼è‡´äº†æ•°æ®åŠ è½½é—®é¢˜ã€‚

## ğŸ“Š å½“å‰æ•°æ®å­—æ®µæ˜ å°„æƒ…å†µ

### 1. AlphaFinä¸­æ–‡æ•°æ®
**æ–‡ä»¶**: `data/alphafin/alphafin_cleaned.json`
**æ ¼å¼**: JSONæ•°ç»„
**å­—æ®µæ˜ å°„**:
```json
{
  "original_context": "å®é™…çš„è´¢åŠ¡æ–°é—»å†…å®¹",
  "original_answer": "ç”Ÿæˆçš„ç­”æ¡ˆ",
  "summary": "æ‘˜è¦å†…å®¹",
  "generated_question": "ç”Ÿæˆçš„é—®é¢˜",
  "company_name": "å…¬å¸åç§°",
  "stock_code": "è‚¡ç¥¨ä»£ç ",
  "report_date": "æŠ¥å‘Šæ—¥æœŸ"
}
```

### 2. TAT-QAè‹±æ–‡æ•°æ®
**æ–‡ä»¶**: `evaluate_mrr/tatqa_knowledge_base.jsonl`
**æ ¼å¼**: JSONLï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
**å­—æ®µæ˜ å°„**:
```json
{
  "doc_id": "æ–‡æ¡£ID",
  "source_type": "æ•°æ®æºç±»å‹",
  "text": "å®é™…çš„è¡¨æ ¼æˆ–æ®µè½å†…å®¹"
}
```

### 3. TAT-QAè¯„ä¼°æ•°æ®
**æ–‡ä»¶**: `evaluate_mrr/tatqa_eval_enhanced.jsonl`
**æ ¼å¼**: JSONLï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
**å­—æ®µæ˜ å°„**:
```json
{
  "query": "æŸ¥è¯¢é—®é¢˜",
  "context": "å®é™…çš„è¡¨æ ¼æˆ–æ®µè½å†…å®¹",
  "answer": "æ ‡å‡†ç­”æ¡ˆ",
  "doc_id": "æ–‡æ¡£ID",
  "relevant_doc_ids": ["ç›¸å…³æ–‡æ¡£IDåˆ—è¡¨"],
  "answer_from": "ç­”æ¡ˆæ¥æºç±»å‹"
}
```

## âš ï¸ é—®é¢˜åˆ†æ

### é—®é¢˜1: å­—æ®µåä¸ä¸€è‡´
- **AlphaFin**: ä½¿ç”¨ `original_context` å­—æ®µ
- **TAT-QAçŸ¥è¯†åº“**: ä½¿ç”¨ `text` å­—æ®µ
- **TAT-QAè¯„ä¼°æ•°æ®**: ä½¿ç”¨ `context` å­—æ®µ
- **æ•°æ®åŠ è½½å™¨**: åªæŸ¥æ‰¾ `context` å­—æ®µ

### é—®é¢˜2: æ•°æ®æ ¼å¼ä¸ä¸€è‡´
- **AlphaFin**: JSONæ•°ç»„æ ¼å¼
- **TAT-QA**: JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªå¯¹è±¡ï¼‰

### é—®é¢˜3: å†…å®¹ç»“æ„ä¸ä¸€è‡´
- **AlphaFin**: åŒ…å«æ¨¡æ¿åŒ–å†…å®¹ï¼ˆå·²æ¸…ç†ï¼‰
- **TAT-QAçŸ¥è¯†åº“**: åŒ…å«Table IDå’ŒParagraph IDæ ‡è¯†
- **TAT-QAè¯„ä¼°æ•°æ®**: åŒ…å«Table IDå’ŒParagraph IDæ ‡è¯†

## ğŸ› ï¸ è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: ç»Ÿä¸€å­—æ®µåï¼ˆæ¨èï¼‰

#### 1.1 ä¿®æ”¹TAT-QAçŸ¥è¯†åº“æ•°æ®æ ¼å¼
å°†TAT-QAçŸ¥è¯†åº“æ•°æ®çš„`text`å­—æ®µé‡å‘½åä¸º`context`ï¼š

```bash
# åˆ›å»ºè½¬æ¢è„šæœ¬
python -c "
import json
with open('evaluate_mrr/tatqa_knowledge_base.jsonl', 'r') as f_in:
    with open('evaluate_mrr/tatqa_knowledge_base_unified.jsonl', 'w') as f_out:
        for line in f_in:
            item = json.loads(line.strip())
            # å°†textå­—æ®µé‡å‘½åä¸ºcontext
            if 'text' in item:
                item['context'] = item.pop('text')
            f_out.write(json.dumps(item, ensure_ascii=False) + '\n')
"
```

#### 1.2 ä¿®æ”¹AlphaFinæ•°æ®æ ¼å¼
å°†AlphaFinæ•°æ®çš„`original_context`å­—æ®µé‡å‘½åä¸º`context`ï¼š

```bash
# åˆ›å»ºè½¬æ¢è„šæœ¬
python -c "
import json
with open('data/alphafin/alphafin_cleaned.json', 'r') as f:
    data = json.load(f)
    
for item in data:
    # å°†original_contextå­—æ®µé‡å‘½åä¸ºcontext
    if 'original_context' in item:
        item['context'] = item.pop('original_context')
    
with open('data/alphafin/alphafin_unified.json', 'w') as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
"
```

### æ–¹æ¡ˆ2: ä¿®æ”¹æ•°æ®åŠ è½½å™¨ï¼ˆå½“å‰é‡‡ç”¨ï¼‰

ä¿®æ”¹`xlm/utils/dual_language_loader.py`ä¸­çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œä½¿å…¶æ”¯æŒå¤šç§å­—æ®µåï¼š

```python
def load_tatqa_context_only(self, file_path: str) -> List[DocumentWithMetadata]:
    """åŠ è½½TAT-QAè‹±æ–‡æ•°æ®ï¼ˆæ”¯æŒå¤šç§å­—æ®µåï¼‰"""
    documents = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for idx, line in enumerate(f):
                try:
                    item = json.loads(line.strip())
                    # æ”¯æŒå¤šç§å­—æ®µåï¼štext, context, original_context
                    context = (item.get('text', '') or 
                             item.get('context', '') or 
                             item.get('original_context', '')).strip()
                    
                    if context:
                        metadata = DocumentMetadata(
                            source="tatqa",
                            language="english",
                            doc_id=f"tatqa_{idx}"
                        )
                        document = DocumentWithMetadata(
                            content=context,
                            metadata=metadata
                        )
                        documents.append(document)
                except Exception as e:
                    print(f"è·³è¿‡ç¬¬{idx+1}è¡Œï¼ŒåŸå› : {e}")
        
        print(f"åŠ è½½äº† {len(documents)} ä¸ªTAT-QAæ–‡æ¡£")
        return documents
        
    except Exception as e:
        print(f"é”™è¯¯: åŠ è½½TAT-QAæ•°æ®å¤±è´¥: {e}")
        return []
```

**å…³é”®å‘ç°**ï¼š
- **TAT-QAçŸ¥è¯†åº“** (`tatqa_knowledge_base.jsonl`): ä½¿ç”¨ `text` å­—æ®µ
- **TAT-QAè¯„ä¼°æ•°æ®** (`tatqa_eval_enhanced.jsonl`): ä½¿ç”¨ `context` å­—æ®µ
- åŒä¸€ä¸ªæ•°æ®é›†çš„ä¸åŒç”¨é€”æ–‡ä»¶ä½¿ç”¨äº†ä¸åŒçš„å­—æ®µåï¼

## ğŸ“‹ æ¨èçš„æ•°æ®æ ‡å‡†

### ç»Ÿä¸€çš„æ•°æ®æ ¼å¼æ ‡å‡†

```json
{
  "doc_id": "å”¯ä¸€æ–‡æ¡£æ ‡è¯†ç¬¦",
  "context": "æ–‡æ¡£å†…å®¹ï¼ˆè¡¨æ ¼ã€æ®µè½æˆ–æ··åˆå†…å®¹ï¼‰",
  "question": "ç›¸å…³é—®é¢˜ï¼ˆå¯é€‰ï¼‰",
  "answer": "æ ‡å‡†ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰",
  "source_type": "æ•°æ®æºç±»å‹ï¼ˆtrain/test/devï¼‰",
  "language": "è¯­è¨€æ ‡è¯†ï¼ˆchinese/englishï¼‰",
  "metadata": {
    "company_name": "å…¬å¸åç§°ï¼ˆä¸­æ–‡æ•°æ®ï¼‰",
    "stock_code": "è‚¡ç¥¨ä»£ç ï¼ˆä¸­æ–‡æ•°æ®ï¼‰",
    "report_date": "æŠ¥å‘Šæ—¥æœŸï¼ˆä¸­æ–‡æ•°æ®ï¼‰",
    "table_id": "è¡¨æ ¼IDï¼ˆè‹±æ–‡æ•°æ®ï¼‰",
    "paragraph_id": "æ®µè½IDï¼ˆè‹±æ–‡æ•°æ®ï¼‰"
  }
}
```

### æ–‡ä»¶æ ¼å¼æ ‡å‡†
- **ç»Ÿä¸€ä½¿ç”¨JSONLæ ¼å¼**ï¼šæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡
- **ç»Ÿä¸€å­—æ®µå**ï¼šä½¿ç”¨`context`ä½œä¸ºå†…å®¹å­—æ®µ
- **ç»Ÿä¸€ç¼–ç **ï¼šUTF-8ç¼–ç 

## ğŸ”§ å®æ–½æ­¥éª¤

### æ­¥éª¤1: åˆ›å»ºç»Ÿä¸€æ•°æ®è½¬æ¢è„šæœ¬

```python
#!/usr/bin/env python3
"""
ç»Ÿä¸€æ•°æ®æ ¼å¼è½¬æ¢è„šæœ¬
"""

import json
from pathlib import Path

def convert_alphafin_to_unified(input_path: str, output_path: str):
    """è½¬æ¢AlphaFinæ•°æ®ä¸ºç»Ÿä¸€æ ¼å¼"""
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        for item in data:
            unified_item = {
                "doc_id": f"alphafin_{item.get('company_name', 'unknown')}",
                "context": item.get('original_context', ''),
                "question": item.get('generated_question', ''),
                "answer": item.get('original_answer', ''),
                "source_type": "train",
                "language": "chinese",
                "metadata": {
                    "company_name": item.get('company_name', ''),
                    "stock_code": item.get('stock_code', ''),
                    "report_date": item.get('report_date', '')
                }
            }
            f.write(json.dumps(unified_item, ensure_ascii=False) + '\n')

def convert_tatqa_to_unified(input_path: str, output_path: str):
    """è½¬æ¢TAT-QAæ•°æ®ä¸ºç»Ÿä¸€æ ¼å¼"""
    with open(input_path, 'r', encoding='utf-8') as f:
        with open(output_path, 'w', encoding='utf-8') as f_out:
            for line in f:
                item = json.loads(line.strip())
                # æ”¯æŒå¤šç§å­—æ®µåï¼štext, context
                context = item.get('text', '') or item.get('context', '')
                unified_item = {
                    "doc_id": item.get('doc_id', 'unknown'),
                    "context": context,
                    "source_type": item.get('source_type', 'train'),
                    "language": "english",
                    "metadata": {}
                }
                f_out.write(json.dumps(unified_item, ensure_ascii=False) + '\n')

if __name__ == "__main__":
    # è½¬æ¢AlphaFinæ•°æ®
    convert_alphafin_to_unified(
        "data/alphafin/alphafin_cleaned.json",
        "data/alphafin/alphafin_unified.jsonl"
    )
    
    # è½¬æ¢TAT-QAçŸ¥è¯†åº“æ•°æ®
    convert_tatqa_to_unified(
        "evaluate_mrr/tatqa_knowledge_base.jsonl",
        "evaluate_mrr/tatqa_knowledge_base_unified.jsonl"
    )
    
    # è½¬æ¢TAT-QAè¯„ä¼°æ•°æ®
    convert_tatqa_to_unified(
        "evaluate_mrr/tatqa_eval_enhanced.jsonl",
        "evaluate_mrr/tatqa_eval_enhanced_unified.jsonl"
    )
    
    print("âœ… æ•°æ®æ ¼å¼ç»Ÿä¸€å®Œæˆï¼")
```

### æ­¥éª¤2: æ›´æ–°é…ç½®æ–‡ä»¶

```python
# config/parameters.py
@dataclass
class DataConfig:
    # ä½¿ç”¨ç»Ÿä¸€æ ¼å¼çš„æ•°æ®æ–‡ä»¶
    chinese_data_path: str = "data/alphafin/alphafin_unified.jsonl"
    english_data_path: str = "evaluate_mrr/tatqa_knowledge_base_unified.jsonl"
```

### æ­¥éª¤3: ç®€åŒ–æ•°æ®åŠ è½½å™¨

```python
def load_unified_data(self, file_path: str, language: str) -> List[DocumentWithMetadata]:
    """åŠ è½½ç»Ÿä¸€æ ¼å¼çš„æ•°æ®"""
    documents = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for idx, line in enumerate(f):
            try:
                item = json.loads(line.strip())
                context = item.get('context', '').strip()
                
                if context:
                    metadata = DocumentMetadata(
                        source=item.get('source_type', 'unknown'),
                        language=language,
                        doc_id=item.get('doc_id', f'doc_{idx}')
                    )
                    
                    document = DocumentWithMetadata(
                        content=context,
                        metadata=metadata
                    )
                    documents.append(document)
                    
            except Exception as e:
                print(f"è·³è¿‡ç¬¬{idx+1}è¡Œï¼ŒåŸå› : {e}")
    
    return documents
```

## ğŸ¯ æ€»ç»“

### å½“å‰çŠ¶æ€
- âœ… **é—®é¢˜å·²è¯†åˆ«**ï¼šå­—æ®µåä¸ä¸€è‡´å¯¼è‡´æ•°æ®åŠ è½½å¤±è´¥
- âœ… **ä¸´æ—¶ä¿®å¤**ï¼šæ•°æ®åŠ è½½å™¨æ”¯æŒå¤šç§å­—æ®µå
- â³ **é•¿æœŸæ–¹æ¡ˆ**ï¼šç»Ÿä¸€æ•°æ®æ ¼å¼æ ‡å‡†

### å»ºè®®
1. **ç«‹å³**ï¼šä½¿ç”¨å½“å‰ä¿®å¤çš„æ•°æ®åŠ è½½å™¨
2. **çŸ­æœŸ**ï¼šåˆ›å»ºç»Ÿä¸€æ ¼å¼çš„æ•°æ®æ–‡ä»¶
3. **é•¿æœŸ**ï¼šå»ºç«‹æ•°æ®æ ¼å¼æ ‡å‡†ï¼Œé¿å…ç±»ä¼¼é—®é¢˜

### éªŒè¯æ–¹æ³•
```bash
# æµ‹è¯•æ•°æ®åŠ è½½
python test_english_data_loading.py
python test_chinese_data_loading.py

# æµ‹è¯•RAGç³»ç»Ÿ
python run_optimized_ui.py
```

é€šè¿‡ç»Ÿä¸€æ•°æ®æ ¼å¼ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š
- ç®€åŒ–æ•°æ®åŠ è½½é€»è¾‘
- æé«˜ç³»ç»Ÿå¯ç»´æŠ¤æ€§
- é¿å…å­—æ®µåä¸ä¸€è‡´é—®é¢˜
- ä¾¿äºåç»­æ•°æ®æ‰©å±• 