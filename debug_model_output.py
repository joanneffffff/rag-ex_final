#!/usr/bin/env python3
"""
è°ƒè¯•æ¨¡å‹è¾“å‡ºï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«<answer>æ ‡ç­¾
"""

import json
import re
from typing import List, Dict, Any
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

# å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGenerator
try:
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥LocalLLMGeneratorï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
    USE_RAG_GENERATOR = False

def extract_final_answer(raw_output: str) -> str:
    """ä»æ¨¡å‹çš„åŸå§‹è¾“å‡ºä¸­æå–<answer>æ ‡ç­¾å†…çš„å†…å®¹"""
    print(f"\nğŸ” åŸå§‹è¾“å‡ºé•¿åº¦: {len(raw_output)}")
    print(f"ğŸ” åŸå§‹è¾“å‡ºå‰200å­—ç¬¦: {raw_output[:200]}...")
    
    # æŸ¥æ‰¾<answer>æ ‡ç­¾
    match = re.search(r'<answer>(.*?)</answer>', raw_output, re.DOTALL)
    if match:
        answer_content = match.group(1).strip()
        print(f"âœ… æ‰¾åˆ°<answer>æ ‡ç­¾ï¼Œå†…å®¹: {answer_content}")
        return answer_content
    
    # æŸ¥æ‰¾<think>æ ‡ç­¾
    think_match = re.search(r'<think>(.*?)</think>', raw_output, re.DOTALL)
    if think_match:
        print(f"âš ï¸ åªæ‰¾åˆ°<think>æ ‡ç­¾ï¼Œæ²¡æœ‰<answer>æ ‡ç­¾")
        print(f"ğŸ” <think>å†…å®¹: {think_match.group(1).strip()[:100]}...")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æœªé—­åˆçš„<answer>æ ‡ç­¾
    if '<answer>' in raw_output and '</answer>' not in raw_output:
        print("âš ï¸ å‘ç°æœªé—­åˆçš„<answer>æ ‡ç­¾")
        # æå–<answer>åçš„æ‰€æœ‰å†…å®¹
        answer_start = raw_output.find('<answer>') + len('<answer>')
        answer_content = raw_output[answer_start:].strip()
        print(f"ğŸ” <answer>åçš„å†…å®¹: {answer_content[:100]}...")
        return answer_content
    
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›æœ€åä¸€è¡Œ
    lines = raw_output.strip().split('\n')
    if lines:
        last_line = lines[-1].strip()
        print(f"âš ï¸ æœªæ‰¾åˆ°æ ‡ç­¾ï¼Œä½¿ç”¨æœ€åä¸€è¡Œ: {last_line}")
        return last_line
    
    print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç­”æ¡ˆå†…å®¹")
    return ""

def get_detailed_english_prompt_messages(context_content: str, question_text: str) -> List[Dict[str, str]]:
    """ç”ŸæˆLLMæœŸæœ›çš„messagesåˆ—è¡¨"""
    try:
        with open('rag_english_template.txt', 'r', encoding='utf-8') as f:
            template_content = f.read().strip()
    except FileNotFoundError:
        print("âš ï¸ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿")
        return [
            {"role": "system", "content": "You are a world-class quantitative financial analyst AI."},
            {"role": "user", "content": f"Context:\n{context_content}\n\nQuestion:\n{question_text}\n\nA:"}
        ]
    
    # è§£æsystemå’Œuseræ ‡ç­¾
    system_match = re.search(r'<system>(.*?)</system>', template_content, re.DOTALL)
    if system_match:
        system_content = system_match.group(1).strip()
    else:
        system_content = "You are a world-class quantitative financial analyst AI."
    
    user_match = re.search(r'<user>(.*?)</user>', template_content, re.DOTALL)
    if user_match:
        user_template = user_match.group(1).strip()
        user_content = user_template.replace('{context}', context_content).replace('{question}', question_text)
    else:
        user_content = f"Context:\n{context_content}\n\nQuestion:\n{question_text}\n\nA:"

    return [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_content}
    ]

def convert_messages_to_text(messages: List[Dict[str, str]]) -> str:
    """å°†messagesè½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"""
    text = ""
    for message in messages:
        role = message["role"]
        content = message["content"]
        if role == "system":
            text += f"System: {content}\n\n"
        elif role == "user":
            text += f"User: {content}\n\n"
        elif role == "assistant":
            text += f"Assistant: {content}\n\n"
    return text.strip()

def test_model_output():
    """æµ‹è¯•æ¨¡å‹è¾“å‡º"""
    print("ğŸš€ å¼€å§‹è°ƒè¯•æ¨¡å‹è¾“å‡º...")
    
    # åŠ è½½ä¸€ä¸ªç®€å•çš„æµ‹è¯•æ ·æœ¬
    test_context = "Table ID: 1\nCompany: Apple Inc.\nRevenue: $394.3 billion\nProfit: $96.9 billion"
    test_question = "What is Apple's revenue?"
    test_answer = "$394.3 billion"
    
    print(f"ğŸ“ æµ‹è¯•é—®é¢˜: {test_question}")
    print(f"ğŸ“Š æµ‹è¯•ä¸Šä¸‹æ–‡: {test_context}")
    print(f"âœ… æœŸæœ›ç­”æ¡ˆ: {test_answer}")
    
    # æ„å»ºprompt
    messages = get_detailed_english_prompt_messages(test_context, test_question)
    prompt_text = convert_messages_to_text(messages)
    
    print(f"\nğŸ“‹ å®Œæ•´Prompt:")
    print("="*60)
    print(prompt_text)
    print("="*60)
    
    if USE_RAG_GENERATOR:
        try:
            # ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator
            llm_generator = LocalLLMGenerator(
                model_name="SUFE-AIFLM-Lab/Fin-R1",
                device="auto"
            )
            
            # è®¾ç½®ç”Ÿæˆå‚æ•°
            generation_params = {
                "max_new_tokens": 2048,
                "do_sample": False,
                "repetition_penalty": 1.1,
                "temperature": 0.1  # é™ä½æ¸©åº¦ä»¥è·å¾—æ›´ç¡®å®šçš„è¾“å‡º
            }
            
            print(f"\nğŸ”§ ç”Ÿæˆå‚æ•°: {generation_params}")
            
            # ç”Ÿæˆå›ç­”
            start_time = time.time()
            generated_text = llm_generator.generate([prompt_text])[0]
            generation_time = time.time() - start_time
            
            print(f"\nâ±ï¸ ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
            print(f"ğŸ“ ç”Ÿæˆæ–‡æœ¬é•¿åº¦: {len(generated_text)}")
            
            # åˆ†æè¾“å‡º
            print(f"\nğŸ¤– æ¨¡å‹å®Œæ•´è¾“å‡º:")
            print("="*80)
            print(generated_text)
            print("="*80)
            
            # æå–ç­”æ¡ˆ
            final_answer = extract_final_answer(generated_text)
            
            print(f"\nğŸ¯ æå–çš„æœ€ç»ˆç­”æ¡ˆ: '{final_answer}'")
            print(f"ğŸ¯ æœŸæœ›ç­”æ¡ˆ: '{test_answer}'")
            
            # æ£€æŸ¥åŒ¹é…
            if final_answer.strip().lower() == test_answer.strip().lower():
                print("âœ… ç­”æ¡ˆå®Œå…¨åŒ¹é…ï¼")
            elif test_answer.strip().lower() in final_answer.strip().lower():
                print("âœ… ç­”æ¡ˆåŒ…å«æœŸæœ›å†…å®¹ï¼")
            else:
                print("âŒ ç­”æ¡ˆä¸åŒ¹é…")
                
        except Exception as e:
            print(f"âŒ RAGç”Ÿæˆå™¨é”™è¯¯: {e}")
    else:
        print("âŒ RAGç”Ÿæˆå™¨ä¸å¯ç”¨")

if __name__ == "__main__":
    test_model_output() 