#!/usr/bin/env python3
"""
æµ‹è¯•MRRè®¡ç®—æ˜¯å¦æ­£ç¡® (ä¿®å¤ç‰ˆ)
ä½¿ç”¨æ­£ç¡®çš„doc_idåŒ¹é…é€»è¾‘
"""

import json
import numpy as np
import torch
from sentence_transformers import SentenceTransformer

def test_mrr_calculation_fixed():
    """æµ‹è¯•MRRè®¡ç®—é€»è¾‘ (ä¿®å¤ç‰ˆ)"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•MRRè®¡ç®— (ä¿®å¤ç‰ˆ)...")
    
    # åŠ è½½ä¸€äº›è¯„ä¼°æ•°æ®
    eval_data = []
    with open("evaluate_mrr/alphafin_eval.jsonl", 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 10:  # å–å‰10ä¸ªæ ·æœ¬è¿›è¡Œæµ‹è¯•
                break
            item = json.loads(line)
            eval_data.append({
                'query': item.get('generated_question', ''),
                'context': item.get('summary', ''),
                'doc_id': item.get('doc_id', '')
            })
    
    print(f"ğŸ“Š åŠ è½½äº† {len(eval_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬çš„ä¿¡æ¯
    for i, item in enumerate(eval_data[:3]):
        print(f"\næ ·æœ¬ {i+1}:")
        print(f"  doc_id: {item['doc_id']}")
        print(f"  query: {item['query'][:80]}...")
        print(f"  context: {item['context'][:80]}...")
    
    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ¤– åŠ è½½æ¨¡å‹...")
    model = SentenceTransformer("Langboat/mengzi-bert-base-fin")
    
    # ç¼–ç æ‰€æœ‰ä¸Šä¸‹æ–‡
    contexts = [item['context'] for item in eval_data]
    print(f"ç¼–ç  {len(contexts)} ä¸ªä¸Šä¸‹æ–‡...")
    context_embeddings = model.encode(contexts, convert_to_tensor=True)
    
    # åˆ›å»ºdoc_idåˆ°ç´¢å¼•çš„æ˜ å°„
    doc_id_to_idx = {}
    for idx, item in enumerate(eval_data):
        doc_id = item.get('doc_id') or str(idx)
        doc_id_to_idx[doc_id] = idx
    
    print(f"doc_idæ˜ å°„: {doc_id_to_idx}")
    
    # æµ‹è¯•æ¯ä¸ªæŸ¥è¯¢
    mrrs = []
    for i, item in enumerate(eval_data):
        print(f"\n--- æµ‹è¯•æŸ¥è¯¢ {i+1} ---")
        print(f"doc_id: {item['doc_id']}")
        print(f"query: {item['query'][:60]}...")
        
        # ç¼–ç æŸ¥è¯¢
        query_emb = model.encode(item['query'], convert_to_tensor=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦ - ä¿®å¤ï¼šè®¡ç®—æŸ¥è¯¢ä¸æ‰€æœ‰ä¸Šä¸‹æ–‡çš„ç›¸ä¼¼åº¦
        scores = torch.cosine_similarity(query_emb.unsqueeze(0), context_embeddings).cpu().numpy()
        
        # ä½¿ç”¨doc_idæ‰¾åˆ°ç›®æ ‡ä¸Šä¸‹æ–‡çš„ç´¢å¼•
        target_doc_id = item.get('doc_id') or str(i)
        target_context_idx = doc_id_to_idx.get(target_doc_id, i)
        
        # æ’åº
        sorted_indices = np.argsort(scores)[::-1]
        print(f"ç›¸ä¼¼åº¦åˆ†æ•°: {scores}")  # æ˜¾ç¤ºæ‰€æœ‰åˆ†æ•°
        print(f"æ’åºåçš„ç´¢å¼•: {sorted_indices}")  # æ˜¾ç¤ºæ‰€æœ‰ç´¢å¼•
        print(f"ç›®æ ‡ç´¢å¼•: {target_context_idx}")
        
        # æ‰¾åˆ°æ’å
        rank = -1
        for r, idx in enumerate(sorted_indices):
            if idx == target_context_idx:
                rank = r + 1
                break
        
        print(f"ç›®æ ‡æ’å: {rank}")
        
        if rank != -1:
            mrr_score = 1.0 / rank
            mrrs.append(mrr_score)
            print(f"MRRåˆ†æ•°: {mrr_score:.4f}")
        else:
            mrrs.append(0.0)
            print(f"MRRåˆ†æ•°: 0.0000 (æœªæ‰¾åˆ°)")
    
    # è®¡ç®—å¹³å‡MRR
    avg_mrr = float(np.mean(mrrs)) if mrrs else 0.0
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"  å„æ ·æœ¬MRR: {[f'{mrr:.4f}' for mrr in mrrs]}")
    print(f"  å¹³å‡MRR: {avg_mrr:.4f}")
    
    # åˆ†æç»“æœ
    if avg_mrr > 0.5:
        print("âœ… MRRè®¡ç®—æ­£ç¡®ï¼Œæ¨¡å‹èƒ½å¤Ÿæ­£ç¡®åŒ¹é…æŸ¥è¯¢å’Œä¸Šä¸‹æ–‡")
    elif avg_mrr > 0.1:
        print("âš ï¸  MRRè¾ƒä½ï¼Œä½†è®¡ç®—é€»è¾‘æ­£ç¡®ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒ")
    else:
        print("âŒ MRRæä½ï¼Œå¯èƒ½å­˜åœ¨ä»¥ä¸‹é—®é¢˜:")
        print("   1. å­—æ®µæ˜ å°„é”™è¯¯")
        print("   2. æ•°æ®è´¨é‡é—®é¢˜")
        print("   3. æ¨¡å‹éœ€è¦æ›´å¤šè®­ç»ƒ")
    
    return avg_mrr

if __name__ == "__main__":
    test_mrr_calculation_fixed() 