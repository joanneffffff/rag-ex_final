#!/usr/bin/env python3
"""
RAGç³»ç»Ÿç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬
æ¨¡æ‹ŸçœŸå®ç”¨æˆ·ä¸RAGç³»ç»Ÿçš„å®Œæ•´äº¤äº’æµç¨‹ï¼Œè¯„ä¼°æ•´ä½“æ€§èƒ½
"""

import sys
import os
import time
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import argparse
from tqdm import tqdm
import numpy as np
import jieba

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥RAGç³»ç»Ÿç»„ä»¶
from xlm.ui.optimized_rag_ui import OptimizedRagUI
from config.parameters import config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('e2e_test.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class RagSystemAdapter:
    """
    RAGç³»ç»Ÿé€‚é…å™¨ï¼Œæä¾›ç»Ÿä¸€çš„æ¥å£æ¥æµ‹è¯•æ•´ä¸ªRAGç³»ç»Ÿ
    """
    
    def __init__(self, enable_reranker: bool = True, enable_stock_prediction: bool = False):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿé€‚é…å™¨
        
        Args:
            enable_reranker: æ˜¯å¦å¯ç”¨é‡æ’åºå™¨
            enable_stock_prediction: æ˜¯å¦å¯ç”¨è‚¡ç¥¨é¢„æµ‹æ¨¡å¼
        """
        self.enable_reranker = enable_reranker
        self.enable_stock_prediction = enable_stock_prediction
        self.rag_ui = None
        self.initialized = False
        
    def initialize(self):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        try:
            logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–RAGç³»ç»Ÿ...")
            
            # åˆå§‹åŒ–RAG UIç³»ç»Ÿ
            self.rag_ui = OptimizedRagUI(
                enable_reranker=self.enable_reranker,
                use_existing_embedding_index=True  # ä½¿ç”¨ç°æœ‰ç´¢å¼•ä»¥åŠ å¿«æµ‹è¯•
            )
            
            # åˆå§‹åŒ–ç»„ä»¶
            self.rag_ui._init_components()
            
            # éªŒè¯é€»è¾‘ä¸€è‡´æ€§
            if self.verify_rag_logic_consistency():
                self.initialized = True
                logger.info("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œé€»è¾‘ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
            else:
                logger.warning("âš ï¸ RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œä½†é€»è¾‘ä¸€è‡´æ€§éªŒè¯å¤±è´¥")
                self.initialized = True  # ä»ç„¶å…è®¸ä½¿ç”¨ï¼Œä½†è®°å½•è­¦å‘Š
            
        except Exception as e:
            logger.error(f"âŒ RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def process_query(self, query: str, datasource: str = "auto") -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œè¿”å›å®Œæ•´çš„RAGç³»ç»Ÿå“åº”
        ä½¿ç”¨ä¸RAGç³»ç»Ÿå®Œå…¨ç›¸åŒçš„é€»è¾‘
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            datasource: æ•°æ®æºï¼ˆautoè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        if not self.initialized:
            raise RuntimeError("RAGç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initialize()")
        
        start_time = time.time()
        
        try:
            # ä½¿ç”¨ä¸RAGç³»ç»Ÿå®Œå…¨ç›¸åŒçš„å¤„ç†é€»è¾‘
            # ç›´æ¥è°ƒç”¨RAGç³»ç»Ÿçš„_process_questionæ–¹æ³•ï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†ï¼š
            # 1. è¯­è¨€æ£€æµ‹
            # 2. æ ¹æ®è¯­è¨€é€‰æ‹©å¤„ç†æ–¹å¼ï¼ˆä¸­æ–‡ç”¨å¤šé˜¶æ®µæ£€ç´¢ï¼Œè‹±æ–‡ç”¨ä¼ ç»ŸRAGï¼‰
            # 3. è‚¡ç¥¨é¢„æµ‹æ¨¡å¼çš„å¤„ç†
            # 4. é‡æ’åºå™¨çš„å¤„ç†
            answer, html_content = self.rag_ui._process_question(
                question=query,
                datasource=datasource,
                reranker_checkbox=self.enable_reranker,
                stock_prediction_checkbox=self.enable_stock_prediction
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # æå–æ€§èƒ½æŒ‡æ ‡ï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…RAGç³»ç»Ÿçš„æ—¥å¿—æ¥è·å–ï¼‰
            # ç”±äºRAGç³»ç»Ÿå†…éƒ¨æ²¡æœ‰ç›´æ¥æš´éœ²è¿™äº›æŒ‡æ ‡ï¼Œæˆ‘ä»¬éœ€è¦ä»æ—¥å¿—ä¸­è§£æ
            performance_metrics = self._extract_performance_metrics()
            
            return {
                "query": query,
                "answer": answer,
                "html_content": html_content,
                "processing_time": processing_time,
                "performance_metrics": performance_metrics,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return {
                "query": query,
                "answer": f"å¤„ç†å¤±è´¥: {str(e)}",
                "html_content": "",
                "processing_time": time.time() - start_time,
                "performance_metrics": {},
                "success": False,
                "error": str(e)
            }
    
    def _extract_performance_metrics(self) -> Dict[str, Any]:
        """
        ä»RAGç³»ç»Ÿæ—¥å¿—ä¸­æå–æ€§èƒ½æŒ‡æ ‡
        è¿™é‡Œè¿”å›ä¸€ä¸ªç¤ºä¾‹ï¼Œå®é™…å®ç°éœ€è¦æ ¹æ®RAGç³»ç»Ÿçš„æ—¥å¿—æ ¼å¼æ¥è§£æ
        """
        return {
            "retrieval_time": 0.0,  # æ£€ç´¢æ—¶é—´
            "generation_time": 0.0,  # ç”Ÿæˆæ—¶é—´
            "total_tokens": 0,  # æ€»tokenæ•°
            "retrieved_docs": 0,  # æ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°
            "reranker_enabled": self.enable_reranker,
            "stock_prediction_enabled": self.enable_stock_prediction
        }
    
    def verify_rag_logic_consistency(self) -> bool:
        """
        éªŒè¯RagSystemAdapteræ˜¯å¦ä¸RAGç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„é€»è¾‘
        """
        if not self.initialized:
            logger.warning("âš ï¸ RAGç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ— æ³•éªŒè¯é€»è¾‘ä¸€è‡´æ€§")
            return False
        
        try:
            # æ£€æŸ¥RAGç³»ç»Ÿæ˜¯å¦åŒ…å«æ‰€æœ‰å¿…è¦çš„ç»„ä»¶
            required_components = [
                'retriever',
                'generator', 
                'reranker',
                'chinese_retrieval_system',
                'config'
            ]
            
            missing_components = []
            for component in required_components:
                if not hasattr(self.rag_ui, component):
                    missing_components.append(component)
            
            if missing_components:
                logger.warning(f"âš ï¸ ç¼ºå°‘RAGç³»ç»Ÿç»„ä»¶: {missing_components}")
                return False
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…è¦çš„æ–¹æ³•
            required_methods = [
                '_process_question',
                '_unified_rag_processing_with_prompt',
                '_unified_rag_processing',
                '_generate_answer_with_context'
            ]
            
            missing_methods = []
            for method in required_methods:
                if not hasattr(self.rag_ui, method):
                    missing_methods.append(method)
            
            if missing_methods:
                logger.warning(f"âš ï¸ ç¼ºå°‘RAGç³»ç»Ÿæ–¹æ³•: {missing_methods}")
                return False
            
            logger.info("âœ… RAGç³»ç»Ÿé€»è¾‘ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            logger.error(f"âŒ RAGç³»ç»Ÿé€»è¾‘ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}")
            return False


def normalize_answer_chinese(s: str) -> str:
    """
    æ ‡å‡†åŒ–ä¸­æ–‡ç­”æ¡ˆï¼Œç”¨äºè®¡ç®—F1-scoreå’ŒExact Match
    """
    if not s:
        return ""
    
    # ç§»é™¤"è§£æ"åŠå…¶åé¢çš„å†…å®¹
    import re
    # æŸ¥æ‰¾"è§£æ"çš„ä½ç½®ï¼Œç§»é™¤å®ƒåŠå…¶åé¢çš„æ‰€æœ‰å†…å®¹
    parse_index = s.find("è§£æ")
    if parse_index != -1:
        s = s[:parse_index]
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    s = ' '.join(s.split())
    
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼ˆä¿ç•™ä¸­æ–‡æ ‡ç‚¹ï¼‰
    s = re.sub(r'[^\u4e00-\u9fff\w\s]', '', s)
    
    return s.strip()


def get_tokens_chinese(s: str) -> List[str]:
    """
    ä½¿ç”¨jiebaåˆ†è¯è·å–ä¸­æ–‡tokenåˆ—è¡¨
    """
    return list(jieba.cut(s))


def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """
    è®¡ç®—F1-score
    """
    pred_tokens = set(get_tokens_chinese(normalize_answer_chinese(prediction)))
    gt_tokens = set(get_tokens_chinese(normalize_answer_chinese(ground_truth)))
    
    if not gt_tokens:
        return 1.0 if not pred_tokens else 0.0
    
    intersection = pred_tokens & gt_tokens
    precision = len(intersection) / len(pred_tokens) if pred_tokens else 0.0
    recall = len(intersection) / len(gt_tokens)
    
    if precision + recall == 0:
        return 0.0
    
    return 2 * precision * recall / (precision + recall)


def calculate_exact_match(prediction: str, ground_truth: str) -> float:
    """
    è®¡ç®—Exact Match
    """
    pred_normalized = normalize_answer_chinese(prediction)
    gt_normalized = normalize_answer_chinese(ground_truth)
    
    return 1.0 if pred_normalized == gt_normalized else 0.0


def load_test_dataset(data_path: str, sample_size: Optional[int] = None) -> List[Dict[str, Any]]:
    """
    åŠ è½½æµ‹è¯•æ•°æ®é›†
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        sample_size: é‡‡æ ·æ•°é‡ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®
        
    Returns:
        æµ‹è¯•æ•°æ®åˆ—è¡¨
    """
    logger.info(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®é›†: {data_path}")
    
    dataset = []
    
    if data_path.endswith('.jsonl'):
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    dataset.append(json.loads(line))
    elif data_path.endswith('.json'):
        with open(data_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
    
    if sample_size and sample_size < len(dataset):
        import random
        random.seed(42)  # å›ºå®šéšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
        dataset = random.sample(dataset, sample_size)
        logger.info(f"ğŸ“Š éšæœºé‡‡æ · {sample_size} ä¸ªæ ·æœ¬")
    
    logger.info(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return dataset


def run_e2e_test(
    data_path: str,
    output_path: str,
    sample_size: Optional[int] = None,
    enable_reranker: bool = True,
    enable_stock_prediction: bool = False
) -> Dict[str, Any]:
    """
    è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•
    
    Args:
        data_path: æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„
        output_path: ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„
        sample_size: é‡‡æ ·æ•°é‡
        enable_reranker: æ˜¯å¦å¯ç”¨é‡æ’åºå™¨
        enable_stock_prediction: æ˜¯å¦å¯ç”¨è‚¡ç¥¨é¢„æµ‹æ¨¡å¼
        
    Returns:
        æµ‹è¯•ç»“æœæ‘˜è¦
    """
    logger.info("ğŸš€ å¼€å§‹ç«¯åˆ°ç«¯æµ‹è¯•")
    logger.info(f"æ•°æ®è·¯å¾„: {data_path}")
    logger.info(f"è¾“å‡ºè·¯å¾„: {output_path}")
    logger.info(f"é‡æ’åºå™¨: {'å¯ç”¨' if enable_reranker else 'ç¦ç”¨'}")
    logger.info(f"è‚¡ç¥¨é¢„æµ‹: {'å¯ç”¨' if enable_stock_prediction else 'ç¦ç”¨'}")
    
    # 1. åŠ è½½æµ‹è¯•æ•°æ®
    test_dataset = load_test_dataset(data_path, sample_size)
    
    # 2. åˆå§‹åŒ–RAGç³»ç»Ÿé€‚é…å™¨
    rag_adapter = RagSystemAdapter(
        enable_reranker=enable_reranker,
        enable_stock_prediction=enable_stock_prediction
    )
    rag_adapter.initialize()
    
    # 3. è¿è¡Œæµ‹è¯•
    results = []
    total_processing_time = 0.0
    
    logger.info("ğŸ”„ å¼€å§‹å¤„ç†æµ‹è¯•æ ·æœ¬...")
    
    for i, test_item in enumerate(tqdm(test_dataset, desc="å¤„ç†æµ‹è¯•æ ·æœ¬")):
        # è·å–æŸ¥è¯¢å’Œæ ‡å‡†ç­”æ¡ˆ
        query = test_item.get("query", "") or test_item.get("question", "") or test_item.get("generated_question", "")
        ground_truth = test_item.get("answer", "") or test_item.get("expected_answer", "")
        
        if not query:
            logger.warning(f"âš ï¸ æ ·æœ¬ {i} ç¼ºå°‘æŸ¥è¯¢ï¼Œè·³è¿‡")
            continue
        
        # å¤„ç†æŸ¥è¯¢
        result = rag_adapter.process_query(query)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        if result["success"] and ground_truth:
            f1_score = calculate_f1_score(result["answer"], ground_truth)
            exact_match = calculate_exact_match(result["answer"], ground_truth)
        else:
            f1_score = 0.0
            exact_match = 0.0
        
        # è®°å½•ç»“æœ
        test_result = {
            "sample_id": i,
            "query": query,
            "ground_truth": ground_truth,
            "predicted_answer": result["answer"],
            "f1_score": f1_score,
            "exact_match": exact_match,
            "processing_time": result["processing_time"],
            "success": result["success"],
            "performance_metrics": result["performance_metrics"]
        }
        
        if not result["success"]:
            test_result["error"] = result.get("error", "æœªçŸ¥é”™è¯¯")
        
        results.append(test_result)
        total_processing_time += result["processing_time"]
        
        # æ¯å¤„ç†10ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡è¿›åº¦
        if (i + 1) % 10 == 0:
            avg_time = total_processing_time / (i + 1)
            avg_f1 = np.mean([r["f1_score"] for r in results])
            avg_em = np.mean([r["exact_match"] for r in results])
            logger.info(f"ğŸ“Š è¿›åº¦: {i+1}/{len(test_dataset)}, å¹³å‡F1: {avg_f1:.4f}, å¹³å‡EM: {avg_em:.4f}, å¹³å‡æ—¶é—´: {avg_time:.2f}s")
    
    # 4. è®¡ç®—æ€»ä½“ç»Ÿè®¡
    successful_results = [r for r in results if r["success"]]
    
    if successful_results:
        avg_f1 = np.mean([r["f1_score"] for r in successful_results])
        avg_em = np.mean([r["exact_match"] for r in successful_results])
        avg_time = np.mean([r["processing_time"] for r in successful_results])
        total_time = sum([r["processing_time"] for r in successful_results])
    else:
        avg_f1 = avg_em = avg_time = total_time = 0.0
    
    # 5. ç”Ÿæˆæµ‹è¯•æ‘˜è¦
    test_summary = {
        "test_config": {
            "data_path": data_path,
            "sample_size": sample_size,
            "enable_reranker": enable_reranker,
            "enable_stock_prediction": enable_stock_prediction
        },
        "overall_metrics": {
            "total_samples": len(test_dataset),
            "successful_samples": len(successful_results),
            "success_rate": len(successful_results) / len(test_dataset) if test_dataset else 0.0,
            "average_f1_score": avg_f1,
            "average_exact_match": avg_em,
            "average_processing_time": avg_time,
            "total_processing_time": total_time
        },
        "detailed_results": results
    }
    
    # 6. ä¿å­˜ç»“æœ
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(test_summary, f, ensure_ascii=False, indent=2)
    
    # 7. è¾“å‡ºæ‘˜è¦
    logger.info("ğŸ‰ ç«¯åˆ°ç«¯æµ‹è¯•å®Œæˆï¼")
    logger.info(f"ğŸ“Š æµ‹è¯•æ‘˜è¦:")
    logger.info(f"   æ€»æ ·æœ¬æ•°: {len(test_dataset)}")
    logger.info(f"   æˆåŠŸæ ·æœ¬æ•°: {len(successful_results)}")
    logger.info(f"   æˆåŠŸç‡: {test_summary['overall_metrics']['success_rate']:.2%}")
    logger.info(f"   å¹³å‡F1-score: {avg_f1:.4f}")
    logger.info(f"   å¹³å‡Exact Match: {avg_em:.4f}")
    logger.info(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_time:.2f}ç§’")
    logger.info(f"   æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
    logger.info(f"ğŸ“ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    return test_summary


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿç«¯åˆ°ç«¯æµ‹è¯•")
    parser.add_argument("--data_path", type=str, required=True, 
                       help="æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„ (jsonlæˆ–jsonæ ¼å¼)")
    parser.add_argument("--output_path", type=str, default="e2e_test_results.json",
                       help="ç»“æœè¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--sample_size", type=int, default=None,
                       help="é‡‡æ ·æ•°é‡ (é»˜è®¤ä½¿ç”¨å…¨éƒ¨æ•°æ®)")
    parser.add_argument("--disable_reranker", action="store_true",
                       help="ç¦ç”¨é‡æ’åºå™¨")
    parser.add_argument("--enable_stock_prediction", action="store_true",
                       help="å¯ç”¨è‚¡ç¥¨é¢„æµ‹æ¨¡å¼")
    
    args = parser.parse_args()
    
    # è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•
    test_summary = run_e2e_test(
        data_path=args.data_path,
        output_path=args.output_path,
        sample_size=args.sample_size,
        enable_reranker=not args.disable_reranker,
        enable_stock_prediction=args.enable_stock_prediction
    )
    
    # è¾“å‡ºæœ€ç»ˆç»“æœ
    print("\n" + "="*50)
    print("ğŸ¯ ç«¯åˆ°ç«¯æµ‹è¯•æœ€ç»ˆç»“æœ")
    print("="*50)
    print(f"å¹³å‡F1-score: {test_summary['overall_metrics']['average_f1_score']:.4f}")
    print(f"å¹³å‡Exact Match: {test_summary['overall_metrics']['average_exact_match']:.4f}")
    print(f"æˆåŠŸç‡: {test_summary['overall_metrics']['success_rate']:.2%}")
    print(f"å¹³å‡å¤„ç†æ—¶é—´: {test_summary['overall_metrics']['average_processing_time']:.2f}ç§’")
    print("="*50)


if __name__ == "__main__":
    main() 