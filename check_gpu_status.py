#!/usr/bin/env python3
"""
æ£€æŸ¥GPUçŠ¶æ€å’Œé…ç½®
"""

import torch
import os
from config.parameters import config

def check_gpu_status():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    print("ğŸ” æ£€æŸ¥GPUçŠ¶æ€...")
    print("="*50)
    
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    print(f"CUDA available: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        # æ£€æŸ¥GPUæ•°é‡
        gpu_count = torch.cuda.device_count()
        print(f"GPU count: {gpu_count}")
        
        # æ£€æŸ¥æ¯ä¸ªGPUçš„ä¿¡æ¯
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}: {gpu_name}, Memory: {gpu_memory:.1f} GB")
        
        # æ£€æŸ¥å½“å‰è®¾å¤‡
        current_device = torch.cuda.current_device()
        print(f"Current CUDA device: {current_device}")
        
        # æ£€æŸ¥é»˜è®¤è®¾å¤‡
        default_device = torch.cuda.get_device_name(0)
        print(f"Default CUDA device: {default_device}")
    else:
        print("âŒ CUDAä¸å¯ç”¨")
    
    print("\nğŸ”§ æ£€æŸ¥é…ç½®...")
    print("="*50)
    
    # æ£€æŸ¥ç”Ÿæˆå™¨é…ç½®
    generator_config = config.generator
    print(f"Generator device: {generator_config.device}")
    print(f"Generator use_quantization: {generator_config.use_quantization}")
    print(f"Generator quantization_type: {generator_config.quantization_type}")
    print(f"Generator max_new_tokens: {generator_config.max_new_tokens}")
    print(f"Generator max_generation_time: {generator_config.max_generation_time}")
    
    # æ£€æŸ¥ç¼–ç å™¨é…ç½®
    encoder_config = config.encoder
    print(f"Encoder device: {encoder_config.device}")
    
    # æ£€æŸ¥é‡æ’åºå™¨é…ç½®
    reranker_config = config.reranker
    print(f"Reranker device: {reranker_config.device}")
    
    print("\nğŸš€ æµ‹è¯•GPUè®¿é—®...")
    print("="*50)
    
    if torch.cuda.is_available():
        try:
            # æµ‹è¯•GPU 0
            device_0 = torch.device("cuda:0")
            test_tensor_0 = torch.randn(100, 100).to(device_0)
            print(f"âœ… GPU 0 è®¿é—®æˆåŠŸ: {test_tensor_0.device}")
            
            # æµ‹è¯•GPU 1 (å¦‚æœå­˜åœ¨)
            if torch.cuda.device_count() > 1:
                device_1 = torch.device("cuda:1")
                test_tensor_1 = torch.randn(100, 100).to(device_1)
                print(f"âœ… GPU 1 è®¿é—®æˆåŠŸ: {test_tensor_1.device}")
            else:
                print("âš ï¸ åªæœ‰ä¸€ä¸ªGPUï¼Œæ— æ³•æµ‹è¯•GPU 1")
                
        except Exception as e:
            print(f"âŒ GPUè®¿é—®å¤±è´¥: {e}")
    else:
        print("âŒ æ— æ³•æµ‹è¯•GPUè®¿é—®ï¼ŒCUDAä¸å¯ç”¨")

if __name__ == "__main__":
    check_gpu_status() 