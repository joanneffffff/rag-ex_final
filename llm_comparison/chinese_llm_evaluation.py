#!/usr/bin/env python3
"""
ç”Ÿæˆæ¨¡å—æ€§èƒ½è¯„ä¼°è„šæœ¬ - å¯¹æ¯” Fin-R1 å’Œ Qwen2-7B-Instruct åœ¨ä¸­æ–‡æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚
æ”¯æŒæ‰¹é‡éšæœºæ ·æœ¬æµ‹è¯•ï¼Œå¹¶è¾“å‡ºè¯¦ç»†æ—¥å¿—ã€‚
ä¼˜åŒ–äº†æ˜¾å­˜å ç”¨ï¼Œæ¨¡å‹æŒ‰é¡ºåºåŠ è½½å’Œå¸è½½ã€‚
Prompt Template å†…å®¹ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½ã€‚
"""

import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time
import re
import gc
import json 
import argparse 
from tqdm import tqdm 
import numpy as np 
from typing import List, Optional, Dict, Any
from collections import Counter

# å¯¼å…¥é…ç½®æ–‡ä»¶
try:
    from config.parameters import config
    print(f"âœ… ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç¼“å­˜è·¯å¾„: {config.generator.cache_dir}")
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ç¼“å­˜è·¯å¾„")
    config = None 

# ====================================================================================
# åå¤„ç†æ¨¡å—å®šä¹‰ (ä¸“é—¨é’ˆå¯¹ä¸­æ–‡)
# ====================================================================================

def _fix_company_name_translation(text: str) -> str:
    """
    ä¿®æ­£å…¬å¸åç§°ç¿»è¯‘é—®é¢˜å’Œå¹´ä»½é—®é¢˜ (ä»…é™ä¸­æ–‡)ã€‚
    """
    company_translations = {
        r'å¾·èµ›\s*battery\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰',
        r'å¾·èµ›\s*Battery\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰',
        r'å¾·èµ›\s*BATTERY\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰',
        r'å¾·èµ›\s*battery': 'å¾·èµ›ç”µæ± ',
        r'å¾·èµ›\s*Battery': 'å¾·èµ›ç”µæ± ',
        r'å¾·èµ›\s*BATTERY': 'å¾·èµ›ç”µæ± ',
        r'å¾·èµ›\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰', 
        r'å¾·å¡ç”µæ± ': 'å¾·èµ›ç”µæ± ', 
        
        r'iPhone\s*\+\s*ProMax': 'iPhone 12 Pro Max',
        r'iPhon\s*e12ProMax': 'iPhone 12 Pro Max',
        r'iPhone\s*X\s*ç³»åˆ—': 'iPhone 12 Pro Max', 
        r'iPhone\s*1\s*\(Pro\s*Max\s*\)': 'iPhone 12 Pro Max',
        r'iPhone\s*1\s*Pro\s*Max': 'iPhone 12 Pro Max',
        r'iPhone\s*2\s*ProMax': 'iPhone 12 Pro Max',
    }
    for pattern, replacement in company_translations.items():
        text = re.sub(pattern, replacement, text, flags=re.DOTALL | re.IGNORECASE)
    
    text = re.sub(r'20\s*\(\s*\d{2}\?\)\s*å¹´åº¦', r'2021å¹´åº¦', text, flags=re.IGNORECASE)
    text = text.replace('20XXå¹´', '2021å¹´')
    text = text.replace('20+', '2021')
    text = text.replace('2OI Iå¹´', '2021å¹´')
    text = text.replace('20 I Iå¹´', '2021å¹´')
        
    return text


def clean_response(text: str) -> str:
    """
    å¼ºåˆ¶åå¤„ç†æ¨¡å—ï¼šæ¸…é™¤æ‰€æœ‰æ±¡æŸ“å†…å®¹ (ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è§„åˆ™)ã€‚
    """
    # print("ğŸ§¹ å¼€å§‹å¼ºåˆ¶åå¤„ç†...") # åœ¨å¾ªç¯ä¸­æ‰“å°ä¼šå¾ˆåµ
    
    text = _fix_company_name_translation(text) 
    
    patterns_to_remove = [
        r'æˆ‘éœ€è¦æ£€æŸ¥è¿™ä¸ªå›ç­”æ˜¯å¦ç¬¦åˆè¦æ±‚.*?====', 
        r'\*\*æ³¨æ„\*\*:.*?æ”¹è¿›åçš„ç‰ˆæœ¬[:ï¼š]', 
        r'ä¸Šé¢çš„ç­”æ¡ˆè™½ç„¶ç¬¦åˆè¦æ±‚.*?ä»¥ä¸‹æ˜¯æ”¹è¿›åçš„ç‰ˆæœ¬:', 
        r'###\s*æ”¹è¿›ç‰ˆç­”æ¡ˆ', 
        r'###\s*å›ç­”', 
        r'å›ç­”å®Œæˆåç«‹å³åœæ­¢ç”Ÿæˆ', 
        r'å›ç­”å®Œæˆå¹¶åœæ­¢', 
        r'ç¡®ä¿å›ç­”', 
        r'ç”¨æˆ·å¯èƒ½', 
        r'æ€»ç»“ä¸€ä¸‹', 
        r'è¯·ç”¨ç®€æ´', 
        r'è¿›ä¸€æ­¥ç®€åŒ–', 
        r'å†ç®€åŒ–çš„ç‰ˆæœ¬', 
        r'æœ€ç»ˆç­”æ¡ˆå®šç¨¿å¦‚ä¸‹', 
        r'è¿™ä¸ªæ€»ç»“å…¨é¢', 
        r'æ ¸å¿ƒç‚¹æ€»ç»“[:ï¼š]?', 
        r'ä»¥ä¸Šåˆ†ææ˜¯å¦æ­£ç¡®ï¼Ÿè¿˜æœ‰å“ªäº›æ–¹é¢å¯ä»¥æ”¹è¿›ï¼Ÿ', 
        r'æ‚¨çš„åˆ†æåŸºæœ¬åˆç†ï¼Œä½†åœ¨æŸäº›åœ°æ–¹å¯ä»¥è¿›ä¸€æ­¥å®Œå–„å’Œç»†åŒ–ã€‚ä»¥ä¸‹æ˜¯å‡ ç‚¹æ”¹è¿›å»ºè®®ï¼š',
        r'ï¼ˆå‚é˜…ç¬¬ä¸‰éƒ¨åˆ†ï¼‰', # ä¿®æ­£ï¼šå¤šäº†ä¸€ä¸ªå³æ‹¬å·
        r'ï¼ˆè¯¦æƒ…è§ç¬¬â‘¡æ®µï¼‰',
        r'è¿™äº›é—®é¢˜çš„ç­”æ¡ˆéœ€è¦ç»“åˆå…·ä½“çš„ç ”ç©¶æŠ¥å‘Šå†…å®¹è¿›è¡Œè¯¦ç»†åˆ†æã€‚',
        r'ä¸Šè¿°ç­”æ¡ˆæ¶µç›–äº†æŠ¥å‘Šä¸­æåŠçš„å…³é”®å› ç´ ï¼Œå¹¶è¿›è¡Œäº†é€‚å½“å½’çº³ã€‚',
        r'å¦‚æœ‰éœ€è¦è¿›ä¸€æ­¥ç»†åŒ–æŸä¸€æ–¹é¢çš„å†…å®¹ï¼Œè¯·å‘ŠçŸ¥ã€‚',
        r'æ³¨æ„ï¼šä»¥ä¸Šè®ºæ–­å®Œå…¨ä¾èµ–äºå·²å…¬å¼€æŠ«éœ²çš„ä¿¡æ¯èµ„æº ; å¯¹æœªæ¥çš„å…·ä½“å‰æ™¯å°šéœ€ç»“åˆæ›´å¤šå®æ—¶æ•°æ®åŠ ä»¥éªŒè¯å’Œå®Œå–„', 
        r'ï¼ˆæ³¨æ„æ­¤æ®µæ–‡å­—è™½è¯¦ç»†é˜è¿°äº†å‡ æ–¹é¢å› ç´ åŠå…¶ç›¸äº’ä½œç”¨æœºåˆ¶ï¼Œä½†ç”±äºé¢˜å¹²è¦æ±‚é«˜åº¦æµ“ç¼©ä¸ºä¸€å¥è¯å†…å®Œæˆè¡¨è¿°ï¼Œæ•…åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†é€‚å½“ç®€åŒ–å‹ç¼©ï¼‰', 
        r'è¯·æ³¨æ„ï¼Œä»¥ä¸Šå†…å®¹æ˜¯å¯¹.*?å±•æœ›ï¼Œå¹¶éç»å¯¹ç»“è®ºã€‚', 
        r'å®é™…èµ°åŠ¿è¿˜éœ€ç»“åˆå®é™…æƒ…å†µä¸æ–­è¯„ä¼°è°ƒæ•´ã€‚å¸Œæœ›è¿™ä¸ªå›ç­”å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼', 
        r'è¦é¢„æµ‹.*?åšå‡ºåˆ¤æ–­[:ï¼š]?', 
        r'ä»¥ä¸‹æ˜¯å‡ ä¸ªå…³é”®å› ç´ å’Œæ­¥éª¤[:ï¼š]?',
        r'ç»¼ä¸Šæ‰€è¿°[:ï¼š]?', 
        r'æœ€ç»ˆç»“è®º[:ï¼š]?',
        r'ç­”æ¡ˆç¤ºä¾‹[:ï¼šï¼š]?', 
        r'æœ€ç»ˆç¡®è®¤[:ï¼š]?',
        r'ç­”æ¡ˆå¿ å®åœ°åæ˜ äº†åŸå§‹æ–‡æ¡£çš„å†…å®¹è€Œæ— å¤šä½™æ¨æ–­',
        r'å›ç­”[:ï¼š]\s*$', 
        r'å›ç­”æ˜¯ï¼š\s*', 
        r'ä»¥ä¸‹æ˜¯åŸå› ï¼š\s*',

        r'<\|[^>]+\|>', 
        r'\\boxed\{.*?\}', 
        r'\\text\{.*?\}', 
        r'\\s*', 
        r'[\u2460-\u2469]\s*', 

        r'===SYSTEM===[\s\S]*?===USER===', 
        r'---[\s\S]*?---', 
        r'ã€å…¬å¸è´¢åŠ¡æŠ¥å‘Šæ‘˜è¦ã€‘[\s\S]*?ã€å®Œæ•´å…¬å¸è´¢åŠ¡æŠ¥å‘Šç‰‡æ®µã€‘', 
        r'ã€ç”¨æˆ·é—®é¢˜ã€‘[\s\S]*?ã€å›ç­”ã€‘', 

        r'Question:\n.*?\nTable Context:', 
        r'Table Context:\n.*?\nText Context:', 
        r'Text Context:\n.*?\nQuestion:', 
        r'Context:\n.*?\nQuestion:', 
        r'Assistant\'s Response:', 
        r'--- END OF EXAMPLES ---', 
    ]
    
    for pattern in patterns_to_remove:
        text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
    
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text) 
    text = re.sub(r'\*(.*?)\*', r'\1', text) 
    text = text.replace("---", "").replace("===", "") 
    text = re.sub(r'^\s*[\d]+\.\s*', '', text, flags=re.MULTILINE) 
    text = re.sub(r'^\s*[-*â€¢Â·]\s*', '', text, flags=re.MULTILINE) 
    text = re.sub(r'^\s*\((\w|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])+\)\s*', '', text, flags=re.MULTILINE) 
    text = re.sub(r'[ï¼Œï¼›,;]$', '', text) 

    text = re.sub(r'\n+', ' ', text).strip() 
    text = re.sub(r'\s+', ' ', text).strip() 

    sentence_endings = r'(?<=[ã€‚ï¼Ÿï¼ï¼›])\s*'
    default_ending = 'ã€‚'

    sentences = re.split(sentence_endings, text)
    sentences = [s.strip() for s in sentences if s.strip()]
    
    if len(sentences) > 3: 
        sentences = sentences[:3]
    
    final_text = ' '.join(sentences) 

    if final_text and not final_text.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')): 
        final_text += default_ending

    return final_text


# ====================================================================================
# Prompt æ„é€ è¾…åŠ©å‡½æ•° (ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½)
# ====================================================================================

def _load_template_content_from_file(template_file_name: str) -> str:
    """ä»æŒ‡å®šæ–‡ä»¶ä¸­åŠ è½½Promptæ¨¡æ¿çš„å®Œæ•´å­—ç¬¦ä¸²å†…å®¹"""
    template_path = Path("data/prompt_templates") / template_file_name # ç›´æ¥ä½¿ç”¨æ–‡ä»¶åï¼Œä¸æ·»åŠ chineseå­ç›®å½•
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚")
        sys.exit(1)

def get_messages_for_test(summary: str, context: str, query: str, template_file_name: str = "chinese_test_template.txt") -> List[Dict[str, str]]:
    """
    æ„å»ºç”¨äºæµ‹è¯•çš„ messages åˆ—è¡¨ï¼Œä»æŒ‡å®šæ¨¡æ¿æ–‡ä»¶åŠ è½½å†…å®¹ã€‚
    Args:
        context (str): å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå·²åŒ…å«æ‘˜è¦ï¼‰ã€‚
        query (str): ç”¨æˆ·é—®é¢˜ã€‚
        template_file_name (str): è¦åŠ è½½çš„æ¨¡æ¿æ–‡ä»¶å (ä¾‹å¦‚ "chinese_test_template.txt")ã€‚
    Returns:
        List[Dict[str, str]]: æ„å»ºå¥½çš„ messages åˆ—è¡¨ã€‚
    """
    template_full_string = _load_template_content_from_file(template_file_name)
    
    messages = []
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ‰€æœ‰éƒ¨åˆ†ï¼Œå¹¶ä¿ç•™åˆ†éš”ç¬¦å†…å®¹
    parts = re.split(r'(===SYSTEM===|===USER===|===ASSISTANT===)', template_full_string, flags=re.DOTALL)
    
    # ç§»é™¤ç¬¬ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰å’Œå¤šä½™çš„ç©ºç™½
    parts = [p.strip() for p in parts if p.strip()]

    # éå† parts åˆ—è¡¨ï¼Œé‡æ–°ç»„åˆ role å’Œ content
    for i in range(0, len(parts), 2):
        if i + 1 < len(parts):
            role_tag_raw = parts[i].strip()
            content = parts[i+1].strip()
            
            role = None
            if role_tag_raw == "===SYSTEM===": role = "system"
            elif role_tag_raw == "===USER===": role = "user"
            elif role_tag_raw == "===ASSISTANT===": role = "assistant"
            
            if role and content:
                # æ›¿æ¢å ä½ç¬¦ (åªé’ˆå¯¹ 'user' è§’è‰²æ¶ˆæ¯è¿›è¡Œæ›¿æ¢)
                if role == "user":
                    content = content.replace('{query}', query)
                    # æ›¿æ¢ {summary} å’Œ {context}
                    # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„summaryå’Œcontextå‚æ•°
                    content = content.replace('{summary}', summary).replace('{context}', context)
                
                messages.append({"role": role, "content": content})
                
    return messages


def _convert_messages_to_chatml(messages: List[Dict[str, str]]) -> str:
    """
    å°† messages åˆ—è¡¨è½¬æ¢ä¸º Fin-R1 (Qwen2.5 based) æœŸæœ›çš„ChatMLæ ¼å¼å­—ç¬¦ä¸²ã€‚
    """
    if not messages:
        return ""
    
    formatted_prompt = ""
    for message in messages:
        role = message.get("role", "")
        content = message.get("content", "")
        
        if role == "system":
            formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n"
        elif role == "user":
            formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n"
        elif role == "assistant":
            formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n"
    
    formatted_prompt += "<|im_start|>assistant\n" 
    
    return formatted_prompt


# ====================================================================================
# æ¨¡å‹åŠ è½½å’Œç”Ÿæˆå™¨åŒ…è£…ç±»
# ====================================================================================

class ModelLoader:
    """è´Ÿè´£åŠ è½½å’Œå¸è½½æ¨¡å‹ï¼Œå¹¶æä¾›ç”Ÿæˆæ¥å£"""
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.is_loaded = False

        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç¼“å­˜è·¯å¾„
        cache_dir = config.generator.cache_dir if config else "/users/sgjfei3/data/huggingface"
        
        if "Fin-R1" in model_name: 
            # æ£€æŸ¥æœ¬åœ°ç¼“å­˜è·¯å¾„
            local_fin_r1_path = f"{cache_dir}/models--SUFE-AIFLM-Lab--Fin-R1/snapshots/026768c4a015b591b54b240743edeac1de0970fa"
            if os.path.exists(local_fin_r1_path):
                self.model_path = local_fin_r1_path
                print(f"âœ… ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„Fin-R1æ¨¡å‹: {self.model_path}")
            else:
                self.model_path = "SUFE-AIFLM-Lab/Fin-R1"
                print(f"âš ï¸ æœ¬åœ°ç¼“å­˜æœªæ‰¾åˆ°ï¼Œå°†ä»Hubä¸‹è½½: {self.model_path}")
        elif "Qwen3-8B" in model_name:
            # æ£€æŸ¥æœ¬åœ°ç¼“å­˜è·¯å¾„ - ä½¿ç”¨æ­£ç¡®çš„Qwen3-8Bè·¯å¾„
            local_qwen_path = f"{cache_dir}/models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5"
            if os.path.exists(local_qwen_path):
                self.model_path = local_qwen_path
                print(f"âœ… ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„Qwen3-8Bæ¨¡å‹: {self.model_path}")
            else:
                self.model_path = "Qwen/Qwen3-8B"
                print(f"âš ï¸ æœ¬åœ°ç¼“å­˜æœªæ‰¾åˆ°ï¼Œå°†ä»Hubä¸‹è½½: {self.model_path}")
        else:
            self.model_path = model_name 
            print(f"âš ï¸ æ¨¡å‹è·¯å¾„ '{model_name}' æœªçŸ¥ï¼Œå°è¯•ä»Hugging Face HubåŠ è½½ã€‚å»ºè®®æå‰ä¸‹è½½åˆ°æœ¬åœ°ã€‚")

    def load_model(self):
        if self.is_loaded:
            print(f"âœ… {self.model_name} å·²åŠ è½½ï¼Œæ— éœ€é‡å¤åŠ è½½ã€‚")
            return
        
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {self.model_name} ä» {self.model_path}")
        is_local_path = isinstance(self.model_path, str) and ("snapshots" in self.model_path or "models--" in self.model_path) # åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°ç¼“å­˜è·¯å¾„

        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç¼“å­˜è®¾ç½®
        cache_dir = config.generator.cache_dir if config else None
        tokenizer_args = {"trust_remote_code": True, "local_files_only": is_local_path, "cache_dir": cache_dir}
        model_args = {"torch_dtype": torch.float16, "device_map": "auto", "trust_remote_code": True, 
                      "load_in_8bit": True, "local_files_only": is_local_path, "cache_dir": cache_dir} 

        try:
            print("ğŸ”§ åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_args)
            if self.tokenizer.pad_token is None: self.tokenizer.pad_token = self.tokenizer.eos_token
            if self.tokenizer.pad_token_id is None: self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            print(f"âœ… {self.model_name} TokenizeråŠ è½½å®Œæˆ. Chat Template: {self.tokenizer.chat_template}")

            print("ğŸ”§ åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(self.model_path, **model_args)
            self.model.eval()
            print(f"âœ… {self.model_name} æ¨¡å‹åŠ è½½å®Œæˆ. è®¾å¤‡: {self.model.device}, é‡åŒ–: 8bit")
            self.is_loaded = True
        except Exception as e:
            print(f"âŒ {self.model_name} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.unload_model() # ç¡®ä¿å¤±è´¥æ—¶ä¹Ÿæ¸…ç†
            raise

    def unload_model(self):
        if not self.is_loaded:
            return
        
        print(f"ğŸ—‘ï¸ å¸è½½æ¨¡å‹: {self.model_name} å¹¶æ¸…ç†æ˜¾å­˜...")
        try:
            if self.model:
                # å¯¹äº8ä½é‡åŒ–æ¨¡å‹ï¼Œç›´æ¥åˆ é™¤è€Œä¸ä½¿ç”¨.to()æ–¹æ³•
                del self.model
            if self.tokenizer:
                del self.tokenizer
            self.model = None
            self.tokenizer = None
            torch.cuda.empty_cache() # æ¸…ç†CUDAç¼“å­˜
            gc.collect() # åƒåœ¾å›æ”¶
            self.is_loaded = False
            print(f"âœ… {self.model_name} æ˜¾å­˜å·²æ¸…ç†ã€‚")
        except Exception as e:
            print(f"âŒ å¸è½½ {self.model_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def generate(self, prompt_string: str, max_new_tokens: int = 150, do_sample: bool = False, repetition_penalty: float = 1.1) -> str:
        """ç”Ÿæˆæ–‡æœ¬ï¼ŒæœŸæœ›è¾“å…¥å·²ç»æ˜¯ ChatML æ ¼å¼çš„å­—ç¬¦ä¸²"""
        if not self.is_loaded:
            raise RuntimeError(f"æ¨¡å‹ {self.model_name} æœªåŠ è½½ã€‚è¯·å…ˆè°ƒç”¨ load_model()ã€‚")

        inputs = self.tokenizer(prompt_string, return_tensors="pt", padding=True, truncation=True).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate( 
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=repetition_penalty
            )
        
        generated_tokens = outputs[0, inputs["input_ids"].shape[1]:]
        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True) 
        
        return generated_text

# ====================================================================================
# è¯„ä¼°æŒ‡æ ‡è®¡ç®— (ä½ éœ€è¦å®ç°è¿™äº›å‡½æ•°ï¼Œè¿™é‡Œæ˜¯å ä½ç¬¦)
# ====================================================================================
# Placeholder for F1 and EM if not imported
def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—F1åˆ†æ•°ï¼Œè¿™é‡Œæ˜¯å ä½ç¬¦ã€‚"""
    return 1.0 if prediction == ground_truth else 0.0

def calculate_exact_match(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…ç‡ï¼Œè¿™é‡Œæ˜¯å ä½ç¬¦ã€‚"""
    return 1.0 if prediction == ground_truth else 0.0


# ====================================================================================
# ä¸»æµ‹è¯•é€»è¾‘
# ====================================================================================

def run_chinese_comparison_test(args):
    print("ğŸš€ ä¸­æ–‡æ¨¡å‹å¯¹æ¯”æµ‹è¯•å¼€å§‹...")
    
    # --- é…ç½®è¦æµ‹è¯•çš„æ¨¡å‹ ---
    model_loaders = {
        "Fin-R1": ModelLoader("Fin-R1"),
        "Qwen3-8B": ModelLoader("Qwen3-8B")
    }

    # --- æµ‹è¯•é…ç½® (ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°) ---
    data_path = args.data_path # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    sample_size = args.sample_size # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°
    # æ¨¡æ¿æ–‡ä»¶åï¼Œéœ€è¦ä¸ data/prompt_templates/chinese/ ä¸‹çš„æ–‡ä»¶åä¸€è‡´
    template_file_name = "multi_stage_chinese_template.txt"  # åªä½¿ç”¨æ–‡ä»¶åï¼Œè·¯å¾„åœ¨å‡½æ•°ä¸­æ‹¼æ¥ 
    
    # --- åŠ è½½æ•°æ®é›† ---
    print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {data_path}")
    try:
        from utils.data_loader import load_json_or_jsonl, sample_data 
        dataset = load_json_or_jsonl(data_path)
        
        if sample_size > 0:
            dataset = sample_data(dataset, sample_size, 42)
            print(f"âœ… éšæœºé‡‡æ · {len(dataset)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
        else:
            print(f"âœ… åŠ è½½äº†å…¨éƒ¨ {len(dataset)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
            
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return

    all_results_data = [] # å­˜å‚¨æ‰€æœ‰æ¨¡å‹çš„è¯„ä¼°ç»“æœ

    # --- é€ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼° ---
    for model_name, loader in model_loaders.items():
        print(f"\nğŸ”„ å¼€å§‹è¯„ä¼°æ¨¡å‹: {model_name}")
        print(f"ğŸ“Š å½“å‰GPUå†…å­˜çŠ¶æ€:")
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                print(f"   GPU {i}: å·²åˆ†é… {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")
        
        current_model_results = []
        total_f1_model = 0.0
        total_em_model = 0.0
        total_generation_time_model = 0.0
        
        try:
            print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            loader.load_model() # åŠ è½½å½“å‰æ¨¡å‹
            print(f"âœ… æ¨¡å‹ {model_name} åŠ è½½å®Œæˆï¼Œå¼€å§‹è¯„ä¼°...")
            
            pbar = tqdm(dataset, desc=f"è¯„ä¼° {model_name}")
            for i, item in enumerate(pbar):
                # å…¼å®¹å¤šç§æŸ¥è¯¢å­—æ®µå
                query = item.get("query", "") or item.get("generated_question", "") or item.get("question", "")
                summary = item.get("summary", "") # è·å–summaryå­—æ®µ
                context = item.get("context", "") # è·å–contextå­—æ®µ
                expected_answer = item.get("answer", "") # è·å–å‚è€ƒç­”æ¡ˆ

                # æ„å»º messages åˆ—è¡¨ï¼Œä»å¤–éƒ¨æ¨¡æ¿æ–‡ä»¶åŠ è½½
                # ä¼ é€’summaryå’Œcontextï¼Œè®©æ¨¡æ¿å‡½æ•°æ­£ç¡®å¤„ç†
                messages = get_messages_for_test(summary, context, query, template_file_name)
                
                # è½¬æ¢ä¸º ChatML æ ¼å¼
                prompt_string_for_model = _convert_messages_to_chatml(messages)
                
                start_time = time.time()
                generated_text = loader.generate( # è°ƒç”¨ loader å†…éƒ¨çš„ generate
                    prompt_string=prompt_string_for_model,
                    max_new_tokens=150, # ä½¿ç”¨ç¡¬ç¼–ç çš„max_new_tokens
                    do_sample=False, 
                    repetition_penalty=1.1
                )
                generation_time = time.time() - start_time
                
                final_answer = clean_response(generated_text) # åå¤„ç† (ç¡¬ç¼–ç ä¸ºä¸­æ–‡)
                
                f1 = calculate_f1_score(final_answer, expected_answer)
                em = calculate_exact_match(final_answer, expected_answer)

                total_f1_model += f1
                total_em_model += em
                total_generation_time_model += generation_time

                current_model_results.append({
                    "model": model_name,
                    "sample_id": i,
                    "query": query,
                    "expected_answer": expected_answer,
                    "raw_generated_text": generated_text,
                    "final_answer": final_answer,
                    "f1_score": f1,
                    "exact_match": em,
                    "generation_time": generation_time
                })

            # æ‰“å°å½“å‰æ¨¡å‹çš„æ±‡æ€»ç»“æœ
            num_samples_evaluated = len(dataset)
            avg_f1 = total_f1_model / num_samples_evaluated
            avg_em = total_em_model / num_samples_evaluated
            avg_gen_time = total_generation_time_model / num_samples_evaluated

            print(f"\n--- {model_name} è¯„ä¼°æ€»ç»“ ---")
            print(f"æ€»æ ·æœ¬æ•°: {num_samples_evaluated}")
            print(f"å¹³å‡ F1-score: {avg_f1:.4f}")
            print(f"å¹³å‡ Exact Match: {avg_em:.4f}")
            print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_gen_time:.2f} ç§’/æ ·æœ¬")
            print("--------------------")
            
            all_results_data.extend(current_model_results)

        except Exception as e:
            print(f"âŒ è¯„ä¼° {model_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            print(f"ğŸ—‘ï¸ æ­£åœ¨å¸è½½æ¨¡å‹: {model_name}")
            loader.unload_model() # ç¡®ä¿æ¯æ¬¡å¾ªç¯éƒ½å¸è½½æ¨¡å‹
            print(f"âœ… æ¨¡å‹ {model_name} å¸è½½å®Œæˆ")
            print(f"ğŸ“Š å¸è½½åGPUå†…å­˜çŠ¶æ€:")
            if torch.cuda.is_available():
                for i in range(torch.cuda.device_count()):
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    cached = torch.cuda.memory_reserved(i) / 1024**3
                    print(f"   GPU {i}: å·²åˆ†é… {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")

    # --- è¯„ä¼°å®Œæˆï¼Œä¿å­˜æ‰€æœ‰ç»“æœ ---
    output_filename = f"comparison_results_chinese_{os.path.basename(data_path).replace('.jsonl', '')}_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(all_results_data, f, ensure_ascii=False, indent=4)
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä¸­æ–‡æ¨¡å‹å¯¹æ¯”æµ‹è¯•è„šæœ¬")
    parser.add_argument("--model", type=str, default="SUFE-AIFLM-Lab/Fin-R1", help="è¦è¯„ä¼°çš„LLMåç§° (Fin-R1 æˆ– Qwen3-8B)")
    parser.add_argument("--data_path", type=str, required=True, help="è¯„ä¼°æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (jsonlæ ¼å¼ï¼Œä¾‹å¦‚ evaluate_mrr/alphafin_eval_optimized.jsonl)")
    parser.add_argument("--sample_size", type=int, default=500, help="éšæœºé‡‡æ ·çš„æ ·æœ¬æ•°é‡ (0è¡¨ç¤ºè¯„ä¼°å…¨éƒ¨)")
    parser.add_argument("--device", type=str, default="cuda:0", help="æ¨¡å‹éƒ¨ç½²çš„è®¾å¤‡ (ä¾‹å¦‚ 'cuda:0' æˆ– 'cpu')")
    parser.add_argument("--max_new_tokens", type=int, default=150, help="æ¨¡å‹ç”Ÿæˆæœ€å¤§æ–°Tokenæ•°")
    parser.add_argument("--do_sample", type=bool, default=False, help="æ˜¯å¦ä½¿ç”¨é‡‡æ ·ç”Ÿæˆ (True/False)")
    parser.add_argument("--repetition_penalty", type=float, default=1.1, help="é‡å¤æƒ©ç½šç³»æ•°")
    
    args = parser.parse_args()
    run_chinese_comparison_test(args)  # ä¼ é€’argså‚æ•°