#!/usr/bin/env python3
"""
ç”Ÿæˆæ¨¡å—æ€§èƒ½è¯„ä¼°è„šæœ¬ - å¯¹æ¯” Fin-R1 å’Œ Qwen3-8B åœ¨ä¸­æ–‡æ•°æ®é›†ä¸Šçš„è¡¨ç°ã€‚
æ”¯æŒæ‰¹é‡éšæœºæ ·æœ¬æµ‹è¯•ï¼Œå¹¶è¾“å‡ºè¯¦ç»†æ—¥å¿—ã€‚
åˆ©ç”¨åŒ GPU è¿›è¡Œæ¨¡å‹å¹¶è¡ŒåŠ è½½å’Œè¯„ä¼°ä»¥åŠ é€Ÿã€‚
Prompt Template å†…å®¹ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½ã€‚
å¢åŠ äº† F1-score å’Œ Exact Match çš„æ­£ç¡®è®¡ç®—ï¼ˆæ”¯æŒä¸­æ–‡åˆ†è¯ï¼‰ã€‚
ç»Ÿè®¡äº†è¾“å…¥/è¾“å‡º Token æ•°å’Œçº¯ç”Ÿæˆæ—¶é—´ã€‚
"""

import sys
import os
from pathlib import Path
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import time
import re
import gc
import json
import argparse
from tqdm import tqdm
import numpy as np
from typing import List, Optional, Dict, Any
from collections import Counter
import string
import jieba # å¼•å…¥jiebaåº“

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥é…ç½®æ–‡ä»¶ (è¯·ç¡®ä¿ config/parameters.py å­˜åœ¨å¹¶å®šä¹‰äº† config.generator.cache_dir)
try:
    from config.parameters import config
    print(f"âœ… ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç¼“å­˜è·¯å¾„: {config.generator.cache_dir}")
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ç¼“å­˜è·¯å¾„ '/users/sgjfei3/data/huggingface'")
    class Config: # å®šä¹‰ä¸€ä¸ªå‡çš„configç±»ï¼Œé˜²æ­¢æŠ¥é”™
        class Generator:
            cache_dir = "/users/sgjfei3/data/huggingface"
    config = Config()

# ç¡®ä¿ bitsandbytes, accelerate å·²å®‰è£…
# pip install bitsandbytes accelerate

# ====================================================================================
# åå¤„ç†æ¨¡å—å®šä¹‰ (ä¸“é—¨é’ˆå¯¹ä¸­æ–‡)
# ====================================================================================

def _fix_company_name_translation(text: str) -> str:
    """
    ä¿®æ­£å…¬å¸åç§°ç¿»è¯‘é—®é¢˜å’Œå¹´ä»½é—®é¢˜ (ä»…é™ä¸­æ–‡)ã€‚
    """
    company_translations = {
        r'å¾·èµ›\s*battery\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰',
        r'å¾·èµ›\s*Battery\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰',
        r'å¾·èµ›\s*BATTERY\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰',
        r'å¾·èµ›\s*battery': 'å¾·èµ›ç”µæ± ',
        r'å¾·èµ›\s*Battery': 'å¾·èµ›ç”µæ± ',
        r'å¾·èµ›\s*BATTERY': 'å¾·èµ›ç”µæ± ',
        r'å¾·èµ›\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰',
        r'å¾·å¡ç”µæ± ': 'å¾·èµ›ç”µæ± ',

        r'iPhone\s*\+\s*ProMax': 'iPhone 12 Pro Max',
        r'iPhon\s*e12ProMax': 'iPhone 12 Pro Max',
        r'iPhone\s*X\s*ç³»åˆ—': 'iPhone 12 Pro Max',
        r'iPhone\s*1\s*\(Pro\s*Max\s*\)': 'iPhone 12 Pro Max',
        r'iPhone\s*1\s*Pro\s*Max': 'iPhone 12 Pro Max',
        r'iPhone\s*2\s*ProMax': 'iPhone 12 Pro Max',
    }
    for pattern, replacement in company_translations.items():
        text = re.sub(pattern, replacement, text, flags=re.DOTALL | re.IGNORECASE)

    text = re.sub(r'20\s*\(\s*\d{2}\?\)\s*å¹´åº¦', r'2021å¹´åº¦', text, flags=re.IGNORECASE)
    text = text.replace('20XXå¹´', '2021å¹´')
    text = text.replace('20+', '2021')
    text = text.replace('2OI Iå¹´', '2021å¹´')
    text = text.replace('20 I Iå¹´', '2021å¹´')

    return text


def clean_response(text: str) -> str:
    """
    å¼ºåˆ¶åå¤„ç†æ¨¡å—ï¼šæ¸…é™¤æ‰€æœ‰æ±¡æŸ“å†…å®¹ (ä¸“é—¨é’ˆå¯¹ä¸­æ–‡è§„åˆ™)ã€‚
    """
    text = _fix_company_name_translation(text)

    patterns_to_remove = [
        r'æˆ‘éœ€è¦æ£€æŸ¥è¿™ä¸ªå›ç­”æ˜¯å¦ç¬¦åˆè¦æ±‚.*?====',
        r'\*\*æ³¨æ„\*\*:.*?æ”¹è¿›åçš„ç‰ˆæœ¬[:ï¼š]',
        r'ä¸Šé¢çš„ç­”æ¡ˆè™½ç„¶ç¬¦åˆè¦æ±‚.*?ä»¥ä¸‹æ˜¯æ”¹è¿›åçš„ç‰ˆæœ¬:',
        r'###\s*æ”¹è¿›ç‰ˆç­”æ¡ˆ',
        r'###\s*å›ç­”',
        r'å›ç­”å®Œæˆåç«‹å³åœæ­¢ç”Ÿæˆ',
        r'å›ç­”å®Œæˆå¹¶åœæ­¢',
        r'ç¡®ä¿å›ç­”',
        r'ç”¨æˆ·å¯èƒ½',
        r'æ€»ç»“ä¸€ä¸‹',
        r'è¯·ç”¨ç®€æ´',
        r'è¿›ä¸€æ­¥ç®€åŒ–',
        r'å†ç®€åŒ–çš„ç‰ˆæœ¬',
        r'æœ€ç»ˆç­”æ¡ˆå®šç¨¿å¦‚ä¸‹',
        r'è¿™ä¸ªæ€»ç»“å…¨é¢',
        r'æ ¸å¿ƒç‚¹æ€»ç»“[:ï¼š]?',
        r'ä»¥ä¸Šåˆ†ææ˜¯å¦æ­£ç¡®ï¼Ÿè¿˜æœ‰å“ªäº›æ–¹é¢å¯ä»¥æ”¹è¿›ï¼Ÿ',
        r'æ‚¨çš„åˆ†æåŸºæœ¬åˆç†ï¼Œä½†åœ¨æŸäº›åœ°æ–¹å¯ä»¥è¿›ä¸€æ­¥å®Œå–„å’Œç»†åŒ–ã€‚ä»¥ä¸‹æ˜¯å‡ ç‚¹æ”¹è¿›å»ºè®®ï¼š',
        r'ï¼ˆå‚é˜…ç¬¬ä¸‰éƒ¨åˆ†ï¼‰',
        r'ï¼ˆè¯¦æƒ…è§ç¬¬â‘¡æ®µï¼‰',
        r'è¿™äº›é—®é¢˜çš„ç­”æ¡ˆéœ€è¦ç»“åˆå…·ä½“çš„ç ”ç©¶æŠ¥å‘Šå†…å®¹è¿›è¡Œè¯¦ç»†åˆ†æã€‚',
        r'ä¸Šè¿°ç­”æ¡ˆæ¶µç›–äº†æŠ¥å‘Šä¸­æåŠçš„å…³é”®å› ç´ ï¼Œå¹¶è¿›è¡Œäº†é€‚å½“å½’çº³ã€‚',
        r'å¦‚æœ‰éœ€è¦è¿›ä¸€æ­¥ç»†åŒ–æŸä¸€æ–¹é¢çš„å†…å®¹ï¼Œè¯·å‘ŠçŸ¥ã€‚',
        r'æ³¨æ„ï¼šä»¥ä¸Šè®ºæ–­å®Œå…¨ä¾èµ–äºå·²å…¬å¼€æŠ«éœ²çš„ä¿¡æ¯èµ„æº ; å¯¹æœªæ¥çš„å…·ä½“å‰æ™¯å°šéœ€ç»“åˆæ›´å¤šå®æ—¶æ•°æ®åŠ ä»¥éªŒè¯å’Œå®Œå–„',
        r'ï¼ˆæ³¨æ„æ­¤æ®µæ–‡å­—è™½è¯¦ç»†é˜è¿°äº†å‡ æ–¹é¢å› ç´ åŠå…¶ç›¸äº’ä½œç”¨æœºåˆ¶ï¼Œä½†ç”±äºé¢˜å¹²è¦æ±‚é«˜åº¦æµ“ç¼©ä¸ºä¸€å¥è¯å†…å®Œæˆè¡¨è¿°ï¼Œæ•…åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†é€‚å½“ç®€åŒ–å‹ç¼©ï¼‰',
        r'è¯·æ³¨æ„ï¼Œä»¥ä¸Šå†…å®¹æ˜¯å¯¹.*?å±•æœ›ï¼Œå¹¶éç»å¯¹ç»“è®ºã€‚',
        r'å®é™…èµ°åŠ¿è¿˜éœ€ç»“åˆå®é™…æƒ…å†µä¸æ–­è¯„ä¼°è°ƒæ•´ã€‚å¸Œæœ›è¿™ä¸ªå›ç­”å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼',
        r'è¦é¢„æµ‹.*?åšå‡ºåˆ¤æ–­[:ï¼š]?',
        r'ä»¥ä¸‹æ˜¯å‡ ä¸ªå…³é”®å› ç´ å’Œæ­¥éª¤[:ï¼š]?',
        r'ç»¼ä¸Šæ‰€è¿°[:ï¼š]?',
        r'æœ€ç»ˆç»“è®º[:ï¼š]?',
        r'ç­”æ¡ˆç¤ºä¾‹[:ï¼šï¼š]?',
        r'æœ€ç»ˆç¡®è®¤[:ï¼š]?',
        r'ç­”æ¡ˆå¿ å®åœ°åæ˜ äº†åŸå§‹æ–‡æ¡£çš„å†…å®¹è€Œæ— å¤šä½™æ¨æ–­',
        r'å›ç­”[:ï¼š]\s*$',
        r'å›ç­”æ˜¯ï¼š\s*',
        r'ä»¥ä¸‹æ˜¯åŸå› ï¼š\s*',

        r'<\|[^>]+\|>',
        r'\\boxed\{.*?\}',
        r'\\text\{.*?\}',
        r'\\s*',
        r'[\u2460-\u2469]\s*',

        r'===SYSTEM===[\s\S]*?===USER===',
        r'---[\s\S]*?---',
        r'ã€å…¬å¸è´¢åŠ¡æŠ¥å‘Šæ‘˜è¦ã€‘[\s\S]*?ã€å®Œæ•´å…¬å¸è´¢åŠ¡æŠ¥å‘Šç‰‡æ®µã€‘',
        r'ã€ç”¨æˆ·é—®é¢˜ã€‘[\s\S]*?ã€å›ç­”ã€‘',

        r'Question:\n.*?\nTable Context:',
        r'Table Context:\n.*?\nText Context:',
        r'Text Context:\n.*?\nQuestion:',
        r'Context:\n.*?\nQuestion:',
        r'Assistant\'s Response:',
        r'--- END OF EXAMPLES ---',
    ]

    for pattern in patterns_to_remove:
        text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()

    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)
    text = re.sub(r'\*(.*?)\*', r'\1', text)
    text = text.replace("---", "").replace("===", "")
    text = re.sub(r'^\s*[\d]+\.\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*[-*â€¢Â·]\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*\((\w|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])+\)\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'[ï¼Œï¼›,;]$', '', text)

    text = re.sub(r'\n+', ' ', text).strip()
    text = re.sub(r'\s+', ' ', text).strip()

    sentence_endings = r'(?<=[ã€‚ï¼Ÿï¼ï¼›])\s*'
    default_ending = 'ã€‚'

    sentences = re.split(sentence_endings, text)
    sentences = [s.strip() for s in sentences if s.strip()]

    if len(sentences) > 3:
        sentences = sentences[:3]

    final_text = ' '.join(sentences)

    if final_text and not final_text.endswith(('.', '!', '?', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
        final_text += default_ending

    return final_text

# ====================================================================================
# Prompt æ„é€ è¾…åŠ©å‡½æ•° (ä»å¤–éƒ¨æ–‡ä»¶åŠ è½½)
# ====================================================================================

def _load_template_content_from_file(template_file_name: str) -> str:
    """ä»æŒ‡å®šæ–‡ä»¶ä¸­åŠ è½½Promptæ¨¡æ¿çš„å®Œæ•´å­—ç¬¦ä¸²å†…å®¹"""
    template_path = Path("data/prompt_templates") / template_file_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚")
        sys.exit(1)

def get_messages_for_test(summary: str, context: str, query: str, template_file_name: str = "chinese_test_template.txt") -> List[Dict[str, str]]:
    """
    æ„å»ºç”¨äºæµ‹è¯•çš„ messages åˆ—è¡¨ï¼Œä»æŒ‡å®šæ¨¡æ¿æ–‡ä»¶åŠ è½½å†…å®¹ã€‚
    Args:
        context (str): å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå·²åŒ…å«æ‘˜è¦ï¼‰ã€‚
        query (str): ç”¨æˆ·é—®é¢˜ã€‚
        template_file_name (str): è¦åŠ è½½çš„æ¨¡æ¿æ–‡ä»¶å (ä¾‹å¦‚ "chinese_test_template.txt")ã€‚
    Returns:
        List[Dict[str, str]]: æ„å»ºå¥½çš„ messages åˆ—è¡¨ã€‚
    """
    template_full_string = _load_template_content_from_file(template_file_name)

    messages = []
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ‰€æœ‰éƒ¨åˆ†ï¼Œå¹¶ä¿ç•™åˆ†éš”ç¬¦å†…å®¹
    parts = re.split(r'(===SYSTEM===|===USER===|===ASSISTANT===)', template_full_string, flags=re.DOTALL)

    # ç§»é™¤ç¬¬ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰å’Œå¤šä½™çš„ç©ºç™½
    parts = [p.strip() for p in parts if p.strip()]

    # éå† parts åˆ—è¡¨ï¼Œé‡æ–°ç»„åˆ role å’Œ content
    for i in range(0, len(parts), 2):
        if i + 1 < len(parts):
            role_tag_raw = parts[i].strip()
            content = parts[i+1].strip()

            role = None
            if role_tag_raw == "===SYSTEM===": role = "system"
            elif role_tag_raw == "===USER===": role = "user"
            elif role_tag_raw == "===ASSISTANT===": role = "assistant"

            if role and content:
                # æ›¿æ¢å ä½ç¬¦ (åªé’ˆå¯¹ 'user' è§’è‰²æ¶ˆæ¯è¿›è¡Œæ›¿æ¢)
                if role == "user":
                    content = content.replace('{query}', query)
                    # ç›´æ¥ä½¿ç”¨ä¼ å…¥çš„summaryå’Œcontextå‚æ•°
                    content = content.replace('{summary}', summary).replace('{context}', context)

                messages.append({"role": role, "content": content})

    return messages


def _convert_messages_to_chatml(messages: List[Dict[str, str]]) -> str:
    """
    å°† messages åˆ—è¡¨è½¬æ¢ä¸º Fin-R1 (Qwen2.5 based) æœŸæœ›çš„ChatMLæ ¼å¼å­—ç¬¦ä¸²ã€‚
    æ³¨æ„ï¼šè¿™é‡Œçš„ `im_im_end` å¯èƒ½æ˜¯ç¬”è¯¯ï¼ŒQwenç³»åˆ—æ ‡å‡†åº”è¯¥æ˜¯ `im_end`
    """
    if not messages:
        return ""

    formatted_prompt = ""
    for message in messages:
        role = message.get("role", "")
        content = message.get("content", "")

        if role == "system":
            formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n" # æ›´æ­£ä¸ºim_end
        elif role == "user":
            formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n" # æ›´æ­£ä¸ºim_end
        elif role == "assistant":
            formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n" # æ›´æ­£ä¸ºim_end

    formatted_prompt += "<|im_start|>assistant\n"

    return formatted_prompt


# ====================================================================================
# æ¨¡å‹åŠ è½½å’Œç”Ÿæˆå™¨åŒ…è£…ç±»
# ====================================================================================

class ModelLoader:
    """è´Ÿè´£åŠ è½½å’Œå¸è½½æ¨¡å‹ï¼Œå¹¶æä¾›ç”Ÿæˆæ¥å£"""
    def __init__(self, model_name: str, device: str): # æ–°å¢ device å‚æ•°
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = device # ä½¿ç”¨ä¼ å…¥çš„ device
        self.is_loaded = False

        cache_dir = config.generator.cache_dir

        if "Fin-R1" in model_name:
            # æ£€æŸ¥æœ¬åœ°ç¼“å­˜è·¯å¾„ - ä½¿ç”¨ Fin-R1 çš„æ­£ç¡®è·¯å¾„
            local_fin_r1_path = f"{cache_dir}/models--SUFE-AIFLM-Lab--Fin-R1/snapshots/026768c4a015b591b54b240743edeac1de0970fa"
            if os.path.exists(local_fin_r1_path):
                self.model_path = local_fin_r1_path
                print(f"âœ… [{self.model_name}] ä½¿ç”¨æœ¬åœ°ç¼“å­˜æ¨¡å‹: {self.model_path}")
            else:
                self.model_path = "SUFE-AIFLM-Lab/Fin-R1"
                print(f"âš ï¸ [{self.model_name}] æœ¬åœ°ç¼“å­˜æœªæ‰¾åˆ°ï¼Œå°†ä»Hubä¸‹è½½: {self.model_path}")
        elif "Qwen3-8B" in model_name:
            # æ£€æŸ¥æœ¬åœ°ç¼“å­˜è·¯å¾„ - ä½¿ç”¨ Qwen3-8B çš„æ­£ç¡®è·¯å¾„
            local_qwen_path = f"{cache_dir}/models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5" # å‡è®¾è¿™æ˜¯Qwen3-8Bçš„æŸä¸ªç¨³å®šå¿«ç…§
            if os.path.exists(local_qwen_path):
                self.model_path = local_qwen_path
                print(f"âœ… [{self.model_name}] ä½¿ç”¨æœ¬åœ°ç¼“å­˜æ¨¡å‹: {self.model_path}")
            else:
                self.model_path = "Qwen/Qwen3-8B"
                print(f"âš ï¸ [{self.model_name}] æœ¬åœ°ç¼“å­˜æœªæ‰¾åˆ°ï¼Œå°†ä»Hubä¸‹è½½: {self.model_path}")
        else:
            self.model_path = model_name
            print(f"âš ï¸ [{self.model_name}] æ¨¡å‹è·¯å¾„ '{model_name}' æœªçŸ¥ï¼Œå°è¯•ä»Hugging Face HubåŠ è½½ã€‚å»ºè®®æå‰ä¸‹è½½åˆ°æœ¬åœ°ã€‚")

        # 4-bit é‡åŒ–é…ç½® (ç¡®ä¿ `bitsandbytes` å·²å®‰è£…)
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4", # NormalFloat 4-bit
            bnb_4bit_compute_dtype=torch.float16, # è®¡ç®—æ•°æ®ç±»å‹ï¼Œé€šå¸¸è®¾ç½®ä¸º float16
            bnb_4bit_use_double_quant=False, # ä¸ä½¿ç”¨åŒé‡åŒ–
        )

    def load_model(self):
        if self.is_loaded:
            print(f"âœ… [{self.model_name}] å·²åŠ è½½åˆ° {self.device}ï¼Œæ— éœ€é‡å¤åŠ è½½ã€‚")
            return

        print(f"ğŸ”„ [{self.model_name}] æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° {self.device} ä» {self.model_path}")
        # åˆ¤æ–­æ˜¯å¦ä¸ºæœ¬åœ°è·¯å¾„ï¼Œå½±å“ local_files_only å‚æ•°
        is_local_path = Path(self.model_path).exists() and Path(self.model_path).is_dir()

        cache_dir = config.generator.cache_dir
        tokenizer_args = {"trust_remote_code": True, "local_files_only": is_local_path, "cache_dir": cache_dir}
        model_args = {
            "torch_dtype": torch.float16,
            "device_map": self.device, # ç›´æ¥æŒ‡å®šè®¾å¤‡ï¼ŒBitsAndBytesConfig ä¼šå¤„ç†åˆ†é…
            "trust_remote_code": True,
            "quantization_config": self.quantization_config, # ä½¿ç”¨ 4-bit é‡åŒ–é…ç½®
            "local_files_only": is_local_path,
            "cache_dir": cache_dir
        }

        try:
            print(f"ğŸ”§ [{self.model_name}] åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_args)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            if self.tokenizer.pad_token_id is None:
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            print(f"âœ… [{self.model_name}] TokenizeråŠ è½½å®Œæˆ. Chat Template: {self.tokenizer.chat_template}")

            print(f"ğŸ”§ [{self.model_name}] åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(self.model_path, **model_args)
            self.model.eval()
            print(f"âœ… [{self.model_name}] æ¨¡å‹åŠ è½½å®Œæˆ. è®¾å¤‡: {self.model.device.type}:{self.model.device.index}, é‡åŒ–: 4bit")
            self.is_loaded = True
        except Exception as e:
            print(f"âŒ [{self.model_name}] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.unload_model() # ç¡®ä¿å¤±è´¥æ—¶ä¹Ÿæ¸…ç†
            raise

    def unload_model(self):
        if not self.is_loaded:
            return

        print(f"ğŸ—‘ï¸ [{self.model_name}] å¸è½½æ¨¡å‹å¹¶æ¸…ç†æ˜¾å­˜...")
        try:
            if self.model:
                del self.model
            if self.tokenizer:
                del self.tokenizer
            self.model = None
            self.tokenizer = None
            # æ¸…ç†CUDAç¼“å­˜å¹¶å¼ºåˆ¶åƒåœ¾å›æ”¶
            torch.cuda.empty_cache()
            gc.collect()
            self.is_loaded = False
            print(f"âœ… [{self.model_name}] æ˜¾å­˜å·²æ¸…ç†ã€‚")
        except Exception as e:
            print(f"âŒ å¸è½½ [{self.model_name}] æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def generate(self, prompt_string: str, max_new_tokens: int = 150, do_sample: bool = False, repetition_penalty: float = 1.1) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ–‡æœ¬ï¼ŒæœŸæœ›è¾“å…¥å·²ç»æ˜¯ ChatML æ ¼å¼çš„å­—ç¬¦ä¸²ã€‚
        è¿”å›åŒ…å«ç”Ÿæˆæ–‡æœ¬ã€è¾“å…¥å’Œè¾“å‡ºtokenæ•°çš„å­—å…¸ã€‚
        """
        if not self.is_loaded:
            raise RuntimeError(f"æ¨¡å‹ {self.model_name} æœªåŠ è½½ã€‚è¯·å…ˆè°ƒç”¨ load_model()ã€‚")

        # ç¡®ä¿è¾“å…¥åœ¨æ¨¡å‹æ‰€åœ¨çš„æ­£ç¡®è®¾å¤‡ä¸Š
        inputs = self.tokenizer(prompt_string, return_tensors="pt", padding=True, truncation=True).to(self.model.device)

        start_gen_time = time.time() # è®°å½•çº¯ç”Ÿæˆå¼€å§‹æ—¶é—´
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=repetition_penalty
            )
        end_gen_time = time.time() # è®°å½•çº¯ç”Ÿæˆç»“æŸæ—¶é—´

        # è§£ç ç”Ÿæˆçš„ tokens
        # æ³¨æ„ï¼šè¿™é‡Œ outputs[0] åŒ…å«äº† prompt_ids + generated_ids
        generated_tokens_ids = outputs[0, inputs["input_ids"].shape[1]:] # ä»…å–ç”Ÿæˆçš„æ–°tokens
        generated_text = self.tokenizer.decode(generated_tokens_ids, skip_special_tokens=True)

        input_token_count = inputs["input_ids"].shape[1]
        output_token_count = generated_tokens_ids.shape[0] # ç”Ÿæˆçš„tokensæ•°é‡

        return {
            "generated_text": generated_text,
            "input_token_count": input_token_count,
            "output_token_count": output_token_count,
            "generation_time_pure": end_gen_time - start_gen_time # çº¯ç”Ÿæˆæ—¶é—´
        }

# ====================================================================================
# è¯„ä¼°æŒ‡æ ‡è®¡ç®— (åŸºäºè¯é‡å ï¼Œé’ˆå¯¹ä¸­æ–‡éœ€è¦ jieba)
# ====================================================================================

def normalize_answer_chinese(s: str) -> str:
    """
    é’ˆå¯¹ä¸­æ–‡è¿›è¡Œç­”æ¡ˆå½’ä¸€åŒ–ï¼šç§»é™¤æ ‡ç‚¹ã€è½¬æ¢å…¨è§’å­—ç¬¦ä¸ºåŠè§’ã€å»é™¤å¤šä½™ç©ºæ ¼ã€åˆ†è¯å¹¶å°å†™ã€‚
    """
    if not s:
        return ""

    # è½¬æ¢ä¸ºå°å†™å¹¶å»é™¤ä¸¤ç«¯ç©ºç™½
    s = s.strip().lower()

    # å°†å…¨è§’æ ‡ç‚¹æ›¿æ¢ä¸ºåŠè§’
    s = s.replace('ï¼Œ', ',').replace('ã€‚', '.').replace('ï¼', '!').replace('ï¼Ÿ', '?').replace('ï¼›', ';')
    s = s.replace('ï¼ˆ', '(').replace('ï¼‰', ')')

    # ç§»é™¤æ‰€æœ‰å¸¸è§æ ‡ç‚¹ç¬¦å·
    # è¿™é‡Œéœ€è¦ç¡®ä¿æ¶µç›–äº†å¸¸è§çš„ä¸­æ–‡å’Œè‹±æ–‡æ ‡ç‚¹
    punctuation_pattern = r'[!"#$%&\'()*+,-./:;<=>?@[\]^_`{|}~â€œâ€â€˜â€™ã€ã€‘ã€ã€ã€Šã€‹â€”â€¦Â·ï½ã€Œã€ï½ï¿¥%#@ï¼&ï¼ˆï¼‰ã€Šã€‹]'
    s = re.sub(punctuation_pattern, '', s)


    # ä½¿ç”¨ jieba è¿›è¡Œåˆ†è¯
    tokens = jieba.cut(s)
    # è¿‡æ»¤æ‰åˆ†è¯ç»“æœä¸­çš„ç©ºæ ¼å’Œç©ºå­—ç¬¦ä¸²
    normalized_tokens = [token for token in tokens if token.strip()]
    return " ".join(normalized_tokens)


def get_tokens_chinese(s: str) -> List[str]:
    """è·å–ä¸­æ–‡åˆ†è¯åçš„tokensåˆ—è¡¨ã€‚"""
    # ç›´æ¥è¿”å› normalize_answer_chinese åçš„ split ç»“æœ
    return normalize_answer_chinese(s).split()

def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—F1åˆ†æ•° (åŸºäºè¯é‡å )ã€‚"""
    gold_tokens = get_tokens_chinese(ground_truth)
    pred_tokens = get_tokens_chinese(prediction)

    common = Counter(gold_tokens) & Counter(pred_tokens)
    num_common = sum(common.values())

    if len(gold_tokens) == 0 and len(pred_tokens) == 0:
        return 1.0 # å¦‚æœä¸¤è€…éƒ½ä¸ºç©ºï¼ŒF1 ä¸º 1
    if len(gold_tokens) == 0 or len(pred_tokens) == 0:
        return 0.0 # å…¶ä¸­ä¸€ä¸ªä¸ºç©ºï¼Œå¦ä¸€ä¸ªä¸ä¸ºç©ºï¼ŒF1 ä¸º 0

    precision = num_common / len(pred_tokens)
    recall = num_common / len(gold_tokens)

    if precision + recall == 0:
        return 0.0
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def calculate_exact_match(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…ç‡ã€‚"""
    return float(normalize_answer_chinese(prediction) == normalize_answer_chinese(ground_truth))

# ====================================================================================
# ä¸»æµ‹è¯•é€»è¾‘
# ====================================================================================

# å¤šè¿›ç¨‹/å¤šçº¿ç¨‹å¤„ç†
from concurrent.futures import ThreadPoolExecutor, as_completed

def run_chinese_comparison_test(args):
    print("ğŸš€ ä¸­æ–‡æ¨¡å‹å¯¹æ¯”æµ‹è¯•å¼€å§‹...")

    # æ£€æŸ¥ GPU æ•°é‡ï¼Œå¹¶åˆ†é…è®¾å¤‡
    num_gpus = torch.cuda.device_count()
    if num_gpus < 2:
        print(f"âŒ è­¦å‘Šï¼šæ£€æµ‹åˆ° {num_gpus} å—å¯ç”¨ GPU (å°‘äº 2 å—)ã€‚å°†é€€åŒ–ä¸ºå•å¡é¡ºåºè¯„ä¼°æ¨¡å¼ã€‚")
        model_configs = [
            ("Fin-R1", "cuda:0"),
            ("Qwen3-8B", "cuda:0") # éƒ½ä¼šåŠ è½½åˆ° cuda:0ï¼Œä½†ä¼šæŒ‰é¡ºåºåŠ è½½å’Œå¸è½½
        ]
        # åœ¨å•å¡æ¨¡å¼ä¸‹ï¼Œé€€åŒ–å›é¡ºåºåŠ è½½ï¼Œé˜²æ­¢æ˜¾å­˜ä¸è¶³
        single_gpu_sequential_mode = True
    else:
        print(f"âœ… æ£€æµ‹åˆ° {num_gpus} å— GPUã€‚å°è¯•åˆ†é… Fin-R1 åˆ° cuda:0ï¼ŒQwen3-8B åˆ° cuda:1ã€‚")
        model_configs = [
            ("Fin-R1", "cuda:0"),
            ("Qwen3-8B", "cuda:1")
        ]
        single_gpu_sequential_mode = False

    model_loaders = {}
    for name, dev in model_configs:
        model_loaders[name] = ModelLoader(name, dev)

    data_path = args.data_path
    sample_size = args.sample_size
    template_file_name = "multi_stage_chinese_template.txt"

    print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {data_path}")
    try:
        # å‡è®¾ utils.data_loader å­˜åœ¨å¹¶æä¾›äº† load_json_or_jsonl å’Œ sample_data
        from utils.data_loader import load_json_or_jsonl, sample_data
        dataset = load_json_or_jsonl(data_path)

        if sample_size > 0:
            dataset = sample_data(dataset, sample_size, 42) # ä½¿ç”¨å›ºå®šéšæœºç§å­ä¿è¯å¯å¤ç°æ€§
            print(f"âœ… éšæœºé‡‡æ · {len(dataset)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
        else:
            print(f"âœ… åŠ è½½äº†å…¨éƒ¨ {len(dataset)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return

    all_results_data = []

    if single_gpu_sequential_mode:
        print("\n--- è¿›å…¥å• GPU é¡ºåºè¯„ä¼°æ¨¡å¼ ---")
        for model_name, loader in model_loaders.items():
            try:
                print(f"\nğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name} åˆ° {loader.device}")
                loader.load_model() # åŠ è½½å½“å‰æ¨¡å‹
                print(f"âœ… æ¨¡å‹ {model_name} åŠ è½½å®Œæˆï¼Œå¼€å§‹è¯„ä¼°...")
                model_specific_results = evaluate_model_on_dataset(
                    model_name, loader, dataset, template_file_name,
                    args.max_new_tokens, args.do_sample, args.repetition_penalty
                )
                all_results_data.extend(model_specific_results)
                print(f"\n--- {model_name} è¯„ä¼°å®Œæˆ ---")
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model_name} è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
            finally:
                loader.unload_model() # ç¡®ä¿æ¯æ¬¡å¾ªç¯éƒ½å¸è½½æ¨¡å‹
                print(f"âœ… æ¨¡å‹ {model_name} å¸è½½å®Œæˆ")
                print(f"ğŸ“Š å¸è½½åGPUå†…å­˜çŠ¶æ€:")
                if torch.cuda.is_available():
                    for i in range(torch.cuda.device_count()):
                        allocated = torch.cuda.memory_allocated(i) / 1024**3
                        cached = torch.cuda.memory_reserved(i) / 1024**3
                        print(f"   GPU {i}: å·²åˆ†é… {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")
    else: # åŒ GPU å¹¶è¡Œæ¨¡å¼
        # å¹¶è¡ŒåŠ è½½æ¨¡å‹
        loaded_models = {}
        for model_name, loader in model_loaders.items():
            try:
                # æ˜¾å¼åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šçš„ GPU
                loader.load_model()
                loaded_models[model_name] = loader
            except Exception as e:
                print(f"âŒ æ¨¡å‹ {model_name} åŠ è½½å¤±è´¥ï¼Œè·³è¿‡è¯¥æ¨¡å‹: {e}")
                if model_name in loaded_models:
                    del loaded_models[model_name]
                loader.unload_model() # ç¡®ä¿æ¸…ç†æ˜¾å­˜
                continue

        if not loaded_models:
            print("âŒ æ²¡æœ‰æ¨¡å‹æˆåŠŸåŠ è½½ï¼Œé€€å‡ºè¯„ä¼°ã€‚")
            return

        print("\nâœ… æ‰€æœ‰æˆåŠŸåŠ è½½çš„æ¨¡å‹å·²å°±ç»ªï¼Œå¼€å§‹å¹¶è¡Œè¯„ä¼°...")

        # ä½¿ç”¨ ThreadPoolExecutor å¹¶è¡Œå¤„ç†æ¯ä¸ªæ¨¡å‹çš„è¯„ä¼°
        # æ¯ä¸ªæ¨¡å‹ä¸€ä¸ªçº¿ç¨‹ï¼Œç¡®ä¿æ¨¡å‹åœ¨è‡ªå·±çš„GPUä¸Šè¿è¡Œ
        from concurrent.futures import ThreadPoolExecutor, as_completed
        with ThreadPoolExecutor(max_workers=len(loaded_models)) as executor:
            futures = {executor.submit(evaluate_model_on_dataset, model_name, loader, dataset, template_file_name, args.max_new_tokens, args.do_sample, args.repetition_penalty): model_name
                       for model_name, loader in loaded_models.items()}

            for future in as_completed(futures):
                model_name = futures[future]
                try:
                    model_specific_results = future.result()
                    all_results_data.extend(model_specific_results)
                    print(f"\n--- {model_name} è¯„ä¼°å®Œæˆ ---")
                except Exception as e:
                    print(f"âŒ æ¨¡å‹ {model_name} è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                    import traceback
                    traceback.print_exc()
                finally:
                    # è¯„ä¼°å®Œæˆåå¸è½½æ¨¡å‹
                    if model_name in loaded_models:
                        loaded_models[model_name].unload_model()
        
        # æ‰“å°å¹¶è¡Œæ¨¡å¼ä¸‹å¸è½½åçš„æœ€ç»ˆGPUå†…å­˜çŠ¶æ€
        print(f"\nğŸ“Š å¹¶è¡Œæ¨¡å¼è¯„ä¼°åï¼Œæœ€ç»ˆGPUå†…å­˜çŠ¶æ€:")
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i) / 1024**3
                cached = torch.cuda.memory_reserved(i) / 1024**3
                print(f"   GPU {i}: å·²åˆ†é… {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")


    # --- è¯„ä¼°å®Œæˆï¼Œä¿å­˜æ‰€æœ‰ç»“æœ ---
    output_filename = f"comparison_results_chinese_{os.path.basename(data_path).replace('.jsonl', '')}_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(all_results_data, f, ensure_ascii=False, indent=4)
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")

    # æ±‡æ€»å¹¶æ‰“å°æœ€ç»ˆå¯¹æ¯”ç»“æœ
    print("\n--- æœ€ç»ˆæ¨¡å‹å¯¹æ¯”æ‘˜è¦ ---")
    model_summaries = {}
    for result in all_results_data:
        model_name = result["model"]
        if model_name not in model_summaries:
            model_summaries[model_name] = {
                "total_f1": 0.0,
                "total_em": 0.0,
                "total_gen_time": 0.0,
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "count": 0
            }

        model_summaries[model_name]["total_f1"] += result["f1_score"]
        model_summaries[model_name]["total_em"] += result["exact_match"]
        model_summaries[model_name]["total_gen_time"] += result["generation_time_pure"] # ä½¿ç”¨çº¯ç”Ÿæˆæ—¶é—´
        model_summaries[model_name]["total_input_tokens"] += result["input_token_count"]
        model_summaries[model_name]["total_output_tokens"] += result["output_token_count"]
        model_summaries[model_name]["count"] += 1

    for model_name, data in model_summaries.items():
        if data["count"] > 0:
            avg_f1 = data["total_f1"] / data["count"]
            avg_em = data["total_em"] / data["count"]
            avg_gen_time = data["total_gen_time"] / data["count"]
            avg_input_tokens = data["total_input_tokens"] / data["count"]
            avg_output_tokens = data["total_output_tokens"] / data["count"]
        else:
            avg_f1, avg_em, avg_gen_time, avg_input_tokens, avg_output_tokens = 0.0, 0.0, 0.0, 0.0, 0.0

        print(f"\næ¨¡å‹: {model_name}")
        print(f"  è¯„ä¼°æ ·æœ¬æ•°: {data['count']}")
        print(f"  å¹³å‡ F1-score: {avg_f1:.4f}")
        print(f"  å¹³å‡ Exact Match: {avg_em:.4f}")
        print(f"  å¹³å‡ç”Ÿæˆè€—æ—¶ (çº¯æ¨ç†): {avg_gen_time:.2f} ç§’/æ ·æœ¬")
        print(f"  å¹³å‡è¾“å…¥ Token æ•°: {avg_input_tokens:.1f}")
        print(f"  å¹³å‡è¾“å‡º Token æ•°: {avg_output_tokens:.1f}")
    print("----------------------------")


def evaluate_model_on_dataset(model_name: str, loader: ModelLoader, dataset: List[Dict[str, Any]], template_file_name: str, max_new_tokens: int, do_sample: bool, repetition_penalty: float) -> List[Dict[str, Any]]:
    """
    åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¯„ä¼°å•ä¸ªæ¨¡å‹ã€‚æ­¤å‡½æ•°å°†åœ¨ç‹¬ç«‹çš„çº¿ç¨‹ä¸­è¿è¡Œã€‚
    """
    model_results = []

    # æ‰“å°å½“å‰çº¿ç¨‹çš„æ¨¡å‹å’Œå®ƒæ‰€åœ¨çš„GPU
    print(f"\n[çº¿ç¨‹] å¼€å§‹è¯„ä¼° {model_name} åœ¨ {loader.device} ä¸Š...")

    pbar = tqdm(dataset, desc=f"è¯„ä¼° {model_name} ({loader.device})")
    for i, item in enumerate(pbar):
        query = item.get("query", "") or item.get("generated_question", "") or item.get("question", "")
        summary = item.get("summary", "")
        context = item.get("context", "")
        expected_answer = item.get("answer", "")

        messages = get_messages_for_test(summary, context, query, template_file_name)
        prompt_string_for_model = _convert_messages_to_chatml(messages)

        # è°ƒç”¨ loader å†…éƒ¨çš„ generateï¼Œå®ƒç°åœ¨è¿”å›ä¸€ä¸ªå­—å…¸
        try:
            gen_output = loader.generate(
                prompt_string=prompt_string_for_model,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                repetition_penalty=repetition_penalty
            )
            generated_text = gen_output["generated_text"]
            final_answer = clean_response(generated_text) # åå¤„ç†

            f1 = calculate_f1_score(final_answer, expected_answer)
            em = calculate_exact_match(final_answer, expected_answer)

            model_results.append({
                "model": model_name,
                "sample_id": i,
                "query": query,
                "expected_answer": expected_answer,
                "raw_generated_text": generated_text,
                "final_answer": final_answer,
                "f1_score": f1,
                "exact_match": em,
                "generation_time_pure": gen_output["generation_time_pure"],
                "input_token_count": gen_output["input_token_count"],
                "output_token_count": gen_output["output_token_count"],
            })
        except Exception as e:
            print(f"âŒ [çº¿ç¨‹] {model_name} æ ·æœ¬ {i} è¯„ä¼°å¤±è´¥: {e}")
            # å¯ä»¥é€‰æ‹©åœ¨è¿™é‡Œè®°å½•å¤±è´¥æ ·æœ¬æˆ–è·³è¿‡
            model_results.append({
                "model": model_name,
                "sample_id": i,
                "query": query,
                "expected_answer": expected_answer,
                "raw_generated_text": "[ERROR]",
                "final_answer": "[ERROR]",
                "f1_score": 0.0,
                "exact_match": 0.0,
                "generation_time_pure": 0.0,
                "input_token_count": 0,
                "output_token_count": 0,
                "error": str(e)
            })
    return model_results

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä¸­æ–‡æ¨¡å‹å¯¹æ¯”æµ‹è¯•è„šæœ¬")
    parser.add_argument("--data_path", type=str, required=True, help="è¯„ä¼°æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (jsonlæ ¼å¼ï¼Œä¾‹å¦‚ evaluate_mrr/alphafin_eval_optimized.jsonl)")
    parser.add_argument("--sample_size", type=int, default=100, help="éšæœºé‡‡æ ·çš„æ ·æœ¬æ•°é‡ (0è¡¨ç¤ºè¯„ä¼°å…¨éƒ¨ï¼Œé»˜è®¤ä¸º100)")
    parser.add_argument("--max_new_tokens", type=int, default=150, help="æ¨¡å‹ç”Ÿæˆæœ€å¤§æ–°Tokenæ•°")
    parser.add_argument("--do_sample", action='store_true', help="æ˜¯å¦ä½¿ç”¨é‡‡æ ·ç”Ÿæˆ (å¦‚æœè®¾ç½®äº†æ­¤flagï¼Œåˆ™ä¸ºTrueï¼Œé»˜è®¤False)") # ä¿®æ­£å¸ƒå°”å‚æ•°
    parser.add_argument("--repetition_penalty", type=float, default=1.1, help="é‡å¤æƒ©ç½šç³»æ•°")

    args = parser.parse_args()
    run_chinese_comparison_test(args)