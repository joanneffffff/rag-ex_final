import warnings
import logging
import os
import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
import time
import argparse
from collections import Counter # ä»ç„¶ä¿ç•™ï¼Œä»¥é˜²ä¸‡ä¸€éœ€è¦è®¡ç®—æŸäº›ç»Ÿè®¡é‡
import sys
import gc
import signal
import atexit

# ç¯å¢ƒè®¾ç½®
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

# --- æ—¥å¿—é…ç½® ---
log_dir = Path("logs")
log_dir.mkdir(parents=True, exist_ok=True)
log_file_path = log_dir / f"llm_judge_{time.strftime('%Y%m%d_%H%M%S')}.log"

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

if logger.hasHandlers():
    logger.handlers.clear()

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

try:
    from tqdm import tqdm
except ImportError:
    logger.error("âŒ tqdm is not installed. Please run: pip install tqdm")
    sys.exit(1)

from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.utils.quantization_config import BitsAndBytesConfig

# å¯¼å…¥é…ç½®æ–‡ä»¶ (ç¡®ä¿å­˜åœ¨)
try:
    from config.parameters import config
    logger.info(f"âœ… ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç¼“å­˜è·¯å¾„: {config.generator.cache_dir}")
except ImportError:
    logger.warning("âš ï¸ æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ç¼“å­˜è·¯å¾„ '/users/sgjfei3/data/huggingface'")
    class Config:
        class Generator:
            cache_dir = "/users/sgjfei3/data/huggingface"
        generator = Generator()
    config = Config()

# --- æ¨¡å‹åŠ è½½å™¨ (ä¸ä½ è¯„ä¼°è„šæœ¬ä¸­çš„ ModelLoader ä¿æŒä¸€è‡´) ---
class ModelLoader:
    def __init__(self, model_name: str, device: str):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = device
        self.is_loaded = False
        self.cache_dir = config.generator.cache_dir # ä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•

        # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®è·¯å¾„ (Judge é»˜è®¤ä¸º Qwen3-8B)
        if "Qwen3-8B" in model_name:
            local_qwen_path = f"{self.cache_dir}/models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5"
            self.model_path = local_qwen_path if os.path.exists(local_qwen_path) else "Qwen/Qwen3-8B"
        else: # å…è®¸å…¶ä»–æ¨¡å‹ä½œä¸ºJudgeï¼Œä½†é»˜è®¤åªæ”¯æŒQwen3-8B
            self.model_path = model_name
            logger.warning(f"âš ï¸ [{self.model_name}] æ¨¡å‹è·¯å¾„ '{model_name}' æœªçŸ¥ï¼Œå°è¯•ä»HubåŠ è½½ã€‚å»ºè®®Judgeä½¿ç”¨Qwen3-8Bã€‚")

        # 4-bit é‡åŒ–é…ç½®
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=False,
        )

    def load_model(self):
        if not self.is_loaded:
            logger.info(f"ğŸ”„ [{self.model_name}] æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° {self.device} ä» {self.model_path}")
            is_local_path = Path(self.model_path).exists() and Path(self.model_path).is_dir()

            tokenizer_args = {"trust_remote_code": True, "local_files_only": is_local_path, "cache_dir": self.cache_dir}
            model_args = {
                "torch_dtype": torch.float16,
                "device_map": self.device,
                "trust_remote_code": True,
                "quantization_config": self.quantization_config,
                "local_files_only": is_local_path,
                "cache_dir": self.cache_dir
            }

            try:
                logger.info(f"ğŸ”§ [{self.model_name}] åŠ è½½tokenizer...")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_args)
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                if self.tokenizer.pad_token_id is None:
                    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
                logger.info(f"âœ… [{self.model_name}] TokenizeråŠ è½½å®Œæˆ. Chat Template: {self.tokenizer.chat_template}")

                logger.info(f"ğŸ”§ [{self.model_name}] åŠ è½½æ¨¡å‹...")
                self.model = AutoModelForCausalLM.from_pretrained(self.model_path, **model_args)
                self.model.eval()
                logger.info(f"âœ… [{self.model_name}] æ¨¡å‹åŠ è½½å®Œæˆ. è®¾å¤‡: {self.model.device.type}:{self.model.device.index}, é‡åŒ–: 4bit")
                self.is_loaded = True
            except Exception as e:
                logger.exception(f"âŒ [{self.model_name}] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.unload_model()
                raise
        else:
            logger.info(f"âœ… [{self.model_name}] æ¨¡å‹å·²åŠ è½½ã€‚")

    def unload_model(self):
        if self.is_loaded:
            logger.info(f"ğŸ—‘ï¸ [{self.model_name}] å¸è½½æ¨¡å‹å¹¶æ¸…ç†æ˜¾å­˜...")
            try:
                if self.model:
                    del self.model
                if self.tokenizer:
                    del self.tokenizer
                self.model = None
                self.tokenizer = None
                torch.cuda.empty_cache()
                gc.collect()
                self.is_loaded = False
                logger.info(f"âœ… [{self.model_name}] æ˜¾å­˜å·²æ¸…ç†ã€‚")
            except Exception as e:
                logger.error(f"âŒ å¸è½½ [{self.model_name}] æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def generate(self, prompt_string: str, max_new_tokens: int = 512, do_sample: bool = False, repetition_penalty: float = 1.1) -> Dict[str, Any]:
        if not self.is_loaded:
            raise RuntimeError(f"æ¨¡å‹ {self.model_name} æœªåŠ è½½ã€‚è¯·å…ˆè°ƒç”¨ load_model()ã€‚")
        if self.tokenizer is None or self.model is None:
            raise RuntimeError(f"æ¨¡å‹ {self.model_name} çš„tokenizeræˆ–modelä¸ºNoneã€‚è¯·é‡æ–°åŠ è½½æ¨¡å‹ã€‚")

        inputs = self.tokenizer(prompt_string, return_tensors="pt", padding=True, truncation=True).to(self.model.device)

        start_gen_time = time.time()
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=repetition_penalty
            )
        end_gen_time = time.time()

        generated_tokens_ids = outputs[0, inputs["input_ids"].shape[1]:]
        generated_text = self.tokenizer.decode(generated_tokens_ids, skip_special_tokens=True)

        input_token_count = inputs["input_ids"].shape[1]
        output_token_count = generated_tokens_ids.shape[0]

        logger.debug(f"[{self.model_name}] è¾“å…¥tokens: {input_token_count}, è¾“å‡ºtokens: {output_token_count}, ç”Ÿæˆæ—¶é—´: {end_gen_time - start_gen_time:.2f}s")
        return {
            "generated_text": generated_text,
            "input_token_count": input_token_count,
            "output_token_count": output_token_count,
            "generation_time_pure": end_gen_time - start_gen_time
        }

# --- èµ„æºæ¸…ç†æœºåˆ¶ ---
class ResourceManager:
    def __init__(self, *args, **kwargs): # å…¼å®¹ä¸å¸¦å‚æ•°çš„init
        self.model_loaders = {}
        self.cleanup_registered = False
        self._register_cleanup()
    def _register_cleanup(self):
        if not self.cleanup_registered:
            atexit.register(self.cleanup_resources)
            signal.signal(signal.SIGINT, self._signal_handler)
            signal.signal(signal.SIGTERM, self._signal_handler)
            self.cleanup_registered = True
    def _signal_handler(self, signum, frame):
        logger.info(f"\nğŸ›‘ æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
        self.cleanup_resources()
        sys.exit(0)
    def add_model_loader(self, name: str, loader):
        self.model_loaders[name] = loader
    def cleanup_resources(self):
        logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†èµ„æº...")
        try:
            for name, loader in self.model_loaders.items():
                logger.info(f"ğŸ—‘ï¸ æ¸…ç†æ¨¡å‹ {name}...")
                if hasattr(loader, 'unload_model'): 
                    loader.unload_model()
                else:
                    if hasattr(loader, 'model') and loader.model: del loader.model
                    if hasattr(loader, 'tokenizer') and loader.tokenizer: del loader.tokenizer
                    torch.cuda.empty_cache(); gc.collect()
            self.model_loaders.clear()
            if torch.cuda.is_available():
                logger.info("ğŸ—‘ï¸ æ¸…ç†GPUå†…å­˜..."); torch.cuda.empty_cache(); torch.cuda.synchronize()
                for i in range(torch.cuda.device_count()):
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    cached = torch.cuda.memory_reserved(i) / 1024**3
                    logger.info(f"   GPU {i}: Allocated {allocated:.2f}GB, Cached {cached:.2f}GB")
            logger.info("ğŸ—‘ï¸ Forcing garbage collection..."); gc.collect()
            logger.info("âœ… Resource cleanup complete")
        except Exception as e: logger.error(f"âš ï¸ Error during resource cleanup: {e}")

resource_manager = ResourceManager() # åˆå§‹åŒ–èµ„æºç®¡ç†å™¨

# --- Prompt æ„å»ºè¾…åŠ©å‡½æ•° (ç”¨äº LLM-Judge) ---
def _load_template_content_from_file(template_file_name: str) -> str:
    template_path = Path("data/prompt_templates") / template_file_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except FileNotFoundError:
        logger.error(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚")
        sys.exit(1)

def get_judge_messages(query: str, expected_answer: str, model_final_answer: str, template_file_name: str) -> List[Dict[str, str]]:
    template_full_string = _load_template_content_from_file(template_file_name)

    system_part = re.search(r'===SYSTEM===(.*?)===USER===', template_full_string, re.DOTALL)
    user_part = re.search(r'===USER===(.*?)===ASSISTANT===', template_full_string, re.DOTALL)

    messages = []
    if system_part:
        messages.append({"role": "system", "content": system_part.group(1).strip()})
    
    if user_part:
        user_content = user_part.group(1).strip()
        user_content = user_content.replace('{query}', query)
        user_content = user_content.replace('{expected_answer}', expected_answer)
        user_content = user_content.replace('{model_final_answer}', model_final_answer)
        messages.append({"role": "user", "content": user_content})

    messages.append({"role": "assistant", "content": ""}) 

    formatted_prompt = ""
    for msg in messages[:-1]: 
        formatted_prompt += f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>\n"
    formatted_prompt += f"<|im_start|>{messages[-1]['role']}\n" 

    logger.debug(f"Judge Prompt (å‰500å­—ç¬¦):\n{formatted_prompt[:500]}...")
    return formatted_prompt

# ChatML è½¬æ¢ (ç”¨äº Judge)
def _convert_messages_to_chatml(messages: List[Dict[str, str]]) -> str:
    if not messages: return ""
    formatted_prompt = ""
    for message in messages[:-1]:
        role = message.get("role", "")
        content = message.get("content", "")
        if role == "system": formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n"
        elif role == "user": formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n"
        elif role == "assistant": formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n"
    final_message_role = messages[-1].get("role", "")
    final_message_content = messages[-1].get("content", "").strip()
    formatted_prompt += f"<|im_start|>{final_message_role}\n{final_message_content}"
    logger.debug(f"Converted ChatML Prompt (å‰500å­—ç¬¦):\n{formatted_prompt[:500]}...")
    return formatted_prompt


# --- LLM-Judge ä¸»é€»è¾‘ ---
def run_llm_judge_evaluation(args):
    logger.info("ğŸ¤– LLM-Judge è¯„ä¼°å¼€å§‹...")

    # åŠ è½½ Judge æ¨¡å‹ (Qwen3-8Bï¼Œé€šå¸¸æ”¾åœ¨ cuda:0)
    judge_model_name = "Qwen3-8B"
    judge_loader = ModelLoader(judge_model_name, "cuda:0") # è¿™é‡Œå›ºå®šcuda:0
    try:
        judge_loader.load_model()
    except Exception as e:
        logger.error(f"âŒ LLM-Judge æ¨¡å‹ {judge_model_name} åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°: {e}")
        return

    # åŠ è½½ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°ç»“æœæ–‡ä»¶
    if not os.path.exists(args.results_file):
        logger.error(f"âŒ æœªæ‰¾åˆ°ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°ç»“æœæ–‡ä»¶: {args.results_file}")
        judge_loader.unload_model()
        return

    with open(args.results_file, 'r', encoding='utf-8') as f:
        generated_results = json.load(f)

    judge_template_file = args.judge_template_file # ä¾‹å¦‚ "qwen_judge_template.txt"
    judged_results = []

    logger.info(f"ğŸ“Š å¼€å§‹ç”¨ {judge_model_name} è¯„ä¼° {len(generated_results)} ä¸ªç”Ÿæˆç»“æœ...")
    
    # å¯¹æ¯ä¸ªç”Ÿæˆç»“æœè¿›è¡Œ Judge è¯„ä¼°
    pbar = tqdm(generated_results, desc=f"LLM-Judge è¯„ä¼°ä¸­ ({judge_model_name})")
    for item in pbar:
        query = item["query"]
        expected_answer = item["expected_answer"]
        model_final_answer = item["final_answer"]
        model_name = item["model"]
        sample_id = item["sample_id"]

        judge_prompt = get_judge_messages(query, expected_answer, model_final_answer, judge_template_file)

        try:
            judge_output = judge_loader.generate(
                prompt_string=judge_prompt,
                max_new_tokens=args.judge_max_new_tokens, # Judge ç”Ÿæˆé€šå¸¸ä¹Ÿéœ€è¦ä¸€äº›token
                do_sample=False, # Judge è¯„åˆ†é€šå¸¸ä¸ä½¿ç”¨é‡‡æ ·ï¼Œè¿½æ±‚ç¡®å®šæ€§
                repetition_penalty=1.0 # Judge è¯„åˆ†é€šå¸¸æ²¡æœ‰é‡å¤æƒ©ç½š
            )
            judge_raw_text = judge_output["generated_text"]
            
            # å°è¯•è§£æ Judge çš„ JSON è¾“å‡º
            judge_score_data = {}
            try:
                # æå–ç¬¬ä¸€ä¸ªJSONå—ï¼Œå› ä¸ºLLMå¯èƒ½ä¼šåœ¨JSONå‰åè¯´åºŸè¯
                json_match = re.search(r'\{[\s\S]*\}', judge_raw_text)
                if json_match:
                    json_string = json_match.group(0)
                    judge_score_data = json.loads(json_string)
                else:
                    logger.warning(f"âš ï¸ Judge Output for Sample {sample_id} ({model_name}) æ— JSONè¾“å‡º: {judge_raw_text[:100]}...")
                    judge_score_data = {"Accuracy_Score": 0, "Conciseness_Score": 0, "Professionalism_Score": 0, "Reasoning": "Judgeè¾“å‡ºä¸å«JSON"}

            except json.JSONDecodeError as json_e:
                logger.error(f"âŒ Judge Output for Sample {sample_id} ({model_name}) JSONè§£æå¤±è´¥: {json_e} - Raw: {judge_raw_text[:200]}...")
                judge_score_data = {"Accuracy_Score": 0, "Conciseness_Score": 0, "Professionalism_Score": 0, "Reasoning": f"JSONè§£æå¤±è´¥: {judge_raw_text[:50]}"}
            
            judged_results.append({
                "generator_model": model_name,
                "sample_id": sample_id,
                "query": query,
                "expected_answer": expected_answer,
                "generator_final_answer": model_final_answer,
                "judge_raw_output": judge_raw_text,
                "accuracy_score": judge_score_data.get("Accuracy_Score", 0),
                "conciseness_score": judge_score_data.get("Conciseness_Score", 0),
                "professionalism_score": judge_score_data.get("Professionalism_Score", 0),
                "judge_reasoning": judge_score_data.get("Reasoning", "")
            })

        except Exception as e:
            logger.exception(f"âŒ è°ƒç”¨ Judge æ¨¡å‹è¯„ä¼° Sample {sample_id} ({model_name}) å¤±è´¥: {e}")
            judged_results.append({
                "generator_model": model_name,
                "sample_id": sample_id,
                "query": query,
                "expected_answer": expected_answer,
                "generator_final_answer": model_final_answer,
                "judge_raw_output": "[ERROR]",
                "accuracy_score": 0, "conciseness_score": 0, "professionalism_score": 0,
                "judge_reasoning": str(e)
            })
    
    # ä¿å­˜ LLM-Judge è¯„ä¼°ç»“æœ
    output_filename = f"llm_judge_results_{Path(args.results_file).stem}_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(judged_results, f, ensure_ascii=False, indent=4)
    logger.info(f"ğŸ‰ LLM-Judge è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")

    # æ±‡æ€»å¹¶æ‰“å°æœ€ç»ˆ Judge è¯„ä¼°æ‘˜è¦
    logger.info("\n--- LLM-Judge è¯„ä¼°æ‘˜è¦ ---")
    model_summaries = {}
    for result in judged_results:
        model_name = result["generator_model"]
        if model_name not in model_summaries:
            model_summaries[model_name] = {
                "total_accuracy": 0,
                "total_conciseness": 0,
                "total_professionalism": 0,
                "count": 0
            }
        model_summaries[model_name]["total_accuracy"] += result["accuracy_score"]
        model_summaries[model_name]["total_conciseness"] += result["conciseness_score"]
        model_summaries[model_name]["total_professionalism"] += result["professionalism_score"]
        model_summaries[model_name]["count"] += 1

    for model_name, data in model_summaries.items():
        if data["count"] > 0:
            avg_accuracy = data["total_accuracy"] / data["count"]
            avg_conciseness = data["total_conciseness"] / data["count"]
            avg_professionalism = data["total_professionalism"] / data["count"]
        else:
            avg_accuracy, avg_conciseness, avg_professionalism = 0.0, 0.0, 0.0

        logger.info(f"\nç”Ÿæˆæ¨¡å‹: {model_name}")
        logger.info(f" è¯„ä¼°æ ·æœ¬æ•°: {data['count']}")
        logger.info(f" å¹³å‡å‡†ç¡®æ€§åˆ†æ•°: {avg_accuracy:.2f}")
        logger.info(f" å¹³å‡ç®€æ´æ€§åˆ†æ•°: {avg_conciseness:.2f}")
        logger.info(f" å¹³å‡ä¸“ä¸šæ€§åˆ†æ•°: {avg_professionalism:.2f}")
    logger.info("----------------------------")
    
    judge_loader.unload_model() # å¸è½½ Judge æ¨¡å‹

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="LLM-Judge è¯„ä¼°è„šæœ¬")
    parser.add_argument("--results_file", type=str, required=True, help="åŒ…å«ç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„JSONæ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚ comparison_results_chinese_*.json)")
    parser.add_argument("--judge_template_file", type=str, default="qwen_judge_template.txt", help="LLM Judge çš„ Prompt æ¨¡æ¿æ–‡ä»¶ (ä¾‹å¦‚ qwen_judge_template.txt)")
    parser.add_argument("--judge_max_new_tokens", type=int, default=256, help="LLM Judge ç”Ÿæˆçš„æœ€å¤§æ–° Token æ•°")

    args = parser.parse_args()
    run_llm_judge_evaluation(args)