import warnings
import logging
import os
import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
import time
import argparse
from collections import Counter # ä»ç„¶ä¿ç•™ï¼Œä»¥é˜²ä¸‡ä¸€éœ€è¦è®¡ç®—æŸäº›ç»Ÿè®¡é‡
import sys
import gc
import signal
import atexit

# ç¯å¢ƒè®¾ç½®
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

# --- æ—¥å¿—é…ç½® ---
log_dir = Path("logs")
log_dir.mkdir(parents=True, exist_ok=True)
log_file_path = log_dir / f"llm_judge_{time.strftime('%Y%m%d_%H%M%S')}.log"

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')

if logger.hasHandlers():
    logger.handlers.clear()

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

file_handler = logging.FileHandler(log_file_path, encoding='utf-8')
file_handler.setLevel(logging.DEBUG)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

try:
    from tqdm import tqdm
except ImportError:
    logger.error("âŒ tqdm is not installed. Please run: pip install tqdm")
    sys.exit(1)

from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.utils.quantization_config import BitsAndBytesConfig

# å¯¼å…¥é…ç½®æ–‡ä»¶ (ç¡®ä¿å­˜åœ¨)
try:
    from config.parameters import config
    logger.info(f"âœ… ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ç¼“å­˜è·¯å¾„: {config.generator.cache_dir}")
except ImportError:
    logger.warning("âš ï¸ æ— æ³•å¯¼å…¥é…ç½®æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤ç¼“å­˜è·¯å¾„ '/users/sgjfei3/data/huggingface'")
    class Config:
        class Generator:
            cache_dir = "/users/sgjfei3/data/huggingface"
        generator = Generator()
    config = Config()

# --- æ¨¡å‹åŠ è½½å™¨ (ä¸ä½ è¯„ä¼°è„šæœ¬ä¸­çš„ ModelLoader ä¿æŒä¸€è‡´) ---
class ModelLoader:
    def __init__(self, model_name: str, device: str):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = device
        self.is_loaded = False
        self.cache_dir = config.generator.cache_dir # ä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•

        # æ ¹æ®æ¨¡å‹åç§°è®¾ç½®è·¯å¾„ (Judge é»˜è®¤ä¸º Qwen3-8B)
        if "Qwen3-8B" in model_name:
            local_qwen_path = f"{self.cache_dir}/models--Qwen--Qwen3-8B/snapshots/9c925d64d72725edaf899c6cb9c377fd0709d9c5"
            self.model_path = local_qwen_path if os.path.exists(local_qwen_path) else "Qwen/Qwen3-8B"
        else: # å…è®¸å…¶ä»–æ¨¡å‹ä½œä¸ºJudgeï¼Œä½†é»˜è®¤åªæ”¯æŒQwen3-8B
            self.model_path = model_name
            logger.warning(f"âš ï¸ [{self.model_name}] æ¨¡å‹è·¯å¾„ '{model_name}' æœªçŸ¥ï¼Œå°è¯•ä»HubåŠ è½½ã€‚å»ºè®®Judgeä½¿ç”¨Qwen3-8Bã€‚")

        # 4-bit é‡åŒ–é…ç½®
        self.quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=False,
        )

    def load_model(self):
        if not self.is_loaded:
            logger.info(f"ğŸ”„ [{self.model_name}] æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° {self.device} ä» {self.model_path}")
            is_local_path = Path(self.model_path).exists() and Path(self.model_path).is_dir()

            tokenizer_args = {"trust_remote_code": True, "local_files_only": is_local_path, "cache_dir": self.cache_dir}
            model_args = {
                "torch_dtype": torch.float16,
                "device_map": self.device,
                "trust_remote_code": True,
                "quantization_config": self.quantization_config,
                "local_files_only": is_local_path,
                "cache_dir": self.cache_dir
            }

            try:
                logger.info(f"ğŸ”§ [{self.model_name}] åŠ è½½tokenizer...")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_args)
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                if self.tokenizer.pad_token_id is None:
                    self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
                logger.info(f"âœ… [{self.model_name}] TokenizeråŠ è½½å®Œæˆ. Chat Template: {self.tokenizer.chat_template}")

                logger.info(f"ğŸ”§ [{self.model_name}] åŠ è½½æ¨¡å‹...")
                self.model = AutoModelForCausalLM.from_pretrained(self.model_path, **model_args)
                self.model.eval()
                logger.info(f"âœ… [{self.model_name}] æ¨¡å‹åŠ è½½å®Œæˆ. è®¾å¤‡: {self.model.device.type}:{self.model.device.index}, é‡åŒ–: 4bit")
                self.is_loaded = True
            except Exception as e:
                logger.exception(f"âŒ [{self.model_name}] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.unload_model()
                raise
        else:
            logger.info(f"âœ… [{self.model_name}] æ¨¡å‹å·²åŠ è½½ã€‚")

    def unload_model(self):
        if self.is_loaded:
            logger.info(f"ğŸ—‘ï¸ [{self.model_name}] å¸è½½æ¨¡å‹å¹¶æ¸…ç†æ˜¾å­˜...")
            try:
                if self.model:
                    del self.model
                if self.tokenizer:
                    del self.tokenizer
                self.model = None
                self.tokenizer = None
                torch.cuda.empty_cache()
                gc.collect()
                self.is_loaded = False
                logger.info(f"âœ… [{self.model_name}] æ˜¾å­˜å·²æ¸…ç†ã€‚")
            except Exception as e:
                logger.error(f"âŒ å¸è½½ [{self.model_name}] æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def generate(self, prompt_string: str, max_new_tokens: int = 512, do_sample: bool = False, repetition_penalty: float = 1.1) -> Dict[str, Any]:
        if not self.is_loaded:
            raise RuntimeError(f"æ¨¡å‹ {self.model_name} æœªåŠ è½½ã€‚è¯·å…ˆè°ƒç”¨ load_model()ã€‚")
        if self.tokenizer is None or self.model is None:
            raise RuntimeError(f"æ¨¡å‹ {self.model_name} çš„tokenizeræˆ–modelä¸ºNoneã€‚è¯·é‡æ–°åŠ è½½æ¨¡å‹ã€‚")

        inputs = self.tokenizer(prompt_string, return_tensors="pt", padding=True, truncation=True).to(self.model.device)

        start_gen_time = time.time()
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=repetition_penalty
            )
        end_gen_time = time.time()

        generated_tokens_ids = outputs[0, inputs["input_ids"].shape[1]:]
        generated_text = self.tokenizer.decode(generated_tokens_ids, skip_special_tokens=True)

        input_token_count = inputs["input_ids"].shape[1]
        output_token_count = generated_tokens_ids.shape[0]

        logger.debug(f"[{self.model_name}] è¾“å…¥tokens: {input_token_count}, è¾“å‡ºtokens: {output_token_count}, ç”Ÿæˆæ—¶é—´: {end_gen_time - start_gen_time:.2f}s")
        return {
            "generated_text": generated_text,
            "input_token_count": input_token_count,
            "output_token_count": output_token_count,
            "generation_time_pure": end_gen_time - start_gen_time
        }

# --- èµ„æºæ¸…ç†æœºåˆ¶ ---
class ResourceManager:
    def __init__(self, *args, **kwargs): # å…¼å®¹ä¸å¸¦å‚æ•°çš„init
        self.model_loaders = {}
        self.cleanup_registered = False
        self._register_cleanup()
    def _register_cleanup(self):
        if not self.cleanup_registered:
            atexit.register(self.cleanup_resources)
            signal.signal(signal.SIGINT, self._signal_handler)
            signal.signal(signal.SIGTERM, self._signal_handler)
            self.cleanup_registered = True
    def _signal_handler(self, signum, frame):
        logger.info(f"\nğŸ›‘ æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
        self.cleanup_resources()
        sys.exit(0)
    def add_model_loader(self, name: str, loader):
        self.model_loaders[name] = loader
    def cleanup_resources(self):
        logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†èµ„æº...")
        try:
            for name, loader in self.model_loaders.items():
                logger.info(f"ğŸ—‘ï¸ æ¸…ç†æ¨¡å‹ {name}...")
                if hasattr(loader, 'unload_model'): 
                    loader.unload_model()
                else:
                    if hasattr(loader, 'model') and loader.model: del loader.model
                    if hasattr(loader, 'tokenizer') and loader.tokenizer: del loader.tokenizer
                    torch.cuda.empty_cache(); gc.collect()
            self.model_loaders.clear()
            if torch.cuda.is_available():
                logger.info("ğŸ—‘ï¸ æ¸…ç†GPUå†…å­˜..."); torch.cuda.empty_cache(); torch.cuda.synchronize()
                for i in range(torch.cuda.device_count()):
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    cached = torch.cuda.memory_reserved(i) / 1024**3
                    logger.info(f"   GPU {i}: Allocated {allocated:.2f}GB, Cached {cached:.2f}GB")
            logger.info("ğŸ—‘ï¸ Forcing garbage collection..."); gc.collect()
            logger.info("âœ… Resource cleanup complete")
        except Exception as e: logger.error(f"âš ï¸ Error during resource cleanup: {e}")

resource_manager = ResourceManager() # åˆå§‹åŒ–èµ„æºç®¡ç†å™¨

# --- Prompt æ„å»ºè¾…åŠ©å‡½æ•° (ç”¨äº LLM-Judge) ---
def _load_template_content_from_file(template_file_name: str) -> str:
    template_path = Path("data/prompt_templates") / template_file_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except FileNotFoundError:
        logger.warning(f"âš ï¸ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}ï¼Œä½¿ç”¨å†…ç½®æ¨¡æ¿")
        return _get_builtin_judge_template()

def _get_builtin_judge_template() -> str:
    """è¿”å›å†…ç½®çš„Judgeæ¨¡æ¿"""
    return """===SYSTEM===
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¯„ä¼°ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®æä¾›çš„"å‚è€ƒç­”æ¡ˆ"å’Œ"ç”¨æˆ·é—®é¢˜"ï¼Œè¯„ä¼°å¦ä¸€ä¸ª"å¾…è¯„ä¼°ç­”æ¡ˆ"çš„è´¨é‡ã€‚

**é‡è¦ï¼šè¯·ç›´æ¥è¾“å‡ºJSONæ ¼å¼çš„è¯„åˆ†ç»“æœï¼Œä¸è¦è¾“å‡ºä»»ä½•æ€è€ƒè¿‡ç¨‹ã€è§£é‡Šæˆ–å…¶ä»–å†…å®¹ã€‚**

**è¯„åˆ†æ ‡å‡†:**
1.  **å‡†ç¡®æ€§ (Accuracy):** å¾…è¯„ä¼°ç­”æ¡ˆåœ¨å†…å®¹ä¸Šæ˜¯å¦ä¸ç”¨æˆ·é—®é¢˜å’Œå‚è€ƒç­”æ¡ˆä¸€è‡´ï¼Œæ˜¯å¦å›ç­”äº†ç”¨æˆ·é—®é¢˜ï¼Œä¸”æ²¡æœ‰äº‹å®æ€§é”™è¯¯æˆ–å¹»è§‰ï¼Ÿ (0-5åˆ†)
    * 5åˆ†: å®Œç¾å‡†ç¡®ï¼Œå†…å®¹ä¸å‚è€ƒç­”æ¡ˆå®Œå…¨ä¸€è‡´ï¼Œæ— ä»»ä½•é”™è¯¯æˆ–å¹»è§‰ã€‚
    * 4åˆ†: åŸºæœ¬å‡†ç¡®ï¼Œæœ‰å°‘é‡ä¸å½±å“æ ¸å¿ƒæ„ä¹‰çš„æªè¾åå·®ï¼Œæ— é”™è¯¯æˆ–å¹»è§‰ã€‚
    * 3åˆ†: å¤šæ•°å‡†ç¡®ï¼Œä½†æœ‰éƒ¨åˆ†é—æ¼æˆ–è½»å¾®é”™è¯¯ï¼Œæˆ–è½»å¾®å¹»è§‰ã€‚
    * 2åˆ†: åŒ…å«ä¸€äº›æ­£ç¡®ä¿¡æ¯ï¼Œä½†ä¹Ÿæœ‰æ˜æ˜¾é”™è¯¯æˆ–é—æ¼ï¼Œæˆ–æœ‰æ˜æ˜¾å¹»è§‰ã€‚
    * 1åˆ†: åŸºæœ¬ä¸å‡†ç¡®ï¼Œæˆ–ä¸é—®é¢˜æ— å…³ã€‚
    * 0åˆ†: å®Œå…¨é”™è¯¯æˆ–æ— æ³•ç†è§£ã€‚
2.  **ç®€æ´æ€§ (Conciseness):** å¾…è¯„ä¼°ç­”æ¡ˆæ˜¯å¦åœ¨ 3-5 å¥è¯ä»¥å†…ï¼Œä¸è¶…è¿‡ 300 æ±‰å­—ï¼Œä¸”æ²¡æœ‰å†—ä½™ä¿¡æ¯ã€æ— å…³çš„å¼€åœºç™½æˆ–è‡ªæˆ‘åæ€ï¼Ÿ (0-5åˆ†)
    * 5åˆ†: å®Œç¾ç¬¦åˆå­—æ•°å’Œç®€æ´æ€§è¦æ±‚ã€‚
    * 4åˆ†: åŸºæœ¬ç¬¦åˆï¼Œæœ‰å°‘é‡å†—ä½™ä½†ä¸å½±å“ç†è§£ã€‚
    * 3åˆ†: é•¿åº¦é€‚ä¸­ï¼Œä½†æœ‰ä¸€äº›ä¸å¿…è¦çš„é‡å¤æˆ–å†—ä½™ã€‚
    * 2åˆ†: è¿‡äºå†—é•¿æˆ–è¿‡äºç®€çŸ­ï¼Œå½±å“ç†è§£ã€‚
    * 1åˆ†: ä¸¥é‡å†—é•¿æˆ–è¿‡äºç®€çŸ­ã€‚
    * 0åˆ†: å®Œå…¨ä¸ç¬¦åˆç®€æ´æ€§è¦æ±‚ã€‚
3.  **ä¸“ä¸šæ€§ (Professionalism):** å¾…è¯„ä¼°ç­”æ¡ˆæ˜¯å¦ä½¿ç”¨äº†ä¸“ä¸šã€å‡†ç¡®çš„æœ¯è¯­ï¼Œè¯­è¨€è¡¨è¾¾æ˜¯å¦è§„èŒƒï¼Œæ˜¯å¦ç¬¦åˆé‡‘è/è´¢åŠ¡é¢†åŸŸçš„ä¸“ä¸šæ ‡å‡†ï¼Ÿ (0-5åˆ†)
    * 5åˆ†: ä½¿ç”¨ä¸“ä¸šæœ¯è¯­å‡†ç¡®ï¼Œè¡¨è¾¾è§„èŒƒï¼Œå®Œå…¨ç¬¦åˆä¸“ä¸šæ ‡å‡†ã€‚
    * 4åˆ†: åŸºæœ¬ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œè¡¨è¾¾è¾ƒä¸ºè§„èŒƒã€‚
    * 3åˆ†: éƒ¨åˆ†ä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œè¡¨è¾¾åŸºæœ¬è§„èŒƒã€‚
    * 2åˆ†: ä¸“ä¸šæœ¯è¯­ä½¿ç”¨ä¸å½“ï¼Œè¡¨è¾¾ä¸å¤Ÿè§„èŒƒã€‚
    * 1åˆ†: ç¼ºä¹ä¸“ä¸šæ€§ï¼Œè¡¨è¾¾ä¸è§„èŒƒã€‚
    * 0åˆ†: å®Œå…¨ä¸ä¸“ä¸šï¼Œè¡¨è¾¾æ··ä¹±ã€‚

**è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š**
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ï¼š

{
    "Accuracy_Score": åˆ†æ•°,
    "Conciseness_Score": åˆ†æ•°,
    "Professionalism_Score": åˆ†æ•°,
    "Reasoning": "ç®€è¦è¯´æ˜è¯„åˆ†ç†ç”±"
}

===USER===
**ç”¨æˆ·é—®é¢˜:** {query}

**å‚è€ƒç­”æ¡ˆ:** {expected_answer}

**å¾…è¯„ä¼°ç­”æ¡ˆ:** {model_final_answer}

è¯·æ ¹æ®ä¸Šè¿°è¯„åˆ†æ ‡å‡†ï¼Œå¯¹"å¾…è¯„ä¼°ç­”æ¡ˆ"è¿›è¡Œè¯„åˆ†ã€‚ç›´æ¥è¾“å‡ºJSONæ ¼å¼ç»“æœï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–å†…å®¹ã€‚

===ASSISTANT===
"""

def get_judge_messages(query: str, expected_answer: str, model_final_answer: str, template_file_name: str) -> str:
    template_full_string = _load_template_content_from_file(template_file_name)

    system_part = re.search(r'===SYSTEM===(.*?)===USER===', template_full_string, re.DOTALL)
    user_part = re.search(r'===USER===(.*?)===ASSISTANT===', template_full_string, re.DOTALL)

    messages = []
    if system_part:
        messages.append({"role": "system", "content": system_part.group(1).strip()})
    
    if user_part:
        user_content = user_part.group(1).strip()
        user_content = user_content.replace('{query}', query)
        user_content = user_content.replace('{expected_answer}', expected_answer)
        user_content = user_content.replace('{model_final_answer}', model_final_answer)
        messages.append({"role": "user", "content": user_content})

    messages.append({"role": "assistant", "content": ""}) 

    formatted_prompt = ""
    for msg in messages[:-1]: 
        formatted_prompt += f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>\n"
    formatted_prompt += f"<|im_start|>{messages[-1]['role']}\n" 

    logger.debug(f"Judge Prompt (å‰500å­—ç¬¦):\n{formatted_prompt[:500]}...")
    return formatted_prompt

# ChatML è½¬æ¢ (ç”¨äº Judge)
def _convert_messages_to_chatml(messages: List[Dict[str, str]]) -> str:
    if not messages: return ""
    formatted_prompt = ""
    for message in messages[:-1]:
        role = message.get("role", "")
        content = message.get("content", "")
        if role == "system": formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n"
        elif role == "user": formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n"
        elif role == "assistant": formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n"
    final_message_role = messages[-1].get("role", "")
    final_message_content = messages[-1].get("content", "").strip()
    formatted_prompt += f"<|im_start|>{final_message_role}\n{final_message_content}"
    logger.debug(f"Converted ChatML Prompt (å‰500å­—ç¬¦):\n{formatted_prompt[:500]}...")
    return formatted_prompt


# --- LLM-Judge ä¸»é€»è¾‘ ---
def run_llm_judge_evaluation(args):
    logger.info("ğŸ¤– LLM-Judge è¯„ä¼°å¼€å§‹...")

    # åŠ è½½ Judge æ¨¡å‹ (Qwen3-8Bï¼Œé€šå¸¸æ”¾åœ¨ cuda:0)
    judge_model_name = "Qwen3-8B"
    judge_loader = ModelLoader(judge_model_name, "cuda:0") # è¿™é‡Œå›ºå®šcuda:0
    try:
        judge_loader.load_model()
    except Exception as e:
        logger.error(f"âŒ LLM-Judge æ¨¡å‹ {judge_model_name} åŠ è½½å¤±è´¥ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°: {e}")
        return

    # åŠ è½½ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°ç»“æœæ–‡ä»¶
    if not os.path.exists(args.results_file):
        logger.error(f"âŒ æœªæ‰¾åˆ°ç”Ÿæˆæ¨¡å‹çš„è¯„ä¼°ç»“æœæ–‡ä»¶: {args.results_file}")
        judge_loader.unload_model()
        return

    with open(args.results_file, 'r', encoding='utf-8') as f:
        generated_results = json.load(f)

    judge_template_file = args.judge_template_file # ä¾‹å¦‚ "qwen_judge_template.txt"
    judged_results = []

    logger.info(f"ğŸ“Š å¼€å§‹ç”¨ {judge_model_name} è¯„ä¼° {len(generated_results)} ä¸ªç”Ÿæˆç»“æœ...")
    
    # å¯¹æ¯ä¸ªç”Ÿæˆç»“æœè¿›è¡Œ Judge è¯„ä¼°
    pbar = tqdm(generated_results, desc=f"LLM-Judge è¯„ä¼°ä¸­ ({judge_model_name})")
    for item in pbar:
        query = item["query"]
        expected_answer = item["expected_answer"]
        model_final_answer = item["final_answer"]
        model_name = item["model"]
        sample_id = item["sample_id"]

        judge_prompt = get_judge_messages(query, expected_answer, model_final_answer, judge_template_file)

        try:
            judge_output = judge_loader.generate(
                prompt_string=judge_prompt,
                max_new_tokens=args.judge_max_new_tokens, # Judge ç”Ÿæˆé€šå¸¸ä¹Ÿéœ€è¦ä¸€äº›token
                do_sample=False, # Judge è¯„åˆ†é€šå¸¸ä¸ä½¿ç”¨é‡‡æ ·ï¼Œè¿½æ±‚ç¡®å®šæ€§
                repetition_penalty=1.0 # Judge è¯„åˆ†é€šå¸¸æ²¡æœ‰é‡å¤æƒ©ç½š
            )
            judge_raw_text = judge_output["generated_text"]
            
            # å°è¯•è§£æ Judge çš„ JSON è¾“å‡º
            judge_score_data = {}
            try:
                # æå–ç¬¬ä¸€ä¸ªJSONå—ï¼Œå› ä¸ºLLMå¯èƒ½ä¼šåœ¨JSONå‰åè¯´åºŸè¯
                json_match = re.search(r'\{[\s\S]*\}', judge_raw_text)
                if json_match:
                    json_string = json_match.group(0)
                    judge_score_data = json.loads(json_string)
                else:
                    logger.warning(f"âš ï¸ Judge Output for Sample {sample_id} ({model_name}) æ— JSONè¾“å‡º: {judge_raw_text[:100]}...")
                    judge_score_data = {"Accuracy_Score": 0, "Conciseness_Score": 0, "Professionalism_Score": 0, "Reasoning": "Judgeè¾“å‡ºä¸å«JSON"}

            except json.JSONDecodeError as json_e:
                logger.error(f"âŒ Judge Output for Sample {sample_id} ({model_name}) JSONè§£æå¤±è´¥: {json_e} - Raw: {judge_raw_text[:200]}...")
                judge_score_data = {"Accuracy_Score": 0, "Conciseness_Score": 0, "Professionalism_Score": 0, "Reasoning": f"JSONè§£æå¤±è´¥: {judge_raw_text[:50]}"}
            
            judged_results.append({
                "generator_model": model_name,
                "sample_id": sample_id,
                "query": query,
                "expected_answer": expected_answer,
                "generator_final_answer": model_final_answer,
                "judge_raw_output": judge_raw_text,
                "accuracy_score": judge_score_data.get("Accuracy_Score", 0),
                "conciseness_score": judge_score_data.get("Conciseness_Score", 0),
                "professionalism_score": judge_score_data.get("Professionalism_Score", 0),
                "judge_reasoning": judge_score_data.get("Reasoning", "")
            })

        except Exception as e:
            logger.exception(f"âŒ è°ƒç”¨ Judge æ¨¡å‹è¯„ä¼° Sample {sample_id} ({model_name}) å¤±è´¥: {e}")
            judged_results.append({
                "generator_model": model_name,
                "sample_id": sample_id,
                "query": query,
                "expected_answer": expected_answer,
                "generator_final_answer": model_final_answer,
                "judge_raw_output": "[ERROR]",
                "accuracy_score": 0, "conciseness_score": 0, "professionalism_score": 0,
                "judge_reasoning": str(e)
            })
    
    # ä¿å­˜ LLM-Judge è¯„ä¼°ç»“æœ
    output_filename = f"llm_judge_results_{Path(args.results_file).stem}_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(judged_results, f, ensure_ascii=False, indent=4)
    logger.info(f"ğŸ‰ LLM-Judge è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")

    # æ±‡æ€»å¹¶æ‰“å°æœ€ç»ˆ Judge è¯„ä¼°æ‘˜è¦
    logger.info("\n--- LLM-Judge è¯„ä¼°æ‘˜è¦ ---")
    model_summaries = {}
    for result in judged_results:
        model_name = result["generator_model"]
        if model_name not in model_summaries:
            model_summaries[model_name] = {
                "total_accuracy": 0,
                "total_conciseness": 0,
                "total_professionalism": 0,
                "count": 0
            }
        model_summaries[model_name]["total_accuracy"] += result["accuracy_score"]
        model_summaries[model_name]["total_conciseness"] += result["conciseness_score"]
        model_summaries[model_name]["total_professionalism"] += result["professionalism_score"]
        model_summaries[model_name]["count"] += 1

    for model_name, data in model_summaries.items():
        if data["count"] > 0:
            avg_accuracy = data["total_accuracy"] / data["count"]
            avg_conciseness = data["total_conciseness"] / data["count"]
            avg_professionalism = data["total_professionalism"] / data["count"]
        else:
            avg_accuracy, avg_conciseness, avg_professionalism = 0.0, 0.0, 0.0

        logger.info(f"\nç”Ÿæˆæ¨¡å‹: {model_name}")
        logger.info(f" è¯„ä¼°æ ·æœ¬æ•°: {data['count']}")
        logger.info(f" å¹³å‡å‡†ç¡®æ€§åˆ†æ•°: {avg_accuracy:.2f}")
        logger.info(f" å¹³å‡ç®€æ´æ€§åˆ†æ•°: {avg_conciseness:.2f}")
        logger.info(f" å¹³å‡ä¸“ä¸šæ€§åˆ†æ•°: {avg_professionalism:.2f}")
    logger.info("----------------------------")
    
    judge_loader.unload_model() # å¸è½½ Judge æ¨¡å‹

# --- å•ä¾‹LLM Judgeç±» ---
class SingletonLLMJudge:
    _instance = None
    _model_loader = None
    _is_initialized = False
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(SingletonLLMJudge, cls).__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not self._is_initialized:
            self._model_loader = None
            self._is_initialized = True
    
    def initialize(self, model_name: str = "Qwen3-8B", device: str = "cuda:1"):
        """åˆå§‹åŒ–æ¨¡å‹åŠ è½½å™¨ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰"""
        if self._model_loader is None:
            logger.info(f"ğŸ”§ åˆå§‹åŒ–LLM Judgeæ¨¡å‹: {model_name} on {device}")
            self._model_loader = ModelLoader(model_name, device)
            try:
                self._model_loader.load_model()
                logger.info(f"âœ… LLM Judgeæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ LLM Judgeæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
                self._model_loader = None
                raise
    
    def evaluate(self, query: str, expected_answer: str, model_final_answer: str, template_file_name: str = "qwen_judge_template.txt") -> Dict[str, Any]:
        """æ‰§è¡ŒLLM Judgeè¯„ä¼°"""
        if self._model_loader is None:
            raise RuntimeError("LLM Judgeæ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initialize()")
        
        try:
            judge_prompt = get_judge_messages(query, expected_answer, model_final_answer, template_file_name)
            
            judge_output = self._model_loader.generate(
                prompt_string=judge_prompt,
                max_new_tokens=256,
                do_sample=False,
                repetition_penalty=1.0
            )
            
            judge_raw_text = judge_output["generated_text"]
            
            # è§£æJudgeè¾“å‡º
            judge_score_data = {}
            try:
                # é¦–å…ˆå°è¯•ç›´æ¥è§£ææ•´ä¸ªè¾“å‡º
                if judge_raw_text.strip().startswith('{') and judge_raw_text.strip().endswith('}'):
                    judge_score_data = json.loads(judge_raw_text.strip())
                else:
                    # å°è¯•æå–JSONéƒ¨åˆ†
                    json_match = re.search(r'\{[\s\S]*\}', judge_raw_text)
                    if json_match:
                        json_string = json_match.group(0)
                        judge_score_data = json.loads(json_string)
                    else:
                        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰JSONï¼Œå°è¯•ä»æ€è€ƒè¿‡ç¨‹ä¸­æå–è¯„åˆ†
                        logger.warning(f"âš ï¸ Judgeè¾“å‡ºæ— JSONæ ¼å¼ï¼Œå°è¯•ä»æ€è€ƒè¿‡ç¨‹ä¸­æå–è¯„åˆ†: {judge_raw_text[:200]}...")
                        
                        # å°è¯•ä»æ–‡æœ¬ä¸­æå–è¯„åˆ†ä¿¡æ¯ - æ›´çµæ´»çš„åŒ¹é…æ¨¡å¼
                        accuracy_match = re.search(r'å‡†ç¡®æ€§[ï¼š:]\s*(\d+)', judge_raw_text) or re.search(r'å‡†ç¡®[ï¼š:]\s*(\d+)', judge_raw_text) or re.search(r'Accuracy[ï¼š:]\s*(\d+)', judge_raw_text)
                        conciseness_match = re.search(r'ç®€æ´æ€§[ï¼š:]\s*(\d+)', judge_raw_text) or re.search(r'ç®€æ´[ï¼š:]\s*(\d+)', judge_raw_text) or re.search(r'Conciseness[ï¼š:]\s*(\d+)', judge_raw_text)
                        professionalism_match = re.search(r'ä¸“ä¸šæ€§[ï¼š:]\s*(\d+)', judge_raw_text) or re.search(r'ä¸“ä¸š[ï¼š:]\s*(\d+)', judge_raw_text) or re.search(r'Professionalism[ï¼š:]\s*(\d+)', judge_raw_text)
                        
                        # ä¹Ÿå°è¯•åŒ¹é…æ•°å­—æ¨¡å¼
                        if not accuracy_match:
                            accuracy_match = re.search(r'(\d+)\s*åˆ†.*å‡†ç¡®', judge_raw_text) or re.search(r'å‡†ç¡®.*(\d+)', judge_raw_text)
                        if not conciseness_match:
                            conciseness_match = re.search(r'(\d+)\s*åˆ†.*ç®€æ´', judge_raw_text) or re.search(r'ç®€æ´.*(\d+)', judge_raw_text)
                        if not professionalism_match:
                            professionalism_match = re.search(r'(\d+)\s*åˆ†.*ä¸“ä¸š', judge_raw_text) or re.search(r'ä¸“ä¸š.*(\d+)', judge_raw_text)
                        
                        accuracy_score = int(accuracy_match.group(1)) if accuracy_match else 0
                        conciseness_score = int(conciseness_match.group(1)) if conciseness_match else 0
                        professionalism_score = int(professionalism_match.group(1)) if professionalism_match else 0
                        
                        # å¦‚æœè¿˜æ˜¯æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»æ•°å­—ä¸­æ¨æ–­
                        if accuracy_score == 0 and conciseness_score == 0 and professionalism_score == 0:
                            numbers = re.findall(r'\d+', judge_raw_text)
                            found = 0
                            for n in numbers:
                                score = int(n)
                                if 0 <= score <= 5:
                                    if found == 0:
                                        accuracy_score = score
                                    elif found == 1:
                                        conciseness_score = score
                                    elif found == 2:
                                        professionalism_score = score
                                    found += 1
                                    if found >= 3:
                                        break
                            # å¦‚æœåªæ‰¾åˆ°1ä¸ªåˆ†æ•°ï¼Œå…¨éƒ¨ç”¨è¿™ä¸ªåˆ†æ•°
                            if found == 1:
                                conciseness_score = professionalism_score = accuracy_score
                            # å¦‚æœæ²¡æœ‰åˆç†åˆ†æ•°ï¼Œé™çº§ä¸ºé»˜è®¤åˆ†æ•°3
                            if found == 0:
                                accuracy_score = conciseness_score = professionalism_score = 3
                        # å¼ºåˆ¶è£å‰ªåˆ†æ•°åˆ°0~5
                        accuracy_score = min(max(accuracy_score, 0), 5)
                        conciseness_score = min(max(conciseness_score, 0), 5)
                        professionalism_score = min(max(professionalism_score, 0), 5)
                        judge_score_data = {
                            'Accuracy_Score': accuracy_score,
                            'Conciseness_Score': conciseness_score,
                            'Professionalism_Score': professionalism_score,
                            'Reasoning': f'ä»æ€è€ƒè¿‡ç¨‹ä¸­æå–çš„è¯„åˆ† - åŸå§‹è¾“å‡º: {judge_raw_text[:200]}'
                        }
                        
                        if accuracy_score == 0 and conciseness_score == 0 and professionalism_score == 0:
                            judge_score_data = {
                                "Accuracy_Score": 5,  # ç»™ä¸€ä¸ªé»˜è®¤ä¸­ç­‰åˆ†æ•°
                                "Conciseness_Score": 5,
                                "Professionalism_Score": 5,
                                "Reasoning": f"æ— æ³•æå–è¯„åˆ†ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•° - åŸå§‹è¾“å‡º: {judge_raw_text[:100]}..."
                            }
            except json.JSONDecodeError as json_e:
                logger.error(f"âŒ Judgeè¾“å‡ºJSONè§£æå¤±è´¥: {json_e} - Raw: {judge_raw_text[:200]}...")
                judge_score_data = {
                    "Accuracy_Score": 0, 
                    "Conciseness_Score": 0, 
                    "Professionalism_Score": 0, 
                    "Reasoning": f"JSONè§£æå¤±è´¥: {judge_raw_text[:50]}"
                }
            
            return {
                "accuracy": judge_score_data.get("Accuracy_Score", 0),
                "conciseness": judge_score_data.get("Conciseness_Score", 0),
                "professionalism": judge_score_data.get("Professionalism_Score", 0),
                "overall_score": (judge_score_data.get("Accuracy_Score", 0) + 
                                judge_score_data.get("Conciseness_Score", 0) + 
                                judge_score_data.get("Professionalism_Score", 0)) / 3,
                "reasoning": judge_score_data.get("Reasoning", ""),
                "raw_output": judge_raw_text
            }
            
        except Exception as e:
            logger.exception(f"âŒ LLM Judgeè¯„ä¼°å¤±è´¥: {e}")
            return {
                "accuracy": 0,
                "conciseness": 0,
                "professionalism": 0,
                "overall_score": 0,
                "reasoning": f"è¯„ä¼°å¤±è´¥: {str(e)}",
                "raw_output": ""
            }
    
    def cleanup(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        if self._model_loader is not None:
            self._model_loader.unload_model()
            self._model_loader = None
            logger.info("âœ… LLM Judgeæ¨¡å‹å·²æ¸…ç†")

# å…¨å±€å•ä¾‹å®ä¾‹
llm_judge_singleton = SingletonLLMJudge()

# --- ä¸»ç¨‹åºå…¥å£ ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="LLM-Judge è¯„ä¼°è„šæœ¬")
    parser.add_argument("--results_file", type=str, required=True, help="åŒ…å«ç”Ÿæˆæ¨¡å‹è¾“å‡ºçš„JSONæ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚ comparison_results_chinese_*.json)")
    parser.add_argument("--judge_template_file", type=str, default="qwen_judge_template.txt", help="LLM Judge çš„ Prompt æ¨¡æ¿æ–‡ä»¶ (ä¾‹å¦‚ qwen_judge_template.txt)")
    parser.add_argument("--judge_max_new_tokens", type=int, default=256, help="LLM Judge ç”Ÿæˆçš„æœ€å¤§æ–° Token æ•°")

    args = parser.parse_args()
    run_llm_judge_evaluation(args)