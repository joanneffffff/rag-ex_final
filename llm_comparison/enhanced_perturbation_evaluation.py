#!/usr/bin/env python3
"""
å¢å¼ºçš„æ‰°åŠ¨è¯„ä¼°è„šæœ¬
ä½¿ç”¨llm-judgeè¯„ä¼°æ‰°åŠ¨ç»“æœï¼Œå¹¶ä½¿ç”¨F1å’ŒEMé€»è¾‘è®¡ç®—æ‰°åŠ¨ç­”æ¡ˆä¸æœŸæœ›ç­”æ¡ˆã€æ‰°åŠ¨ç­”æ¡ˆä¸åŸå§‹ç­”æ¡ˆçš„å¯¹æ¯”
"""

import sys
import os
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Any, Optional
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥F1å’ŒEMè®¡ç®—é€»è¾‘
from llm_comparison.chinese_llm_evaluation import (
    calculate_f1_score, 
    calculate_exact_match, 
    normalize_answer_chinese
)

# å¯¼å…¥LLM Judge
from llm_comparison.chinese_llm_judge import SingletonLLMJudge

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedPerturbationEvaluator:
    """å¢å¼ºçš„æ‰°åŠ¨è¯„ä¼°å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.llm_judge = None
        self.expected_answers = {}
        
    def load_expected_answers(self, alphafin_data_path: str):
        """ä»alphafinæ•°æ®æ–‡ä»¶åŠ è½½æœŸæœ›ç­”æ¡ˆ"""
        logger.info(f"ğŸ“Š åŠ è½½æœŸæœ›ç­”æ¡ˆä»: {alphafin_data_path}")
        
        try:
            with open(alphafin_data_path, 'r', encoding='utf-8') as f:
                for line_num, line in enumerate(f, 1):
                    if line.strip():
                        try:
                            data = json.loads(line.strip())
                            # ä½¿ç”¨é—®é¢˜ä½œä¸ºkeyï¼ŒæœŸæœ›ç­”æ¡ˆä½œä¸ºvalue
                            question = data.get('generated_question', '')
                            expected_answer = data.get('answer', '')
                            if question and expected_answer:
                                self.expected_answers[question] = expected_answer
                        except json.JSONDecodeError as e:
                            logger.warning(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(self.expected_answers)} ä¸ªæœŸæœ›ç­”æ¡ˆ")
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æœŸæœ›ç­”æ¡ˆå¤±è´¥: {e}")
            raise
    
    def initialize_llm_judge(self, model_name: str = "Qwen3-8B", device: str = "cuda:0"):
        """åˆå§‹åŒ–LLM Judge"""
        logger.info(f"ğŸ¤– åˆå§‹åŒ–LLM Judge: {model_name} on {device}")
        
        try:
            self.llm_judge = SingletonLLMJudge()
            self.llm_judge.initialize(model_name=model_name, device=device)
            logger.info("âœ… LLM Judgeåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ LLM Judgeåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def calculate_enhanced_metrics(self, original_answer: str, perturbed_answer: str, 
                                 expected_answer: str, query: str) -> Dict[str, Any]:
        """è®¡ç®—å¢å¼ºçš„è¯„ä¼°æŒ‡æ ‡"""
        
        # 1. ä½¿ç”¨F1å’ŒEMé€»è¾‘è®¡ç®—å„ç§å¯¹æ¯”
        metrics = {}
        
        # åŸå§‹ç­”æ¡ˆ vs æœŸæœ›ç­”æ¡ˆ
        metrics['f1_original_vs_expected'] = calculate_f1_score(original_answer, expected_answer)
        metrics['em_original_vs_expected'] = calculate_exact_match(original_answer, expected_answer)
        
        # æ‰°åŠ¨ç­”æ¡ˆ vs æœŸæœ›ç­”æ¡ˆ
        metrics['f1_perturbed_vs_expected'] = calculate_f1_score(perturbed_answer, expected_answer)
        metrics['em_perturbed_vs_expected'] = calculate_exact_match(perturbed_answer, expected_answer)
        
        # æ‰°åŠ¨ç­”æ¡ˆ vs åŸå§‹ç­”æ¡ˆ
        metrics['f1_perturbed_vs_original'] = calculate_f1_score(perturbed_answer, original_answer)
        metrics['em_perturbed_vs_original'] = calculate_exact_match(perturbed_answer, original_answer)
        
        # 2. è®¡ç®—F1æ”¹è¿›
        metrics['f1_improvement'] = metrics['f1_perturbed_vs_expected'] - metrics['f1_original_vs_expected']
        
        # 3. ä½¿ç”¨LLM Judgeè¯„ä¼°
        if self.llm_judge:
            try:
                judge_result = self.llm_judge.evaluate(
                    query=query,
                    expected_answer=expected_answer,
                    model_final_answer=perturbed_answer
                )
                metrics['llm_judge_scores'] = judge_result
                logger.debug(f"âœ… LLM Judgeè¯„ä¼°å®Œæˆ: å‡†ç¡®æ€§={judge_result.get('accuracy', 0)}, ç®€æ´æ€§={judge_result.get('conciseness', 0)}, ä¸“ä¸šæ€§={judge_result.get('professionalism', 0)}")
            except Exception as e:
                logger.warning(f"âš ï¸ LLM Judgeè¯„ä¼°å¤±è´¥: {e}")
                metrics['llm_judge_scores'] = {
                    'accuracy': 5,  # ä½¿ç”¨é»˜è®¤ä¸­ç­‰åˆ†æ•°
                    'conciseness': 5,
                    'professionalism': 5,
                    'overall_score': 5,
                    'reasoning': f"è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ•°: {str(e)}",
                    'raw_output': ""
                }
        else:
            metrics['llm_judge_scores'] = {
                'accuracy': 0,
                'conciseness': 0,
                'professionalism': 0,
                'overall_score': 0,
                'reasoning': "LLM Judgeæœªåˆå§‹åŒ–",
                'raw_output': ""
            }
        
        return metrics
    
    def evaluate_perturbation_results(self, perturbation_file: str, output_file: str):
        """è¯„ä¼°æ‰°åŠ¨ç»“æœæ–‡ä»¶"""
        logger.info(f"ğŸ” å¼€å§‹è¯„ä¼°æ‰°åŠ¨ç»“æœ: {perturbation_file}")
        
        # åŠ è½½æ‰°åŠ¨ç»“æœ
        try:
            with open(perturbation_file, 'r', encoding='utf-8') as f:
                perturbation_results = json.load(f)
            logger.info(f"âœ… åŠ è½½äº† {len(perturbation_results)} ä¸ªæ‰°åŠ¨ç»“æœ")
        except Exception as e:
            logger.error(f"âŒ åŠ è½½æ‰°åŠ¨ç»“æœå¤±è´¥: {e}")
            return
        
        # è¯„ä¼°æ¯ä¸ªç»“æœ
        enhanced_results = []
        
        for i, result in enumerate(perturbation_results):
            logger.info(f"ğŸ“Š è¯„ä¼°ç¬¬ {i+1}/{len(perturbation_results)} ä¸ªç»“æœ")
            
            query = result.get('question', '')
            original_answer = result.get('original_answer', '')
            perturbed_answer = result.get('perturbed_answer', '')
            
            # æŸ¥æ‰¾æœŸæœ›ç­”æ¡ˆ
            expected_answer = self.expected_answers.get(query, '')
            if not expected_answer:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°é—®é¢˜ '{query[:50]}...' çš„æœŸæœ›ç­”æ¡ˆ")
                expected_answer = ""
            
            # è®¡ç®—å¢å¼ºæŒ‡æ ‡
            enhanced_metrics = self.calculate_enhanced_metrics(
                original_answer=original_answer,
                perturbed_answer=perturbed_answer,
                expected_answer=expected_answer,
                query=query
            )
            
            # åˆå¹¶ç»“æœ
            enhanced_result = {
                **result,  # ä¿ç•™åŸå§‹æ•°æ®
                **enhanced_metrics,  # æ·»åŠ å¢å¼ºæŒ‡æ ‡
                'expected_answer': expected_answer,
                'evaluation_timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
            enhanced_results.append(enhanced_result)
            
            # æ‰“å°è¿›åº¦
            if (i + 1) % 10 == 0:
                logger.info(f"âœ… å·²å®Œæˆ {i+1}/{len(perturbation_results)} ä¸ªè¯„ä¼°")
        
        # ä¿å­˜å¢å¼ºç»“æœ
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(enhanced_results, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ‰ å¢å¼ºè¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            logger.error(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        
        # æ‰“å°ç»Ÿè®¡æ‘˜è¦
        self.print_evaluation_summary(enhanced_results)
    
    def print_evaluation_summary(self, results: List[Dict[str, Any]]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        logger.info("\n" + "="*60)
        logger.info("ğŸ“Š å¢å¼ºæ‰°åŠ¨è¯„ä¼°æ‘˜è¦")
        logger.info("="*60)
        
        if not results:
            logger.info("âŒ æ²¡æœ‰è¯„ä¼°ç»“æœ")
            return
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_results = len(results)
        
        # F1ç»Ÿè®¡
        f1_original_vs_expected = [r.get('f1_original_vs_expected', 0) for r in results]
        f1_perturbed_vs_expected = [r.get('f1_perturbed_vs_expected', 0) for r in results]
        f1_perturbed_vs_original = [r.get('f1_perturbed_vs_original', 0) for r in results]
        
        # EMç»Ÿè®¡
        em_original_vs_expected = [r.get('em_original_vs_expected', 0) for r in results]
        em_perturbed_vs_expected = [r.get('em_perturbed_vs_expected', 0) for r in results]
        em_perturbed_vs_original = [r.get('em_perturbed_vs_original', 0) for r in results]
        
        # LLM Judgeç»Ÿè®¡
        judge_scores = [r.get('llm_judge_scores', {}).get('overall_score', 0) for r in results]
        
        logger.info(f"ğŸ“ˆ è¯„ä¼°æ ·æœ¬æ€»æ•°: {total_results}")
        logger.info(f"ğŸ“Š F1åˆ†æ•°ç»Ÿè®¡:")
        logger.info(f"   åŸå§‹ç­”æ¡ˆ vs æœŸæœ›ç­”æ¡ˆ: å¹³å‡ {sum(f1_original_vs_expected)/len(f1_original_vs_expected):.4f}")
        logger.info(f"   æ‰°åŠ¨ç­”æ¡ˆ vs æœŸæœ›ç­”æ¡ˆ: å¹³å‡ {sum(f1_perturbed_vs_expected)/len(f1_perturbed_vs_expected):.4f}")
        logger.info(f"   æ‰°åŠ¨ç­”æ¡ˆ vs åŸå§‹ç­”æ¡ˆ: å¹³å‡ {sum(f1_perturbed_vs_original)/len(f1_perturbed_vs_original):.4f}")
        
        logger.info(f"ğŸ“Š EMåˆ†æ•°ç»Ÿè®¡:")
        logger.info(f"   åŸå§‹ç­”æ¡ˆ vs æœŸæœ›ç­”æ¡ˆ: å¹³å‡ {sum(em_original_vs_expected)/len(em_original_vs_expected):.4f}")
        logger.info(f"   æ‰°åŠ¨ç­”æ¡ˆ vs æœŸæœ›ç­”æ¡ˆ: å¹³å‡ {sum(em_perturbed_vs_expected)/len(em_perturbed_vs_expected):.4f}")
        logger.info(f"   æ‰°åŠ¨ç­”æ¡ˆ vs åŸå§‹ç­”æ¡ˆ: å¹³å‡ {sum(em_perturbed_vs_original)/len(em_perturbed_vs_original):.4f}")
        
        logger.info(f"ğŸ¤– LLM Judgeè¯„åˆ†: å¹³å‡ {sum(judge_scores)/len(judge_scores):.2f}")
        
        # æ‰°åŠ¨å™¨ç»Ÿè®¡
        perturber_stats = {}
        for result in results:
            perturber = result.get('perturber_name', 'unknown')
            if perturber not in perturber_stats:
                perturber_stats[perturber] = {
                    'count': 0,
                    'avg_f1_improvement': 0,
                    'avg_judge_score': 0
                }
            
            perturber_stats[perturber]['count'] += 1
            perturber_stats[perturber]['avg_f1_improvement'] += result.get('f1_improvement', 0)
            perturber_stats[perturber]['avg_judge_score'] += result.get('llm_judge_scores', {}).get('overall_score', 0)
        
        logger.info(f"ğŸ”„ æ‰°åŠ¨å™¨ç»Ÿè®¡:")
        for perturber, stats in perturber_stats.items():
            count = stats['count']
            avg_f1_improvement = stats['avg_f1_improvement'] / count if count > 0 else 0
            avg_judge_score = stats['avg_judge_score'] / count if count > 0 else 0
            logger.info(f"   {perturber}: {count}ä¸ªæ ·æœ¬, F1æ”¹è¿›: {avg_f1_improvement:.4f}, Judgeè¯„åˆ†: {avg_judge_score:.2f}")
        
        logger.info("="*60)

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¢å¼ºçš„æ‰°åŠ¨è¯„ä¼°è„šæœ¬")
    parser.add_argument("--perturbation_file", type=str, required=True,
                       help="æ‰°åŠ¨ç»“æœæ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: perturbation_results_incremental.json)")
    parser.add_argument("--alphafin_data", type=str, required=True,
                       help="AlphaFinæ•°æ®æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: data/alphafin/alphafin_eval_samples_updated.jsonl)")
    parser.add_argument("--output_file", type=str, default=None,
                       help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (é»˜è®¤: enhanced_perturbation_evaluation_results.json)")
    parser.add_argument("--judge_model", type=str, default="Qwen3-8B",
                       help="LLM Judgeæ¨¡å‹åç§°")
    parser.add_argument("--judge_device", type=str, default="cuda:1",
                       help="LLM Judgeè®¾å¤‡")
    
    args = parser.parse_args()
    
    # è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶
    if args.output_file is None:
        timestamp = time.strftime('%Y%m%d_%H%M%S')
        args.output_file = f"enhanced_perturbation_evaluation_results_{timestamp}.json"
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = EnhancedPerturbationEvaluator()
    
    try:
        # åŠ è½½æœŸæœ›ç­”æ¡ˆ
        evaluator.load_expected_answers(args.alphafin_data)
        
        # åˆå§‹åŒ–LLM Judge
        evaluator.initialize_llm_judge(args.judge_model, args.judge_device)
        
        # æ‰§è¡Œè¯„ä¼°
        evaluator.evaluate_perturbation_results(args.perturbation_file, args.output_file)
        
        logger.info("ğŸ‰ å¢å¼ºæ‰°åŠ¨è¯„ä¼°å®Œæˆï¼")
        
    except Exception as e:
        logger.error(f"âŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 