#!/usr/bin/env python3
"""
TatQAè‹±æ–‡LLMæ¨¡å‹å¯¹æ¯”è¯„ä¼°è„šæœ¬ - åŸºäºcomprehensive_evaluation_enhanced.pyçš„é€»è¾‘
æ”¯æŒFin-R1å’ŒQwen3-8Båœ¨TatQAè‹±æ–‡æ•°æ®é›†ä¸Šçš„è¡¨ç°å¯¹æ¯”
"""

import warnings
import logging
import os
import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import time
import argparse
from collections import Counter
from difflib import SequenceMatcher
import sys
import gc

# ç¯å¢ƒè®¾ç½®
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

try:
    from tqdm import tqdm
except ImportError:
    print("âŒ tqdmæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tqdm")
    sys.exit(1)

from transformers import AutoTokenizer, AutoModelForCausalLM

# ===================================================================
# æ ¸å¿ƒè¾…åŠ©å‡½æ•° (åŸºäºcomprehensive_evaluation_enhanced.py)
# ===================================================================

def extract_final_answer_with_rescue(raw_output: str) -> str:
    """
    ä»æ¨¡å‹çš„åŸå§‹è¾“å‡ºä¸­æ™ºèƒ½æå–æœ€ç»ˆç­”æ¡ˆ (æ”¯æŒä¸­è‹±æ–‡)
    """
    def _clean_extracted_text(text: str) -> str:
        """å¯¹æå–å‡ºçš„æ–‡æœ¬è¿›è¡Œé€šç”¨æ¸…ç†"""
        text = text.strip()
        # ç§»é™¤æ•°å­—ä¸­çš„é€—å·
        text = text.replace(',', '')
        # ç§»é™¤è´Ÿæ•°æ‹¬å· (ä¾‹å¦‚ "(33)" -> "-33")
        if text.startswith('(') and text.endswith(')'):
            text = '-' + text[1:-1]
        
        # æ ‡å‡†åŒ–ç™¾åˆ†å·
        text = text.replace('%', ' %').strip()
        text = text.replace(' %', '%')

        # ç§»é™¤å¸¸è§çš„å¼•å¯¼è¯å¥
        text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
        
        # ç§»é™¤æœ«å°¾å¯èƒ½çš„å¤šä½™æ ‡ç‚¹ç¬¦å·
        text = re.sub(r'[\.;,]$', '', text).strip()
        
        # ç§»é™¤å¸¸è§çš„è´§å¸ç¬¦å·å’Œå•ä½è¯
        text = re.sub(r'(\$|million|billion|usd|eur|pounds|Â£)', '', text, flags=re.IGNORECASE).strip()

        return text

    # 1. å°è¯•ä» <answer> æ ‡ç­¾ä¸­æå–
    answer_match = re.search(r'<answer>(.*?)</answer>', raw_output, re.DOTALL)
    if answer_match:
        content = answer_match.group(1).strip()
        if content:
            return _clean_extracted_text(content)

    # 2. æ•‘æ´é€»è¾‘ï¼šä» <think> æ ‡ç­¾ä¸­æå–
    think_match = re.search(r'<think>(.*?)(?:</think>|$)', raw_output, re.DOTALL)
    if not think_match:
        # å¦‚æœè¿ <think> æ ‡ç­¾éƒ½æ²¡æœ‰ï¼Œå›é€€åˆ°åŸå§‹è¾“å‡ºçš„æœ€åä¸€è¡Œ
        lines = raw_output.strip().split('\n')
        return _clean_extracted_text(lines[-1]) if lines else ""

    think_content = think_match.group(1)
    
    # å°è¯•å¯»æ‰¾ç»“è®ºæ€§çŸ­è¯­
    conclusion_phrases = [
        r'final\s*answer\s*is[:\s]*', r'the\s*answer\s*is[:\s]*', 
        r'therefore,\s*the\s*answer\s*is[:\s]*', r'the\s*result\s*is[:\s]*', 
        r'equals\s*to[:\s]*', r'is\s*equal\s*to[:\s]*', 
        r'the\s*value\s*is[:\s]*', r'the\s*change\s*is[:\s]*', 
        r'the\s*amount\s*is[:\s]*', r'conclusion[:\s]*', 
        r'final\s*extracted\s*value/calculated\s*result[:\s]*', r'final\s*number[:\s]*',
        r'adjusted\s*net\s*income\s*is[:\s]*', r'percentage\s*change\s*is[:\s]*', 
        r'decreased\s*by[:\s]*', r'increased\s*by[:\s]*',
        r'net\s*change\s*is[:\s]*', r'total\s*is[:\s]*',
        r'resulting\s*in[:\s]*', r'is[:\s]*([-+]?[\d,\.]+%?)'
    ]
    
    for phrase_pattern in conclusion_phrases:
        conclusion_match = re.search(
            f'{phrase_pattern}(.*?)(?:$|<answer>|<think>|\\n\\n|\\Z)', 
            think_content, 
            re.IGNORECASE | re.DOTALL 
        )
        if conclusion_match:
            conclusion = conclusion_match.group(1).strip()
            if conclusion and re.fullmatch(r'\d+\.', conclusion.split('\n')[0].strip()):
                continue
            
            return _clean_extracted_text(conclusion)
    
    # å°è¯•å¯»æ‰¾æœ€åä¸€ä¸ªç¬¦åˆæ•°å€¼/ç™¾åˆ†æ¯”æ ¼å¼çš„å­—ç¬¦ä¸²
    potential_answers_raw = re.findall(r'([-+]?\s*\(?[\d,\.]+\)?%?)\s*$', think_content, re.MULTILINE)
    if not potential_answers_raw:
        potential_answers_raw = re.findall(r'([-+]?\s*\(?[\d,\.]+\)?%?)', think_content)
    
    if potential_answers_raw:
        for item_raw in reversed(potential_answers_raw):
            item = item_raw.strip()
            if not item: continue
            
            if re.fullmatch(r'(\d+\.|\bstep\s*\d+\b)[:\s]*', item, re.IGNORECASE):
                continue

            cleaned_item = _clean_extracted_text(item)
            
            if cleaned_item and len(cleaned_item) > 0 and not re.fullmatch(r'[^\w\s\d%.-]*', cleaned_item):
                return cleaned_item
                
    # æœ€åå›é€€ï¼šå– <think> å†…å®¹çš„æœ€åä¸€è¡Œ
    lines = [line for line in think_content.strip().split('\n') if line.strip()]
    if lines:
        return _clean_extracted_text(lines[-1])
    return ""


def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—F1åˆ†æ•°ï¼ŒåŒ…å«æ›´é²æ£’çš„å½’ä¸€åŒ–"""
    def normalize_for_f1(text):
        text = text.strip()
        
        # ç§»é™¤æ•°å­—ä¸­çš„é€—å·
        text = text.replace(',', '')
        # ç§»é™¤è´Ÿæ•°æ‹¬å·
        if text.startswith('(') and text.endswith(')'):
            text = '-' + text[1:-1]
        
        # æ ‡å‡†åŒ–ç™¾åˆ†å·
        text = text.replace('%', ' %').strip()
        text = text.replace(' %', '%')

        # ç§»é™¤å¸¸è§çš„å¼•å¯¼è¯å¥
        text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
        
        # ç§»é™¤æœ«å°¾å¯èƒ½çš„å¤šä½™æ ‡ç‚¹
        text = text.rstrip('.')
        
        # æœ€ç»ˆå…¨éƒ¨å°å†™å¹¶åˆ†å‰²
        return text.lower().split()

    prediction_tokens = normalize_for_f1(prediction)
    ground_truth_tokens = normalize_for_f1(ground_truth)

    if not ground_truth_tokens: 
        return 1.0 if not prediction_tokens else 0.0
    if not prediction_tokens: 
        return 0.0

    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0: 
        return 0.0

    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1


def calculate_exact_match(prediction: str, ground_truth: str) -> float:
    """è®¡ç®—ç²¾ç¡®åŒ¹é…ç‡"""
    def normalize_for_em(text):
        text = text.strip().lower()
        text = text.replace(',', '')
        if text.startswith('(') and text.endswith(')'):
            text = '-' + text[1:-1]
        text = text.replace('%', ' %').strip().replace(' %', '%')
        text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
        text = text.rstrip('.')
        return text
    
    return 1.0 if normalize_for_em(prediction) == normalize_for_em(ground_truth) else 0.0


# ===================================================================
# ä¸Šä¸‹æ–‡ç±»å‹åˆ¤æ–­å’Œå†³ç­–é€»è¾‘ (åŸºäºcomprehensive_evaluation_enhanced.py)
# ===================================================================

def determine_context_type(context: str) -> str:
    """åˆ¤æ–­ä¸Šä¸‹æ–‡ç±»å‹ï¼štable, text, æˆ– mixed"""
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼ç‰¹å¾
    table_indicators = [
        r'\|\s*[^|]+\s*\|',  # è¡¨æ ¼åˆ†éš”ç¬¦
        r'Table\s*\d+',      # è¡¨æ ¼æ ‡é¢˜
        r'Row\s*\d+',        # è¡Œæ ‡è¯†
        r'Column\s*\d+',     # åˆ—æ ‡è¯†
        r'Header\s*[:\s]',   # è¡¨å¤´
        r'Data\s*[:\s]',     # æ•°æ®æ ‡è¯†
    ]
    
    text_indicators = [
        r'Paragraph\s*\d+',  # æ®µè½æ ‡è¯†
        r'Section\s*\d+',    # ç« èŠ‚æ ‡è¯†
        r'Report\s*[:\s]',   # æŠ¥å‘Šæ ‡è¯†
        r'Summary\s*[:\s]',  # æ‘˜è¦æ ‡è¯†
    ]
    
    table_score = sum(len(re.findall(pattern, context, re.IGNORECASE)) for pattern in table_indicators)
    text_score = sum(len(re.findall(pattern, context, re.IGNORECASE)) for pattern in text_indicators)
    
    if table_score > text_score:
        return "table"
    elif text_score > table_score:
        return "text"
    else:
        return "mixed"


def analyze_query_features(query: str) -> Dict[str, Any]:
    """åˆ†ææŸ¥è¯¢ç‰¹å¾"""
    features = {
        "length": len(query),
        "has_numbers": bool(re.search(r'\d+', query)),
        "has_percentages": bool(re.search(r'\d+%', query)),
        "has_currency": bool(re.search(r'[\$Â£â‚¬]', query)),
        "has_comparison": bool(re.search(r'(higher|lower|more|less|increase|decrease|change|difference)', query, re.IGNORECASE)),
        "has_calculation": bool(re.search(r'(calculate|compute|sum|total|average|mean|percentage)', query, re.IGNORECASE)),
        "question_type": "calculation" if re.search(r'(what\s*is|how\s*much|calculate|compute)', query, re.IGNORECASE) else "extraction"
    }
    return features


def calculate_content_ratio(context: str) -> Dict[str, float]:
    """è®¡ç®—å†…å®¹æ¯”ä¾‹"""
    total_chars = len(context)
    if total_chars == 0:
        return {"table_ratio": 0.0, "text_ratio": 0.0}
    
    # ç®€å•çš„å¯å‘å¼æ–¹æ³•
    table_chars = len(re.findall(r'[|+\-]', context))  # è¡¨æ ¼åˆ†éš”ç¬¦
    text_chars = len(re.findall(r'[a-zA-Z]', context))  # å­—æ¯å­—ç¬¦
    
    table_ratio = table_chars / total_chars if total_chars > 0 else 0.0
    text_ratio = text_chars / total_chars if total_chars > 0 else 0.0
    
    return {"table_ratio": table_ratio, "text_ratio": text_ratio}


def hybrid_decision_enhanced(context: str, query: str) -> Dict[str, Any]:
    """å¢å¼ºçš„æ··åˆå†³ç­–ç®—æ³•"""
    context_type = determine_context_type(context)
    query_features = analyze_query_features(query)
    content_ratio = calculate_content_ratio(context)
    
    # å†³ç­–é€»è¾‘
    decision_factors = {
        "context_type_weight": 0.4,
        "query_features_weight": 0.4,
        "content_ratio_weight": 0.2
    }
    
    # åŸºäºä¸Šä¸‹æ–‡ç±»å‹çš„åˆ†æ•°
    context_scores = {
        "table": 0.8 if context_type == "table" else 0.2,
        "text": 0.8 if context_type == "text" else 0.2,
        "mixed": 0.6
    }
    
    # åŸºäºæŸ¥è¯¢ç‰¹å¾çš„åˆ†æ•°
    query_scores = {
        "table": 0.7 if query_features["has_calculation"] or query_features["has_comparison"] else 0.3,
        "text": 0.7 if query_features["question_type"] == "extraction" else 0.3,
        "mixed": 0.5
    }
    
    # åŸºäºå†…å®¹æ¯”ä¾‹çš„åˆ†æ•°
    ratio_scores = {
        "table": content_ratio["table_ratio"],
        "text": content_ratio["text_ratio"],
        "mixed": 0.5
    }
    
    # è®¡ç®—æœ€ç»ˆåˆ†æ•°
    final_scores = {}
    for context_type_key in ["table", "text", "mixed"]:
        final_scores[context_type_key] = (
            context_scores[context_type_key] * decision_factors["context_type_weight"] +
            query_scores[context_type_key] * decision_factors["query_features_weight"] +
            ratio_scores[context_type_key] * decision_factors["content_ratio_weight"]
        )
    
    # é€‰æ‹©æœ€é«˜åˆ†æ•°çš„ç±»å‹
    best_type = max(final_scores.keys(), key=lambda k: final_scores[k])
    
    return {
        "decision": best_type,
        "confidence": final_scores[best_type],
        "scores": final_scores,
        "context_type": context_type,
        "query_features": query_features,
        "content_ratio": content_ratio
    }


# ===================================================================
# æ¨¡æ¿åŠ è½½å’Œæ ¼å¼åŒ– (åŸºäºcomprehensive_evaluation_enhanced.py)
# ===================================================================

def _load_template_content_from_file(template_file_name: str) -> str:
    """ä»æŒ‡å®šæ–‡ä»¶ä¸­åŠ è½½Promptæ¨¡æ¿çš„å®Œæ•´å­—ç¬¦ä¸²å†…å®¹"""
    template_path = Path("data/prompt_templates") / template_file_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}ï¼Œè¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨ã€‚")
        sys.exit(1)


def _parse_template_string_to_messages(template_full_string: str, query: str, context: str = "", table_context: str = "", text_context: str = "") -> List[Dict[str, str]]:
    """è§£ææ¨¡æ¿å­—ç¬¦ä¸²ä¸ºmessagesæ ¼å¼"""
    messages = []
    
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ‰€æœ‰éƒ¨åˆ†ï¼Œå¹¶ä¿ç•™åˆ†éš”ç¬¦å†…å®¹
    parts = re.split(r'(===SYSTEM===|===USER===|===ASSISTANT===)', template_full_string, flags=re.DOTALL)
    
    # ç§»é™¤ç¬¬ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰å’Œå¤šä½™çš„ç©ºç™½
    parts = [p.strip() for p in parts if p.strip()]

    # éå† parts åˆ—è¡¨ï¼Œé‡æ–°ç»„åˆ role å’Œ content
    for i in range(0, len(parts), 2):
        if i + 1 < len(parts):
            role_tag_raw = parts[i].strip()
            content = parts[i+1].strip()
            
            role = None
            if role_tag_raw == "===SYSTEM===": role = "system"
            elif role_tag_raw == "===USER===": role = "user"
            elif role_tag_raw == "===ASSISTANT===": role = "assistant"
            
            if role and content:
                # æ›¿æ¢å ä½ç¬¦
                if role == "user":
                    content = content.replace('{query}', query)
                    content = content.replace('{context}', context)
                    content = content.replace('{table_context}', table_context)
                    content = content.replace('{text_context}', text_context)
                
                messages.append({"role": role, "content": content})
                
    return messages


def load_and_format_template(template_name: str, context: str, query: str) -> List[Dict[str, str]]:
    """åŠ è½½å¹¶æ ¼å¼åŒ–æ¨¡æ¿"""
    template_full_string = _load_template_content_from_file(template_name)
    return _parse_template_string_to_messages(template_full_string, query, context)


def get_final_prompt(context: str, query: str) -> List[Dict[str, str]]:
    """è·å–æœ€ç»ˆçš„prompt"""
    # ä½¿ç”¨æ··åˆå†³ç­–ç®—æ³•
    decision_result = hybrid_decision_enhanced(context, query)
    decision = decision_result["decision"]
    
    # æ ¹æ®å†³ç­–é€‰æ‹©æ¨¡æ¿ (AlphaFinä½¿ç”¨ä¸­æ–‡æ¨¡æ¿)
    if decision == "table":
        template_name = "template_for_table_answer.txt"
    elif decision == "text":
        template_name = "template_for_text_answer.txt"
    else:  # mixed
        template_name = "template_for_hybrid_answer.txt"
    
    try:
        return load_and_format_template(template_name, context, query)
    except Exception as e:
        print(f"âš ï¸ æ¨¡æ¿åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æ¨¡æ¿: {e}")
        # å›é€€åˆ°é»˜è®¤æ¨¡æ¿
        return load_and_format_template("default_template.txt", context, query)


# ===================================================================
# æ¨¡å‹åŠ è½½å’Œç”Ÿæˆå™¨åŒ…è£…ç±»
# ===================================================================

class ModelLoader:
    """è´Ÿè´£åŠ è½½å’Œå¸è½½æ¨¡å‹ï¼Œå¹¶æä¾›ç”Ÿæˆæ¥å£"""
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"
        self.is_loaded = False

        if "Fin-R1" in model_name: 
            self.model_path = "/users/sgjfei3/data/huggingface/models--SUFE-AIFLM-Lab--Fin-R1/snapshots/026768c4a015b591b54b240743edeac1de0970fa" 
        elif "Qwen3-8B" in model_name:
            self.model_path = "Qwen/Qwen2.5-7B-Instruct"
        else:
            self.model_path = model_name 
            print(f"âš ï¸ æ¨¡å‹è·¯å¾„ '{model_name}' æœªçŸ¥ï¼Œå°è¯•ä»Hugging Face HubåŠ è½½ã€‚")

    def load_model(self):
        if self.is_loaded:
            print(f"âœ… {self.model_name} å·²åŠ è½½ï¼Œæ— éœ€é‡å¤åŠ è½½ã€‚")
            return
        
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹: {self.model_name} ä» {self.model_path}")
        is_local_path = isinstance(self.model_path, str) and "snapshots" in self.model_path

        tokenizer_args = {"trust_remote_code": True, "local_files_only": is_local_path}
        model_args = {"torch_dtype": torch.float16, "device_map": "auto", "trust_remote_code": True, 
                      "load_in_8bit": True, "local_files_only": is_local_path} 

        try:
            print("ğŸ”§ åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path, **tokenizer_args)
            if self.tokenizer.pad_token is None: 
                self.tokenizer.pad_token = self.tokenizer.eos_token
            if self.tokenizer.pad_token_id is None: 
                self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            print(f"âœ… {self.model_name} TokenizeråŠ è½½å®Œæˆ.")

            print("ğŸ”§ åŠ è½½æ¨¡å‹...")
            self.model = AutoModelForCausalLM.from_pretrained(self.model_path, **model_args)
            self.model.eval()
            print(f"âœ… {self.model_name} æ¨¡å‹åŠ è½½å®Œæˆ. è®¾å¤‡: {self.model.device}, é‡åŒ–: 8bit")
            self.is_loaded = True
        except Exception as e:
            print(f"âŒ {self.model_name} æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            self.unload_model()
            raise

    def unload_model(self):
        if not self.is_loaded:
            return
        
        print(f"ğŸ—‘ï¸ å¸è½½æ¨¡å‹: {self.model_name} å¹¶æ¸…ç†æ˜¾å­˜...")
        try:
            if self.model:
                self.model.to('cpu')
                del self.model
            if self.tokenizer:
                del self.tokenizer
            self.model = None
            self.tokenizer = None
            torch.cuda.empty_cache()
            gc.collect()
            self.is_loaded = False
            print(f"âœ… {self.model_name} æ˜¾å­˜å·²æ¸…ç†ã€‚")
        except Exception as e:
            print(f"âŒ å¸è½½ {self.model_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def generate(self, prompt_string: str, max_new_tokens: int = 150, do_sample: bool = False, repetition_penalty: float = 1.1) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        if not self.is_loaded or self.model is None or self.tokenizer is None:
            raise RuntimeError(f"æ¨¡å‹ {self.model_name} æœªåŠ è½½ã€‚è¯·å…ˆè°ƒç”¨ load_model()ã€‚")

        inputs = self.tokenizer(prompt_string, return_tensors="pt", padding=True, truncation=True).to(self.model.device)
        
        with torch.no_grad():
            outputs = self.model.generate( 
                **inputs,
                max_new_tokens=max_new_tokens,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=repetition_penalty
            )
        
        generated_tokens = outputs[0, inputs["input_ids"].shape[1]:]
        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True) 
        
        return generated_text


def _convert_messages_to_chatml(messages: List[Dict[str, str]]) -> str:
    """å°†messagesåˆ—è¡¨è½¬æ¢ä¸ºChatMLæ ¼å¼å­—ç¬¦ä¸²"""
    if not messages:
        return ""
    
    formatted_prompt = ""
    for message in messages:
        role = message.get("role", "")
        content = message.get("content", "")
        
        if role == "system":
            formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n"
        elif role == "user":
            formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n"
        elif role == "assistant":
            formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n"
    
    formatted_prompt += "<|im_start|>assistant\n" 
    
    return formatted_prompt


# ===================================================================
# ä¸»è¯„ä¼°é€»è¾‘
# ===================================================================

def run_english_comparison_test():
    print("ğŸš€ æ¨¡å‹å¯¹æ¯”æµ‹è¯•å¼€å§‹...")
    
    # é…ç½®è¦æµ‹è¯•çš„æ¨¡å‹
    model_loaders = {
        "Fin-R1": ModelLoader("Fin-R1"),
        "Qwen3-8B": ModelLoader("Qwen3-8B")
    }

    # æµ‹è¯•é…ç½®
    data_path = "evaluate_mrr/tatqa_eval_enhanced.jsonl"  # é»˜è®¤ä½¿ç”¨TatQAè‹±æ–‡æ•°æ®é›†
    sample_size = 500  # éšæœºé‡‡æ ·æ•°é‡
    
    # åŠ è½½æ•°æ®é›†
    print(f"ğŸ“Š åŠ è½½æ•°æ®é›†: {data_path}")
    try:
        dataset = []
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    dataset.append(json.loads(line))
        
        if sample_size > 0 and sample_size < len(dataset):
            import random
            random.seed(42)
            dataset = random.sample(dataset, sample_size)
            print(f"âœ… éšæœºé‡‡æ · {len(dataset)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
        else:
            print(f"âœ… åŠ è½½äº†å…¨éƒ¨ {len(dataset)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
            
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return

    all_results_data = []

    # é€ä¸ªæ¨¡å‹è¿›è¡Œè¯„ä¼°
    for model_name, loader in model_loaders.items():
        current_model_results = []
        total_f1_model = 0.0
        total_em_model = 0.0
        total_generation_time_model = 0.0
        
        try:
            loader.load_model()
            
            pbar = tqdm(dataset, desc=f"è¯„ä¼° {model_name}")
            for i, item in enumerate(pbar):
                # å…¼å®¹å¤šç§æŸ¥è¯¢å­—æ®µå
                query = item.get("query", "") or item.get("generated_question", "") or item.get("question", "")
                context_data = item.get("context", "")
                expected_answer = item.get("answer", "")
                doc_id = item.get("doc_id", f"sample_{i}")  # æ·»åŠ doc_idæ”¯æŒ

                # ä½¿ç”¨æ··åˆå†³ç­–ç®—æ³•è·å–prompt
                messages = get_final_prompt(context_data, query)
                
                # è½¬æ¢ä¸ºChatMLæ ¼å¼
                prompt_string_for_model = _convert_messages_to_chatml(messages)
                
                start_time = time.time()
                generated_text = loader.generate(
                    prompt_string=prompt_string_for_model,
                    max_new_tokens=150,
                    do_sample=False, 
                    repetition_penalty=1.1
                )
                generation_time = time.time() - start_time
                
                # ä½¿ç”¨æ™ºèƒ½ç­”æ¡ˆæå–
                final_answer = extract_final_answer_with_rescue(generated_text)
                
                f1 = calculate_f1_score(final_answer, expected_answer)
                em = calculate_exact_match(final_answer, expected_answer)

                total_f1_model += f1
                total_em_model += em
                total_generation_time_model += generation_time

                current_model_results.append({
                    "model": model_name,
                    "sample_id": i,
                    "doc_id": doc_id,
                    "query": query,
                    "expected_answer": expected_answer,
                    "raw_generated_text": generated_text,
                    "final_answer": final_answer,
                    "f1_score": f1,
                    "exact_match": em,
                    "generation_time": generation_time
                })

            # æ‰“å°å½“å‰æ¨¡å‹çš„æ±‡æ€»ç»“æœ
            num_samples_evaluated = len(dataset)
            avg_f1 = total_f1_model / num_samples_evaluated
            avg_em = total_em_model / num_samples_evaluated
            avg_gen_time = total_generation_time_model / num_samples_evaluated

            print(f"\n--- {model_name} è¯„ä¼°æ€»ç»“ ---")
            print(f"æ€»æ ·æœ¬æ•°: {num_samples_evaluated}")
            print(f"å¹³å‡ F1-score: {avg_f1:.4f}")
            print(f"å¹³å‡ Exact Match: {avg_em:.4f}")
            print(f"å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_gen_time:.2f} ç§’/æ ·æœ¬")
            print("--------------------")
            
            all_results_data.extend(current_model_results)

        except Exception as e:
            print(f"âŒ è¯„ä¼° {model_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            loader.unload_model()

    # è¯„ä¼°å®Œæˆï¼Œä¿å­˜æ‰€æœ‰ç»“æœ
    output_filename = f"tatqa_comparison_results_{os.path.basename(data_path).replace('.jsonl', '')}_{time.strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_filename, 'w', encoding='utf-8') as f:
        json.dump(all_results_data, f, ensure_ascii=False, indent=4)
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_filename}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="TatQAè‹±æ–‡æ¨¡å‹å¯¹æ¯”æµ‹è¯•è„šæœ¬")
    parser.add_argument("--data_path", type=str, default="evaluate_mrr/tatqa_eval_enhanced.jsonl", help="è¯„ä¼°æ•°æ®é›†æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--sample_size", type=int, default=500, help="éšæœºé‡‡æ ·çš„æ ·æœ¬æ•°é‡ (0è¡¨ç¤ºè¯„ä¼°å…¨éƒ¨)")
    parser.add_argument("--max_new_tokens", type=int, default=150, help="æ¨¡å‹ç”Ÿæˆæœ€å¤§æ–°Tokenæ•°")
    parser.add_argument("--do_sample", type=bool, default=False, help="æ˜¯å¦ä½¿ç”¨é‡‡æ ·ç”Ÿæˆ")
    parser.add_argument("--repetition_penalty", type=float, default=1.1, help="é‡å¤æƒ©ç½šç³»æ•°")
    
    args = parser.parse_args()
    run_english_comparison_test()
