#!/usr/bin/env python3
"""
æµ‹è¯•UIä¸­çš„é‡å¤contexté—®é¢˜
"""

import sys
from pathlib import Path
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

def test_knowledge_base_duplicates():
    """æµ‹è¯•çŸ¥è¯†åº“ä¸­çš„é‡å¤æƒ…å†µ"""
    print("=" * 80)
    print("æµ‹è¯•çŸ¥è¯†åº“é‡å¤æƒ…å†µ")
    print("=" * 80)
    
    # æ£€æŸ¥çŸ¥è¯†åº“æ–‡ä»¶
    knowledge_base_file = "data/unified/tatqa_knowledge_base_combined.jsonl"
    
    if not Path(knowledge_base_file).exists():
        print(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {knowledge_base_file}")
        return
    
    print(f"ğŸ“– æ£€æŸ¥çŸ¥è¯†åº“æ–‡ä»¶: {knowledge_base_file}")
    
    # è¯»å–çŸ¥è¯†åº“
    documents = []
    with open(knowledge_base_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                try:
                    doc = json.loads(line)
                    documents.append(doc)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
                    continue
    
    print(f"ğŸ“Š çŸ¥è¯†åº“æ€»æ–‡æ¡£æ•°: {len(documents)}")
    
    # æ£€æŸ¥contexté‡å¤
    context_groups = {}
    for doc in documents:
        context = doc.get('context', '')
        if context:
            normalized_context = ' '.join(context.split())
            if normalized_context not in context_groups:
                context_groups[normalized_context] = []
            context_groups[normalized_context].append(doc)
    
    # æ‰¾å‡ºé‡å¤çš„context
    duplicates = {context: docs for context, docs in context_groups.items() if len(docs) > 1}
    
    print(f"\nğŸ“‹ çŸ¥è¯†åº“é‡å¤ç»Ÿè®¡:")
    print(f"   - å”¯ä¸€contextæ•°é‡: {len(context_groups)}")
    print(f"   - æœ‰é‡å¤çš„contextæ•°é‡: {len(duplicates)}")
    print(f"   - é‡å¤æ–‡æ¡£æ€»æ•°: {sum(len(docs) for docs in duplicates.values())}")
    
    if duplicates:
        print(f"\nğŸ” çŸ¥è¯†åº“é‡å¤è¯¦æƒ…:")
        for i, (context, docs) in enumerate(list(duplicates.items())[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"\né‡å¤ç»„ {i+1} (å…±{len(docs)}ä¸ªæ–‡æ¡£):")
            print(f"Contexté¢„è§ˆ: {context[:100]}...")
            
            for j, doc in enumerate(docs):
                doc_id = doc.get('doc_id', '')
                source = doc.get('source', '')
                print(f"  {j+1}. {doc_id} ({source})")
    else:
        print("âœ… çŸ¥è¯†åº“ä¸­æ²¡æœ‰é‡å¤çš„context")

def test_retriever_duplicates():
    """æµ‹è¯•æ£€ç´¢å™¨è¿”å›çš„é‡å¤æƒ…å†µ"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ£€ç´¢å™¨é‡å¤æƒ…å†µ")
    print("=" * 80)
    
    try:
        from xlm.ui.optimized_rag_ui import OptimizedRagUI
        from config.parameters import Config
        
        # åˆå§‹åŒ–UI
        print("ğŸ”„ åˆå§‹åŒ–UI...")
        config = Config()
        ui = OptimizedRagUI(
            cache_dir=config.encoder.cache_dir,
            use_faiss=config.retriever.use_faiss,
            enable_reranker=config.reranker.enabled,
            use_existing_embedding_index=config.retriever.use_existing_embedding_index,
            max_alphafin_chunks=config.retriever.max_alphafin_chunks
        )
        print("âœ… UIåˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•è‹±æ–‡æŸ¥è¯¢
        test_questions = [
            "What is the revenue of Apple in 2023?",
            "What are the financial highlights?",
            "What is the net income?"
        ]
        
        for question in test_questions:
            print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {question}")
            
            # æ£€æµ‹è¯­è¨€
            try:
                from langdetect import detect
                language = detect(question)
                is_chinese = language.startswith('zh')
            except:
                is_chinese = any('\u4e00' <= char <= '\u9fff' for char in question)
            
            print(f"æ£€æµ‹è¯­è¨€: {'ä¸­æ–‡' if is_chinese else 'è‹±æ–‡'}")
            
            # è·å–æ£€ç´¢ç»“æœ
            if is_chinese:
                if hasattr(ui, 'chinese_retrieval_system') and ui.chinese_retrieval_system:
                    # ä¸­æ–‡æ£€ç´¢ç³»ç»Ÿè¿”å›çš„æ˜¯(doc_idx, score)å…ƒç»„
                    faiss_results = ui.chinese_retrieval_system.retrieve(question, top_k=10)
                    retrieved_documents = []
                    retriever_scores = []
                    for doc_idx, score in faiss_results:
                        original_doc = ui.chinese_retrieval_system.data[doc_idx]
                        chunks = ui.chinese_retrieval_system.doc_to_chunks_mapping.get(doc_idx, [])
                        if chunks:
                            from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata
                            doc = DocumentWithMetadata(
                                content=chunks[0],
                                metadata=DocumentMetadata(
                                    source=str(original_doc.get('company_name', '')),
                                    language="chinese",
                                    doc_id=str(original_doc.get('doc_id', str(doc_idx)))
                                )
                            )
                            retrieved_documents.append(doc)
                            retriever_scores.append(score)
                else:
                    print("âŒ ä¸­æ–‡æ£€ç´¢ç³»ç»Ÿæœªåˆå§‹åŒ–")
                    continue
            else:
                if hasattr(ui, 'retriever'):
                    # è‹±æ–‡ä½¿ç”¨BilingualRetrieverçš„retrieveæ–¹æ³•
                    result = ui.retriever.retrieve(
                        text=question,
                        top_k=10,
                        return_scores=True
                    )
                    if isinstance(result, tuple):
                        retrieved_documents, retriever_scores = result
                    else:
                        retrieved_documents = result
                        retriever_scores = [1.0] * len(result) if result else []
                else:
                    print("âŒ æ£€ç´¢å™¨æœªåˆå§‹åŒ–")
                    continue
            
            print(f"æ£€ç´¢åˆ° {len(retrieved_documents)} ä¸ªæ–‡æ¡£")
            
            # æ£€æŸ¥é‡å¤å†…å®¹
            content_hashes = set()
            duplicate_count = 0
            duplicate_contents = []
            
            for i, (doc, score) in enumerate(zip(retrieved_documents, retriever_scores)):
                if hasattr(doc, 'content'):
                    content = doc.content
                else:
                    content = str(doc)
                content_hash = hash(content)
                
                if content_hash in content_hashes:
                    duplicate_count += 1
                    duplicate_contents.append({
                        'index': i,
                        'score': score,
                        'content_preview': content[:100],
                        'content_hash': content_hash
                    })
                else:
                    content_hashes.add(content_hash)
            
            print(f"é‡å¤ç»Ÿè®¡:")
            print(f"   æ€»æ–‡æ¡£æ•°: {len(retrieved_documents)}")
            print(f"   å”¯ä¸€æ–‡æ¡£æ•°: {len(content_hashes)}")
            print(f"   é‡å¤æ–‡æ¡£æ•°: {duplicate_count}")
            if retrieved_documents:
                print(f"   é‡å¤ç‡: {duplicate_count/len(retrieved_documents)*100:.2f}%")
            else:
                print(f"   é‡å¤ç‡: 0.00%")
            
            if duplicate_contents:
                print(f"é‡å¤å†…å®¹è¯¦æƒ…:")
                for dup in duplicate_contents[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"   æ–‡æ¡£ {dup['index']+1}: åˆ†æ•°={dup['score']:.4f}, å†…å®¹={dup['content_preview']}...")
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_ui_processing_duplicates():
    """æµ‹è¯•UIå¤„ç†è¿‡ç¨‹ä¸­çš„é‡å¤æƒ…å†µ"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•UIå¤„ç†é‡å¤æƒ…å†µ")
    print("=" * 80)
    
    try:
        from xlm.ui.optimized_rag_ui import OptimizedRagUI
        from config.parameters import Config
        
        # åˆå§‹åŒ–UI
        print("ğŸ”„ åˆå§‹åŒ–UI...")
        config = Config()
        ui = OptimizedRagUI(
            cache_dir=config.encoder.cache_dir,
            use_faiss=config.retriever.use_faiss,
            enable_reranker=config.reranker.enabled,
            use_existing_embedding_index=config.retriever.use_existing_embedding_index,
            max_alphafin_chunks=config.retriever.max_alphafin_chunks
        )
        print("âœ… UIåˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æŸ¥è¯¢
        test_question = "What is the revenue of Apple in 2023?"
        print(f"\nğŸ” æµ‹è¯•æŸ¥è¯¢: {test_question}")
        
        # æ¨¡æ‹ŸUIå¤„ç†æµç¨‹
        try:
            # æ£€æµ‹è¯­è¨€
            from langdetect import detect
            language = detect(test_question)
            is_chinese = language.startswith('zh')
        except:
            is_chinese = any('\u4e00' <= char <= '\u9fff' for char in test_question)
        
        print(f"æ£€æµ‹è¯­è¨€: {'ä¸­æ–‡' if is_chinese else 'è‹±æ–‡'}")
        
        # è·å–æ£€ç´¢ç»“æœ
        if is_chinese:
            if hasattr(ui, 'chinese_retrieval_system') and ui.chinese_retrieval_system:
                faiss_results = ui.chinese_retrieval_system.retrieve(test_question, top_k=10)
                retrieved_documents = []
                retriever_scores = []
                for doc_idx, score in faiss_results:
                    original_doc = ui.chinese_retrieval_system.data[doc_idx]
                    chunks = ui.chinese_retrieval_system.doc_to_chunks_mapping.get(doc_idx, [])
                    if chunks:
                        from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata
                        doc = DocumentWithMetadata(
                            content=chunks[0],
                            metadata=DocumentMetadata(
                                source=str(original_doc.get('company_name', '')),
                                language="chinese",
                                doc_id=str(original_doc.get('doc_id', str(doc_idx)))
                            )
                        )
                        retrieved_documents.append(doc)
                        retriever_scores.append(score)
            else:
                print("âŒ ä¸­æ–‡æ£€ç´¢ç³»ç»Ÿæœªåˆå§‹åŒ–")
                return
        else:
            if hasattr(ui, 'retriever'):
                # è‹±æ–‡ä½¿ç”¨BilingualRetrieverçš„retrieveæ–¹æ³•
                result = ui.retriever.retrieve(
                    text=test_question,
                    top_k=10,
                    return_scores=True
                )
                if isinstance(result, tuple):
                    retrieved_documents, retriever_scores = result
                else:
                    retrieved_documents = result
                    retriever_scores = [1.0] * len(result) if result else []
            else:
                print("âŒ æ£€ç´¢å™¨æœªåˆå§‹åŒ–")
                return
        
        print(f"æ£€ç´¢åˆ° {len(retrieved_documents)} ä¸ªæ–‡æ¡£")
        
        # æ¨¡æ‹ŸUIå»é‡é€»è¾‘
        print("\nğŸ”„ æ¨¡æ‹ŸUIå»é‡é€»è¾‘...")
        unique_docs = []
        seen_hashes = set()
        
        for doc, score in zip(retrieved_documents, retriever_scores):
            if hasattr(doc, 'content'):
                content = doc.content
            else:
                content = str(doc)
            h = hash(content)
            if h not in seen_hashes:
                unique_docs.append((doc, score))
                seen_hashes.add(h)
            if len(unique_docs) >= ui.config.retriever.rerank_top_k:
                break
        
        print(f"UIå»é‡åæ–‡æ¡£æ•°: {len(unique_docs)}")
        print(f"UIå»é‡æ•ˆæœ: {len(retrieved_documents) - len(unique_docs)} ä¸ªé‡å¤è¢«ç§»é™¤")
        
        # æ£€æŸ¥UIæ–‡æ¡£å¤„ç†
        print("\nğŸ”„ æ£€æŸ¥UIæ–‡æ¡£å¤„ç†...")
        ui_docs = []
        seen_ui_hashes = set()
        seen_table_ids = set()
        seen_paragraph_ids = set()
        
        for doc, score in unique_docs:
            if getattr(doc.metadata, 'language', '') == 'chinese':
                doc_id = str(getattr(doc.metadata, 'origin_doc_id', '') or getattr(doc.metadata, 'doc_id', '')).strip()
                raw_context = ui.docid2context.get(doc_id, "")
                if not raw_context:
                    raw_context = doc.content
            else:
                raw_context = doc.content
            
            # æ£€æŸ¥å†…å®¹ç±»å‹å¹¶åº”ç”¨ç›¸åº”çš„å»é‡é€»è¾‘
            has_table_id = "Table ID:" in raw_context
            has_paragraph_id = "Paragraph ID:" in raw_context
            
            if has_table_id:
                import re
                table_id_match = re.search(r'Table ID:\s*([a-f0-9-]+)', raw_context)
                if table_id_match:
                    table_id = table_id_match.group(1)
                    if table_id in seen_table_ids:
                        print(f"è·³è¿‡é‡å¤çš„Table ID: {table_id}")
                        continue
                    seen_table_ids.add(table_id)
            elif has_paragraph_id:
                import re
                paragraph_id_match = re.search(r'Paragraph ID:\s*([a-f0-9-]+)', raw_context)
                if paragraph_id_match:
                    paragraph_id = paragraph_id_match.group(1)
                    if paragraph_id in seen_paragraph_ids:
                        print(f"è·³è¿‡é‡å¤çš„Paragraph ID: {paragraph_id}")
                        continue
                    seen_paragraph_ids.add(paragraph_id)
            
            # å¯¹raw_contextè¿›è¡Œå»é‡æ£€æŸ¥
            context_hash = hash(raw_context)
            if context_hash in seen_ui_hashes:
                print(f"è·³è¿‡é‡å¤çš„UIæ–‡æ¡£")
                continue
            
            seen_ui_hashes.add(context_hash)
            preview_content = raw_context[:200] + "..." if len(raw_context) > 200 else raw_context
            ui_docs.append((doc, score, preview_content, raw_context))
        
        print(f"UIå¤„ç†åçš„æ–‡æ¡£æ•°: {len(ui_docs)}")
        print(f"Table IDå»é‡: {len(seen_table_ids)} ä¸ªå”¯ä¸€Table ID")
        print(f"Paragraph IDå»é‡: {len(seen_paragraph_ids)} ä¸ªå”¯ä¸€Paragraph ID")
        print(f"Contextå»é‡: {len(seen_ui_hashes)} ä¸ªå”¯ä¸€Context")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # 1. æµ‹è¯•çŸ¥è¯†åº“é‡å¤
    test_knowledge_base_duplicates()
    
    # 2. æµ‹è¯•æ£€ç´¢å™¨é‡å¤
    test_retriever_duplicates()
    
    # 3. æµ‹è¯•UIå¤„ç†é‡å¤
    test_ui_processing_duplicates()
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•å®Œæˆ")
    print("=" * 80) 