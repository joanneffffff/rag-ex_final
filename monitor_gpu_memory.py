#!/usr/bin/env python3
"""
GPU å†…å­˜ç›‘æ§è„šæœ¬
å®æ—¶ç›‘æ§ GPU å†…å­˜ä½¿ç”¨æƒ…å†µ
"""

import torch
import time
import psutil
import os

def get_gpu_memory_info(device_id=1):
    """è·å– GPU å†…å­˜ä¿¡æ¯"""
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(device_id).total_memory
        allocated_memory = torch.cuda.memory_allocated(device_id)
        reserved_memory = torch.cuda.memory_reserved(device_id)
        free_memory = gpu_memory - allocated_memory
        
        return {
            'total': gpu_memory / 1024**3,
            'allocated': allocated_memory / 1024**3,
            'reserved': reserved_memory / 1024**3,
            'free': free_memory / 1024**3,
            'utilization': (allocated_memory / gpu_memory) * 100
        }
    return None

def get_process_memory_info():
    """è·å–å½“å‰è¿›ç¨‹å†…å­˜ä¿¡æ¯"""
    process = psutil.Process()
    memory_info = process.memory_info()
    
    return {
        'rss': memory_info.rss / 1024**3,  # GB
        'vms': memory_info.vms / 1024**3,  # GB
        'percent': process.memory_percent()
    }

def monitor_memory(interval=5, duration=60):
    """ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    print(f"ğŸ” å¼€å§‹ç›‘æ§ GPU å†…å­˜ (é—´éš”: {interval}ç§’, æŒç»­æ—¶é—´: {duration}ç§’)")
    print("=" * 80)
    
    start_time = time.time()
    iteration = 0
    
    while time.time() - start_time < duration:
        iteration += 1
        current_time = time.strftime("%H:%M:%S")
        
        print(f"\nâ° [{current_time}] ç¬¬ {iteration} æ¬¡æ£€æŸ¥:")
        
        # GPU å†…å­˜ä¿¡æ¯
        gpu_info = get_gpu_memory_info(1)
        if gpu_info:
            print(f"ğŸ“Š GPU 1 å†…å­˜:")
            print(f"   - æ€»å†…å­˜: {gpu_info['total']:.1f}GB")
            print(f"   - å·²åˆ†é…: {gpu_info['allocated']:.1f}GB")
            print(f"   - å·²ä¿ç•™: {gpu_info['reserved']:.1f}GB")
            print(f"   - å¯ç”¨: {gpu_info['free']:.1f}GB")
            print(f"   - åˆ©ç”¨ç‡: {gpu_info['utilization']:.1f}%")
        
        # è¿›ç¨‹å†…å­˜ä¿¡æ¯
        process_info = get_process_memory_info()
        print(f"ğŸ’» è¿›ç¨‹å†…å­˜:")
        print(f"   - RSS: {process_info['rss']:.1f}GB")
        print(f"   - VMS: {process_info['vms']:.1f}GB")
        print(f"   - å ç”¨ç‡: {process_info['percent']:.1f}%")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–è¿›ç¨‹ä½¿ç”¨ GPU
        try:
            import subprocess
            result = subprocess.run(['nvidia-smi', '--query-compute-apps=pid,used_memory', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True)
            if result.stdout.strip():
                print("ğŸ” å…¶ä»– GPU è¿›ç¨‹:")
                for line in result.stdout.strip().split('\n'):
                    if line.strip():
                        parts = line.split(', ')
                        if len(parts) == 2:
                            pid, memory = parts
                            print(f"   - PID {pid}: {memory} MiB")
        except:
            pass
        
        time.sleep(interval)
    
    print("\nğŸ ç›‘æ§å®Œæˆ")

def analyze_memory_fragmentation():
    """åˆ†æå†…å­˜ç¢ç‰‡åŒ–æƒ…å†µ"""
    print("ğŸ” åˆ†æ GPU å†…å­˜ç¢ç‰‡åŒ–...")
    
    if torch.cuda.is_available():
        # å°è¯•åˆ†é…ä¸åŒå¤§å°çš„å†…å­˜å—
        sizes = [64, 128, 256, 512, 1024, 2048]  # MB
        
        for size_mb in sizes:
            try:
                size_bytes = size_mb * 1024 * 1024
                tensor = torch.empty(size_bytes // 4, dtype=torch.float32, device='cuda:1')
                print(f"âœ… æˆåŠŸåˆ†é… {size_mb}MB å†…å­˜å—")
                del tensor
                torch.cuda.empty_cache()
            except RuntimeError as e:
                print(f"âŒ æ— æ³•åˆ†é… {size_mb}MB å†…å­˜å—: {e}")
                break

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ GPU å†…å­˜ç›‘æ§å·¥å…·")
    print("=" * 50)
    
    # æ£€æŸ¥ GPU å¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨")
        return
    
    print(f"âœ… æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ª GPU")
    
    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    print("\nğŸ“Š å½“å‰ GPU çŠ¶æ€:")
    gpu_info = get_gpu_memory_info(1)
    if gpu_info:
        print(f"GPU 1: {gpu_info['total']:.1f}GB æ€»å†…å­˜, {gpu_info['free']:.1f}GB å¯ç”¨")
    
    # åˆ†æå†…å­˜ç¢ç‰‡åŒ–
    print("\nğŸ” å†…å­˜ç¢ç‰‡åŒ–åˆ†æ:")
    analyze_memory_fragmentation()
    
    # å¼€å§‹ç›‘æ§
    print("\n" + "=" * 50)
    monitor_memory(interval=3, duration=30)

if __name__ == "__main__":
    main() 