#!/usr/bin/env python3
"""æµ‹è¯•ç»Ÿä¸€ç¼–ç çš„æ•ˆæœ"""

import json
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent))

def test_unified_encoding():
    """æµ‹è¯•ç»Ÿä¸€ç¼–ç çš„æ•ˆæœ"""
    print("=== æµ‹è¯•ç»Ÿä¸€ç¼–ç æ•ˆæœ ===")
    
    try:
        from config.parameters import Config
        from xlm.components.retriever.bilingual_retriever import BilingualRetriever
        from xlm.components.encoder.finbert import FinbertEncoder
        from xlm.utils.optimized_data_loader import OptimizedDataLoader
        
        config = Config()
        
        print("1. åŠ è½½ç¼–ç å™¨...")
        encoder_ch = FinbertEncoder(
            model_name="models/finetuned_alphafin_zh",
            cache_dir=config.encoder.cache_dir,
        )
        encoder_en = FinbertEncoder(
            model_name="models/finetuned_finbert_tatqa",
            cache_dir=config.encoder.cache_dir,
        )
        print("   âœ… ç¼–ç å™¨åŠ è½½æˆåŠŸ")
        
        print("\n2. åŠ è½½åŒ…å«è¯„ä¼°æ•°æ®çš„çŸ¥è¯†åº“...")
        data_loader = OptimizedDataLoader(
            data_dir="data",
            max_samples=100,  # åªåŠ è½½100ä¸ªæ ·æœ¬
            chinese_document_level=True,
            english_chunk_level=True,
            include_eval_data=True  # åŒ…å«è¯„ä¼°æ•°æ®
        )
        
        chinese_chunks = data_loader.chinese_docs
        english_chunks = data_loader.english_docs
        
        print(f"   âœ… çŸ¥è¯†åº“åŠ è½½æˆåŠŸ:")
        print(f"      ä¸­æ–‡chunks: {len(chinese_chunks)}")
        print(f"      è‹±æ–‡chunks: {len(english_chunks)}")
        
        print("\n3. åˆ›å»ºæ£€ç´¢å™¨ï¼ˆç»Ÿä¸€ç¼–ç ï¼‰...")
        retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,
            corpus_documents_en=english_chunks,
            corpus_documents_ch=chinese_chunks,
            use_faiss=True,
            use_gpu=False,
            batch_size=8,
            cache_dir=config.encoder.cache_dir
        )
        print("   âœ… æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        
        print("\n4. æµ‹è¯•ä¸­æ–‡æ£€ç´¢...")
        # åŠ è½½è¯„ä¼°æ•°æ®ç”¨äºæµ‹è¯•
        def load_eval_data(eval_file: str):
            data = []
            with open(eval_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data.append(json.loads(line))
            return data
        
        alphafin_eval = load_eval_data("evaluate_mrr/alphafin_eval.jsonl")
        test_sample = alphafin_eval[0]
        
        query = test_sample['query']
        context = test_sample['context']
        doc_id = test_sample['doc_id']
        
        print(f"   æŸ¥è¯¢: {query}")
        print(f"   æ­£ç¡®ç­”æ¡ˆID: {doc_id}")
        
        # æ£€ç´¢
        retrieved_result = retriever.retrieve(
            text=query, 
            top_k=20, 
            return_scores=True, 
            language='zh'
        )
        
        if isinstance(retrieved_result, tuple):
            retrieved_docs, scores = retrieved_result
        else:
            retrieved_docs = retrieved_result
            scores = []
        
        print(f"   æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªæ–‡æ¡£")
        
        # ä½¿ç”¨æ”¹è¿›çš„åŒ¹é…é€»è¾‘
        from test_retrieval_mrr import find_correct_document_rank
        
        found_rank = find_correct_document_rank(
            context=context,
            retrieved_docs=retrieved_docs,
            sample=test_sample,
            encoder=encoder_ch
        )
        
        print(f"   æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å: {found_rank}")
        
        if found_rank > 0:
            print(f"   âœ… æˆåŠŸæ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆï¼")
            matched_doc = retrieved_docs[found_rank-1]
            print(f"   åŒ¹é…æ–‡æ¡£å†…å®¹: {matched_doc.content[:200]}...")
            
            # æ˜¾ç¤ºåˆ†æ•°
            if scores and found_rank <= len(scores):
                print(f"   åŒ¹é…æ–‡æ¡£åˆ†æ•°: {scores[found_rank-1]:.4f}")
        else:
            print(f"   âŒ æœªæ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆ")
            print(f"   å‰3ä¸ªæ£€ç´¢ç»“æœ:")
            for i, doc in enumerate(retrieved_docs[:3]):
                score_info = f" (åˆ†æ•°: {scores[i]:.4f})" if scores and i < len(scores) else ""
                print(f"     {i+1}. {doc.content[:100]}...{score_info}")
        
        print("\n5. æµ‹è¯•è‹±æ–‡æ£€ç´¢...")
        tatqa_eval = load_eval_data("evaluate_mrr/tatqa_eval.jsonl")
        test_sample_en = tatqa_eval[0]
        
        query_en = test_sample_en['query']
        context_en = test_sample_en['context']
        
        print(f"   æŸ¥è¯¢: {query_en}")
        
        # æ£€ç´¢
        retrieved_result_en = retriever.retrieve(
            text=query_en, 
            top_k=20, 
            return_scores=True, 
            language='en'
        )
        
        if isinstance(retrieved_result_en, tuple):
            retrieved_docs_en, scores_en = retrieved_result_en
        else:
            retrieved_docs_en = retrieved_result_en
            scores_en = []
        
        print(f"   æ£€ç´¢åˆ° {len(retrieved_docs_en)} ä¸ªæ–‡æ¡£")
        
        found_rank_en = find_correct_document_rank(
            context=context_en,
            retrieved_docs=retrieved_docs_en,
            sample=test_sample_en,
            encoder=encoder_en
        )
        
        print(f"   æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å: {found_rank_en}")
        
        if found_rank_en > 0:
            print(f"   âœ… æˆåŠŸæ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆï¼")
            if scores_en and found_rank_en <= len(scores_en):
                print(f"   åŒ¹é…æ–‡æ¡£åˆ†æ•°: {scores_en[found_rank_en-1]:.4f}")
        else:
            print(f"   âŒ æœªæ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆ")
        
        print("\nğŸ‰ ç»Ÿä¸€ç¼–ç æµ‹è¯•å®Œæˆï¼")
        
        # æ€»ç»“ç»“æœ
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœæ€»ç»“:")
        print(f"   ä¸­æ–‡æ£€ç´¢: {'âœ… æˆåŠŸ' if found_rank > 0 else 'âŒ å¤±è´¥'} (æ’å: {found_rank})")
        print(f"   è‹±æ–‡æ£€ç´¢: {'âœ… æˆåŠŸ' if found_rank_en > 0 else 'âŒ å¤±è´¥'} (æ’å: {found_rank_en})")
        
        # ä¼˜åŠ¿åˆ†æ
        print(f"\nğŸš€ ç»Ÿä¸€ç¼–ç çš„ä¼˜åŠ¿:")
        print(f"   âœ… è®­ç»ƒæ•°æ®å’Œè¯„ä¼°æ•°æ®ä¸€èµ·ç¼–ç ï¼Œé¿å…é‡å¤è®¡ç®—")
        print(f"   âœ… æ‰€æœ‰æ•°æ®åœ¨åŒä¸€ä¸ªå‘é‡ç©ºé—´ä¸­ï¼Œæ£€ç´¢æ›´å‡†ç¡®")
        print(f"   âœ… ç®€åŒ–äº†æ•°æ®åŠ è½½æµç¨‹ï¼Œå‡å°‘ä»£ç å¤æ‚åº¦")
        print(f"   âœ… é¿å…äº†æ•°æ®ä¸ä¸€è‡´çš„é—®é¢˜")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_unified_encoding() 