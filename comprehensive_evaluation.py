#!/usr/bin/env python3
"""
å…¨é¢è¯„ä¼°è„šæœ¬ - åœ¨æ›´å¤§æ•°æ®é›†ä¸Šè¿›è¡ŒMRRå’Œç”Ÿæˆè´¨é‡è¯„ä¼°
éªŒè¯è‹±æ–‡Promptæµç¨‹çš„æ³›åŒ–æ€§
"""

# ä¸´æ—¶å…³é—­warningsï¼Œé¿å…transformerså‚æ•°è­¦å‘Š
import warnings
warnings.filterwarnings("ignore")

# æ›´ç²¾ç¡®åœ°è¿‡æ»¤transformersç”Ÿæˆå‚æ•°è­¦å‘Š
import logging
logging.getLogger("transformers.generation.utils").setLevel(logging.ERROR)

# å‡å°‘å…¶ä»–æ¨¡å—çš„æ—¥å¿—è¾“å‡º
logging.getLogger("transformers").setLevel(logging.WARNING)
logging.getLogger("torch").setLevel(logging.WARNING)
logging.getLogger("xlm").setLevel(logging.WARNING)

# è®¾ç½®ç¯å¢ƒå˜é‡å‡å°‘transformersçš„è¯¦ç»†è¾“å‡º
import os
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

import json
import time
import torch
import numpy as np
from typing import List, Dict, Any, Optional
from pathlib import Path
import sys
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGenerator
try:
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
    print("âœ… ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator")
except ImportError:
    USE_RAG_GENERATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGeneratorï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from transformers.utils.quantization_config import BitsAndBytesConfig

from test_english_template import LLMTemplateTester, get_final_optimized_english_prompt_messages

class ComprehensiveEvaluator:
    """å…¨é¢è¯„ä¼°å™¨"""
    
    def __init__(self, model_name: str = "SUFE-AIFLM-Lab/Fin-R1", device: str = "auto"):
        self.tester = LLMTemplateTester(model_name, device)
        self.tester.load_model()
        
    def load_evaluation_data(self, sample_size: int = 500) -> List[Dict[str, Any]]:
        """åŠ è½½è¯„ä¼°æ•°æ®"""
        # åŠ è½½å¢å¼ºç‰ˆè¯„ä¼°æ•°æ®
        eval_data = []
        with open('evaluate_mrr/tatqa_eval_enhanced.jsonl', 'r') as f:
            for line in tqdm(f, desc="ğŸ“– è¯»å–æ•°æ®æ–‡ä»¶", unit="è¡Œ"):
                eval_data.append(json.loads(line))
        
        # éšæœºé‡‡æ ·æŒ‡å®šæ•°é‡çš„æ ·æœ¬
        if len(eval_data) > sample_size:
            np.random.seed(42)  # ç¡®ä¿å¯é‡ç°æ€§
            eval_data = np.random.choice(eval_data, sample_size, replace=False).tolist()
        
        return eval_data
    
    def evaluate_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        try:
            # æ„å»ºPrompt
            messages = get_final_optimized_english_prompt_messages(
                context_content=sample["context"],
                question_text=sample["query"],
                summary_content=sample["context"]
            )
            
            # ç”Ÿæˆå›ç­”
            generation_result = self.tester.generate_response(messages)
            
            # è¯„ä¼°è´¨é‡
            evaluation = self.tester.evaluate_answer_quality(
                generated_answer=generation_result["cleaned_answer"],
                expected_answer=sample["answer"],
                context=sample["context"],
                question=sample["query"],
                raw_answer=generation_result["generated_answer"]  # ä¼ é€’åŸå§‹æœªæ¸…ç†çš„ç­”æ¡ˆ
            )
            
            return {
                "sample_id": sample.get("id", "unknown"),
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "context_type": "table" if "Table ID:" in sample["context"] else "paragraph",
                "generation": generation_result,
                "evaluation": evaluation,
                "success": evaluation["exact_match"] or evaluation["contains_expected"]
            }
            
        except Exception as e:
            # é™é»˜å¤„ç†é”™è¯¯ï¼Œä¸æ‰“å°è¯¦ç»†æ—¥å¿—
            return {
                "sample_id": sample.get("id", "unknown"),
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "error": str(e),
                "success": False
            }
    
    def run_comprehensive_evaluation(self, sample_size: int = 100) -> Dict[str, Any]:
        """è¿è¡Œå…¨é¢è¯„ä¼°"""
        # åŠ è½½æ•°æ®
        eval_data = self.load_evaluation_data(sample_size)
        
        # è¯„ä¼°æ‰€æœ‰æ ·æœ¬
        results = []
        start_time = time.time()
        
        # ä½¿ç”¨tqdmè¿›åº¦æ¡
        for sample in tqdm(eval_data, desc="ğŸ” è¯„ä¼°æ ·æœ¬", unit="æ ·æœ¬", 
                          bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]"):
            result = self.evaluate_single_sample(sample)
            results.append(result)
        
        total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        analysis = self.analyze_results(results, total_time)
        
        return {
            "results": results,
            "analysis": analysis,
            "timestamp": time.time()
        }
    
    def analyze_results(self, results: List[Dict[str, Any]], total_time: float) -> Dict[str, Any]:
        """åˆ†æè¯„ä¼°ç»“æœ"""
        # åŸºç¡€ç»Ÿè®¡
        total_samples = len(results)
        successful_samples = sum(1 for r in results if r.get("success", False))
        failed_samples = total_samples - successful_samples
        
        # è´¨é‡æŒ‡æ ‡
        quality_scores = [r.get("evaluation", {}).get("quality_score", 0) for r in results]
        exact_matches = sum(1 for r in results if r.get("evaluation", {}).get("exact_match", False))
        contains_expected = sum(1 for r in results if r.get("evaluation", {}).get("contains_expected", False))
        semantic_similarities = [r.get("evaluation", {}).get("semantic_similarity", 0) for r in results]
        
        # ç”Ÿæˆæ—¶é—´ç»Ÿè®¡
        generation_times = [r.get("generation", {}).get("generation_time", 0) for r in results]
        
        # æŒ‰ä¸Šä¸‹æ–‡ç±»å‹åˆ†ç»„
        table_results = [r for r in results if r.get("context_type") == "table"]
        paragraph_results = [r for r in results if r.get("context_type") == "paragraph"]
        
        # è®¡ç®—MRR (Mean Reciprocal Rank) - è¿™é‡Œç®€åŒ–ä¸ºç²¾ç¡®åŒ¹é…ç‡
        mrr = exact_matches / total_samples if total_samples > 0 else 0
        
        analysis = {
            "overall_metrics": {
                "total_samples": total_samples,
                "successful_samples": successful_samples,
                "failed_samples": failed_samples,
                "success_rate": successful_samples / total_samples if total_samples > 0 else 0,
                "exact_match_rate": exact_matches / total_samples if total_samples > 0 else 0,
                "contains_expected_rate": contains_expected / total_samples if total_samples > 0 else 0,
                "mrr": mrr,
                "avg_quality_score": np.mean(quality_scores) if quality_scores else 0,
                "avg_semantic_similarity": np.mean(semantic_similarities) if semantic_similarities else 0,
                "avg_generation_time": np.mean(generation_times) if generation_times else 0,
                "total_evaluation_time": total_time
            },
            "context_type_analysis": {
                "table_samples": {
                    "count": len(table_results),
                    "success_rate": sum(1 for r in table_results if r.get("success", False)) / len(table_results) if table_results else 0,
                    "exact_match_rate": sum(1 for r in table_results if r.get("evaluation", {}).get("exact_match", False)) / len(table_results) if table_results else 0,
                    "avg_quality_score": np.mean([r.get("evaluation", {}).get("quality_score", 0) for r in table_results]) if table_results else 0
                },
                "paragraph_samples": {
                    "count": len(paragraph_results),
                    "success_rate": sum(1 for r in paragraph_results if r.get("success", False)) / len(paragraph_results) if paragraph_results else 0,
                    "exact_match_rate": sum(1 for r in paragraph_results if r.get("evaluation", {}).get("exact_match", False)) / len(paragraph_results) if paragraph_results else 0,
                    "avg_quality_score": np.mean([r.get("evaluation", {}).get("quality_score", 0) for r in paragraph_results]) if paragraph_results else 0
                }
            },
            "quality_distribution": {
                "excellent_quality": sum(1 for score in quality_scores if score >= 0.8),
                "good_quality": sum(1 for score in quality_scores if 0.6 <= score < 0.8),
                "fair_quality": sum(1 for score in quality_scores if 0.4 <= score < 0.6),
                "poor_quality": sum(1 for score in quality_scores if score < 0.4)
            },
            "performance_insights": []
        }
        
        # ç”Ÿæˆæ€§èƒ½æ´å¯Ÿ
        if analysis["overall_metrics"]["success_rate"] >= 0.8:
            analysis["performance_insights"].append("ğŸ‰ æ•´ä½“è¡¨ç°ä¼˜ç§€ï¼ŒæˆåŠŸç‡è¾¾åˆ°80%ä»¥ä¸Š")
        elif analysis["overall_metrics"]["success_rate"] >= 0.6:
            analysis["performance_insights"].append("âœ… æ•´ä½“è¡¨ç°è‰¯å¥½ï¼ŒæˆåŠŸç‡è¾¾åˆ°60%ä»¥ä¸Š")
        else:
            analysis["performance_insights"].append("âš ï¸ æ•´ä½“è¡¨ç°éœ€è¦æ”¹è¿›")
        
        if analysis["overall_metrics"]["exact_match_rate"] >= 0.7:
            analysis["performance_insights"].append("ğŸ¯ ç²¾ç¡®åŒ¹é…ç‡å¾ˆé«˜ï¼Œæ¨¡å‹è¾“å‡ºè´¨é‡ä¼˜ç§€")
        
        if analysis["context_type_analysis"]["table_samples"]["success_rate"] > analysis["context_type_analysis"]["paragraph_samples"]["success_rate"]:
            analysis["performance_insights"].append("ğŸ“Š è¡¨æ ¼æ•°æ®è¡¨ç°ä¼˜äºæ®µè½æ•°æ®")
        else:
            analysis["performance_insights"].append("ğŸ“ æ®µè½æ•°æ®è¡¨ç°ä¼˜äºè¡¨æ ¼æ•°æ®")
        
        return analysis
    
    def print_analysis_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("\n" + "="*80)
        print("ğŸ“Š å…¨é¢è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*80)
        
        metrics = analysis["overall_metrics"]
        print(f"ğŸ“ˆ æ•´ä½“æŒ‡æ ‡:")
        print(f"   æ€»æ ·æœ¬æ•°: {metrics['total_samples']}")
        print(f"   æˆåŠŸæ ·æœ¬æ•°: {metrics['successful_samples']}")
        print(f"   æˆåŠŸç‡: {metrics['success_rate']:.3f} ({metrics['success_rate']*100:.1f}%)")
        print(f"   ç²¾ç¡®åŒ¹é…ç‡: {metrics['exact_match_rate']:.3f} ({metrics['exact_match_rate']*100:.1f}%)")
        print(f"   åŒ…å«æœŸæœ›ç­”æ¡ˆç‡: {metrics['contains_expected_rate']:.3f} ({metrics['contains_expected_rate']*100:.1f}%)")
        print(f"   MRR: {metrics['mrr']:.3f}")
        print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {metrics['avg_quality_score']:.3f}")
        print(f"   å¹³å‡è¯­ä¹‰ç›¸ä¼¼åº¦: {metrics['avg_semantic_similarity']:.3f}")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {metrics['avg_generation_time']:.2f}s")
        print(f"   æ€»è¯„ä¼°æ—¶é—´: {metrics['total_evaluation_time']:.2f}s")
        
        print(f"\nğŸ“Š ä¸Šä¸‹æ–‡ç±»å‹åˆ†æ:")
        table_analysis = analysis["context_type_analysis"]["table_samples"]
        paragraph_analysis = analysis["context_type_analysis"]["paragraph_samples"]
        print(f"   è¡¨æ ¼æ•°æ® ({table_analysis['count']} æ ·æœ¬):")
        print(f"     æˆåŠŸç‡: {table_analysis['success_rate']:.3f} ({table_analysis['success_rate']*100:.1f}%)")
        print(f"     ç²¾ç¡®åŒ¹é…ç‡: {table_analysis['exact_match_rate']:.3f} ({table_analysis['exact_match_rate']*100:.1f}%)")
        print(f"     å¹³å‡è´¨é‡åˆ†æ•°: {table_analysis['avg_quality_score']:.3f}")
        print(f"   æ®µè½æ•°æ® ({paragraph_analysis['count']} æ ·æœ¬):")
        print(f"     æˆåŠŸç‡: {paragraph_analysis['success_rate']:.3f} ({paragraph_analysis['success_rate']*100:.1f}%)")
        print(f"     ç²¾ç¡®åŒ¹é…ç‡: {paragraph_analysis['exact_match_rate']:.3f} ({paragraph_analysis['exact_match_rate']*100:.1f}%)")
        print(f"     å¹³å‡è´¨é‡åˆ†æ•°: {paragraph_analysis['avg_quality_score']:.3f}")
        
        print(f"\nğŸ“ˆ è´¨é‡åˆ†å¸ƒ:")
        quality_dist = analysis["quality_distribution"]
        total = sum(quality_dist.values())
        print(f"   ä¼˜ç§€è´¨é‡ (â‰¥0.8): {quality_dist['excellent_quality']} ({quality_dist['excellent_quality']/total*100:.1f}%)")
        print(f"   è‰¯å¥½è´¨é‡ (0.6-0.8): {quality_dist['good_quality']} ({quality_dist['good_quality']/total*100:.1f}%)")
        print(f"   ä¸€èˆ¬è´¨é‡ (0.4-0.6): {quality_dist['fair_quality']} ({quality_dist['fair_quality']/total*100:.1f}%)")
        print(f"   è¾ƒå·®è´¨é‡ (<0.4): {quality_dist['poor_quality']} ({quality_dist['poor_quality']/total*100:.1f}%)")
        
        print(f"\nğŸ’¡ æ€§èƒ½æ´å¯Ÿ:")
        for insight in analysis["performance_insights"]:
            print(f"   {insight}")
        
        print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ComprehensiveEvaluator()
    
    # è¿è¡Œè¯„ä¼° (å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ ·æœ¬æ•°)
    sample_sizes = [100, 500]  # å…ˆæµ‹è¯•100ä¸ªï¼Œå†æµ‹è¯•500ä¸ª
    
    for sample_size in tqdm(sample_sizes, desc="ğŸ“Š è¯„ä¼°ä¸åŒæ ·æœ¬æ•°", unit="æ ·æœ¬æ•°"):
        print(f"\n{'='*60}")
        print(f"ğŸ“Š è¯„ä¼°æ ·æœ¬æ•°: {sample_size}")
        print(f"{'='*60}")
        
        # è¿è¡Œè¯„ä¼°
        evaluation_results = evaluator.run_comprehensive_evaluation(sample_size)
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_analysis_summary(evaluation_results["analysis"])
        
        # ä¿å­˜ç»“æœ
        output_file = f"comprehensive_evaluation_{sample_size}_samples.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(evaluation_results, f, indent=2, ensure_ascii=False)
        print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    print("\nğŸ‰ å…¨é¢è¯„ä¼°å®Œæˆï¼")

if __name__ == "__main__":
    main() 