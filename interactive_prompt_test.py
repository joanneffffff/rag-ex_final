#!/usr/bin/env python3
"""
äº¤äº’å¼ Prompt æµ‹è¯•è„šæœ¬
è®©ç”¨æˆ·é€‰æ‹©ç‰¹å®šçš„ Prompt å˜ä½“å’Œå‚æ•°è¿›è¡Œæµ‹è¯•
"""

import sys
import os
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent))

def show_menu(title, options):
    """æ˜¾ç¤ºèœå•"""
    print(f"\n{title}")
    print("-" * 40)
    for i, (key, value) in enumerate(options.items(), 1):
        if isinstance(value, dict) and 'description' in value:
            print(f"{i}. {key}: {value['description']}")
        else:
            print(f"{i}. {key}")
    print("0. é€€å‡º")
    print("-" * 40)

def get_user_choice(options):
    """è·å–ç”¨æˆ·é€‰æ‹©"""
    while True:
        try:
            choice = input("è¯·é€‰æ‹© (è¾“å…¥æ•°å­—): ").strip()
            if choice == "0":
                return None
            choice_num = int(choice)
            if 1 <= choice_num <= len(options):
                return list(options.keys())[choice_num - 1]
            else:
                print(f"è¯·è¾“å…¥ 1-{len(options)} ä¹‹é—´çš„æ•°å­—")
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—")
        except KeyboardInterrupt:
            print("\né€€å‡ºç¨‹åº")
            return None

def test_specific_combination():
    """æµ‹è¯•ç‰¹å®šçš„ Prompt å’Œå‚æ•°ç»„åˆ"""
    
    print("=== äº¤äº’å¼ Prompt æµ‹è¯• ===")
    print("æµ‹è¯•é—®é¢˜ï¼šå¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰2021å¹´åˆ©æ¶¦æŒç»­å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ")
    print("=" * 60)
    
    try:
        from xlm.components.generator.local_llm_generator import LocalLLMGenerator
        from prompt_variations_library import get_prompt_variations, get_parameter_variations, get_test_scenarios
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        print("1. åˆå§‹åŒ– LLM ç”Ÿæˆå™¨...")
        generator = LocalLLMGenerator()
        print(f"âœ… ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ: {generator.model_name}")
        
        # è·å–æµ‹è¯•æ•°æ®
        scenarios = get_test_scenarios()
        prompt_variations = get_prompt_variations("", "", "")
        parameter_variations = get_parameter_variations()
        
        # é€‰æ‹©æµ‹è¯•åœºæ™¯
        show_menu("é€‰æ‹©æµ‹è¯•åœºæ™¯", scenarios)
        scenario_choice = get_user_choice(scenarios)
        if scenario_choice is None:
            return
        
        scenario = scenarios[scenario_choice]
        print(f"\nâœ… é€‰æ‹©çš„åœºæ™¯: {scenario_choice}")
        print(f"é—®é¢˜: {scenario['query']}")
        
        # é€‰æ‹© Prompt å˜ä½“
        show_menu("é€‰æ‹© Prompt å˜ä½“", prompt_variations)
        prompt_choice = get_user_choice(prompt_variations)
        if prompt_choice is None:
            return
        
        print(f"\nâœ… é€‰æ‹©çš„ Prompt: {prompt_choice}")
        
        # é€‰æ‹©å‚æ•°ç»„åˆ
        show_menu("é€‰æ‹©å‚æ•°ç»„åˆ", parameter_variations)
        param_choice = get_user_choice(parameter_variations)
        if param_choice is None:
            return
        
        print(f"\nâœ… é€‰æ‹©çš„å‚æ•°: {param_choice}")
        
        # ç”Ÿæˆ Prompt
        prompt = prompt_variations[prompt_choice].format(
            context=scenario['context'],
            summary=scenario['summary'],
            query=scenario['query']
        )
        
        # è·å–å‚æ•°
        params = parameter_variations[param_choice]
        
        print(f"\n2. æµ‹è¯•é…ç½®:")
        print(f"   Prompt: {prompt_choice}")
        print(f"   å‚æ•°: {param_choice}")
        print(f"   Temperature: {params['temperature']}")
        print(f"   Top-p: {params['top_p']}")
        print(f"   Max tokens: {params['max_new_tokens']}")
        print(f"   Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        # æ˜¾ç¤º Prompt é¢„è§ˆ
        print(f"\n3. Prompt é¢„è§ˆ:")
        print("-" * 50)
        print(prompt[:500] + "..." if len(prompt) > 500 else prompt)
        print("-" * 50)
        
        # ç¡®è®¤æ˜¯å¦ç»§ç»­
        confirm = input("\næ˜¯å¦å¼€å§‹æµ‹è¯•ï¼Ÿ(y/n): ").strip().lower()
        if confirm != 'y':
            print("æµ‹è¯•å·²å–æ¶ˆ")
            return
        
        # ä¸´æ—¶ä¿®æ”¹å‚æ•°
        original_temp = generator.temperature
        original_top_p = generator.top_p
        original_max_tokens = generator.max_new_tokens
        
        try:
            generator.temperature = params["temperature"]
            generator.top_p = params["top_p"]
            generator.max_new_tokens = params["max_new_tokens"]
            
            # ç”Ÿæˆå“åº”
            print(f"\n4. ç”Ÿæˆå“åº”...")
            print("ğŸš€ å¼€å§‹ç”Ÿæˆï¼Œè¯·ç¨å€™...")
            
            import time
            start_time = time.time()
            responses = generator.generate([prompt])
            end_time = time.time()
            
            response = responses[0] if responses else "ç”Ÿæˆå¤±è´¥"
            generation_time = end_time - start_time
            
            print(f"\n5. ç”Ÿæˆç»“æœ:")
            print("=" * 60)
            print(f"é—®é¢˜: {scenario['query']}")
            print(f"ç­”æ¡ˆ: {response}")
            print("=" * 60)
            print(f"ç”Ÿæˆæ—¶é—´: {generation_time:.2f}ç§’")
            
            # è¯„ä¼°ç»“æœ
            print(f"\n6. è´¨é‡è¯„ä¼°:")
            length = len(response.strip())
            print(f"   å“åº”é•¿åº¦: {length} å­—ç¬¦")
            
            # æ ¹æ®åœºæ™¯è¯„ä¼°å‡†ç¡®æ€§
            if "å¾·èµ›ç”µæ± " in scenario_choice:
                key_terms = ["å¾·èµ›ç”µæ± ", "iPhone", "éœ€æ±‚", "å¢é•¿", "åˆ©æ¶¦", "ä¸šç»©"]
            elif "ç”¨å‹ç½‘ç»œ" in scenario_choice:
                key_terms = ["ç”¨å‹ç½‘ç»œ", "ç°é‡‘æµ", "0.85", "å¢é•¿", "12.5"]
            elif "é¦–é’¢è‚¡ä»½" in scenario_choice:
                key_terms = ["é¦–é’¢è‚¡ä»½", "ä¸šç»©", "ä¸‹é™", "ç–«æƒ…", "é™æœ¬å¢æ•ˆ"]
            else:
                key_terms = []
            
            found_terms = [term for term in key_terms if term in response]
            print(f"   å…³é”®ä¿¡æ¯: {found_terms}")
            print(f"   å‡†ç¡®æ€§: {'âœ…' if len(found_terms) >= 2 else 'âŒ'} (æ‰¾åˆ°{len(found_terms)}ä¸ªå…³é”®è¯)")
            
            unwanted_patterns = ["ã€", "ã€‘", "å›ç­”ï¼š", "Answer:", "---", "===", "___"]
            has_unwanted = any(pattern in response for pattern in unwanted_patterns)
            print(f"   çº¯ç²¹æ€§: {'âœ…' if not has_unwanted else 'âŒ'} (æ— æ ¼å¼æ ‡è®°)")
            
            is_complete = response.strip().endswith(("ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"))
            print(f"   å®Œæ•´æ€§: {'âœ…' if is_complete else 'âŒ'} (å¥å­å®Œæ•´)")
            
            # æ€»ä½“è¯„åˆ†
            score = 0
            if 30 <= length <= 300: score += 25
            if len(found_terms) >= 2: score += 25
            if not has_unwanted: score += 25
            if is_complete: score += 25
            
            print(f"\nğŸ¯ æ€»ä½“è¯„åˆ†: {score}/100 ({score}%)")
            
            if score >= 75:
                print("ğŸ‰ æ•ˆæœå¾ˆå¥½ï¼")
            elif score >= 50:
                print("âš ï¸ æ•ˆæœä¸€èˆ¬ï¼Œå¯ä»¥ç»§ç»­ä¼˜åŒ–")
            else:
                print("âŒ æ•ˆæœä¸ä½³ï¼Œéœ€è¦é‡æ–°è®¾è®¡")
            
            # ä¿å­˜ç»“æœ
            save_result = input("\næ˜¯å¦ä¿å­˜æµ‹è¯•ç»“æœï¼Ÿ(y/n): ").strip().lower()
            if save_result == 'y':
                import json
                result = {
                    "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                    "scenario": scenario_choice,
                    "prompt": prompt_choice,
                    "parameters": param_choice,
                    "query": scenario['query'],
                    "response": response,
                    "generation_time": generation_time,
                    "score": score,
                    "length": length,
                    "found_terms": found_terms,
                    "has_unwanted": has_unwanted,
                    "is_complete": is_complete
                }
                
                filename = f"test_result_{int(time.time())}.json"
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {filename}")
            
        finally:
            # æ¢å¤åŸå§‹å‚æ•°
            generator.temperature = original_temp
            generator.top_p = original_top_p
            generator.max_new_tokens = original_max_tokens
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    while True:
        print("\n" + "=" * 60)
        print("Generator LLM Prompt è°ƒä¼˜å·¥å…·")
        print("=" * 60)
        print("1. å¼€å§‹äº¤äº’å¼æµ‹è¯•")
        print("2. æŸ¥çœ‹ Prompt å˜ä½“åº“")
        print("3. æŸ¥çœ‹å‚æ•°ç»„åˆåº“")
        print("4. æŸ¥çœ‹æµ‹è¯•åœºæ™¯åº“")
        print("0. é€€å‡º")
        print("-" * 60)
        
        choice = input("è¯·é€‰æ‹©: ").strip()
        
        if choice == "1":
            test_specific_combination()
        elif choice == "2":
            from prompt_variations_library import get_prompt_variations
            variations = get_prompt_variations("", "", "")
            print("\nPrompt å˜ä½“åº“:")
            for name in variations.keys():
                print(f"  - {name}")
        elif choice == "3":
            from prompt_variations_library import get_parameter_variations
            parameters = get_parameter_variations()
            print("\nå‚æ•°ç»„åˆåº“:")
            for name, params in parameters.items():
                print(f"  - {name}: {params['description']}")
        elif choice == "4":
            from prompt_variations_library import get_test_scenarios
            scenarios = get_test_scenarios()
            print("\næµ‹è¯•åœºæ™¯åº“:")
            for name, scenario in scenarios.items():
                print(f"  - {name}: {scenario['query']}")
        elif choice == "0":
            print("é€€å‡ºç¨‹åº")
            break
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

if __name__ == "__main__":
    main() 