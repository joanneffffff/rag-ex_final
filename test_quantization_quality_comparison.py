#!/usr/bin/env python3
"""
é‡åŒ–è´¨é‡å¯¹æ¯”æµ‹è¯•
æ¯”è¾ƒ 4bit å’Œ 8bit é‡åŒ–çš„ Fin-R1 å“åº”è´¨é‡
"""

import torch
import time
import sys
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

def load_model_with_quantization(quantization_type="4bit"):
    """åŠ è½½æŒ‡å®šé‡åŒ–ç±»å‹çš„æ¨¡å‹"""
    model_name = "SUFE-AIFLM-Lab/Fin-R1"
    cache_dir = "/users/sgjfei3/data/huggingface"
    
    print(f"ğŸ”§ åŠ è½½ {quantization_type} é‡åŒ–æ¨¡å‹...")
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        cache_dir=cache_dir,
        local_files_only=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # é…ç½®é‡åŒ–
    if quantization_type == "4bit":
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )
    elif quantization_type == "8bit":
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
        )
    else:
        quantization_config = None
    
    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        cache_dir=cache_dir,
        quantization_config=quantization_config,
        device_map="auto",
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True,
        use_cache=False,
        local_files_only=True
    )
    
    return tokenizer, model

def generate_response(tokenizer, model, prompt, max_new_tokens=150):
    """ç”Ÿæˆå“åº”"""
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    
    # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
    if hasattr(model, 'device'):
        device = model.device
    else:
        device = "cuda:1"
    
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    start_time = time.time()
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            temperature=0.1,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    generation_time = time.time() - start_time
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰è¾“å…¥ï¼‰
    generated_text = response[len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)):]
    
    return generated_text.strip(), generation_time

def evaluate_response_quality(response):
    """è¯„ä¼°å“åº”è´¨é‡"""
    quality_score = 0
    feedback = []
    
    # æ£€æŸ¥é•¿åº¦
    if len(response) > 50:
        quality_score += 20
        feedback.append("âœ… å›ç­”é•¿åº¦é€‚ä¸­")
    else:
        feedback.append("âš ï¸ å›ç­”è¿‡çŸ­")
    
    # æ£€æŸ¥ä¸“ä¸šæ€§
    professional_keywords = ["åˆ†æ", "å¢é•¿", "æ”¶å…¥", "åˆ©æ¶¦", "ä¸šåŠ¡", "å¸‚åœº", "è´¢åŠ¡", "ä¸šç»©"]
    professional_count = sum(1 for keyword in professional_keywords if keyword in response)
    if professional_count >= 2:
        quality_score += 30
        feedback.append("âœ… ä½¿ç”¨ä¸“ä¸šæœ¯è¯­")
    else:
        feedback.append("âš ï¸ ä¸“ä¸šæœ¯è¯­è¾ƒå°‘")
    
    # æ£€æŸ¥é€»è¾‘æ€§
    if "å› ä¸º" in response or "ç”±äº" in response or "ä¸»è¦" in response:
        quality_score += 25
        feedback.append("âœ… é€»è¾‘ç»“æ„æ¸…æ™°")
    else:
        feedback.append("âš ï¸ é€»è¾‘ç»“æ„ä¸€èˆ¬")
    
    # æ£€æŸ¥å®Œæ•´æ€§
    if response.endswith(("ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?")):
        quality_score += 15
        feedback.append("âœ… å›ç­”å®Œæ•´")
    else:
        feedback.append("âš ï¸ å›ç­”å¯èƒ½ä¸å®Œæ•´")
    
    # æ£€æŸ¥ç›¸å…³æ€§
    if "å¾·èµ›ç”µæ± " in response or "iPhone" in response or "Aå®¢æˆ·" in response:
        quality_score += 10
        feedback.append("âœ… å†…å®¹ç›¸å…³æ€§å¼º")
    else:
        feedback.append("âš ï¸ å†…å®¹ç›¸å…³æ€§ä¸€èˆ¬")
    
    return quality_score, feedback

def test_quantization_comparison():
    """æµ‹è¯•é‡åŒ–å¯¹æ¯”"""
    print("ğŸ¯ Fin-R1 é‡åŒ–è´¨é‡å¯¹æ¯”æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_context = """å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰çš„ä¸šç»©é¢„å‘Šè¶…å‡ºé¢„æœŸï¼Œä¸»è¦å¾—ç›ŠäºiPhone 12 Pro Maxéœ€æ±‚å¼ºåŠ²å’Œæ–°å“ç›ˆåˆ©èƒ½åŠ›æå‡ã€‚é¢„è®¡2021å¹´åˆ©æ¶¦å°†æŒç»­å¢é•¿ï¼ŒæºäºAå®¢æˆ·çš„ä¸šåŠ¡æˆé•¿ã€éæ‰‹æœºä¸šåŠ¡çš„å¢é•¿ä»¥åŠå¹¶è¡¨æ¯”ä¾‹çš„å¢åŠ ã€‚

ç ”æŠ¥æ˜¾ç¤ºï¼šå¾·èµ›ç”µæ± å‘å¸ƒ20å¹´ä¸šç»©é¢„å‘Šï¼Œ20å¹´è¥æ”¶çº¦193.9äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿5%ï¼Œå½’æ¯å‡€åˆ©æ¶¦6.3-6.9äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿25.5%-37.4%ã€‚21å¹´åˆ©æ¶¦æŒç»­å¢é•¿ï¼ŒæºäºAå®¢æˆ·åŠéæ‰‹æœºä¸šåŠ¡æˆé•¿åŠå¹¶è¡¨æ¯”ä¾‹å¢åŠ ã€‚å…¬å¸è®¤ä¸ºè¶…é¢„æœŸä¸»è¦æºäºiPhone 12 Pro Maxæ–°æœºéœ€æ±‚ä½³åŠæ–°å“ç›ˆåˆ©èƒ½åŠ›æå‡ã€‚å±•æœ›21å¹´ï¼Œ5G iPhoneå‘¨æœŸå åŠ éæ‰‹æœºä¸šåŠ¡å¢é‡ï¼ŒWatchã€AirPodséœ€æ±‚é‡å¢é•¿ï¼ŒiPadã€Macä»½é¢æå‡ï¼Œæœ›é©±åŠ¨Aå®¢æˆ·ä¸šåŠ¡æˆé•¿ã€‚"""
    
    test_query = "å¾·èµ›ç”µæ± 2021å¹´åˆ©æ¶¦æŒç»­å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"
    
    test_prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹å…¬å¸è´¢åŠ¡æŠ¥å‘Šä¿¡æ¯ï¼Œå‡†ç¡®ã€ç®€æ´åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ã€å…¬å¸è´¢åŠ¡æŠ¥å‘Šæ‘˜è¦ã€‘
{test_context}

ã€ç”¨æˆ·é—®é¢˜ã€‘
{test_query}

è¯·æä¾›å‡†ç¡®ã€ä¸“ä¸šçš„åˆ†æå›ç­”ï¼š"""
    
    results = {}
    
    # æµ‹è¯• 4bit é‡åŒ–
    try:
        print("\nğŸ”§ æµ‹è¯• 4bit é‡åŒ–...")
        tokenizer_4bit, model_4bit = load_model_with_quantization("4bit")
        
        response_4bit, time_4bit = generate_response(tokenizer_4bit, model_4bit, test_prompt)
        quality_4bit, feedback_4bit = evaluate_response_quality(response_4bit)
        
        results["4bit"] = {
            "response": response_4bit,
            "time": time_4bit,
            "quality": quality_4bit,
            "feedback": feedback_4bit
        }
        
        print(f"âœ… 4bit å“åº”: {response_4bit}")
        print(f"â±ï¸ ç”Ÿæˆæ—¶é—´: {time_4bit:.2f}ç§’")
        print(f"ğŸ“Š è´¨é‡è¯„åˆ†: {quality_4bit}/100")
        
        # æ¸…ç†å†…å­˜
        del model_4bit
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âŒ 4bit æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯• 8bit é‡åŒ–
    try:
        print("\nğŸ”§ æµ‹è¯• 8bit é‡åŒ–...")
        tokenizer_8bit, model_8bit = load_model_with_quantization("8bit")
        
        response_8bit, time_8bit = generate_response(tokenizer_8bit, model_8bit, test_prompt)
        quality_8bit, feedback_8bit = evaluate_response_quality(response_8bit)
        
        results["8bit"] = {
            "response": response_8bit,
            "time": time_8bit,
            "quality": quality_8bit,
            "feedback": feedback_8bit
        }
        
        print(f"âœ… 8bit å“åº”: {response_8bit}")
        print(f"â±ï¸ ç”Ÿæˆæ—¶é—´: {time_8bit:.2f}ç§’")
        print(f"ğŸ“Š è´¨é‡è¯„åˆ†: {quality_8bit}/100")
        
        # æ¸…ç†å†…å­˜
        del model_8bit
        torch.cuda.empty_cache()
        
    except Exception as e:
        print(f"âŒ 8bit æµ‹è¯•å¤±è´¥: {e}")
    
    # å¯¹æ¯”ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š é‡åŒ–å¯¹æ¯”ç»“æœ")
    print("=" * 60)
    
    if "4bit" in results and "8bit" in results:
        print(f"ğŸ” è´¨é‡è¯„åˆ†å¯¹æ¯”:")
        print(f"   4bit: {results['4bit']['quality']}/100")
        print(f"   8bit: {results['8bit']['quality']}/100")
        print(f"   å·®å¼‚: {results['8bit']['quality'] - results['4bit']['quality']}")
        
        print(f"\nâ±ï¸ ç”Ÿæˆæ—¶é—´å¯¹æ¯”:")
        print(f"   4bit: {results['4bit']['time']:.2f}ç§’")
        print(f"   8bit: {results['8bit']['time']:.2f}ç§’")
        print(f"   é€Ÿåº¦æ¯”: {results['8bit']['time'] / results['4bit']['time']:.2f}x")
        
        print(f"\nğŸ“ å“åº”å†…å®¹å¯¹æ¯”:")
        print(f"   4bit: {results['4bit']['response'][:100]}...")
        print(f"   8bit: {results['8bit']['response'][:100]}...")
        
        # è´¨é‡è¯„ä¼°
        quality_diff = results['8bit']['quality'] - results['4bit']['quality']
        if quality_diff > 10:
            print(f"\nâš ï¸ 8bit è´¨é‡æ˜æ˜¾ä¼˜äº 4bit (å·®å¼‚: {quality_diff})")
        elif quality_diff > 5:
            print(f"\nğŸ“Š 8bit è´¨é‡ç•¥ä¼˜äº 4bit (å·®å¼‚: {quality_diff})")
        elif abs(quality_diff) <= 5:
            print(f"\nâœ… 4bit å’Œ 8bit è´¨é‡ç›¸å½“ (å·®å¼‚: {quality_diff})")
        else:
            print(f"\nğŸ‰ 4bit è´¨é‡ä¼˜äº 8bit (å·®å¼‚: {quality_diff})")
    
    elif "4bit" in results:
        print("âœ… ä»… 4bit æµ‹è¯•æˆåŠŸ")
        print(f"è´¨é‡è¯„åˆ†: {results['4bit']['quality']}/100")
        print(f"ç”Ÿæˆæ—¶é—´: {results['4bit']['time']:.2f}ç§’")
    
    else:
        print("âŒ æ‰€æœ‰æµ‹è¯•éƒ½å¤±è´¥")

def main():
    """ä¸»å‡½æ•°"""
    test_quantization_comparison()

if __name__ == "__main__":
    main() 