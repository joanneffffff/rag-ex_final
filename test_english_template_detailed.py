#!/usr/bin/env python3
"""
è¯¦ç»†æ¨¡æ¿è‹±æ–‡æµ‹è¯•è„šæœ¬
ä½¿ç”¨å®Œæ•´çš„Chain-of-Thoughtç¤ºä¾‹è¿›è¡ŒTATQAè¯„ä¼°
"""

# ä¸´æ—¶å…³é—­warningsï¼Œé¿å…transformerså‚æ•°è­¦å‘Š
import warnings
warnings.filterwarnings("ignore")

# æ›´ç²¾ç¡®åœ°è¿‡æ»¤transformersç”Ÿæˆå‚æ•°è­¦å‘Š
import logging
logging.getLogger("transformers.generation.utils").setLevel(logging.ERROR)

import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional
import random
import time
from difflib import SequenceMatcher

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
import sys
sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGenerator
try:
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
    print("âœ… ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator")
except ImportError:
    USE_RAG_GENERATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGeneratorï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ")
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from transformers.utils.quantization_config import BitsAndBytesConfig

from test_english_template import LLMTemplateTester, load_sample_data

def get_detailed_english_prompt_messages(context_content: str, question_text: str, summary_content: Optional[str] = None) -> List[Dict[str, str]]:
    """
    ç”Ÿæˆ LLM æœŸæœ›çš„ messages åˆ—è¡¨ã€‚
    è¿™æ˜¯ä¸ºé«˜éš¾åº¦ TATQA æ•°æ®é›†ç‰¹åŒ–çš„é«˜æ€§èƒ½æ¨¡æ¿ï¼Œæ ¸å¿ƒæ˜¯åˆ©ç”¨æ€ç»´é“¾ (Chain-of-Thought) å¼•å¯¼æ¨¡å‹è¿›è¡Œå¤æ‚æ¨ç†ã€‚
    """
    
    # TATQAä¸“ç”¨é«˜ç²¾åº¦æ¨¡æ¿
    system_message_content = """You are a world-class quantitative financial analyst AI. Your mission is to solve complex financial questions with extreme precision, based on a given context that may include both tables and text. You must emulate the thinking process of an expert analyst before giving the final answer.

### Core Directives

1.  **Reasoning Process (Internal Thought)**: For every question, you MUST first perform a step-by-step reasoning process, like the examples below. Break down the question, identify necessary data from the table and text, formulate the calculation, and derive the solution. This is your internal monologue.
2.  **Final Output (Your Public Answer)**: Your final, visible output MUST BE the answer ONLY. It should be stripped of all reasoning, explanations, units (unless asked), and introductory phrases. The thinking process is for you to arrive at the correct answer, but it should not be part of your final output.
3.  **Output Format**:
    * For numerical or list-based answers, separate items with a semicolon and a space (e.g., `Value1; Value2`).
    * For text-based answers, provide only the minimal, essential phrase.
    * If the answer is impossible to find, state exactly: `The answer cannot be found in the provided context.`

### Annotated Reasoning Examples (Chain-of-Thought Demonstration)

---
**Example 1: Multi-Step Calculation**
**Q**: What was the percentage increase / (decrease) in capital expenditure from 2018 to 2019?
**Context**:
Table: Capital expenditures 1: 2019 is $2,807; 2018 is $2,790.
**Thought**:
1.  **Objective**: Calculate the percentage change in capital expenditure between 2018 and 2019.
2.  **Data Extraction**:
    * New Value (2019): 2,807
    * Old Value (2018): 2,790
3.  **Formula**: Percentage Change = ((New Value - Old Value) / Old Value) * 100
4.  **Calculation**:
    * Change = 2,807 - 2,790 = 17
    * Ratio = 17 / 2,790 â‰ˆ 0.006093
    * Percentage = 0.006093 * 100 â‰ˆ 0.6093%
5.  **Final Formatting**: The question asks for a percentage. Rounding to two decimal places is standard.
**A**: 0.61%

---
**Example 2: Table and Text Integration**
**Q**: What was the adjusted operating income, excluding one-time restructuring charges?
**Context**:
Text: "Our adjusted metrics provide a clearer view of core performance by excluding special items, such as restructuring charges."
Table: Operating Income: $500M; Restructuring Charges: $20M.
**Thought**:
1.  **Objective**: Find the adjusted operating income.
2.  **Definition**: The text defines "adjusted" as excluding (i.e., adding back) restructuring charges to the reported operating income.
3.  **Data Extraction**:
    * Operating Income: 500
    * Restructuring Charges: 20
4.  **Formula**: Adjusted Operating Income = Reported Operating Income + Restructuring Charges
5.  **Calculation**: 500 + 20 = 520
6.  **Final Formatting**: Provide the final number.
**A**: 520

---
**Example 3: Filtering and Aggregation**
**Q**: What is the total R&D and G&A expense for the year ended July 27, 2019?
**Context**:
Table (Columns: Expense Type, July 27, 2019, July 28, 2018)
Row_R&D: $6,577; $6,332
Row_Sales: $9,571; $9,242
Row_G&A: $1,827; $2,144
**Thought**:
1.  **Objective**: Sum the R&D and G&A expenses for the specific year 2019.
2.  **Filtering**: I need to focus only on the column "July 27, 2019" and the rows "R&D" and "G&A".
3.  **Data Extraction**:
    * R&D expense for 2019: 6,577
    * G&A expense for 2019: 1,827
4.  **Formula**: Total = R&D + G&A
5.  **Calculation**: 6,577 + 1,827 = 8,404
6.  **Final Formatting**: Provide the final number.
**A**: 8404
---"""

    user_message = f"""Context:
{context_content}

Question:
{question_text}

A:"""

    return [
        {"role": "system", "content": system_message_content},
        {"role": "user", "content": user_message}
    ]

def main():
    print("ğŸš€ è¯¦ç»†æ¨¡æ¿è‹±æ–‡æµ‹è¯•å¼€å§‹")
    print("ä½¿ç”¨å®Œæ•´çš„Chain-of-Thoughtç¤ºä¾‹")
    print("="*60)
    
    # åˆå§‹åŒ–LLMæµ‹è¯•å™¨ï¼Œå¢åŠ max_length
    tester = LLMTemplateTester(
        model_name="SUFE-AIFLM-Lab/Fin-R1",
        device="auto"
    )
    tester.max_length = 4096  # å¢åŠ max_lengthä»¥æ”¯æŒè¯¦ç»†æ¨¡æ¿
    
    try:
        tester.load_model()
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return
    
    # åŠ è½½æ ·æœ¬æ•°æ®
    sample_data = load_sample_data()
    print(f"âœ… åŠ è½½äº† {len(sample_data)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    # å­˜å‚¨æ‰€æœ‰ç»“æœ
    all_results = []
    
    # æµ‹è¯•è¯¦ç»†æ¨¡æ¿
    template_name_to_test = "Detailed English Template with CoT"
    
    for i, sample in enumerate(sample_data):
        print(f"\n--- æ ·æœ¬ {i+1} ---")
        print(f"é—®é¢˜: {sample['question']}")
        print(f"é¢„æœŸç­”æ¡ˆ: {sample['answer']}")

        # æ„å»ºè¯¦ç»†Promptæ¶ˆæ¯åˆ—è¡¨
        messages_for_llm = get_detailed_english_prompt_messages(
            context_content=sample["context"], 
            question_text=sample["question"],
            summary_content=sample["context"]
        )
        
        # è°ƒç”¨LLMç”Ÿæˆå›ç­”
        generation_result = tester.generate_response(messages_for_llm)
        
        # å°†æœ¬æ¬¡æµ‹è¯•çš„ç»“æœæ·»åŠ åˆ°all_resultsåˆ—è¡¨
        result_for_analysis = {
            "template_name": template_name_to_test,
            "template": messages_for_llm,
            "context": sample["context"],
            "question": sample["question"],
            "expected_answer": sample["answer"],
            "template_length": len(tester._convert_messages_to_text(messages_for_llm)),
            "generation": generation_result,
            "evaluation": tester.evaluate_answer_quality(
                generated_answer=generation_result["cleaned_answer"],
                expected_answer=sample["answer"],
                context=sample["context"],
                question=sample["question"]
            )
        }
        all_results.append(result_for_analysis)

        # æ‰“å°è¯¦ç»†ç»“æœå’Œè°ƒè¯•ä¿¡æ¯
        print(f"\n--- å‘é€ç»™æ¨¡å‹çš„å®Œæ•´Prompt ---")
        print(tester._convert_messages_to_text(messages_for_llm))
        print(f"--- Prompt ç»“æŸ ---")
        
        print(f"\nâœ… {tester.model_name} åŸå§‹å›ç­” (åå¤„ç†å‰):")
        print(f"{'='*50}")
        print(generation_result["generated_answer"].strip())
        print(f"{'='*50}")
        
        print(f"\nâœ… {tester.model_name} åå¤„ç†å›ç­” (æœ€ç»ˆ):")
        print(f"{'='*50}")
        print(generation_result["cleaned_answer"])
        print(f"{'='*50}")
        print(f"ğŸ“ æœ€ç»ˆé•¿åº¦: {len(generation_result['cleaned_answer'])} å­—ç¬¦")
        
        # è·å–è¯„ä¼°ç»“æœ
        evaluation_result = result_for_analysis["evaluation"]
        
        print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
        print(f"   - è´¨é‡åˆ†æ•°: {evaluation_result['quality_score']:.3f}")
        print(f"   - ç²¾ç¡®åŒ¹é…: {evaluation_result['exact_match']}")
        print(f"   - è¯­ä¹‰ç›¸ä¼¼åº¦: {evaluation_result['semantic_similarity']:.3f}")
        if evaluation_result['format_violations']:
            print(f"   - æ ¼å¼è¿è§„: {', '.join(evaluation_result['format_violations'])}")

    # åˆ†æç»“æœ
    print(f"\nğŸ“Š åˆ†æ {len(all_results)} ä¸ªæµ‹è¯•ç»“æœ...")
    
    # è®¡ç®—å¹³å‡æŒ‡æ ‡
    avg_quality = sum(r["evaluation"]["quality_score"] for r in all_results) / len(all_results)
    avg_time = sum(r["generation"]["generation_time"] for r in all_results) / len(all_results)
    exact_match_rate = sum(1 for r in all_results if r["evaluation"]["exact_match"]) / len(all_results)
    format_violation_rate = sum(1 for r in all_results if r["evaluation"]["format_violations"]) / len(all_results)
    
    print(f"\nğŸ“Š {template_name_to_test} æ€»ç»“:")
    print(f"   å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality:.3f}")
    print(f"   ç²¾ç¡®åŒ¹é…ç‡: {exact_match_rate:.3f}")
    print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f}s")
    print(f"   æ ¼å¼è¿è§„ç‡: {format_violation_rate:.3f}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = "detailed_template_test_results.json"
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump({
            "results": all_results,
            "summary": {
                "template_name": template_name_to_test,
                "avg_quality": avg_quality,
                "avg_time": avg_time,
                "exact_match_rate": exact_match_rate,
                "format_violation_rate": format_violation_rate
            },
            "timestamp": time.time()
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    print("ğŸ‰ è¯¦ç»†æ¨¡æ¿æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    main() 