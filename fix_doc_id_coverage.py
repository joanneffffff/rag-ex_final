#!/usr/bin/env python3
"""
ä¿®å¤çŸ¥è¯†åº“æ•°æ®çš„doc_idè¦†ç›–ç‡é—®é¢˜
"""

import json
import hashlib
from pathlib import Path
from typing import Dict, List, Any

def fix_knowledge_base_doc_ids():
    """ä¿®å¤çŸ¥è¯†åº“æ•°æ®çš„doc_idè¦†ç›–ç‡"""
    
    # 1. ä¿®å¤ä¸­æ–‡çŸ¥è¯†åº“
    print("ğŸ”§ ä¿®å¤ä¸­æ–‡çŸ¥è¯†åº“doc_id...")
    chinese_kb_path = "data/alphafin/alphafin_final_clean.json"
    
    if Path(chinese_kb_path).exists():
        with open(chinese_kb_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        fixed_count = 0
        for i, record in enumerate(data):
            if not record.get('doc_id'):
                # ä½¿ç”¨å†…å®¹å“ˆå¸Œä½œä¸ºdoc_id
                content = record.get('content', '') or record.get('context', '')
                if content:
                    doc_id = hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
                    record['doc_id'] = doc_id
                    fixed_count += 1
        
        # ä¿å­˜ä¿®å¤åçš„æ•°æ®
        with open(chinese_kb_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ä¸­æ–‡çŸ¥è¯†åº“ä¿®å¤å®Œæˆï¼šæ·»åŠ äº† {fixed_count} ä¸ªdoc_id")
    else:
        print(f"âŒ ä¸­æ–‡çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼š{chinese_kb_path}")
    
    # 2. ä¿®å¤è‹±æ–‡çŸ¥è¯†åº“
    print("ğŸ”§ ä¿®å¤è‹±æ–‡çŸ¥è¯†åº“doc_id...")
    english_kb_path = "data/tatqa/tatqa_knowledge_base_combined.jsonl"
    
    if Path(english_kb_path).exists():
        fixed_records = []
        fixed_count = 0
        
        with open(english_kb_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    record = json.loads(line)
                    if not record.get('doc_id'):
                        # ä½¿ç”¨å†…å®¹å“ˆå¸Œä½œä¸ºdoc_id
                        content = record.get('content', '') or record.get('context', '')
                        if content:
                            doc_id = hashlib.md5(content.encode('utf-8')).hexdigest()[:16]
                            record['doc_id'] = doc_id
                            fixed_count += 1
                    fixed_records.append(record)
        
        # ä¿å­˜ä¿®å¤åçš„æ•°æ®
        with open(english_kb_path, 'w', encoding='utf-8') as f:
            for record in fixed_records:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
        
        print(f"âœ… è‹±æ–‡çŸ¥è¯†åº“ä¿®å¤å®Œæˆï¼šæ·»åŠ äº† {fixed_count} ä¸ªdoc_id")
    else:
        print(f"âŒ è‹±æ–‡çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼š{english_kb_path}")
    
    # 3. ä¿®å¤TatQAè¯„æµ‹æ•°æ®
    print("ğŸ”§ ä¿®å¤TatQAè¯„æµ‹æ•°æ®doc_id...")
    tatqa_eval_path = "evaluate_mrr/tatqa_eval.jsonl"
    
    if Path(tatqa_eval_path).exists():
        fixed_records = []
        fixed_count = 0
        
        with open(tatqa_eval_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    record = json.loads(line)
                    if not record.get('doc_id'):
                        # ä½¿ç”¨contextçš„å“ˆå¸Œä½œä¸ºdoc_id
                        context = record.get('context', '')
                        if context:
                            doc_id = hashlib.md5(context.encode('utf-8')).hexdigest()[:16]
                            record['doc_id'] = doc_id
                            fixed_count += 1
                    fixed_records.append(record)
        
        # ä¿å­˜ä¿®å¤åçš„æ•°æ®
        with open(tatqa_eval_path, 'w', encoding='utf-8') as f:
            for record in fixed_records:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
        
        print(f"âœ… TatQAè¯„æµ‹æ•°æ®ä¿®å¤å®Œæˆï¼šæ·»åŠ äº† {fixed_count} ä¸ªdoc_id")
    else:
        print(f"âŒ TatQAè¯„æµ‹æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼š{tatqa_eval_path}")

def verify_fixes():
    """éªŒè¯ä¿®å¤æ•ˆæœ"""
    print("\nğŸ” éªŒè¯ä¿®å¤æ•ˆæœ...")
    
    # æ£€æŸ¥ä¸­æ–‡çŸ¥è¯†åº“
    chinese_kb_path = "data/alphafin/alphafin_final_clean.json"
    if Path(chinese_kb_path).exists():
        with open(chinese_kb_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        doc_id_count = sum(1 for record in data if record.get('doc_id'))
        total_count = len(data)
        coverage = doc_id_count / total_count * 100 if total_count > 0 else 0
        
        print(f"ğŸ“Š ä¸­æ–‡çŸ¥è¯†åº“doc_idè¦†ç›–ç‡: {coverage:.2f}% ({doc_id_count}/{total_count})")
    
    # æ£€æŸ¥è‹±æ–‡çŸ¥è¯†åº“
    english_kb_path = "data/tatqa/tatqa_knowledge_base_combined.jsonl"
    if Path(english_kb_path).exists():
        doc_id_count = 0
        total_count = 0
        
        with open(english_kb_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    record = json.loads(line)
                    total_count += 1
                    if record.get('doc_id'):
                        doc_id_count += 1
        
        coverage = doc_id_count / total_count * 100 if total_count > 0 else 0
        print(f"ğŸ“Š è‹±æ–‡çŸ¥è¯†åº“doc_idè¦†ç›–ç‡: {coverage:.2f}% ({doc_id_count}/{total_count})")
    
    # æ£€æŸ¥TatQAè¯„æµ‹æ•°æ®
    tatqa_eval_path = "evaluate_mrr/tatqa_eval.jsonl"
    if Path(tatqa_eval_path).exists():
        doc_id_count = 0
        total_count = 0
        
        with open(tatqa_eval_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    record = json.loads(line)
                    total_count += 1
                    if record.get('doc_id'):
                        doc_id_count += 1
        
        coverage = doc_id_count / total_count * 100 if total_count > 0 else 0
        print(f"ğŸ“Š TatQAè¯„æµ‹æ•°æ®doc_idè¦†ç›–ç‡: {coverage:.2f}% ({doc_id_count}/{total_count})")

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ä¿®å¤çŸ¥è¯†åº“æ•°æ®çš„doc_idè¦†ç›–ç‡é—®é¢˜...")
    
    # æ‰§è¡Œä¿®å¤
    fix_knowledge_base_doc_ids()
    
    # éªŒè¯ä¿®å¤æ•ˆæœ
    verify_fixes()
    
    print("\nâœ… ä¿®å¤å®Œæˆï¼ç°åœ¨å¯ä»¥æ­£å¸¸è¿è¡Œè¯„æµ‹äº†ã€‚") 