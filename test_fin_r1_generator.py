#!/usr/bin/env python3
"""
æµ‹è¯• Fin-R1 ç”Ÿæˆå™¨æ¨¡å—
éªŒè¯æ›´æ–°åçš„ local_llm_generator.py åŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from xlm.components.generator.local_llm_generator import LocalLLMGenerator

def test_fin_r1_generator():
    """æµ‹è¯• Fin-R1 ç”Ÿæˆå™¨"""
    print("ğŸš€ å¼€å§‹æµ‹è¯• Fin-R1 ç”Ÿæˆå™¨...")
    
    try:
        # åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼Œä½¿ç”¨ Fin-R1 æ¨¡å‹
        generator = LocalLLMGenerator(
            model_name="SUFE-AIFLM-Lab/Fin-R1",
            device="cuda:0",  # ä½¿ç”¨ GPU
            use_quantization=True,
            quantization_type="8bit"
        )
        
        print("âœ… ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # è¯»å– multi_stage_chinese_template.txt
        template_path = "data/prompt_templates/multi_stage_chinese_template.txt"
        with open(template_path, 'r', encoding='utf-8') as f:
            template_content = f.read()
        
        print(f"ğŸ“‹ æ¨¡æ¿æ–‡ä»¶é•¿åº¦: {len(template_content)} å­—ç¬¦")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        test_context = """å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰çš„ä¸šç»©é¢„å‘Šè¶…å‡ºé¢„æœŸï¼Œä¸»è¦å¾—ç›ŠäºiPhone 12 Pro Maxéœ€æ±‚å¼ºåŠ²å’Œæ–°å“ç›ˆåˆ©èƒ½åŠ›æå‡ã€‚é¢„è®¡2021å¹´åˆ©æ¶¦å°†æŒç»­å¢é•¿ï¼ŒæºäºAå®¢æˆ·çš„ä¸šåŠ¡æˆé•¿ã€éæ‰‹æœºä¸šåŠ¡çš„å¢é•¿ä»¥åŠå¹¶è¡¨æ¯”ä¾‹çš„å¢åŠ ã€‚

ç ”æŠ¥æ˜¾ç¤ºï¼šå¾·èµ›ç”µæ± å‘å¸ƒ20å¹´ä¸šç»©é¢„å‘Šï¼Œ20å¹´è¥æ”¶çº¦193.9äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿5%ï¼Œå½’æ¯å‡€åˆ©æ¶¦6.3-6.9äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿25.5%-37.4%ã€‚21å¹´åˆ©æ¶¦æŒç»­å¢é•¿ï¼ŒæºäºAå®¢æˆ·åŠéæ‰‹æœºä¸šåŠ¡æˆé•¿åŠå¹¶è¡¨æ¯”ä¾‹å¢åŠ ã€‚å…¬å¸è®¤ä¸ºè¶…é¢„æœŸä¸»è¦æºäºiPhone 12 Pro Maxæ–°æœºéœ€æ±‚ä½³åŠæ–°å“ç›ˆåˆ©èƒ½åŠ›æå‡ã€‚å±•æœ›21å¹´ï¼Œ5G iPhoneå‘¨æœŸå åŠ éæ‰‹æœºä¸šåŠ¡å¢é‡ï¼ŒWatchã€AirPodséœ€æ±‚é‡å¢é•¿ï¼ŒiPadã€Macä»½é¢æå‡ï¼Œæœ›é©±åŠ¨Aå®¢æˆ·ä¸šåŠ¡æˆé•¿ã€‚"""
        
        test_query = "å¾·èµ›ç”µæ± 2021å¹´åˆ©æ¶¦æŒç»­å¢é•¿çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ"
        
        # æ ¼å¼åŒ–æ¨¡æ¿
        formatted_prompt = template_content.format(
            summary=test_context,
            context=test_context,
            query=test_query
        )
        
        print(f"ğŸ“ æ ¼å¼åŒ–å Prompt é•¿åº¦: {len(formatted_prompt)} å­—ç¬¦")
        
        # æµ‹è¯• convert_to_json_chat_format
        print("\nğŸ”§ æµ‹è¯• convert_to_json_chat_format...")
        json_chat = generator.convert_to_json_chat_format(formatted_prompt)
        print(f"âœ… JSON èŠå¤©æ ¼å¼è½¬æ¢æˆåŠŸï¼Œé•¿åº¦: {len(json_chat)} å­—ç¬¦")
        
        # æµ‹è¯• convert_json_to_model_format
        print("\nğŸ”§ æµ‹è¯• convert_json_to_model_format...")
        model_format = generator.convert_json_to_model_format(json_chat)
        print(f"âœ… æ¨¡å‹æ ¼å¼è½¬æ¢æˆåŠŸï¼Œé•¿åº¦: {len(model_format)} å­—ç¬¦")
        
        # æµ‹è¯•ç”Ÿæˆ
        print("\nğŸ¤– å¼€å§‹ç”Ÿæˆå›ç­”...")
        responses = generator.generate([formatted_prompt])
        
        if responses:
            print(f"\nâœ… ç”ŸæˆæˆåŠŸï¼å›ç­”é•¿åº¦: {len(responses[0])} å­—ç¬¦")
            print(f"\nğŸ“„ ç”Ÿæˆçš„å›ç­”:")
            print("="*50)
            print(responses[0])
            print("="*50)
        else:
            print("âŒ ç”Ÿæˆå¤±è´¥ï¼Œæœªè·å¾—å›ç­”")
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_fin_r1_generator() 