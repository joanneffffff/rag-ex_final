#!/usr/bin/env python3
"""
ä¼˜åŒ–Few-Shot COTç­–ç•¥
åœ¨æœ‰é™Tokené¢„ç®—å†…æœ€å¤§åŒ–Few-Shot COTçš„ä»·å€¼
"""

import json
from typing import List, Dict, Any
from collections import Counter

def load_bad_samples(file_path: str = "comprehensive_evaluation_100_samples.json") -> List[Dict[str, Any]]:
    """åŠ è½½å¹¶ç­›é€‰ä½è´¨é‡æ ·æœ¬"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            results = data.get("results", [])
    except FileNotFoundError:
        print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {file_path}")
        return []
    
    # ç­›é€‰è´¨é‡åˆ†æ•°ä½äº0.5çš„æ ·æœ¬
    bad_samples = []
    for result in results:
        quality_score = result.get("evaluation", {}).get("quality_score", 0)
        if quality_score < 0.5:
            bad_samples.append(result)
    
    return bad_samples

def categorize_failure_patterns(bad_samples: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    """åˆ†ç±»å¤±è´¥æ¨¡å¼"""
    patterns = {
        "å¤æ‚è®¡ç®—": [],
        "å¤šè·³æ¨ç†": [],
        "è¡¨æ ¼ç†è§£": [],
        "å®ä½“æŠ½å–": [],
        "æ•°å€¼æå–": [],
        "å…¶ä»–": []
    }
    
    for sample in bad_samples:
        query = sample.get("query", "").lower()
        context = sample.get("context", "").lower()
        expected_answer = sample.get("expected_answer", "")
        
        # å¤æ‚è®¡ç®—ç±»
        if any(keyword in query for keyword in ["calculate", "compute", "sum", "total", "difference", "percentage", "average", "net"]):
            if "percentage" in query or "average" in query:
                patterns["å¤æ‚è®¡ç®—"].append(sample)
            else:
                patterns["å¤æ‚è®¡ç®—"].append(sample)
        
        # å¤šè·³æ¨ç†ç±»
        elif any(keyword in query for keyword in ["respectively", "both", "and", "or", "compare", "which"]):
            patterns["å¤šè·³æ¨ç†"].append(sample)
        
        # è¡¨æ ¼ç†è§£ç±»
        elif "table id:" in context:
            patterns["è¡¨æ ¼ç†è§£"].append(sample)
        
        # å®ä½“æŠ½å–ç±»
        elif any(keyword in query for keyword in ["what does", "method", "company", "name"]):
            patterns["å®ä½“æŠ½å–"].append(sample)
        
        # æ•°å€¼æå–ç±»
        elif any(keyword in query for keyword in ["what is", "how much", "amount", "value"]):
            patterns["æ•°å€¼æå–"].append(sample)
        
        else:
            patterns["å…¶ä»–"].append(sample)
    
    return patterns

def select_representative_samples(patterns: Dict[str, List[Dict[str, Any]]], max_samples: int = 5) -> List[Dict[str, Any]]:
    """é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„æ ·æœ¬"""
    selected_samples = []
    
    # æŒ‰å¤±è´¥æ¨¡å¼çš„é‡è¦æ€§æ’åº
    priority_order = ["å¤æ‚è®¡ç®—", "å¤šè·³æ¨ç†", "è¡¨æ ¼ç†è§£", "å®ä½“æŠ½å–", "æ•°å€¼æå–"]
    
    for pattern in priority_order:
        samples = patterns.get(pattern, [])
        if samples:
            # é€‰æ‹©è´¨é‡åˆ†æ•°æœ€ä½çš„æ ·æœ¬ï¼ˆæœ€éœ€è¦æ”¹è¿›çš„ï¼‰
            samples.sort(key=lambda x: x.get("evaluation", {}).get("quality_score", 0))
            selected_samples.append({
                "pattern": pattern,
                "sample": samples[0],
                "priority": len(priority_order) - priority_order.index(pattern)
            })
    
    # æŒ‰ä¼˜å…ˆçº§æ’åºå¹¶é™åˆ¶æ•°é‡
    selected_samples.sort(key=lambda x: x["priority"], reverse=True)
    return selected_samples[:max_samples]

def create_optimized_cot_examples(selected_samples: List[Dict[str, Any]]) -> str:
    """åˆ›å»ºä¼˜åŒ–çš„Few-Shot COTç¤ºä¾‹"""
    cot_examples = """Below are some examples of how to reason step by step and extract the final answer. For your answer, only output the final A: part, do not repeat the reasoning.

"""
    
    for i, item in enumerate(selected_samples, 1):
        sample = item["sample"]
        pattern = item["pattern"]
        
        # ç²¾ç®€context - åªä¿ç•™å¿…è¦ä¿¡æ¯
        context = sample.get("context", "")
        if "table id:" in context.lower():
            # å¯¹äºè¡¨æ ¼ï¼Œåªä¿ç•™å…³é”®è¡Œ
            lines = context.split('\n')
            table_lines = [line for line in lines if 'is ' in line and ('$' in line or '%' in line or any(char.isdigit() for char in line))]
            context = '\n'.join(table_lines[:6])  # é™åˆ¶è¡¨æ ¼è¡Œæ•°
        
        # ç²¾ç®€question
        question = sample.get("query", "")
        
        # åˆ›å»ºç²¾ç®€çš„Thoughtè¿‡ç¨‹
        expected_answer = sample.get("expected_answer", "")
        thought = create_optimized_thought(question, context, expected_answer, pattern)
        
        # æ„å»ºç¤ºä¾‹
        cot_examples += f"""Q: {question}
Context: {context[:200]}{'...' if len(context) > 200 else ''}
Thought: {thought}
A: {expected_answer}

"""
    
    return cot_examples

def create_optimized_thought(question: str, context: str, expected_answer: str, pattern: str) -> str:
    """åˆ›å»ºä¼˜åŒ–çš„Thoughtè¿‡ç¨‹"""
    if pattern == "å¤æ‚è®¡ç®—":
        if "percentage" in question.lower():
            return "Extract values from context, calculate percentage: (new-old)/old*100"
        elif "average" in question.lower():
            return "Extract values, calculate average: sum/count"
        else:
            return "Extract values, perform required calculation"
    
    elif pattern == "å¤šè·³æ¨ç†":
        return "Identify multiple entities, extract values for each, compare or combine as needed"
    
    elif pattern == "è¡¨æ ¼ç†è§£":
        return "Locate relevant rows/columns in table, extract specific values"
    
    elif pattern == "å®ä½“æŠ½å–":
        return "Find definition or description in context, extract key information"
    
    elif pattern == "æ•°å€¼æå–":
        return "Locate specific value in context, extract exact number"
    
    else:
        return "Extract relevant information from context"

def estimate_token_count(text: str) -> int:
    """ä¼°ç®—Tokenæ•°é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰"""
    # ç®€å•ä¼°ç®—ï¼šè‹±æ–‡çº¦4ä¸ªå­—ç¬¦1ä¸ªtoken
    return len(text) // 4

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ ä¼˜åŒ–Few-Shot COTç­–ç•¥åˆ†æ")
    print("="*60)
    
    # åŠ è½½bad samples
    bad_samples = load_bad_samples()
    if not bad_samples:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä½è´¨é‡æ ·æœ¬")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(bad_samples)} ä¸ªä½è´¨é‡æ ·æœ¬")
    
    # åˆ†ç±»å¤±è´¥æ¨¡å¼
    patterns = categorize_failure_patterns(bad_samples)
    
    print(f"\nğŸ“Š å¤±è´¥æ¨¡å¼åˆ†å¸ƒ:")
    for pattern, samples in patterns.items():
        if samples:
            print(f"   {pattern}: {len(samples)} ä¸ªæ ·æœ¬")
    
    # é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬
    selected_samples = select_representative_samples(patterns, max_samples=5)
    
    print(f"\nğŸ¯ é€‰æ‹©çš„ä»£è¡¨æ€§æ ·æœ¬:")
    for i, item in enumerate(selected_samples, 1):
        sample = item["sample"]
        pattern = item["pattern"]
        quality_score = sample.get("evaluation", {}).get("quality_score", 0)
        print(f"   {i}. {pattern}: è´¨é‡åˆ†æ•° {quality_score:.3f}")
        print(f"      é—®é¢˜: {sample.get('query', '')[:80]}...")
        print(f"      æœŸæœ›ç­”æ¡ˆ: {sample.get('expected_answer', '')}")
    
    # åˆ›å»ºä¼˜åŒ–çš„COTç¤ºä¾‹
    optimized_cot = create_optimized_cot_examples(selected_samples)
    
    # ä¼°ç®—Tokenä½¿ç”¨é‡
    estimated_tokens = estimate_token_count(optimized_cot)
    
    print(f"\nğŸ“ ä¼˜åŒ–çš„Few-Shot COTç¤ºä¾‹:")
    print(f"   ä¼°ç®—Tokenæ•°é‡: {estimated_tokens}")
    print(f"   ç¤ºä¾‹æ•°é‡: {len(selected_samples)}")
    print(f"   æ˜¯å¦åœ¨é¢„ç®—å†…: {'âœ…' if estimated_tokens < 800 else 'âŒ'}")
    
    print(f"\n{optimized_cot}")
    
    # ä¿å­˜ä¼˜åŒ–ç»“æœ
    optimization_result = {
        "selected_samples": [
            {
                "pattern": item["pattern"],
                "sample_id": item["sample"].get("sample_id", "unknown"),
                "query": item["sample"].get("query", ""),
                "expected_answer": item["sample"].get("expected_answer", ""),
                "quality_score": item["sample"].get("evaluation", {}).get("quality_score", 0)
            }
            for item in selected_samples
        ],
        "optimized_cot": optimized_cot,
        "estimated_tokens": estimated_tokens,
        "total_bad_samples": len(bad_samples)
    }
    
    with open("cot_optimization_result.json", "w", encoding="utf-8") as f:
        json.dump(optimization_result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ä¼˜åŒ–ç»“æœå·²ä¿å­˜åˆ° cot_optimization_result.json")

if __name__ == "__main__":
    main() 