#!/usr/bin/env python3
"""
æµ‹è¯•ä¸åŒç®€æ´ç¨‹åº¦promptçš„æ•ˆæœ
"""

import sys
import os
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent))

def test_clean_prompts():
    """æµ‹è¯•ä¸åŒç®€æ´ç¨‹åº¦promptçš„æ•ˆæœ"""
    try:
        from config.parameters import Config
        from xlm.registry.generator import load_generator
        from xlm.components.rag_system.rag_system import (
            PROMPT_TEMPLATE_ZH, 
            PROMPT_TEMPLATE_ZH_CLEAN,
            PROMPT_TEMPLATE_ZH_SIMPLE
        )
        
        print("ğŸ§ª æµ‹è¯•ä¸åŒç®€æ´ç¨‹åº¦Promptæ•ˆæœ...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        
        # åŠ è½½é…ç½®
        config = Config()
        print(f"ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å‹: {config.generator.model_name}")
        print(f"é‡åŒ–ç±»å‹: {config.generator.quantization_type}")
        print(f"max_new_tokens: {config.generator.max_new_tokens}")
        
        # åŠ è½½ç”Ÿæˆå™¨
        print("\nğŸ”§ åŠ è½½ç”Ÿæˆå™¨...")
        generator = load_generator(
            generator_model_name=config.generator.model_name,
            use_local_llm=True,
            use_gpu=True,
            gpu_device="cuda:1",
            cache_dir=config.generator.cache_dir
        )
        
        # æµ‹è¯•é—®é¢˜
        test_question = "é¦–é’¢è‚¡ä»½çš„ä¸šç»©è¡¨ç°å¦‚ä½•ï¼Ÿ"
        
        # æ¨¡æ‹Ÿæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        test_context = """
        é¦–é’¢è‚¡ä»½2023å¹´ä¸ŠåŠå¹´ä¸šç»©æŠ¥å‘Šæ˜¾ç¤ºï¼Œå…¬å¸å®ç°è¥ä¸šæ”¶å…¥1,234.56äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™21.7%ï¼›
        å‡€åˆ©æ¶¦ä¸º-3.17äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™77.14%ï¼Œæ¯è‚¡äºæŸ0.06å…ƒã€‚
        å…¬å¸è¡¨ç¤ºå°†é€šè¿‡æ³¨å…¥ä¼˜è´¨èµ„äº§æ¥æå‡é•¿æœŸç›ˆåˆ©èƒ½åŠ›ï¼Œå›é¦ˆè‚¡ä¸œã€‚
        """
        
        # æµ‹è¯•ä¸‰ç§prompt
        prompts = [
            ("Few-Shot", PROMPT_TEMPLATE_ZH),
            ("Clean", PROMPT_TEMPLATE_ZH_CLEAN),
            ("Simple", PROMPT_TEMPLATE_ZH_SIMPLE)
        ]
        
        results = {}
        
        for prompt_name, prompt_template in prompts:
            print(f"\n" + "="*60)
            print(f"ğŸ”§ æµ‹è¯• {prompt_name} Prompt")
            print("="*60)
            
            # æ„å»ºprompt
            prompt = prompt_template.format(context=test_context, question=test_question)
            print(f"Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            # ç”Ÿæˆå›ç­”
            print(f"\nğŸ¤– ç”Ÿæˆå›ç­”ä¸­...")
            generated_responses = generator.generate(texts=[prompt])
            answer = generated_responses[0] if generated_responses else "ç”Ÿæˆå¤±è´¥"
            
            print(f"\nâœ… {prompt_name} å›ç­”:")
            print("-" * 40)
            print(answer)
            print("-" * 40)
            
            # åˆ†æå›ç­”
            answer_length = len(answer)
            word_count = len(answer.split())
            
            print(f"\nğŸ“Š å›ç­”åˆ†æ:")
            print(f"å­—ç¬¦æ•°: {answer_length}")
            print(f"è¯æ•°: {word_count}")
            
            # è´¨é‡è¯„ä¼°
            quality_score = 0
            issues = []
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
            if "é¦–é’¢" in answer and ("ä¸šç»©" in answer or "åˆ©æ¶¦" in answer or "æ”¶å…¥" in answer):
                quality_score += 2
            else:
                issues.append("ç¼ºå°‘å…³é”®ä¿¡æ¯")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æ•°æ®
            if any(char.isdigit() for char in answer):
                quality_score += 1
            else:
                issues.append("ç¼ºå°‘å…·ä½“æ•°æ®")
            
            # æ£€æŸ¥æ˜¯å¦å®Œæ•´
            if answer.endswith(('.', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
                quality_score += 1
            else:
                issues.append("å›ç­”ä¸å®Œæ•´")
            
            # æ£€æŸ¥é•¿åº¦æ˜¯å¦åˆé€‚
            if prompt_name == "Simple":
                # Simpleæ¨¡å¼åº”è¯¥å¾ˆçŸ­
                if 20 <= answer_length <= 100:
                    quality_score += 1
                elif answer_length < 20:
                    issues.append("å›ç­”è¿‡çŸ­")
                else:
                    issues.append("å›ç­”è¿‡é•¿")
            elif prompt_name == "Clean":
                # Cleanæ¨¡å¼åº”è¯¥é€‚ä¸­
                if 50 <= answer_length <= 150:
                    quality_score += 1
                elif answer_length < 50:
                    issues.append("å›ç­”è¿‡çŸ­")
                else:
                    issues.append("å›ç­”è¿‡é•¿")
            else:
                # Few-Shotæ¨¡å¼å¯ä»¥ç¨é•¿
                if 50 <= answer_length <= 300:
                    quality_score += 1
                elif answer_length < 50:
                    issues.append("å›ç­”è¿‡çŸ­")
                else:
                    issues.append("å›ç­”è¿‡é•¿")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ ¼å¼æ ‡è®°
            format_indicators = ["---", "æ³¨æ„", "\\boxed", "**", "1.", "2.", "3."]
            has_format = any(indicator in answer for indicator in format_indicators)
            if not has_format:
                quality_score += 1
            else:
                issues.append("åŒ…å«æ ¼å¼æ ‡è®°")
            
            print(f"è´¨é‡è¯„åˆ†: {quality_score}/6")
            if issues:
                print(f"é—®é¢˜: {', '.join(issues)}")
            
            # ä¿å­˜ç»“æœ
            results[prompt_name] = {
                "answer": answer,
                "length": answer_length,
                "word_count": word_count,
                "quality_score": quality_score,
                "issues": issues,
                "prompt_length": len(prompt)
            }
        
        # æ¯”è¾ƒç»“æœ
        print(f"\n" + "="*60)
        print("ğŸ“Š ç»¼åˆæ¯”è¾ƒç»“æœ")
        print("="*60)
        
        print(f"{'Prompt':<12} {'å­—ç¬¦æ•°':<8} {'è¯æ•°':<6} {'è´¨é‡è¯„åˆ†':<8} {'Prompté•¿åº¦':<12} {'çŠ¶æ€'}")
        print("-" * 65)
        
        best_score = 0
        best_prompt = None
        
        for prompt_name in ["Few-Shot", "Clean", "Simple"]:
            result = results[prompt_name]
            status = "âœ… æ¨è" if result["quality_score"] >= 5 else "âš ï¸ ä¸€èˆ¬" if result["quality_score"] >= 4 else "âŒ è¾ƒå·®"
            
            if result["quality_score"] > best_score:
                best_score = result["quality_score"]
                best_prompt = prompt_name
            
            print(f"{prompt_name:<12} {result['length']:<8} {result['word_count']:<6} {result['quality_score']:<8} {result['prompt_length']:<12} {status}")
        
        print(f"\nğŸ† æœ€ä½³Prompt: {best_prompt} (è´¨é‡è¯„åˆ†: {best_score}/6)")
        
        # æ¨è
        print(f"\nğŸ’¡ æ¨è:")
        if best_prompt == "Clean":
            print("âœ… æ¨èä½¿ç”¨ Clean Prompt")
            print("   ç†ç”±: å¹³è¡¡äº†ç®€æ´æ€§å’Œä¿¡æ¯å®Œæ•´æ€§")
        elif best_prompt == "Simple":
            print("âœ… æ¨èä½¿ç”¨ Simple Prompt")
            print("   ç†ç”±: æœ€ç®€æ´ï¼Œé€‚åˆå¿«é€Ÿé—®ç­”")
        else:
            print("âœ… æ¨èä½¿ç”¨ Few-Shot Prompt")
            print("   ç†ç”±: é€šè¿‡ç¤ºä¾‹å­¦ä¹ ï¼Œè´¨é‡ç¨³å®š")
        
        return results
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    test_clean_prompts() 