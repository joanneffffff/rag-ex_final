#!/usr/bin/env python3
"""
æ‰°åŠ¨å®éªŒç»“æœåˆ†æå·¥å…·
æä¾›è¯¦ç»†çš„ç»“æœåˆ†æå’Œå¯è§†åŒ–
"""

import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Any
from dataclasses import asdict
import argparse

class PerturbationResultAnalyzer:
    """æ‰°åŠ¨å®éªŒç»“æœåˆ†æå™¨"""
    
    def __init__(self, results_file: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            results_file: ç»“æœæ–‡ä»¶è·¯å¾„
        """
        self.results_file = results_file
        self.results = self.load_results()
        self.df = self.create_dataframe()
    
    def load_results(self) -> List[Dict[str, Any]]:
        """åŠ è½½ç»“æœæ–‡ä»¶"""
        try:
            with open(self.results_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            return []
    
    def create_dataframe(self) -> pd.DataFrame:
        """åˆ›å»ºDataFrameç”¨äºåˆ†æ"""
        if not self.results:
            return pd.DataFrame()
        
        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(self.results)
        
        # æ·»åŠ è®¡ç®—åˆ—
        df['f1_drop'] = df['f1_original_vs_expected'] - df['f1_perturbed_vs_expected']
        df['f1_ratio'] = df['f1_perturbed_vs_expected'] / df['f1_original_vs_expected'].replace(0, 1)
        
        return df
    
    def basic_statistics(self) -> Dict[str, Any]:
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        if self.df.empty:
            return {}
        
        stats = {
            'total_experiments': len(self.df),
            'perturber_counts': self.df['perturber_name'].value_counts().to_dict(),
            'avg_f1_scores': {
                'original_vs_expected': self.df['f1_original_vs_expected'].mean(),
                'perturbed_vs_expected': self.df['f1_perturbed_vs_expected'].mean(),
                'perturbed_vs_original': self.df['f1_perturbed_vs_original'].mean()
            },
            'avg_f1_drop': self.df['f1_drop'].mean(),
            'avg_f1_ratio': self.df['f1_ratio'].mean()
        }
        
        return stats
    
    def perturber_analysis(self) -> Dict[str, Any]:
        """æ‰°åŠ¨å™¨åˆ†æ"""
        if self.df.empty:
            return {}
        
        analysis = {}
        
        for perturber in self.df['perturber_name'].unique():
            perturber_data = self.df[self.df['perturber_name'] == perturber]
            
            analysis[perturber] = {
                'count': len(perturber_data),
                'avg_f1_original': perturber_data['f1_original_vs_expected'].mean(),
                'avg_f1_perturbed': perturber_data['f1_perturbed_vs_expected'].mean(),
                'avg_f1_drop': perturber_data['f1_drop'].mean(),
                'avg_f1_ratio': perturber_data['f1_ratio'].mean(),
                'success_rate': (perturber_data['f1_perturbed_vs_expected'] > 0.5).mean()
            }
        
        return analysis
    
    def llm_judge_analysis(self) -> Dict[str, Any]:
        """LLM Judgeåˆ†æ"""
        if self.df.empty:
            return {}
        
        # è¿‡æ»¤æœ‰LLM Judgeåˆ†æ•°çš„æ•°æ®
        judge_data = self.df.dropna(subset=['llm_judge_score_accuracy'])
        
        if judge_data.empty:
            return {'error': 'æ²¡æœ‰LLM Judgeæ•°æ®'}
        
        analysis = {
            'total_judged': len(judge_data),
            'avg_scores': {
                'accuracy': judge_data['llm_judge_score_accuracy'].mean(),
                'completeness': judge_data['llm_judge_score_completeness'].mean(),
                'professionalism': judge_data['llm_judge_score_professionalism'].mean()
            },
            'perturber_judge_scores': {}
        }
        
        # æŒ‰æ‰°åŠ¨å™¨åˆ†ç»„åˆ†æ
        for perturber in judge_data['perturber_name'].unique():
            perturber_data = judge_data[judge_data['perturber_name'] == perturber]
            analysis['perturber_judge_scores'][perturber] = {
                'count': len(perturber_data),
                'avg_accuracy': perturber_data['llm_judge_score_accuracy'].mean(),
                'avg_completeness': perturber_data['llm_judge_score_completeness'].mean(),
                'avg_professionalism': perturber_data['llm_judge_score_professionalism'].mean()
            }
        
        return analysis
    
    def generate_visualizations(self, output_dir: str = "analysis_plots"):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        if self.df.empty:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä»¥å¯è§†åŒ–")
            return
        
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        # 1. F1åˆ†æ•°å¯¹æ¯”å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # F1åˆ†æ•°åˆ†å¸ƒ
        axes[0, 0].hist(self.df['f1_original_vs_expected'], alpha=0.7, label='åŸå§‹ç­”æ¡ˆ', bins=20)
        axes[0, 0].hist(self.df['f1_perturbed_vs_expected'], alpha=0.7, label='æ‰°åŠ¨ç­”æ¡ˆ', bins=20)
        axes[0, 0].set_title('F1åˆ†æ•°åˆ†å¸ƒå¯¹æ¯”')
        axes[0, 0].legend()
        
        # æ‰°åŠ¨å™¨F1åˆ†æ•°å¯¹æ¯”
        perturber_f1 = self.df.groupby('perturber_name')['f1_perturbed_vs_expected'].mean()
        perturber_f1.plot(kind='bar', ax=axes[0, 1])
        axes[0, 1].set_title('å„æ‰°åŠ¨å™¨å¹³å‡F1åˆ†æ•°')
        axes[0, 1].tick_params(axis='x', rotation=45)
        
        # F1ä¸‹é™å¹…åº¦
        axes[1, 0].hist(self.df['f1_drop'], bins=20)
        axes[1, 0].set_title('F1åˆ†æ•°ä¸‹é™å¹…åº¦åˆ†å¸ƒ')
        axes[1, 0].axvline(x=0, color='red', linestyle='--', label='æ— å˜åŒ–')
        axes[1, 0].legend()
        
        # LLM Judgeåˆ†æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'llm_judge_score_accuracy' in self.df.columns:
            judge_data = self.df.dropna(subset=['llm_judge_score_accuracy'])
            if not judge_data.empty:
                judge_scores = ['llm_judge_score_accuracy', 'llm_judge_score_completeness', 'llm_judge_score_professionalism']
                judge_means = [judge_data[col].mean() for col in judge_scores]
                axes[1, 1].bar(['å‡†ç¡®æ€§', 'å®Œæ•´æ€§', 'ä¸“ä¸šæ€§'], judge_means)
                axes[1, 1].set_title('LLM Judgeå¹³å‡åˆ†æ•°')
                axes[1, 1].set_ylim(0, 10)
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/f1_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. æ‰°åŠ¨å™¨æ•ˆæœå¯¹æ¯”
        fig, axes = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ‰°åŠ¨å™¨æˆåŠŸç‡
        success_rates = {}
        for perturber in self.df['perturber_name'].unique():
            perturber_data = self.df[self.df['perturber_name'] == perturber]
            success_rate = (perturber_data['f1_perturbed_vs_expected'] > 0.5).mean()
            success_rates[perturber] = success_rate
        
        axes[0].bar(success_rates.keys(), success_rates.values())
        axes[0].set_title('å„æ‰°åŠ¨å™¨æˆåŠŸç‡ (F1 > 0.5)')
        axes[0].tick_params(axis='x', rotation=45)
        axes[0].set_ylim(0, 1)
        
        # æ‰°åŠ¨å™¨F1æ¯”ç‡
        f1_ratios = self.df.groupby('perturber_name')['f1_ratio'].mean()
        axes[1].bar(f1_ratios.index, f1_ratios.values)
        axes[1].set_title('å„æ‰°åŠ¨å™¨F1æ¯”ç‡ (æ‰°åŠ¨/åŸå§‹)')
        axes[1].tick_params(axis='x', rotation=45)
        axes[1].axhline(y=1, color='red', linestyle='--', label='æ— å˜åŒ–')
        axes[1].legend()
        
        plt.tight_layout()
        plt.savefig(f'{output_dir}/perturber_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° {output_dir}/")
    
    def export_summary_report(self, output_file: str = "perturbation_analysis_report.json"):
        """å¯¼å‡ºåˆ†ææŠ¥å‘Š"""
        report = {
            'basic_statistics': self.basic_statistics(),
            'perturber_analysis': self.perturber_analysis(),
            'llm_judge_analysis': self.llm_judge_analysis(),
            'metadata': {
                'results_file': self.results_file,
                'total_experiments': len(self.df) if not self.df.empty else 0,
                'analysis_timestamp': pd.Timestamp.now().isoformat()
            }
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“‹ åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        return report
    
    def print_summary(self):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        print("ğŸ“Š æ‰°åŠ¨å®éªŒç»“æœåˆ†ææ‘˜è¦")
        print("=" * 50)
        
        # åŸºç¡€ç»Ÿè®¡
        stats = self.basic_statistics()
        if stats:
            print(f"æ€»å®éªŒæ•°: {stats['total_experiments']}")
            print(f"æ‰°åŠ¨å™¨åˆ†å¸ƒ: {stats['perturber_counts']}")
            print(f"å¹³å‡F1åˆ†æ•°:")
            print(f"  åŸå§‹ç­”æ¡ˆ vs æœŸæœ›ç­”æ¡ˆ: {stats['avg_f1_scores']['original_vs_expected']:.3f}")
            print(f"  æ‰°åŠ¨ç­”æ¡ˆ vs æœŸæœ›ç­”æ¡ˆ: {stats['avg_f1_scores']['perturbed_vs_expected']:.3f}")
            print(f"  æ‰°åŠ¨ç­”æ¡ˆ vs åŸå§‹ç­”æ¡ˆ: {stats['avg_f1_scores']['perturbed_vs_original']:.3f}")
            print(f"å¹³å‡F1ä¸‹é™: {stats['avg_f1_drop']:.3f}")
            print(f"å¹³å‡F1æ¯”ç‡: {stats['avg_f1_ratio']:.3f}")
        
        # æ‰°åŠ¨å™¨åˆ†æ
        perturber_analysis = self.perturber_analysis()
        if perturber_analysis:
            print(f"\næ‰°åŠ¨å™¨è¯¦ç»†åˆ†æ:")
            for perturber, analysis in perturber_analysis.items():
                print(f"  {perturber}:")
                print(f"    å®éªŒæ•°: {analysis['count']}")
                print(f"    å¹³å‡F1åˆ†æ•°: {analysis['avg_f1_perturbed']:.3f}")
                print(f"    å¹³å‡F1ä¸‹é™: {analysis['avg_f1_drop']:.3f}")
                print(f"    æˆåŠŸç‡: {analysis['success_rate']:.1%}")
        
        # LLM Judgeåˆ†æ
        judge_analysis = self.llm_judge_analysis()
        if judge_analysis and 'error' not in judge_analysis:
            print(f"\nLLM Judgeåˆ†æ:")
            print(f"  è¯„ä¼°æ ·æœ¬æ•°: {judge_analysis['total_judged']}")
            print(f"  å¹³å‡åˆ†æ•°:")
            print(f"    å‡†ç¡®æ€§: {judge_analysis['avg_scores']['accuracy']:.2f}")
            print(f"    å®Œæ•´æ€§: {judge_analysis['avg_scores']['completeness']:.2f}")
            print(f"    ä¸“ä¸šæ€§: {judge_analysis['avg_scores']['professionalism']:.2f}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ‰°åŠ¨å®éªŒç»“æœåˆ†æå·¥å…·')
    parser.add_argument('results_file', help='ç»“æœæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='analysis_report.json', 
                       help='åˆ†ææŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--plots', action='store_true', help='ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨')
    parser.add_argument('--plots_dir', type=str, default='analysis_plots', 
                       help='å›¾è¡¨è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = PerturbationResultAnalyzer(args.results_file)
    
    # æ‰“å°æ‘˜è¦
    analyzer.print_summary()
    
    # å¯¼å‡ºæŠ¥å‘Š
    analyzer.export_summary_report(args.output)
    
    # ç”Ÿæˆå¯è§†åŒ–
    if args.plots:
        analyzer.generate_visualizations(args.plots_dir)
    
    print(f"\nâœ… åˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main() 