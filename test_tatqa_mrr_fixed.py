#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆTatQA MRRæµ‹è¯•è„šæœ¬
æ­£ç¡®åŠ è½½åŸå§‹è®­ç»ƒæ•°æ®ï¼Œæ„å»ºå®Œæ•´çš„çŸ¥è¯†åº“ï¼Œç„¶åæ·»åŠ eval context
"""

import sys
import json
import numpy as np
from pathlib import Path
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent))

def load_eval_data(eval_file: str):
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    data = []
    with open(eval_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def calculate_mrr(ranks):
    """è®¡ç®—MRR"""
    if not ranks:
        return 0.0
    reciprocal_ranks = [1.0 / rank if rank > 0 else 0.0 for rank in ranks]
    return float(np.mean(reciprocal_ranks))

def calculate_hit_rate(ranks, k=1):
    """è®¡ç®—Hit@k"""
    if not ranks:
        return 0.0
    hits = [1 if rank <= k and rank > 0 else 0 for rank in ranks]
    return float(np.mean(hits))

def test_tatqa_mrr_fixed():
    """ä¿®å¤ç‰ˆTatQA MRRæµ‹è¯•"""
    print("=" * 60)
    print("ä¿®å¤ç‰ˆTatQA MRRæµ‹è¯•")
    print("=" * 60)
    
    try:
        from config.parameters import Config
        from xlm.components.encoder.finbert import FinbertEncoder
        from xlm.dto.dto import DocumentWithMetadata
        from xlm.utils.optimized_data_loader import OptimizedDataLoader
        from enhanced_evaluation_functions import find_correct_document_rank_enhanced
        import faiss
        
        config = Config()
        
        print("1. åŠ è½½ç¼–ç å™¨ï¼ˆCPUæ¨¡å¼ï¼‰...")
        encoder = FinbertEncoder(
            model_name="models/finetuned_finbert_tatqa",
            cache_dir=config.encoder.cache_dir,
            device="cpu"
        )
        print("   âœ… ç¼–ç å™¨åŠ è½½æˆåŠŸ")
        
        print("\n2. åŠ è½½åŸå§‹è®­ç»ƒæ•°æ®...")
        data_loader = OptimizedDataLoader(
            data_dir="data",
            max_samples=-1,  # åŠ è½½æ‰€æœ‰æ•°æ®
            chinese_document_level=True,
            english_chunk_level=True,
            include_eval_data=False  # ä¸åŒ…å«è¯„ä¼°æ•°æ®ï¼Œæˆ‘ä»¬åé¢æ‰‹åŠ¨æ·»åŠ 
        )
        
        english_chunks = data_loader.english_docs
        print(f"   âœ… è‹±æ–‡è®­ç»ƒæ•°æ®: {len(english_chunks)} ä¸ªchunks")
        
        print("\n3. åŠ è½½TatQAå¢å¼ºç‰ˆè¯„ä¼°æ•°æ®...")
        tatqa_eval = load_eval_data("evaluate_mrr/tatqa_eval_enhanced.jsonl")
        print(f"   âœ… TatQAè¯„ä¼°æ ·æœ¬: {len(tatqa_eval)}")
        
        print("\n4. å‡†å¤‡eval contextæ–‡æ¡£...")
        # ä»è¯„ä¼°æ•°æ®ä¸­æå–æ‰€æœ‰å”¯ä¸€çš„context
        eval_knowledge_base = {}
        for i, sample in enumerate(tatqa_eval):
            context = sample.get('context', '').strip()
            if context and context not in eval_knowledge_base:
                doc_id = f"eval_doc_{len(eval_knowledge_base)}"
                eval_knowledge_base[context] = {
                    'id': doc_id,
                    'content': context,
                    'relevant_doc_ids': sample.get('relevant_doc_ids', [])
                }
        
        print(f"   âœ… eval contextæ–‡æ¡£æ•°: {len(eval_knowledge_base)}")
        
        print("\n5. æ„å»ºå®Œæ•´çŸ¥è¯†åº“...")
        # æ„å»ºå®Œæ•´çš„çŸ¥è¯†åº“ï¼ˆåŸå§‹è®­ç»ƒæ•°æ® + eval contextï¼‰
        all_documents = []
        all_doc_info = []
        
        # æ·»åŠ åŸå§‹è®­ç»ƒæ•°æ®
        for i, doc in enumerate(english_chunks):
            all_documents.append(doc.content)
            # ä½¿ç”¨ç®€å•çš„æ–‡æ¡£ID
            doc_id = f"train_doc_{i}"
            
            all_doc_info.append({
                'id': doc_id,
                'content': doc.content,
                'relevant_doc_ids': []
            })
        
        # æ·»åŠ eval context
        for context_text, doc_info in eval_knowledge_base.items():
            all_documents.append(context_text)
            all_doc_info.append(doc_info)
        
        print(f"   âœ… å®Œæ•´çŸ¥è¯†åº“: {len(all_documents)} ä¸ªæ–‡æ¡£")
        print(f"      - è®­ç»ƒæ•°æ®: {len(english_chunks)} ä¸ª")
        print(f"      - eval context: {len(eval_knowledge_base)} ä¸ª")
        
        print("\n6. ç¼–ç æ‰€æœ‰æ–‡æ¡£...")
        # åˆ†æ‰¹ç¼–ç ä»¥é€‚åº”CPU
        batch_size = 8
        all_embeddings = []
        
        for i in tqdm(range(0, len(all_documents), batch_size), desc="ç¼–ç æ–‡æ¡£"):
            batch_docs = all_documents[i:i+batch_size]
            batch_embeddings = encoder.encode(batch_docs)
            all_embeddings.extend(batch_embeddings)
        
        all_embeddings_array = np.array(all_embeddings, dtype=np.float32)
        print(f"   âœ… ç¼–ç å®Œæˆï¼Œç»´åº¦: {all_embeddings_array.shape}")
        
        print("\n7. åˆ›å»ºFAISSç´¢å¼•...")
        # åˆ›å»ºFAISSç´¢å¼•
        dimension = all_embeddings_array.shape[1]
        index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç´¢å¼•
        index.add(all_embeddings_array)
        
        print(f"   âœ… FAISSç´¢å¼•åˆ›å»ºå®Œæˆï¼Œæ€»å‘é‡æ•°: {index.ntotal}")
        
        # ä¿å­˜ç´¢å¼•å’ŒåµŒå…¥
        index_path = "models/embedding_cache/finetuned_finbert_tatqa_complete.faiss"
        embeddings_path = "models/embedding_cache/finetuned_finbert_tatqa_complete.npy"
        
        faiss.write_index(index, index_path)
        np.save(embeddings_path, all_embeddings_array)
        
        print(f"   âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {index_path}")
        print(f"   âœ… åµŒå…¥å·²ä¿å­˜åˆ°: {embeddings_path}")
        
        print("\n8. å¼€å§‹è¯„ä¼°TatQA...")
        ranks = []
        found_count = 0
        total_samples = len(tatqa_eval)  # è¯„ä¼°å…¨éƒ¨æ ·æœ¬
        
        print(f"   å°†è¯„ä¼°å…¨éƒ¨ {total_samples} ä¸ªTatQAæ ·æœ¬...")
        
        for i, sample in enumerate(tqdm(tatqa_eval, desc="è¯„ä¼°TatQA")):
            query = sample.get('query', '')
            context = sample.get('context', '')
            relevant_doc_ids = sample.get('relevant_doc_ids', [])
            
            if not query or not context:
                continue
            
            try:
                # ç¼–ç æŸ¥è¯¢
                query_embedding = encoder.encode([query])[0].reshape(1, -1)
                
                # æ£€ç´¢
                scores, indices = index.search(query_embedding, k=20)
                
                # æ„å»ºæ£€ç´¢ç»“æœ
                retrieved_docs = []
                for idx in indices[0]:
                    if idx < len(all_doc_info):
                        doc_info = all_doc_info[idx]
                        # åˆ›å»ºDocumentWithMetadataå¯¹è±¡ï¼Œä½¿ç”¨sourceå­—æ®µå­˜å‚¨doc_id
                        from xlm.dto.dto import DocumentMetadata
                        metadata = DocumentMetadata(source=doc_info['id'])
                        doc = DocumentWithMetadata(
                            content=doc_info['content'],
                            metadata=metadata
                        )
                        retrieved_docs.append(doc)
                
                # ä½¿ç”¨å¢å¼ºç‰ˆå‡½æ•°æŸ¥æ‰¾æ’å
                found_rank = find_correct_document_rank_enhanced(
                    context=context,
                    retrieved_docs=retrieved_docs,
                    sample=sample,
                    encoder=encoder
                )
                
                ranks.append(found_rank)
                if found_rank > 0:
                    found_count += 1
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                if i < 3:
                    print(f"\næ ·æœ¬ {i+1}:")
                    print(f"  é—®é¢˜: {query[:80]}...")
                    print(f"  ç›¸å…³æ–‡æ¡£ID: {relevant_doc_ids}")
                    print(f"  æ‰¾åˆ°æ’å: {found_rank}")
                    if found_rank > 0:
                        retrieved_doc = retrieved_docs[found_rank-1]
                        print(f"  ç›¸å…³æ–‡æ¡£: {retrieved_doc.content[:100]}...")
                        print(f"  æ–‡æ¡£ID: {retrieved_doc.metadata.source}")
                
                # æ¯100ä¸ªæ ·æœ¬æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if (i + 1) % 100 == 0:
                    current_mrr = calculate_mrr(ranks)
                    current_recall = found_count / len(ranks)
                    print(f"\n   è¿›åº¦: {i+1}/{total_samples}, å½“å‰MRR: {current_mrr:.4f}, å¬å›ç‡: {current_recall:.4f}")
                
            except Exception as e:
                print(f"   æ ·æœ¬ {i} å¤„ç†å¤±è´¥: {e}")
                ranks.append(0)
        
        # è®¡ç®—æŒ‡æ ‡
        mrr = calculate_mrr(ranks)
        hit_at_1 = calculate_hit_rate(ranks, k=1)
        hit_at_3 = calculate_hit_rate(ranks, k=3)
        hit_at_5 = calculate_hit_rate(ranks, k=5)
        hit_at_10 = calculate_hit_rate(ranks, k=10)
        
        print(f"\n" + "=" * 60)
        print("ä¿®å¤ç‰ˆTatQA MRRè¯„ä¼°ç»“æœ")
        print("=" * 60)
        print(f"æ€»æ ·æœ¬æ•°: {len(ranks)}")
        print(f"æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆ: {found_count}")
        print(f"å¬å›ç‡: {found_count/len(ranks):.4f}")
        print(f"MRR: {mrr:.4f}")
        print(f"Hit@1: {hit_at_1:.4f}")
        print(f"Hit@3: {hit_at_3:.4f}")
        print(f"Hit@5: {hit_at_5:.4f}")
        print(f"Hit@10: {hit_at_10:.4f}")
        print(f"çŸ¥è¯†åº“å¤§å°: {len(all_documents)} ä¸ªæ–‡æ¡£")
        
        # ä¿å­˜ç»“æœ
        results = {
            "dataset": "TatQAå¢å¼ºç‰ˆ",
            "total_samples": len(ranks),
            "found_samples": found_count,
            "recall": found_count/len(ranks),
            "mrr": mrr,
            "hit_at_1": hit_at_1,
            "hit_at_3": hit_at_3,
            "hit_at_5": hit_at_5,
            "hit_at_10": hit_at_10,
            "mode": "CPU",
            "enhanced_evaluation": True,
            "knowledge_base_size": len(all_documents),
            "training_docs": len(english_chunks),
            "eval_docs": len(eval_knowledge_base),
            "index_path": index_path
        }
        
        with open("tatqa_mrr_results_fixed.json", "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: tatqa_mrr_results_fixed.json")
        print(f"å®Œæ•´ç´¢å¼•: {index_path}")
        print("\nğŸ‰ ä¿®å¤ç‰ˆTatQA MRRæµ‹è¯•å®Œæˆï¼")
        
        return results
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    test_tatqa_mrr_fixed() 