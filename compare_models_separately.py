#!/usr/bin/env python3
"""
åˆ†ç¦»çš„æ¨¡å‹æ¯”è¾ƒè„šæœ¬
åˆ†åˆ«æµ‹è¯•Qwen3-8Bå’ŒFin-R1ï¼Œé¿å…å†…å­˜å†²çª
"""

import os
import sys
import torch
import time
import json
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.parameters import Config
from xlm.components.generator.local_llm_generator import LocalLLMGenerator


def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        print("ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")


def test_model_separately(model_name: str, device: str = "cuda:1") -> Dict[str, Any]:
    """å•ç‹¬æµ‹è¯•ä¸€ä¸ªæ¨¡å‹"""
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"   è®¾å¤‡: {device}")
    
    # æ¸…ç†å†…å­˜
    clear_gpu_memory()
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "ä»€ä¹ˆæ˜¯è‚¡ç¥¨æŠ•èµ„ï¼Ÿ",
        "è¯·è§£é‡Šå€ºåˆ¸çš„åŸºæœ¬æ¦‚å¿µ", 
        "åŸºé‡‘æŠ•èµ„ä¸è‚¡ç¥¨æŠ•èµ„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯å¸‚ç›ˆç‡ï¼Ÿ",
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯ETFåŸºé‡‘"
    ]
    
    results = {
        "model_name": model_name,
        "device": device,
        "questions": [],
        "success_count": 0,
        "total_time": 0,
        "avg_tokens": 0,
        "memory_usage": 0
    }
    
    try:
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        print(f"ğŸ”§ åˆå§‹åŒ– {model_name}...")
        generator = LocalLLMGenerator(
            model_name=model_name,
            device=device,
            use_quantization=True,
            quantization_type="4bit"
        )
        print(f"âœ… {model_name} åˆå§‹åŒ–æˆåŠŸ")
        
        # è®°å½•å†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated(device=int(device.split(':')[1])) / 1024**3
            results["memory_usage"] = gpu_memory
            print(f"ğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {gpu_memory:.2f}GB")
        
        # æµ‹è¯•æ¯ä¸ªé—®é¢˜
        for i, question in enumerate(test_questions):
            print(f"\n   ğŸ” é—®é¢˜ {i+1}: {question}")
            
            try:
                # æ„å»ºprompt
                prompt = f"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}"
                
                # è®°å½•å¼€å§‹æ—¶é—´
                start_time = time.time()
                
                # ç”Ÿæˆå›ç­”
                response = generator.generate(texts=[prompt])[0]
                
                # è®°å½•ç»“æŸæ—¶é—´
                end_time = time.time()
                generation_time = end_time - start_time
                
                # ç»Ÿè®¡tokenæ•°é‡
                response_tokens = len(response.split())
                
                print(f"   âœ… ç”ŸæˆæˆåŠŸ")
                print(f"      å›ç­”: {response[:100]}...")
                print(f"      é•¿åº¦: {response_tokens} tokens")
                print(f"      æ—¶é—´: {generation_time:.2f}s")
                
                results["questions"].append({
                    "question": question,
                    "response": response,
                    "tokens": response_tokens,
                    "time": generation_time,
                    "success": True
                })
                
                results["success_count"] += 1
                results["total_time"] += generation_time
                
            except Exception as e:
                print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
                results["questions"].append({
                    "question": question,
                    "response": f"ç”Ÿæˆå¤±è´¥: {e}",
                    "tokens": 0,
                    "time": 0,
                    "success": False
                })
        
        # è®¡ç®—å¹³å‡tokenæ•°
        successful_responses = [q for q in results["questions"] if q["success"]]
        if successful_responses:
            results["avg_tokens"] = sum(q["tokens"] for q in successful_responses) / len(successful_responses)
        
        # æ¸…ç†å†…å­˜
        del generator
        clear_gpu_memory()
        
    except Exception as e:
        print(f"âŒ {model_name} åˆå§‹åŒ–å¤±è´¥: {e}")
        results["error"] = str(e)
    
    return results


def save_results(results: Dict[str, Any], filename: str):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")


def compare_results(qwen_results: Dict[str, Any], fin_results: Dict[str, Any]):
    """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„ç»“æœ"""
    print(f"\nğŸ“Š æ¨¡å‹æ¯”è¾ƒç»“æœ:")
    print(f"=" * 60)
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ“‹ æ¨¡å‹ä¿¡æ¯:")
    print(f"   Qwen3-8B: {qwen_results.get('model_name', 'N/A')}")
    print(f"   Fin-R1: {fin_results.get('model_name', 'N/A')}")
    
    # æˆåŠŸç‡
    qwen_success = qwen_results.get('success_count', 0)
    fin_success = fin_results.get('success_count', 0)
    total_questions = len(qwen_results.get('questions', []))
    
    print(f"\nâœ… æˆåŠŸç‡:")
    print(f"   Qwen3-8B: {qwen_success}/{total_questions} ({qwen_success/total_questions*100:.1f}%)")
    print(f"   Fin-R1: {fin_success}/{total_questions} ({fin_success/total_questions*100:.1f}%)")
    
    # æ€§èƒ½æŒ‡æ ‡
    if 'error' not in qwen_results:
        print(f"\nâ±ï¸ æ€§èƒ½æŒ‡æ ‡ (Qwen3-8B):")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {qwen_results.get('total_time', 0)/qwen_success:.2f}s")
        print(f"   å¹³å‡tokenæ•°: {qwen_results.get('avg_tokens', 0):.1f}")
        print(f"   GPUå†…å­˜ä½¿ç”¨: {qwen_results.get('memory_usage', 0):.2f}GB")
    
    if 'error' not in fin_results:
        print(f"\nâ±ï¸ æ€§èƒ½æŒ‡æ ‡ (Fin-R1):")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {fin_results.get('total_time', 0)/fin_success:.2f}s")
        print(f"   å¹³å‡tokenæ•°: {fin_results.get('avg_tokens', 0):.1f}")
        print(f"   GPUå†…å­˜ä½¿ç”¨: {fin_results.get('memory_usage', 0):.2f}GB")
    
    # å›ç­”è´¨é‡æ¯”è¾ƒ
    if 'error' not in qwen_results and 'error' not in fin_results:
        print(f"\nğŸ“ å›ç­”è´¨é‡æ¯”è¾ƒ:")
        for i, (qwen_q, fin_q) in enumerate(zip(qwen_results['questions'], fin_results['questions'])):
            if qwen_q['success'] and fin_q['success']:
                print(f"\n   é—®é¢˜ {i+1}: {qwen_q['question']}")
                print(f"   Qwen3-8B: {qwen_q['response'][:100]}...")
                print(f"   Fin-R1: {fin_q['response'][:100]}...")
                print(f"   é•¿åº¦å¯¹æ¯”: {qwen_q['tokens']} vs {fin_q['tokens']} tokens")
    
    # æ€»ç»“
    print(f"\nğŸ¯ æ€»ç»“:")
    if qwen_success > fin_success:
        print(f"   Qwen3-8B è¡¨ç°æ›´å¥½ï¼ŒæˆåŠŸç‡æ›´é«˜")
    elif fin_success > qwen_success:
        print(f"   Fin-R1 è¡¨ç°æ›´å¥½ï¼ŒæˆåŠŸç‡æ›´é«˜")
    else:
        print(f"   ä¸¤ä¸ªæ¨¡å‹æˆåŠŸç‡ç›¸åŒ")
    
    if 'error' in qwen_results:
        print(f"   Qwen3-8B å­˜åœ¨é—®é¢˜: {qwen_results['error']}")
    if 'error' in fin_results:
        print(f"   Fin-R1 å­˜åœ¨é—®é¢˜: {fin_results['error']}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª åˆ†ç¦»æ¨¡å‹æ¯”è¾ƒæµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨ï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        return
    
    # æµ‹è¯•Qwen3-8B
    print(f"\n{'='*20} æµ‹è¯•Qwen3-8B {'='*20}")
    qwen_results = test_model_separately("Qwen/Qwen3-8B", "cuda:1")
    save_results(qwen_results, "qwen3_8b_test_results.json")
    
    # ç­‰å¾…ç”¨æˆ·ç¡®è®¤
    try:
        choice = input(f"\nğŸ¤” æ˜¯å¦ç»§ç»­æµ‹è¯•Fin-R1ï¼Ÿ(y/n): ").lower().strip()
        if choice not in ['y', 'yes', 'æ˜¯']:
            print("ğŸ‘‹ æµ‹è¯•ç»“æŸ")
            return
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        return
    
    # æµ‹è¯•Fin-R1
    print(f"\n{'='*20} æµ‹è¯•Fin-R1 {'='*20}")
    fin_results = test_model_separately("SUFE-AIFLM-Lab/Fin-R1", "cuda:1")
    save_results(fin_results, "fin_r1_test_results.json")
    
    # æ¯”è¾ƒç»“æœ
    compare_results(qwen_results, fin_results)
    
    print(f"\nğŸ‰ åˆ†ç¦»æ¨¡å‹æ¯”è¾ƒæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶:")
    print(f"   - qwen3_8b_test_results.json")
    print(f"   - fin_r1_test_results.json")


if __name__ == "__main__":
    main() 