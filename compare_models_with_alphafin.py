#!/usr/bin/env python3
"""
ä½¿ç”¨AlphaFinæ•°æ®é›†ä¸­çš„é—®é¢˜æ¯”è¾ƒä¸åŒæ¨¡å‹
æ”¯æŒé€šè¿‡--model_nameå‚æ•°æŒ‡å®šä¸åŒçš„æ¨¡å‹
"""

import os
import sys
import json
import torch
import time
import argparse
import random
from typing import Dict, List, Any, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.parameters import Config
from xlm.components.generator.local_llm_generator import LocalLLMGenerator


def load_alphafin_questions(data_path: str, max_questions: int = 10) -> List[str]:
    """ä»AlphaFinæ•°æ®é›†åŠ è½½é—®é¢˜ï¼Œæ”¯æŒjsonlå’Œjsonæ•°ç»„æ ¼å¼ï¼Œä¼˜å…ˆä½¿ç”¨generated_questionå­—æ®µ"""
    questions = []
    try:
        # åˆ¤æ–­æ˜¯å¦ä¸ºjsonæ•°ç»„
        with open(data_path, 'r', encoding='utf-8') as f:
            first_char = f.read(1)
            f.seek(0)
            if first_char == '[':
                # JSONæ•°ç»„æ ¼å¼
                data_list = json.load(f)
                # ä¼˜å…ˆä½¿ç”¨generated_questionå­—æ®µ
                all_questions = [item.get('generated_question') for item in data_list if item.get('generated_question')]
                # å¦‚æœä¸è¶³max_questionsï¼Œå°è¯•ç”¨original_questionè¡¥å……
                if len(all_questions) < max_questions:
                    all_questions += [item.get('original_question') for item in data_list if item.get('original_question')]
                # éšæœºæŠ½å–max_questionsä¸ª
                questions = random.sample(all_questions, min(max_questions, len(all_questions)))
            else:
                # JSONLæ ¼å¼
                for i, line in enumerate(f):
                    if i >= max_questions:
                        break
                    try:
                        data = json.loads(line.strip())
                        if 'generated_question' in data:
                            questions.append(data['generated_question'])
                        elif 'question' in data:
                            questions.append(data['question'])
                        elif 'query' in data:
                            questions.append(data['query'])
                    except json.JSONDecodeError:
                        continue
    except Exception as e:
        print(f"âŒ åŠ è½½AlphaFinæ•°æ®å¤±è´¥: {e}")
        # ä½¿ç”¨é»˜è®¤é—®é¢˜ä½œä¸ºå¤‡é€‰
        questions = [
            "ä»€ä¹ˆæ˜¯è‚¡ç¥¨æŠ•èµ„ï¼Ÿ",
            "è¯·è§£é‡Šå€ºåˆ¸çš„åŸºæœ¬æ¦‚å¿µ",
            "åŸºé‡‘æŠ•èµ„ä¸è‚¡ç¥¨æŠ•èµ„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯å¸‚ç›ˆç‡ï¼Ÿ",
            "è¯·è§£é‡Šä»€ä¹ˆæ˜¯ETFåŸºé‡‘"
        ]
    print(f"âœ… åŠ è½½äº† {len(questions)} ä¸ªé—®é¢˜")
    return questions


def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        print("ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")


def test_model_with_alphafin_questions(
    model_name: str, 
    questions: List[str], 
    device: str = "cuda:1",
    max_new_tokens: int = 600,
    temperature: float = 0.2,
    top_p: float = 0.8
) -> Dict[str, Any]:
    """ä½¿ç”¨AlphaFiné—®é¢˜æµ‹è¯•æŒ‡å®šæ¨¡å‹"""
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯•æ¨¡å‹: {model_name}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   é—®é¢˜æ•°é‡: {len(questions)}")
    
    # æ¸…ç†å†…å­˜
    clear_gpu_memory()
    
    results = {
        "model_name": model_name,
        "device": device,
        "questions": [],
        "success_count": 0,
        "total_time": 0,
        "avg_tokens": 0,
        "memory_usage": 0,
        "config": {
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "top_p": top_p
        }
    }
    
    try:
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        print(f"ğŸ”§ åˆå§‹åŒ– {model_name}...")
        generator = LocalLLMGenerator(
            model_name=model_name,
            device=device,
            use_quantization=True,
            quantization_type="4bit"
        )
        print(f"âœ… {model_name} åˆå§‹åŒ–æˆåŠŸ")
        
        # è®°å½•å†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated(device=int(device.split(':')[1])) / 1024**3
            results["memory_usage"] = gpu_memory
            print(f"ğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {gpu_memory:.2f}GB")
        
        # æµ‹è¯•æ¯ä¸ªé—®é¢˜
        for i, question in enumerate(questions):
            print(f"\n   ğŸ” é—®é¢˜ {i+1}: {question}")
            
            try:
                # æ„å»ºprompt
                prompt = f"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}"
                
                # è®°å½•å¼€å§‹æ—¶é—´
                start_time = time.time()
                
                # ç”Ÿæˆå›ç­”
                response = generator.generate(texts=[prompt])[0]
                
                # è®°å½•ç»“æŸæ—¶é—´
                end_time = time.time()
                generation_time = end_time - start_time
                
                # ç»Ÿè®¡tokenæ•°é‡
                response_tokens = len(response.split())
                
                print(f"   âœ… ç”ŸæˆæˆåŠŸ")
                print(f"      å›ç­”: {response[:100]}...")
                print(f"      é•¿åº¦: {response_tokens} tokens")
                print(f"      æ—¶é—´: {generation_time:.2f}s")
                
                results["questions"].append({
                    "question": question,
                    "response": response,
                    "tokens": response_tokens,
                    "time": generation_time,
                    "success": True
                })
                
                results["success_count"] += 1
                results["total_time"] += generation_time
                
            except Exception as e:
                print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
                results["questions"].append({
                    "question": question,
                    "response": f"ç”Ÿæˆå¤±è´¥: {e}",
                    "tokens": 0,
                    "time": 0,
                    "success": False
                })
        
        # è®¡ç®—å¹³å‡tokenæ•°
        successful_responses = [q for q in results["questions"] if q["success"]]
        if successful_responses:
            results["avg_tokens"] = sum(q["tokens"] for q in successful_responses) / len(successful_responses)
        
        # æ¸…ç†å†…å­˜
        del generator
        clear_gpu_memory()
        
    except Exception as e:
        print(f"âŒ {model_name} åˆå§‹åŒ–å¤±è´¥: {e}")
        results["error"] = str(e)
    
    return results


def save_results(results: Dict[str, Any], filename: str):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")


def compare_multiple_models(
    model_names: List[str], 
    questions: List[str],
    device: str = "cuda:1"
) -> Dict[str, Dict[str, Any]]:
    """æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„ç»“æœ"""
    all_results = {}
    
    for model_name in model_names:
        print(f"\n{'='*20} æµ‹è¯• {model_name} {'='*20}")
        results = test_model_with_alphafin_questions(
            model_name=model_name,
            questions=questions,
            device=device
        )
        all_results[model_name] = results
        
        # ä¿å­˜å•ä¸ªæ¨¡å‹ç»“æœ
        safe_model_name = model_name.replace('/', '_').replace('-', '_')
        save_results(results, f"{safe_model_name}_alphafin_results.json")
        
        # ç­‰å¾…ç”¨æˆ·ç¡®è®¤æ˜¯å¦ç»§ç»­
        if model_name != model_names[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ªæ¨¡å‹
            try:
                choice = input(f"\nğŸ¤” æ˜¯å¦ç»§ç»­æµ‹è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹ï¼Ÿ(y/n): ").lower().strip()
                if choice not in ['y', 'yes', 'æ˜¯']:
                    print("ğŸ‘‹ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
                    break
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
                break
    
    return all_results


def generate_comparison_report(all_results: Dict[str, Dict[str, Any]], output_file: str = "model_comparison_report.md"):
    """ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š"""
    print(f"\nğŸ“Š ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š...")
    
    report = f"""# æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š - AlphaFinæ•°æ®é›†

## ğŸ“‹ æµ‹è¯•æ¦‚è¿°

- æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
- æµ‹è¯•é—®é¢˜æ•°é‡: {len(all_results[list(all_results.keys())[0]]['questions']) if all_results else 0}
- æµ‹è¯•æ¨¡å‹æ•°é‡: {len(all_results)}

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | æˆåŠŸç‡ | å¹³å‡æ—¶é—´(s) | å¹³å‡Tokenæ•° | GPUå†…å­˜(GB) | çŠ¶æ€ |
|------|--------|-------------|-------------|-------------|------|
"""
    
    for model_name, results in all_results.items():
        success_count = results.get('success_count', 0)
        total_questions = len(results.get('questions', []))
        success_rate = f"{success_count}/{total_questions} ({success_count/total_questions*100:.1f}%)" if total_questions > 0 else "0/0 (0%)"
        
        avg_time = results.get('total_time', 0) / success_count if success_count > 0 else 0
        avg_tokens = results.get('avg_tokens', 0)
        memory_usage = results.get('memory_usage', 0)
        
        status = "âœ… æˆåŠŸ" if 'error' not in results else "âŒ å¤±è´¥"
        
        report += f"| {model_name} | {success_rate} | {avg_time:.2f} | {avg_tokens:.1f} | {memory_usage:.2f} | {status} |\n"
    
    report += f"""
## ğŸ“ è¯¦ç»†ç»“æœ

"""
    
    for model_name, results in all_results.items():
        report += f"### {model_name}\n\n"
        
        if 'error' in results:
            report += f"**é”™è¯¯**: {results['error']}\n\n"
        else:
            report += f"- **æˆåŠŸç‡**: {results['success_count']}/{len(results['questions'])}\n"
            report += f"- **å¹³å‡æ—¶é—´**: {results['total_time']/results['success_count']:.2f}s\n"
            report += f"- **å¹³å‡Tokenæ•°**: {results['avg_tokens']:.1f}\n"
            report += f"- **GPUå†…å­˜**: {results['memory_usage']:.2f}GB\n\n"
            
            report += "**ç¤ºä¾‹å›ç­”**:\n\n"
            for i, q_result in enumerate(results['questions'][:3]):  # åªæ˜¾ç¤ºå‰3ä¸ª
                if q_result['success']:
                    report += f"{i+1}. **é—®é¢˜**: {q_result['question']}\n"
                    report += f"   **å›ç­”**: {q_result['response'][:200]}...\n\n"
    
    report += f"""
## ğŸ¯ æ€»ç»“

"""
    
    # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
    best_model = None
    best_score = 0
    
    for model_name, results in all_results.items():
        if 'error' not in results:
            score = results['success_count'] / len(results['questions'])
            if score > best_score:
                best_score = score
                best_model = model_name
    
    if best_model:
        report += f"- **æœ€ä½³æ¨¡å‹**: {best_model} (æˆåŠŸç‡: {best_score*100:.1f}%)\n"
    
    report += f"- **æµ‹è¯•å®Œæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ æ¯”è¾ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="ä½¿ç”¨AlphaFinæ•°æ®é›†æ¯”è¾ƒä¸åŒæ¨¡å‹")
    parser.add_argument("--model_names", nargs="+", 
                       default=["Qwen/Qwen3-8B", "SUFE-AIFLM-Lab/Fin-R1"],
                       help="è¦æµ‹è¯•çš„æ¨¡å‹åç§°åˆ—è¡¨")
    parser.add_argument("--data_path", type=str, 
                       default="evaluate_mrr/alphafin_train_qc.jsonl",
                       help="AlphaFinæ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max_questions", type=int, default=5,
                       help="æœ€å¤§æµ‹è¯•é—®é¢˜æ•°é‡")
    parser.add_argument("--device", type=str, default="cuda:1",
                       help="GPUè®¾å¤‡")
    parser.add_argument("--output_dir", type=str, default="model_comparison_results",
                       help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    print("ğŸ§ª AlphaFinæ•°æ®é›†æ¨¡å‹æ¯”è¾ƒæµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨ï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        args.device = "cpu"
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åŠ è½½AlphaFiné—®é¢˜
    print(f"\nğŸ“š åŠ è½½AlphaFiné—®é¢˜...")
    questions = load_alphafin_questions(args.data_path, args.max_questions)
    
    # æ˜¾ç¤ºé—®é¢˜
    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜:")
    for i, question in enumerate(questions):
        print(f"   {i+1}. {question}")
    
    # æ¯”è¾ƒæ¨¡å‹
    print(f"\nğŸ” å¼€å§‹æ¯”è¾ƒæ¨¡å‹: {args.model_names}")
    all_results = compare_multiple_models(
        model_names=args.model_names,
        questions=questions,
        device=args.device
    )
    
    # ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š
    report_path = os.path.join(args.output_dir, "model_comparison_report.md")
    generate_comparison_report(all_results, report_path)
    
    print(f"\nğŸ‰ æ¨¡å‹æ¯”è¾ƒæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {args.output_dir}")


if __name__ == "__main__":
    main() 