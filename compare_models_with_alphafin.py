#!/usr/bin/env python3
"""
ä½¿ç”¨AlphaFinæ•°æ®é›†ä¸­çš„é—®é¢˜ï¼Œæµ‹è¯•RAGåœºæ™¯ä¸‹çš„Generator LLMæ•ˆæœ
æ¨¡å‹å°†æ¥æ”¶é—®é¢˜å’Œä¸Šä¸‹æ–‡ï¼Œå¹¶ç”Ÿæˆå›ç­”
"""

import os
import sys
import json
import torch
import time
import argparse
import random
import textwrap
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from xlm.components.generator.local_llm_generator import LocalLLMGenerator


def load_alphafin_qa_pairs(data_path: str, max_questions: int = 10) -> List[Dict[str, str]]:
    """
    ä»AlphaFinæ•°æ®é›†åŠ è½½é—®ç­”å¯¹ï¼ŒåŒ…å«é—®é¢˜ã€ä¸Šä¸‹æ–‡å’Œç­”æ¡ˆã€‚
    æ”¯æŒjsonlå’Œjsonæ•°ç»„æ ¼å¼ï¼Œä¼˜å…ˆä½¿ç”¨generated_questionå­—æ®µã€‚
    """
    qa_pairs = []
    try:
        with open(data_path, 'r', encoding='utf-8') as f:
            first_char = f.read(1)
            f.seek(0)
            if first_char == '[':
                data_list = json.load(f)
            else: # Assume JSONL format
                data_list = [json.loads(line.strip()) for line in f if line.strip()]
        
        candidates = []
        for item in data_list:
            question_text = item.get('generated_question') # LLMç”Ÿæˆçš„é—®é¢˜
            context_text = item.get('original_context') # åŸå§‹ä¸Šä¸‹æ–‡
            answer_text = item.get('original_answer') # åŸå§‹ç­”æ¡ˆ

            # Fallback to original_question/query if generated_question is missing or null
            if not question_text:
                question_text = item.get('original_question') or item.get('query')

            if question_text and context_text and answer_text:
                qa_pairs.append({
                    "question": question_text,
                    "context": context_text,
                    "answer": answer_text # Include original answer for reference/analysis
                })
        
        # Randomly sample max_questions pairs if the list is too long
        if len(qa_pairs) > max_questions:
            qa_pairs = random.sample(qa_pairs, max_questions)
        
        print(f"âœ… åŠ è½½äº† {len(qa_pairs)} ä¸ªé—®ç­”å¯¹")
        return qa_pairs

    except Exception as e:
        print(f"âŒ åŠ è½½AlphaFinæ•°æ®å¤±è´¥: {e}")
        # Fallback to default questions if data loading fails
        return [
            {"question": "ä»€ä¹ˆæ˜¯è‚¡ç¥¨æŠ•èµ„ï¼Ÿ", "context": "è‚¡ç¥¨æ˜¯è‚¡ä»½å…¬å¸æ‰€æœ‰æƒçš„ä¸€éƒ¨åˆ†ï¼Œä¹Ÿæ˜¯å‘è¡Œçš„æ‰€æœ‰æƒå‡­è¯ï¼Œæ˜¯è‚¡ä»½å…¬å¸ä¸ºç­¹é›†èµ„é‡‘è€Œå‘è¡Œç»™å„ä¸ªè‚¡ä¸œä½œä¸ºæŒè‚¡å‡­è¯å¹¶å€Ÿä»¥å–å¾—è‚¡æ¯å’Œçº¢åˆ©çš„ä¸€ç§æœ‰ä»·è¯åˆ¸ã€‚", "answer": "è‚¡ç¥¨æ˜¯è‚¡ä»½å…¬å¸ä¸ºç­¹é›†èµ„é‡‘è€Œå‘è¡Œç»™è‚¡ä¸œä½œä¸ºæŒè‚¡å‡­è¯çš„ä¸€ç§æœ‰ä»·è¯åˆ¸ã€‚"},
            {"question": "è¯·è§£é‡Šå€ºåˆ¸çš„åŸºæœ¬æ¦‚å¿µ", "context": "å€ºåˆ¸æ˜¯ä¸€ç§æœ‰ä»·è¯åˆ¸ï¼Œæ˜¯ç¤¾ä¼šå„ç±»ç»æµä¸»ä½“ä¸ºç­¹é›†èµ„é‡‘è€Œå‘æŠ•èµ„è€…å‘è¡Œï¼Œæ‰¿è¯ºæŒ‰ä¸€å®šåˆ©ç‡æ”¯ä»˜åˆ©æ¯å¹¶æŒ‰çº¦å®šæ¡ä»¶å¿è¿˜æœ¬é‡‘çš„å€ºæƒå€ºåŠ¡å‡­è¯ã€‚", "answer": "å€ºåˆ¸æ˜¯å‘è¡Œè€…æ‰¿è¯ºæŒ‰ä¸€å®šåˆ©ç‡æ”¯ä»˜åˆ©æ¯å¹¶æŒ‰çº¦å®šæ¡ä»¶å¿è¿˜æœ¬é‡‘çš„å€ºæƒå€ºåŠ¡å‡­è¯ã€‚"}
        ]


def clear_gpu_memory():
    """æ¸…ç†GPUå†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        print("ğŸ§¹ GPUå†…å­˜å·²æ¸…ç†")


# --- æ–°å¢çš„åæœŸå¤„ç†å‡½æ•° (éœ€è¦æ ¹æ®ä½ çš„æ¨¡å‹å®é™…è¾“å‡ºè°ƒæ•´) ---
def post_process_generator_response(raw_response: str, prompt_template: str) -> str:
    """
    å¯¹Generator LLMçš„åŸå§‹è¾“å‡ºè¿›è¡ŒåæœŸå¤„ç†ï¼Œç§»é™¤ä¸å¿…è¦çš„å…ƒè¯„è®ºå’Œæ ¼å¼æ ‡è®°ï¼Œ
    åªä¿ç•™æ ¸å¿ƒç­”æ¡ˆã€‚
    """
    cleaned_text = raw_response.strip()

    # 1. ç§»é™¤æ¨¡å‹å¯¹Promptçš„å¤è¿°æˆ–å‰å¯¼è¯­
    # æ¯”å¦‚æ¨¡å‹å¯èƒ½é‡å¤ "è¯·åŸºäºä»¥ä¸‹æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
    # æ‰¾åˆ°Promptä¸­æœ€åçš„ç”¨æˆ·é—®é¢˜å’Œç­”æ¡ˆæŒ‡ç¤ºï¼Œç„¶ååœ¨æ­¤ä¹‹åå¼€å§‹æˆªå–
    # å¯»æ‰¾ Answer: è¿™æ ·çš„æ˜ç¡®æŒ‡ç¤ºï¼Œå¹¶åœ¨å…¶åå¼€å§‹
    answer_marker_in_prompt = re.search(r"ç­”æ¡ˆï¼š\s*$", prompt_template, re.MULTILINE)
    if answer_marker_in_prompt:
        # å°è¯•ä» Prompt ç»“æŸçš„åœ°æ–¹å¼€å§‹æˆªå–
        start_pos_in_raw = raw_response.find(prompt_template[answer_marker_in_prompt.start():])
        if start_pos_in_raw != -1:
            cleaned_text = raw_response[start_pos_in_raw:].strip()
            # ç§»é™¤ Answer: è‡ªèº«
            cleaned_text = re.sub(r"^\s*ç­”æ¡ˆï¼š\s*", "", cleaned_text, flags=re.MULTILINE).strip()


    # 2. å®šä¹‰å¸¸è§çš„å…ƒè¯„è®º/ä¸éœ€è¦å†…å®¹å¼€å¤´æ¨¡å¼ï¼ˆæ³¨æ„é¡ºåºï¼Œä»é•¿åˆ°çŸ­æˆ–ä»å…·ä½“åˆ°ä¸€èˆ¬ï¼‰
    unwanted_patterns = [
        r"^\s*\[\s*åˆ é™¤äº†\"[^\]]+\"åçš„å¥å­åŠæ‰€æœ‰éå¿…è¦æ–‡å­—\s*\]\s*", # "[åˆ é™¤äº†...]" è¿™ç§è‡ªæˆ‘è¯„ä»·
        r"^\s*æ³¨æ„ï¼šä¸¥æ ¼éµå¾ªæŒ‡ä»¤è¦æ±‚ï¼Œå»é™¤æ‰€æœ‰ä¸å¿…è¦çš„å†…å®¹ï¼Œä»…ä¿ç•™æ ¸å¿ƒä¿¡æ¯ã€‚\s*", # æ¨¡å‹ç»™è‡ªå·±ä¸‹æŒ‡ä»¤
        r"^\s*æ€»ç»“\s*", # æ€»ç»“æ ‡é¢˜
        r"^\s*è¯·æ³¨æ„ï¼Œä¸Šè¿°åˆ†ææ˜¯åŸºäºç»™å®šçš„ä¿¡æ¯è¿›è¡Œçš„è§£è¯»ï¼Œå¹¶æœªå¼•å…¥é¢å¤–çš„æ•°æ®æˆ–å‡è®¾ã€‚\s*", # å…è´£å£°æ˜
        r"^\s*[\-]{3,}\s*", # --- åˆ†éš”çº¿
        r"^\s*\\boxed{", # åŒ¹é… \boxed{
        r"^\s*æ ¹æ®è¦æ±‚ï¼Œ", # å¸¸è§çš„å‰ç¼€
        r"^\s*ä½†å¯¹ç…§ç¤ºä¾‹\dçš„ç»“æ„ï¼š", # æ¯”è¾ƒç¤ºä¾‹
        r"^\s*æœ€ç»ˆç‰ˆ", # æœ€ç»ˆç‰ˆæ ‡è®°
        r"^\s*è¿›ä¸€æ­¥å‹ç¼©", # å‹ç¼©è¯´æ˜
        r"^\s*ç„¶è€Œï¼Œ", # è¿™ç§è½¬æŠ˜è¯ï¼Œå¦‚æœä¸æ˜¯æ ¸å¿ƒç­”æ¡ˆä¸€éƒ¨åˆ†
        r"^\s*å› æ­¤ï¼Œ",
        r"^\s*æ‰€ä»¥ï¼Œ",
        r"^\s*ç»¼ä¸Šæ‰€è¿°ï¼Œ",
        r"^\s*è¯·æ ¹æ®ä¸Šè¿°è§„åˆ™å’Œç¤ºä¾‹ç»™å‡ºæ­£ç¡®ç­”æ¡ˆã€‚", # Clean Prompt çš„å¼€å¤´
        r"^\s*å†…å®¹ç•¥å»", # Clean Prompt çš„å¼€å¤´
        r"^\s*æŒ‰ç…§è§„å®šï¼Œç­”æ¡ˆä¸å¾—è¶…è¿‡\d+å­—ã€‚", # é•¿åº¦æç¤º
        r"^\s*\*\*é‡è¦è¦æ±‚ï¼š.*", # åŒ¹é…Promptä¸­çš„é‡è¦è¦æ±‚
        r"^\s*åŸºäºä¸Šè¿°ä¿¡æ¯çš„å›ç­”ï¼š\s*", # Simple Promptçš„å¼€å§‹
        r"^\s*ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆã€‚", # ç³»ç»Ÿçš„è‡ªæˆ‘ä»‹ç»
        r"^\s*è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š", # å¦‚æœæ¨¡å‹å¤è¿°äº†é—®é¢˜
        r"^\s*AIåŠ©æ‰‹éœ€è¦æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯åˆ†æ", # CoTçš„å¼€å¤´
        r"^\s*###.*", # Markdown æ ‡é¢˜
        r"^\s*[\-\*]\s+.*", # Markdown åˆ—è¡¨é¡¹
        r"^\s*\(\s*æ³¨ï¼š.*?\)" # å¼€å¤´å¸¦æ‹¬å·çš„æ³¨é‡Š
    ]
    
    # å¾ªç¯ç§»é™¤è¿™äº›æ¨¡å¼ï¼Œç›´åˆ°ä¸å†åŒ¹é…
    # ä½¿ç”¨ re.DOTALL å’Œ re.MULTILINE ç¡®ä¿èƒ½åŒ¹é…å¤šè¡Œå’Œè¡Œå¼€å¤´
    # ä½¿ç”¨ re.IGNORECASE å¿½ç•¥å¤§å°å†™
    for pattern_str in unwanted_patterns:
        pattern = re.compile(pattern_str, re.IGNORECASE | re.DOTALL | re.MULTILINE)
        old_text = cleaned_text
        cleaned_text = pattern.sub("", cleaned_text).strip()
        if cleaned_text == old_text and pattern_str.startswith(r"^\s*"): # ä¼˜åŒ–ï¼šå¦‚æœæ²¡å˜åŒ–ä¸”æ˜¯å¼€å¤´æ¨¡å¼ï¼Œåˆ™åœæ­¢å¾ªç¯
            break
            
    # å¦‚æœå›ç­”ä»¥ "æ€»ç»“ï¼š" å¼€å¤´ï¼Œä¸”ä¸åº”å¦‚æ­¤ï¼Œå°è¯•åˆ é™¤
    cleaned_text = re.sub(r"^\s*æ€»ç»“ï¼š\s*", "", cleaned_text, flags=re.MULTILINE).strip()

    # ç§»é™¤æ‰€æœ‰å¯èƒ½çš„å°¾éƒ¨ä¸å®Œæ•´å¥æˆ–æ ¼å¼æ ‡è®°
    # ä»æœ«å°¾å‘å‰åŒ¹é…å¹¶åˆ é™¤å¸¸è§çš„æ”¶å°¾å…ƒè¯„è®º
    unwanted_tail_patterns = [
        r"[\s\S]*?(?:---|\*+\s*æ€»ç»“|\s*\\boxed)", # åŒ¹é… ---, **æ€»ç»“**, \boxed ä»¥åŠä¹‹å‰æ‰€æœ‰å†…å®¹
        r"\s*æ ¹æ®è¦æ±‚ï¼Œ.*", # åŒ¹é… "æ ¹æ®è¦æ±‚ï¼Œ" åŠä¹‹åæ‰€æœ‰å†…å®¹
        r"\s*è¯·æ³¨æ„ï¼Œä¸Šè¿°åˆ†ææ˜¯åŸºäºç»™å®šçš„ä¿¡æ¯è¿›è¡Œçš„è§£è¯».*", # åŒ¹é… "è¯·æ³¨æ„ï¼Œ..." åŠä¹‹åæ‰€æœ‰å†…å®¹
        r"\s*ç»¼ä¸Šæ‰€è¿°ï¼Œ.*", # åŒ¹é… "ç»¼ä¸Šæ‰€è¿°ï¼Œ" åŠä¹‹åæ‰€æœ‰å†…å®¹
        r"\s*ä½†å¯¹ç…§ç¤ºä¾‹\dçš„ç»“æ„ï¼š.*", # åŒ¹é… "ä½†å¯¹ç…§ç¤ºä¾‹..." åŠä¹‹åæ‰€æœ‰å†…å®¹
        r"\s*æœ€ç»ˆç‰ˆ.*", # åŒ¹é… "æœ€ç»ˆç‰ˆ" åŠä¹‹åæ‰€æœ‰å†…å®¹
        r"\s*è¿›ä¸€æ­¥å‹ç¼©.*", # åŒ¹é… "è¿›ä¸€æ­¥å‹ç¼©" åŠä¹‹åæ‰€æœ‰å†…å®¹
        r"\s*è¯·æ ¹æ®ä¸Šè¿°è§„åˆ™å’Œç¤ºä¾‹ç»™å‡ºæ­£ç¡®ç­”æ¡ˆ.*",
        r"\s*å¦‚æœä½ èƒ½æä¾›æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯.*",
        r"\s*æˆ‘æ— æ³•ç›´æ¥è®¿é—®æœ€æ–°çš„å¸‚åœºç ”ç©¶æŠ¥å‘Šæˆ–æ–°é—»åŠ¨æ€.*",
        r"\s*ä»¥ä¸‹æ˜¯è¯¦ç»†åˆ†æ.*", # åŒ¹é…"ä»¥ä¸‹æ˜¯è¯¦ç»†åˆ†æ"åŠä¹‹åæ‰€æœ‰å†…å®¹
        r"\s*æœ€ç»ˆç­”æ¡ˆï¼š.*", # åŒ¹é…"æœ€ç»ˆç­”æ¡ˆï¼š"åŠä¹‹åæ‰€æœ‰å†…å®¹
        r"\s*ç”¨æˆ·é—®é¢˜ï¼š.*", # åŒ¹é…"ç”¨æˆ·é—®é¢˜ï¼š"åŠä¹‹åæ‰€æœ‰å†…å®¹
    ]
    for pattern_str in unwanted_tail_patterns:
        pattern = re.compile(pattern_str, re.IGNORECASE | re.DOTALL) # DOTALL for multiline match
        match = pattern.search(cleaned_text)
        if match:
            # æ‰¾åˆ°åŒ¹é…é¡¹ï¼Œä»åŒ¹é…çš„å¼€å¤´å¤„æˆªæ–­
            cleaned_text = cleaned_text[:match.start()].strip()
            # å¦‚æœæˆªæ–­åä¸ºç©ºï¼Œåˆ™å¯èƒ½æ˜¯æ•´ä¸ªå›ç­”éƒ½æ˜¯åƒåœ¾ï¼Œå°è¯•ä¿ç•™åŸå§‹å›ç­”çš„ç¬¬ä¸€å¥
            if not cleaned_text:
                first_sentence_match = re.search(r'^(.+?[ã€‚ï¼Ÿï¼])', raw_response.strip(), re.DOTALL)
                if first_sentence_match:
                    cleaned_text = first_sentence_match.group(1).strip()
    
    # ç§»é™¤æ‰€æœ‰å¯èƒ½çš„å°¾éƒ¨ä¸å®Œæ•´å¥æˆ–æ ¼å¼æ ‡è®°çš„å‰©ä½™éƒ¨åˆ†
    cleaned_text = re.sub(r'[\s\S]*?\s*(?:è¯·ç»™å‡ºè¯¦ç»†åˆ†æ|å…³é”®ä¿¡æ¯ç‚¹åŒ…æ‹¬|ç°åœ¨ï¼Œè¯·ä½ ä»¥æ›´ç®€ç»ƒçš„æ–¹å¼|å¥½çš„ï¼Œæˆ‘éœ€è¦å¤„ç†ç”¨æˆ·çš„é—®é¢˜|ä»¥ä¸‹æ˜¯æˆ‘çš„åˆ†æ|æ€»ç»“|\*\*æ€»ç»“\*\*|^\s*\d+\.\s+\*\*.*\*\*|\*\*æ€»ç»“\*\*|^\s*ç»¼ä¸Šæ‰€è¿°|^\s*è¯·æ³¨æ„)', '', cleaned_text, flags=re.DOTALL | re.MULTILINE).strip()
    
    # ç§»é™¤æ‰€æœ‰ Markdown æ ¼å¼æ ‡è®°ï¼ˆ#ï¼Œ**ï¼Œ*ï¼Œ-ï¼Œæ•°å­—åˆ—è¡¨å¼€å¤´ï¼‰
    cleaned_text = re.sub(r'^\s*#+\s*', '', cleaned_text, flags=re.MULTILINE).strip() # ç§»é™¤æ ‡é¢˜
    cleaned_text = re.sub(r'\*\*', '', cleaned_text).strip() # ç§»é™¤ç²—ä½“
    cleaned_text = re.sub(r'\*\s*', '', cleaned_text).strip() # ç§»é™¤åˆ—è¡¨é¡¹æ˜Ÿå·
    cleaned_text = re.sub(r'^\s*\d+\.\s*', '', cleaned_text, flags=re.MULTILINE).strip() # ç§»é™¤æ•°å­—åˆ—è¡¨å¼€å¤´
    cleaned_text = re.sub(r'^\s*-\s*', '', cleaned_text, flags=re.MULTILINE).strip() # ç§»é™¤åˆ—è¡¨é¡¹æ¨ªçº¿

    # ç§»é™¤ç©ºè¡Œå’Œå¤šä½™çš„ç©ºç™½
    cleaned_text = re.sub(r'\n\s*\n+', '\n\n', cleaned_text).strip()
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

    # ç¡®ä¿å›ç­”ä»¥å®Œæ•´å¥å­ç»“æŸï¼Œå¦‚æœæ²¡æœ‰ï¼Œå°è¯•æˆªå–åˆ°æœ€è¿‘çš„å®Œæ•´å¥
    if cleaned_text and not re.search(r'[ã€‚ï¼Ÿï¼]$', cleaned_text):
        last_sentence_match = re.search(r'(.+?[ã€‚ï¼Ÿï¼])', cleaned_text[::-1], re.DOTALL) # å€’åºæ‰¾ç¬¬ä¸€ä¸ªå¥å·ç­‰
        if last_sentence_match:
            cleaned_text = last_sentence_match.group(1)[::-1].strip() # å€’åºåè½¬å›æ¥å–å›å®Œæ•´å¥å­
        else:
            # å¦‚æœæ‰¾ä¸åˆ°å®Œæ•´çš„å¥å­ï¼Œå°è¯•å–åˆ°æœ€åä¸€ä¸ªå®Œæ•´è¯è¯­çš„æœ«å°¾ï¼Œé¿å…è¢«æˆªæ–­çš„ä¹±ç 
            cleaned_text = re.sub(r'[^ã€‚ï¼Ÿï¼\s]*$', '', cleaned_text).strip()

    # å¦‚æœæ¸…ç†åå˜ä¸ºç©ºï¼Œå°è¯•ä¿ç•™åŸå§‹å›ç­”çš„ç¬¬ä¸€å¥æˆ–ç¬¬ä¸€æ®µ
    if not cleaned_text and raw_response.strip():
        first_paragraph_match = re.search(r'^([^\n]+\n)*[^\n]*?[ã€‚ï¼Ÿï¼]', raw_response.strip(), re.DOTALL)
        if first_paragraph_match:
            cleaned_text = first_paragraph_match.group(0).strip()
        else: # å®åœ¨ä¸è¡Œï¼Œå°±å–åŸå§‹å›ç­”çš„å‰100å­—
            cleaned_text = raw_response.strip()[:150] + "..." if len(raw_response.strip()) > 150 else raw_response.strip()


    return cleaned_text

# --- ä¸»è¦å‡½æ•° (ç”¨äºæµ‹è¯•Generator LLM) ---
def test_generator_llm(
    model_name: str, 
    qa_pairs: List[Dict[str, str]], 
    device: str = "cuda:1", 
    max_new_tokens: int = 500, 
    temperature: float = 0.2,
    top_p: float = 0.8,
    use_quantization: bool = True,
    quantization_type: str = "4bit"
) -> Dict[str, Any]:
    print(f"\nğŸš€ å¼€å§‹æµ‹è¯•Generator LLMæ¨¡å‹: {model_name}")
    print(f"   è®¾å¤‡: {device}")
    print(f"   æµ‹è¯•é—®ç­”å¯¹æ•°é‡: {len(qa_pairs)}")
    
    clear_gpu_memory()
    
    results = {
        "model_name": model_name,
        "device": device,
        "qa_results": [], 
        "success_count": 0,
        "total_time": 0,
        "avg_tokens": 0,
        "memory_usage": 0,
        "config": {
            "max_new_tokens": max_new_tokens,
            "temperature": temperature,
            "top_p": top_p,
            "use_quantization": use_quantization,
            "quantization_type": quantization_type
        }
    }
    
    try:
        print(f"ğŸ”§ åˆå§‹åŒ– {model_name}...")
        generator = LocalLLMGenerator(
            model_name=model_name,
            device=device,
            use_quantization=use_quantization,
            quantization_type=quantization_type
        )
        print(f"âœ… {model_name} åˆå§‹åŒ–æˆåŠŸ")
        
        if torch.cuda.is_available():
            gpu_id = int(device.split(':')[1]) if ':' in device else 0
            results["memory_usage"] = torch.cuda.memory_allocated(device=gpu_id) / 1024**3
            print(f"ğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {results['memory_usage']:.2f}GB")
        
        for i, qa_pair in enumerate(qa_pairs):
            question = qa_pair["question"]
            context = qa_pair["context"]
            original_answer = qa_pair["answer"] 

            print(f"\n   ğŸ” é—®é¢˜ {i+1}: {question}")
            print(f"     ä¸Šä¸‹æ–‡ (å‰100å­—): {context[:100]}...")
            print(f"     åŸå§‹ç­”æ¡ˆ: {original_answer}")
            
            prompt_template = textwrap.dedent(f"""
            ä½ æ˜¯ä¸€åä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ï¼Œç›´æ¥ã€å‡†ç¡®åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
            
            é‡è¦è¦æ±‚ï¼š
            1. **åªä½¿ç”¨ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘å›ç­”é—®é¢˜ï¼Œä¸è¦æ·»åŠ ä»»ä½•å¤–éƒ¨çŸ¥è¯†æˆ–çŒœæµ‹ã€‚**
            2. **å¦‚æœã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ä¸­æ²¡æœ‰è¶³å¤Ÿçš„ç›¸å…³ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„ä¿¡æ¯æ— æ³•å›ç­”æ­¤é—®é¢˜"ã€‚**
            3. **å›ç­”å¿…é¡»ç®€æ´ã€ç›´æ¥ã€å®Œæ•´ï¼Œç”¨è‡ªç„¶çš„ä¸­æ–‡è¡¨è¾¾ã€‚**
            4. **æåº¦é‡è¦ï¼šä½ çš„è¾“å‡ºå¿…é¡»æ˜¯çº¯ç²¹ã€ç›´æ¥çš„å›ç­”ï¼Œä¸åŒ…å«ä»»ä½•è‡ªæˆ‘åæ€ã€æ€è€ƒè¿‡ç¨‹ã€å¯¹Promptçš„åˆ†æã€ä¸å›ç­”æ— å…³çš„é¢å¤–æ³¨é‡Šã€ä»»ä½•æ ¼å¼æ ‡è®°ï¼ˆå¦‚ \\boxed{{}}ã€æ•°å­—åˆ—è¡¨ã€åŠ ç²—ï¼‰ã€æˆ–ä»»ä½•å½¢å¼çš„å…ƒè¯„è®ºã€‚è¯·å‹¿å¼•ç”¨æˆ–å¤è¿°Promptå†…å®¹ã€‚ä½ çš„å›ç­”å¿…é¡»ç›´æ¥ã€ç®€æ´åœ°ç»“æŸï¼Œä¸å¸¦ä»»ä½•å¼•å¯¼è¯­æˆ–åç»­è¯´æ˜ã€‚**
            5. **å›ç­”ä¸­åŠ¡å¿…æåŠã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ä¸­æ¶‰åŠåˆ°çš„å…¬å¸åç§°å’Œè‚¡ç¥¨ä»£ç ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œä»¥æé«˜å¯è§£é‡Šæ€§ã€‚**
            
            ã€ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‘ï¼š
            {context}
            
            ç”¨æˆ·é—®é¢˜ï¼š{question}
            
            ç­”æ¡ˆï¼š
            """).strip()

            start_time = time.time()
            # Pass max_new_tokens directly to generate, as it's a generator setting
            response = generator.generate(
                texts=[prompt_template]
            )[0] 
            end_time = time.time()
            generation_time = end_time - start_time
            
            clean_response = post_process_generator_response(response, prompt_template) # è°ƒç”¨åæœŸå¤„ç†å‡½æ•°
            
            response_tokens = len(generator.tokenizer.encode(clean_response)) # ä½¿ç”¨æ¨¡å‹åˆ†è¯å™¨è®¡ç®—tokens
            
            print(f"   âœ… ç”ŸæˆæˆåŠŸ")
            print(f"     åŸå§‹å›ç­” (å‰200å­—): {response[:200]}...")
            print(f"     æ¸…ç†åå›ç­” (å‰200å­—): {clean_response[:200]}...")
            print(f"     é•¿åº¦ (æ¸…ç†å): {response_tokens} tokens") # æŒ‡æ˜æ˜¯æ¸…ç†åçš„tokens
            print(f"     æ—¶é—´: {generation_time:.2f}s")
            
            results["qa_results"].append({
                "question": question,
                "context": context, 
                "original_answer": original_answer, 
                "raw_response": response, 
                "clean_response": clean_response, 
                "tokens": response_tokens,
                "time": generation_time,
                "success": True 
            })
            results["success_count"] += 1
            results["total_time"] += generation_time
            
        successful_responses_tokens = [q["tokens"] for q in results["qa_results"] if q["success"]]
        if successful_responses_tokens:
            results["avg_tokens"] = sum(successful_responses_tokens) / len(successful_responses_tokens)
        
        del generator
        clear_gpu_memory()
        
    except Exception as e:
        print(f"âŒ {model_name} åˆå§‹åŒ–æˆ–ç”Ÿæˆå¤±è´¥: {e}")
        results["error"] = str(e)
    
    return results

def save_results(results: Dict[str, Any], filename: str):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {filename}")


def compare_multiple_models(
    model_names: List[str], 
    qa_pairs: List[Dict[str, str]],
    device: str = "cuda:1" # Default device set for testing
) -> Dict[str, Dict[str, Any]]:
    all_results = {}
    
    for model_name in model_names:
        print(f"\n{'='*20} å¼€å§‹æµ‹è¯• {model_name} {'='*20}")
        results = test_generator_llm(
            model_name=model_name,
            qa_pairs=qa_pairs,
            device=device
        )
        all_results[model_name] = results
        
        safe_model_name = model_name.replace('/', '_').replace('-', '_')
        save_results(results, f"{safe_model_name}_generator_llm_results.json")
        
        if model_name != model_names[-1]: 
            try:
                choice = input(f"\nğŸ¤” æ˜¯å¦ç»§ç»­æµ‹è¯•ä¸‹ä¸€ä¸ªæ¨¡å‹ï¼Ÿ(y/n): ").lower().strip()
                if choice not in ['y', 'yes', 'æ˜¯']:
                    print("ğŸ‘‹ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
                    break
            except KeyboardInterrupt:
                print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
                break
    
    return all_results


def generate_comparison_report(all_results: Dict[str, Dict[str, Any]], output_file: str = "generator_llm_comparison_report.md"):
    print(f"\nğŸ“Š ç”Ÿæˆæ¯”è¾ƒæŠ¥å‘Š...")
    
    report = f"""# Generator LLM æ¨¡å‹æ¯”è¾ƒæŠ¥å‘Š - AlphaFinæ•°æ®é›†

## ğŸ“‹ æµ‹è¯•æ¦‚è¿°

- æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}
- æµ‹è¯•é—®ç­”å¯¹æ•°é‡: {len(all_results[list(all_results.keys())[0]]['qa_results']) if all_results and 'qa_results' in all_results[list(all_results.keys())[0]] else 0}
- æµ‹è¯•æ¨¡å‹æ•°é‡: {len(all_results)}

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | æˆåŠŸç‡ | å¹³å‡æ—¶é—´(s) | å¹³å‡Tokenæ•° | GPUå†…å­˜(GB) | çŠ¶æ€ |
|------|--------|-------------|-------------|-------------|------|
"""
    
    for model_name, results in all_results.items():
        success_count = results.get('success_count', 0)
        total_questions = len(results.get('qa_results', [])) # Changed "questions" to "qa_results"
        success_rate = f"{success_count}/{total_questions} ({success_count/total_questions*100:.1f}%)" if total_questions > 0 else "0/0 (0%)"
        
        avg_time = results.get('total_time', 0) / success_count if success_count > 0 else 0
        avg_tokens = results.get('avg_tokens', 0)
        memory_usage = results.get('memory_usage', 0)
        
        status = "âœ… æˆåŠŸ" if 'error' not in results else "âŒ å¤±è´¥"
        
        report += f"| {model_name} | {success_rate} | {avg_time:.2f} | {avg_tokens:.1f} | {memory_usage:.2f} | {status} |\n"
    
    report += f"""
## ğŸ“ è¯¦ç»†ç»“æœ

"""
    
    for model_name, results in all_results.items():
        report += f"### {model_name}\n\n"
        
        if 'error' in results:
            report += f"**é”™è¯¯**: {results['error']}\n\n"
        else:
            report += f"- **æˆåŠŸç‡**: {results['success_count']}/{len(results['qa_results'])}\n" # Changed "questions" to "qa_results"
            report += f"- **å¹³å‡æ—¶é—´**: {results['total_time']/results['success_count']:.2f}s\n"
            report += f"- **å¹³å‡Tokenæ•°**: {results['avg_tokens']:.1f}\n"
            report += f"- **GPUå†…å­˜**: {results['memory_usage']:.2f}GB\n\n"
            
            report += "**ç¤ºä¾‹å›ç­”**:\n\n"
            for i, q_result in enumerate(results['qa_results'][:3]): # Changed "questions" to "qa_results"
                if q_result['success']:
                    report += f"{i+1}. **é—®é¢˜**: {q_result['question']}\n"
                    report += f"   **åŸå§‹ä¸Šä¸‹æ–‡ (å‰100å­—)**: {q_result['context'][:100]}...\n" # Added context for reference
                    report += f"   **åŸå§‹ç­”æ¡ˆ**: {q_result['original_answer']}\n" # Added original answer
                    report += f"   **åŸå§‹æ¨¡å‹å›ç­” (å‰200å­—)**: {q_result['raw_response'][:200]}...\n" # Raw response
                    report += f"   **æ¸…ç†åå›ç­” (å‰200å­—)**: {q_result['clean_response'][:200]}...\n\n" # Cleaned response
    
    report += f"""
## ğŸ¯ æ€»ç»“

"""
    
    best_model = None
    best_success_rate = -1
    
    for model_name, results in all_results.items():
        if 'error' not in results:
            success_rate = results['success_count'] / len(results['qa_results']) if results['qa_results'] else 0
            if success_rate > best_success_rate:
                best_success_rate = success_rate
                best_model = model_name
    
    if best_model:
        report += f"- **æœ€ä½³æ¨¡å‹**: {best_model} (æˆåŠŸç‡: {best_success_rate*100:.1f}%)\n"
    
    report += f"- **æµ‹è¯•å®Œæˆæ—¶é—´**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n"
    
    save_dir = Path(output_file).parent
    save_dir.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ æ¯”è¾ƒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


def main():
    print("ğŸ§ª AlphaFinæ•°æ®é›†Generator LLMæ¯”è¾ƒæµ‹è¯•")
    print("=" * 50)
    
    parser = argparse.ArgumentParser(description="ä½¿ç”¨AlphaFinæ•°æ®é›†æ¯”è¾ƒä¸åŒæ¨¡å‹")
    parser.add_argument("--model_names", nargs="+", 
                         default=["SUFE-AIFLM-Lab/Fin-R1", "Qwen/Qwen3-8B"], # Added Fin-R1 and Qwen3-8B for comparison
                         help="è¦æµ‹è¯•çš„æ¨¡å‹åç§°åˆ—è¡¨")
    parser.add_argument("--data_path", type=str, 
                         default="data/alphafin/alphafin_summarized_and_structured_qa_0627_colab_backward.json", # Updated default data path
                         help="AlphaFinæ•°æ®æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…å«é—®é¢˜ã€ä¸Šä¸‹æ–‡å’ŒåŸå§‹ç­”æ¡ˆ")
    parser.add_argument("--max_questions", type=int, default=5, # Reduced default questions to avoid high compute units on testing
                         help="æœ€å¤§æµ‹è¯•é—®é¢˜æ•°é‡")
    parser.add_argument("--device", type=str, default="cuda:1",
                         help="GPUè®¾å¤‡")
    parser.add_argument("--output_dir", type=str, default="model_comparison_results",
                         help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # Check CUDA availability
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨ï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        args.device = "cpu"
    
    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)
    
    # Load AlphaFin questions
    print(f"\nğŸ“š åŠ è½½AlphaFiné—®ç­”å¯¹...")
    qa_pairs = load_alphafin_qa_pairs(args.data_path, args.max_questions)
    
    # Display questions (now qa_pairs contain context and answer)
    print(f"\nğŸ“ æµ‹è¯•é—®ç­”å¯¹:")
    for i, qa_pair in enumerate(qa_pairs):
        print(f"   {i+1}. é—®é¢˜: {qa_pair['question']}")
        print(f"      ä¸Šä¸‹æ–‡ (å‰50å­—): {qa_pair['context'][:50]}...")
        print(f"      åŸå§‹ç­”æ¡ˆ: {qa_pair['answer']}")
    
    # Compare models
    print(f"\nğŸ” å¼€å§‹æ¯”è¾ƒæ¨¡å‹: {args.model_names}")
    all_results = compare_multiple_models(
        model_names=args.model_names,
        qa_pairs=qa_pairs,
        device=args.device
    )
    
    # Generate comparison report
    report_path = os.path.join(args.output_dir, "generator_llm_comparison_report.md")
    generate_comparison_report(all_results, report_path)
    
    print(f"\nğŸ‰ æ¨¡å‹æ¯”è¾ƒæµ‹è¯•å®Œæˆï¼")
    print(f"ğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: {args.output_dir}")


if __name__ == "__main__":
    main()