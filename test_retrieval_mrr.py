#!/usr/bin/env python3
"""
æµ‹è¯•æ£€ç´¢è´¨é‡ - MRRè¯„ä¼°
ä½¿ç”¨evaluate_mrr/alphafin_eval.jsonlå’Œtatqa_eval.jsonlä½œä¸ºæµ‹è¯•é›†
æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. è¯„ä¼°æ•°æ®contextåŠ å…¥çŸ¥è¯†åº“ - æµ‹è¯•çœŸå®æ£€ç´¢èƒ½åŠ›
2. è¯„ä¼°æ•°æ®contextä¸åŠ å…¥çŸ¥è¯†åº“ - é¿å…æ•°æ®æ³„éœ²

æ”¹è¿›çš„åŒ¹é…ç­–ç•¥ï¼š
1. IDåŒ¹é…ï¼ˆæœ€é²æ£’ï¼‰
2. å†…å®¹å“ˆå¸ŒåŒ¹é…
3. ç›¸ä¼¼åº¦åŒ¹é…
4. æ¨¡ç³Šæ–‡æœ¬åŒ¹é…
"""

import sys
import os
import json
import numpy as np
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from tqdm import tqdm

sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥å¿…è¦çš„ç±»å‹
from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata

def load_eval_data(eval_file: str) -> List[Dict[str, Any]]:
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    data = []
    with open(eval_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def calculate_mrr(ranks: List[int]) -> float:
    """è®¡ç®—MRR (Mean Reciprocal Rank)"""
    if not ranks:
        return 0.0
    reciprocal_ranks = [1.0 / rank if rank > 0 else 0.0 for rank in ranks]
    return float(np.mean(reciprocal_ranks))

def calculate_hit_rate(ranks: List[int], k: int = 1) -> float:
    """è®¡ç®—Hit@k"""
    if not ranks:
        return 0.0
    hits = [1 if rank <= k and rank > 0 else 0 for rank in ranks]
    return float(np.mean(hits))

def calculate_content_hash(text: str) -> str:
    """è®¡ç®—æ–‡æœ¬å†…å®¹çš„å“ˆå¸Œå€¼"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def find_correct_document_rank(
    context: str, 
    retrieved_docs: List[DocumentWithMetadata], 
    sample: Dict[str, Any],
    encoder=None
) -> int:
    """
    ä½¿ç”¨å¤šç§ç­–ç•¥æŸ¥æ‰¾æ­£ç¡®ç­”æ¡ˆçš„æ’å
    
    Args:
        context: æ­£ç¡®ç­”æ¡ˆçš„context
        retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        sample: è¯„ä¼°æ ·æœ¬
        encoder: ç¼–ç å™¨ï¼ˆç”¨äºç›¸ä¼¼åº¦è®¡ç®—ï¼‰
    
    Returns:
        æ‰¾åˆ°çš„æ’åï¼Œ0è¡¨ç¤ºæœªæ‰¾åˆ°
    """
    if not context or not retrieved_docs:
        return 0
    
    # ç­–ç•¥1: IDåŒ¹é…ï¼ˆæœ€é²æ£’ï¼‰- ä»…é€‚ç”¨äºä¸­æ–‡æ•°æ®
    correct_doc_id = sample.get('doc_id') or sample.get('id') or sample.get('document_id')
    if correct_doc_id:
        for rank, doc in enumerate(retrieved_docs, 1):
            # å°è¯•ä»æ–‡æ¡£å†…å®¹ä¸­æå–doc_idï¼ˆå¦‚æœæ˜¯JSONæ ¼å¼ï¼‰
            try:
                if doc.content.startswith('{'):
                    doc_data = json.loads(doc.content)
                    doc_id = doc_data.get('doc_id') or doc_data.get('id')
                    if doc_id == correct_doc_id:
                        return rank
            except:
                pass
            
            # å°è¯•ä»å…ƒæ•°æ®ä¸­è·å–doc_id
            doc_id = getattr(doc, 'id', None) or getattr(doc.metadata, 'id', None) or getattr(doc.metadata, 'doc_id', None)
            if doc_id == correct_doc_id:
                return rank
    
    # ç­–ç•¥2: å†…å®¹å“ˆå¸ŒåŒ¹é…
    context_hash = calculate_content_hash(context.strip())
    for rank, doc in enumerate(retrieved_docs, 1):
        # å¤„ç†JSONæ ¼å¼çš„æ–‡æ¡£å†…å®¹
        doc_content = doc.content
        try:
            if doc.content.startswith('{'):
                doc_data = json.loads(doc.content)
                # æå–contextå­—æ®µ
                doc_context = doc_data.get('context', '')
                if doc_context:
                    doc_content = doc_context
        except:
            pass
        
        doc_hash = calculate_content_hash(doc_content.strip())
        if doc_hash == context_hash:
            return rank
    
    # ç­–ç•¥3: ç²¾ç¡®æ–‡æœ¬åŒ¹é…ï¼ˆæ”¹è¿›ç‰ˆï¼‰
    context_clean = context.strip().lower()
    for rank, doc in enumerate(retrieved_docs, 1):
        # å¤„ç†JSONæ ¼å¼çš„æ–‡æ¡£å†…å®¹
        doc_content = doc.content
        try:
            if doc.content.startswith('{'):
                doc_data = json.loads(doc.content)
                # æå–contextå­—æ®µ
                doc_context = doc_data.get('context', '')
                if doc_context:
                    doc_content = doc_context
        except:
            pass
        
        doc_content_clean = doc_content.strip().lower()
        
        # æ£€æŸ¥contextæ˜¯å¦åŒ…å«åœ¨æ–‡æ¡£ä¸­ï¼Œæˆ–æ–‡æ¡£æ˜¯å¦åŒ…å«åœ¨contextä¸­
        if (context_clean in doc_content_clean or 
            doc_content_clean in context_clean or
            context_clean == doc_content_clean):
            return rank
    
    # ç­–ç•¥4: æ¨¡ç³Šæ–‡æœ¬åŒ¹é…ï¼ˆä½¿ç”¨å…³é”®è¯ï¼‰
    context_words = set(context_clean.split())
    if len(context_words) > 3:  # è‡³å°‘éœ€è¦3ä¸ªè¯
        for rank, doc in enumerate(retrieved_docs, 1):
            # å¤„ç†JSONæ ¼å¼çš„æ–‡æ¡£å†…å®¹
            doc_content = doc.content
            try:
                if doc.content.startswith('{'):
                    doc_data = json.loads(doc.content)
                    # æå–contextå­—æ®µ
                    doc_context = doc_data.get('context', '')
                    if doc_context:
                        doc_content = doc_context
            except:
                pass
            
            doc_content_clean = doc_content.strip().lower()
            doc_words = set(doc_content_clean.split())
            
            # è®¡ç®—è¯æ±‡é‡å åº¦
            overlap = len(context_words.intersection(doc_words))
            overlap_ratio = overlap / len(context_words)
            
            # å¦‚æœé‡å åº¦è¶…è¿‡70%ï¼Œè®¤ä¸ºåŒ¹é…
            if overlap_ratio > 0.7:
                return rank
    
    # ç­–ç•¥5: ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå¦‚æœæœ‰ç¼–ç å™¨ï¼‰
    if encoder and len(context) > 10:  # ç¡®ä¿contextè¶³å¤Ÿé•¿
        try:
            context_embedding = encoder.encode([context])
            
            # å‡†å¤‡æ–‡æ¡£å†…å®¹ç”¨äºç¼–ç 
            doc_contents = []
            for doc in retrieved_docs:
                doc_content = doc.content
                try:
                    if doc.content.startswith('{'):
                        doc_data = json.loads(doc.content)
                        # æå–contextå­—æ®µ
                        doc_context = doc_data.get('context', '')
                        if doc_context:
                            doc_content = doc_context
                except:
                    pass
                doc_contents.append(doc_content)
            
            doc_embeddings = encoder.encode(doc_contents)
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = []
            for doc_emb in doc_embeddings:
                cos_sim = np.dot(context_embedding[0], doc_emb) / (
                    np.linalg.norm(context_embedding[0]) * np.linalg.norm(doc_emb)
                )
                similarities.append(cos_sim)
            
            # æ‰¾åˆ°æœ€é«˜ç›¸ä¼¼åº¦çš„æ–‡æ¡£
            max_sim_idx = int(np.argmax(similarities))
            max_similarity = similarities[max_sim_idx]
            
            # å¦‚æœç›¸ä¼¼åº¦è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºåŒ¹é…
            if max_similarity > 0.8:
                return max_sim_idx + 1
                
        except Exception as e:
            print(f"ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {e}")
    
    return 0

def test_retrieval_with_eval_context(include_eval_data: bool = True):
    """æµ‹è¯•æ£€ç´¢è´¨é‡ - å¯é€‰æ‹©æ˜¯å¦åŒ…å«è¯„ä¼°æ•°æ®åˆ°çŸ¥è¯†åº“"""
    mode = "åŒ…å«è¯„ä¼°æ•°æ®" if include_eval_data else "ä¸åŒ…å«è¯„ä¼°æ•°æ®"
    print("=" * 60)
    print(f"æµ‹è¯•æ£€ç´¢è´¨é‡ - MRRè¯„ä¼° ({mode})")
    print("=" * 60)
    
    try:
        from config.parameters import Config
        from xlm.components.retriever.bilingual_retriever import BilingualRetriever
        from xlm.components.encoder.finbert import FinbertEncoder
        from xlm.utils.optimized_data_loader import OptimizedDataLoader
        
        config = Config()
        
        print("1. åŠ è½½ç¼–ç å™¨...")
        encoder_ch = FinbertEncoder(
            model_name="models/finetuned_alphafin_zh",
            cache_dir=config.encoder.cache_dir,
        )
        encoder_en = FinbertEncoder(
            model_name="models/finetuned_finbert_tatqa",
            cache_dir=config.encoder.cache_dir,
        )
        print("   âœ… ç¼–ç å™¨åŠ è½½æˆåŠŸ")
        
        print("\n2. åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆçŸ¥è¯†åº“ï¼‰...")
        data_loader = OptimizedDataLoader(
            data_dir="data",
            max_samples=-1,  # åŠ è½½æ‰€æœ‰æ•°æ®
            chinese_document_level=True,
            english_chunk_level=True,
            include_eval_data=include_eval_data  # ç›´æ¥æ§åˆ¶æ˜¯å¦åŒ…å«è¯„ä¼°æ•°æ®
        )
        
        chinese_chunks = data_loader.chinese_docs
        english_chunks = data_loader.english_docs
        
        print(f"   âœ… è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"      ä¸­æ–‡chunks: {len(chinese_chunks)}")
        print(f"      è‹±æ–‡chunks: {len(english_chunks)}")
        
        print("\n3. åŠ è½½è¯„ä¼°æ•°æ®...")
        alphafin_eval = load_eval_data("evaluate_mrr/alphafin_eval.jsonl")
        tatqa_eval = load_eval_data("evaluate_mrr/tatqa_eval.jsonl")
        
        print(f"   âœ… è¯„ä¼°æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"      AlphaFinè¯„ä¼°æ ·æœ¬: {len(alphafin_eval)}")
        print(f"      TatQAè¯„ä¼°æ ·æœ¬: {len(tatqa_eval)}")
        
        print("\n4. åˆ›å»ºæ£€ç´¢å™¨...")
        retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,
            corpus_documents_en=english_chunks,
            corpus_documents_ch=chinese_chunks,
            use_faiss=True,
            use_gpu=False,
            batch_size=8,
            cache_dir=config.encoder.cache_dir
        )
        print("   âœ… æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        
        print("\n5. æµ‹è¯•ä¸­æ–‡æ£€ç´¢è´¨é‡ (AlphaFin)...")
        chinese_ranks = []
        chinese_queries = []
        
        for i, sample in enumerate(tqdm(alphafin_eval[:100], desc="æµ‹è¯•ä¸­æ–‡æ£€ç´¢")):  # æµ‹è¯•å‰100ä¸ªæ ·æœ¬
            query = sample.get('question', '')
            context = sample.get('context', '')
            
            if not query or not context:
                continue
                
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_result = retriever.retrieve(
                text=query, 
                top_k=20, 
                return_scores=True, 
                language='zh'
            )
            
            # å¤„ç†è¿”å›å€¼ï¼šå¯èƒ½æ˜¯å…ƒç»„(documents, scores)æˆ–åªæ˜¯documents
            if isinstance(retrieved_result, tuple):
                retrieved_docs, scores = retrieved_result
            else:
                retrieved_docs = retrieved_result
                scores = []
            
            # æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨æ£€ç´¢ç»“æœä¸­
            found_rank = find_correct_document_rank(
                context, retrieved_docs, sample, encoder_ch
            )
            
            chinese_ranks.append(found_rank)
            chinese_queries.append(query)
            
            if i < 5:  # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                print(f"   æŸ¥è¯¢ {i+1}: {query[:50]}...")
                print(f"   æ‰¾åˆ°ä½ç½®: {found_rank}")
                if found_rank > 0:
                    print(f"   ç›¸å…³æ–‡æ¡£: {retrieved_docs[found_rank-1].content[:100]}...")
                print()
        
        print("\n6. æµ‹è¯•è‹±æ–‡æ£€ç´¢è´¨é‡ (TatQA)...")
        english_ranks = []
        english_queries = []
        
        for i, sample in enumerate(tqdm(tatqa_eval[:100], desc="æµ‹è¯•è‹±æ–‡æ£€ç´¢")):  # æµ‹è¯•å‰100ä¸ªæ ·æœ¬
            query = sample.get('question', '')
            context = sample.get('context', '')
            
            if not query or not context:
                continue
                
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_result = retriever.retrieve(
                text=query, 
                top_k=20, 
                return_scores=True, 
                language='en'
            )
            
            # å¤„ç†è¿”å›å€¼ï¼šå¯èƒ½æ˜¯å…ƒç»„(documents, scores)æˆ–åªæ˜¯documents
            if isinstance(retrieved_result, tuple):
                retrieved_docs, scores = retrieved_result
            else:
                retrieved_docs = retrieved_result
                scores = []
            
            # æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨æ£€ç´¢ç»“æœä¸­
            found_rank = find_correct_document_rank(
                context, retrieved_docs, sample, encoder_en
            )
            
            english_ranks.append(found_rank)
            english_queries.append(query)
            
            if i < 5:  # æ˜¾ç¤ºå‰5ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                print(f"   Query {i+1}: {query[:50]}...")
                print(f"   Found at rank: {found_rank}")
                if found_rank > 0:
                    print(f"   Relevant doc: {retrieved_docs[found_rank-1].content[:100]}...")
                print()
        
        print("\n" + "=" * 60)
        print(f"æ£€ç´¢è´¨é‡è¯„ä¼°ç»“æœ ({mode})")
        print("=" * 60)
        
        # è®¡ç®—ä¸­æ–‡æ£€ç´¢æŒ‡æ ‡
        chinese_mrr = calculate_mrr(chinese_ranks)
        chinese_hit1 = calculate_hit_rate(chinese_ranks, k=1)
        chinese_hit5 = calculate_hit_rate(chinese_ranks, k=5)
        chinese_hit10 = calculate_hit_rate(chinese_ranks, k=10)
        
        print(f"ä¸­æ–‡æ£€ç´¢ (AlphaFin):")
        print(f"  æ ·æœ¬æ•°: {len(chinese_ranks)}")
        print(f"  MRR: {chinese_mrr:.4f}")
        print(f"  Hit@1: {chinese_hit1:.4f}")
        print(f"  Hit@5: {chinese_hit5:.4f}")
        print(f"  Hit@10: {chinese_hit10:.4f}")
        
        # è®¡ç®—è‹±æ–‡æ£€ç´¢æŒ‡æ ‡
        english_mrr = calculate_mrr(english_ranks)
        english_hit1 = calculate_hit_rate(english_ranks, k=1)
        english_hit5 = calculate_hit_rate(english_ranks, k=5)
        english_hit10 = calculate_hit_rate(english_ranks, k=10)
        
        print(f"\nè‹±æ–‡æ£€ç´¢ (TatQA):")
        print(f"  æ ·æœ¬æ•°: {len(english_ranks)}")
        print(f"  MRR: {english_mrr:.4f}")
        print(f"  Hit@1: {english_hit1:.4f}")
        print(f"  Hit@5: {english_hit5:.4f}")
        print(f"  Hit@10: {english_hit10:.4f}")
        
        # æ€»ä½“æŒ‡æ ‡
        all_ranks = chinese_ranks + english_ranks
        overall_mrr = calculate_mrr(all_ranks)
        overall_hit1 = calculate_hit_rate(all_ranks, k=1)
        overall_hit5 = calculate_hit_rate(all_ranks, k=5)
        overall_hit10 = calculate_hit_rate(all_ranks, k=10)
        
        print(f"\næ€»ä½“æ£€ç´¢:")
        print(f"  æ ·æœ¬æ•°: {len(all_ranks)}")
        print(f"  MRR: {overall_mrr:.4f}")
        print(f"  Hit@1: {overall_hit1:.4f}")
        print(f"  Hit@5: {overall_hit5:.4f}")
        print(f"  Hit@10: {overall_hit10:.4f}")
        
        # ä¿å­˜ç»“æœ
        suffix = "with_eval" if include_eval_data else "without_eval"
        results = {
            "mode": mode,
            "chinese": {
                "mrr": chinese_mrr,
                "hit1": chinese_hit1,
                "hit5": chinese_hit5,
                "hit10": chinese_hit10,
                "sample_count": len(chinese_ranks)
            },
            "english": {
                "mrr": english_mrr,
                "hit1": english_hit1,
                "hit5": english_hit5,
                "hit10": english_hit10,
                "sample_count": len(english_ranks)
            },
            "overall": {
                "mrr": overall_mrr,
                "hit1": overall_hit1,
                "hit5": overall_hit5,
                "hit10": overall_hit10,
                "sample_count": len(all_ranks)
            }
        }
        
        filename = f"retrieval_mrr_results_{suffix}.json"
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"\nç»“æœå·²ä¿å­˜åˆ°: {filename}")
        
        return results
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def compare_retrieval_modes():
    """å¯¹æ¯”ä¸¤ç§æ£€ç´¢æ¨¡å¼çš„æ•ˆæœ"""
    print("=" * 80)
    print("å¯¹æ¯”æ£€ç´¢æ¨¡å¼ï¼šåŒ…å«è¯„ä¼°æ•°æ® vs ä¸åŒ…å«è¯„ä¼°æ•°æ®")
    print("=" * 80)
    
    # æµ‹è¯•åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    print("\nğŸ” æµ‹è¯•æ¨¡å¼1ï¼šåŒ…å«è¯„ä¼°æ•°æ®åˆ°çŸ¥è¯†åº“")
    results_with_eval = test_retrieval_with_eval_context(include_eval_data=True)
    
    print("\n" + "=" * 80)
    
    # æµ‹è¯•ä¸åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    print("\nğŸ” æµ‹è¯•æ¨¡å¼2ï¼šä¸åŒ…å«è¯„ä¼°æ•°æ®åˆ°çŸ¥è¯†åº“")
    results_without_eval = test_retrieval_with_eval_context(include_eval_data=False)
    
    # å¯¹æ¯”ç»“æœ
    if results_with_eval and results_without_eval:
        print("\n" + "=" * 80)
        print("æ¨¡å¼å¯¹æ¯”ç»“æœ")
        print("=" * 80)
        
        print(f"{'æŒ‡æ ‡':<15} {'åŒ…å«è¯„ä¼°æ•°æ®':<15} {'ä¸åŒ…å«è¯„ä¼°æ•°æ®':<15} {'å·®å¼‚':<10}")
        print("-" * 60)
        
        # ä¸­æ–‡å¯¹æ¯”
        ch_zh_mrr_diff = results_with_eval["chinese"]["mrr"] - results_without_eval["chinese"]["mrr"]
        ch_zh_hit1_diff = results_with_eval["chinese"]["hit1"] - results_without_eval["chinese"]["hit1"]
        
        print(f"{'ä¸­æ–‡MRR':<15} {results_with_eval['chinese']['mrr']:<15.4f} {results_without_eval['chinese']['mrr']:<15.4f} {ch_zh_mrr_diff:+.4f}")
        print(f"{'ä¸­æ–‡Hit@1':<15} {results_with_eval['chinese']['hit1']:<15.4f} {results_without_eval['chinese']['hit1']:<15.4f} {ch_zh_hit1_diff:+.4f}")
        
        # è‹±æ–‡å¯¹æ¯”
        ch_en_mrr_diff = results_with_eval["english"]["mrr"] - results_without_eval["english"]["mrr"]
        ch_en_hit1_diff = results_with_eval["english"]["hit1"] - results_without_eval["english"]["hit1"]
        
        print(f"{'è‹±æ–‡MRR':<15} {results_with_eval['english']['mrr']:<15.4f} {results_without_eval['english']['mrr']:<15.4f} {ch_en_mrr_diff:+.4f}")
        print(f"{'è‹±æ–‡Hit@1':<15} {results_with_eval['english']['hit1']:<15.4f} {results_without_eval['english']['hit1']:<15.4f} {ch_en_hit1_diff:+.4f}")
        
        # æ€»ä½“å¯¹æ¯”
        ch_overall_mrr_diff = results_with_eval["overall"]["mrr"] - results_without_eval["overall"]["mrr"]
        ch_overall_hit1_diff = results_with_eval["overall"]["hit1"] - results_without_eval["overall"]["hit1"]
        
        print(f"{'æ€»ä½“MRR':<15} {results_with_eval['overall']['mrr']:<15.4f} {results_without_eval['overall']['mrr']:<15.4f} {ch_overall_mrr_diff:+.4f}")
        print(f"{'æ€»ä½“Hit@1':<15} {results_with_eval['overall']['hit1']:<15.4f} {results_without_eval['overall']['hit1']:<15.4f} {ch_overall_hit1_diff:+.4f}")
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        comparison_results = {
            "with_eval_data": results_with_eval,
            "without_eval_data": results_without_eval,
            "differences": {
                "chinese_mrr_diff": ch_zh_mrr_diff,
                "chinese_hit1_diff": ch_zh_hit1_diff,
                "english_mrr_diff": ch_en_mrr_diff,
                "english_hit1_diff": ch_en_hit1_diff,
                "overall_mrr_diff": ch_overall_mrr_diff,
                "overall_hit1_diff": ch_overall_hit1_diff
            }
        }
        
        with open("retrieval_comparison_results.json", "w", encoding="utf-8") as f:
            json.dump(comparison_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nå¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ°: retrieval_comparison_results.json")
        
        # åˆ†æå»ºè®®
        print(f"\nğŸ“Š åˆ†æå»ºè®®:")
        if ch_overall_mrr_diff > 0.1:
            print(f"   âœ… åŒ…å«è¯„ä¼°æ•°æ®æ˜¾è‘—æå‡äº†æ£€ç´¢è´¨é‡ (MRRæå‡ {ch_overall_mrr_diff:.4f})")
            print(f"   ğŸ’¡ å»ºè®®ï¼šè¯„ä¼°æ•°æ®çš„contextåº”è¯¥åŠ å…¥çŸ¥è¯†åº“ä»¥æµ‹è¯•çœŸå®æ£€ç´¢èƒ½åŠ›")
        elif ch_overall_mrr_diff < -0.1:
            print(f"   âš ï¸  åŒ…å«è¯„ä¼°æ•°æ®é™ä½äº†æ£€ç´¢è´¨é‡ (MRRä¸‹é™ {abs(ch_overall_mrr_diff):.4f})")
            print(f"   ğŸ’¡ å»ºè®®ï¼šå¯èƒ½å­˜åœ¨æ•°æ®æ³„éœ²é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
        else:
            print(f"   ğŸ” ä¸¤ç§æ¨¡å¼æ•ˆæœç›¸è¿‘ï¼Œå·®å¼‚ä¸å¤§")
            print(f"   ğŸ’¡ å»ºè®®ï¼šå¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©æ¨¡å¼")

def test_retrieval_quality():
    """åŸå§‹æµ‹è¯•å‡½æ•° - ä¿æŒå‘åå…¼å®¹"""
    return test_retrieval_with_eval_context(include_eval_data=False)

def evaluate_retrieval_quality(include_eval_data=True, max_eval_samples=None):
    """
    è¯„ä¼°æ£€ç´¢è´¨é‡
    
    Args:
        include_eval_data: æ˜¯å¦å°†è¯„ä¼°æ•°æ®åŒ…å«åœ¨çŸ¥è¯†åº“ä¸­
        max_eval_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºè¯„ä¼°æ‰€æœ‰æ ·æœ¬
    """
    print("=== æ£€ç´¢è´¨é‡è¯„ä¼° ===")
    print(f"åŒ…å«è¯„ä¼°æ•°æ®åˆ°çŸ¥è¯†åº“: {include_eval_data}")
    print(f"æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°: {max_eval_samples if max_eval_samples else 'å…¨éƒ¨'}")
    
    try:
        from config.parameters import Config
        from xlm.components.retriever.bilingual_retriever import BilingualRetriever
        from xlm.components.encoder.finbert import FinbertEncoder
        from xlm.utils.optimized_data_loader import OptimizedDataLoader
        
        config = Config()
        
        print("\n1. åŠ è½½ç¼–ç å™¨...")
        encoder_ch = FinbertEncoder(
            model_name="models/finetuned_alphafin_zh",
            cache_dir=config.encoder.cache_dir,
        )
        encoder_en = FinbertEncoder(
            model_name="models/finetuned_finbert_tatqa",
            cache_dir=config.encoder.cache_dir,
        )
        print("   âœ… ç¼–ç å™¨åŠ è½½æˆåŠŸ")
        
        print("\n2. åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆçŸ¥è¯†åº“ï¼‰...")
        data_loader = OptimizedDataLoader(
            data_dir="data",
            max_samples=-1,  # åŠ è½½æ‰€æœ‰æ•°æ®
            chinese_document_level=True,
            english_chunk_level=True,
            include_eval_data=include_eval_data  # ç›´æ¥æ§åˆ¶æ˜¯å¦åŒ…å«è¯„ä¼°æ•°æ®
        )
        
        chinese_chunks = data_loader.chinese_docs
        english_chunks = data_loader.english_docs
        
        print(f"   âœ… è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"      ä¸­æ–‡chunks: {len(chinese_chunks)}")
        print(f"      è‹±æ–‡chunks: {len(english_chunks)}")
        
        print("\n3. åŠ è½½è¯„ä¼°æ•°æ®...")
        alphafin_eval = load_eval_data("evaluate_mrr/alphafin_eval.jsonl")
        tatqa_eval = load_eval_data("evaluate_mrr/tatqa_eval.jsonl")
        
        # å¦‚æœæŒ‡å®šäº†æœ€å¤§æ ·æœ¬æ•°ï¼Œåˆ™è¿›è¡Œé‡‡æ ·
        if max_eval_samples:
            alphafin_eval = alphafin_eval[:max_eval_samples]
            tatqa_eval = tatqa_eval[:max_eval_samples]
        
        print(f"   âœ… è¯„ä¼°æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"      AlphaFinè¯„ä¼°æ ·æœ¬: {len(alphafin_eval)}")
        print(f"      TatQAè¯„ä¼°æ ·æœ¬: {len(tatqa_eval)}")
        
        print("\n4. åˆ›å»ºæ£€ç´¢å™¨...")
        retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,
            corpus_documents_en=english_chunks,
            corpus_documents_ch=chinese_chunks,
            use_faiss=True,
            use_gpu=False,
            batch_size=8,
            cache_dir=config.encoder.cache_dir
        )
        print("   âœ… æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        
        print("\n5. å¼€å§‹è¯„ä¼°...")
        
        # è¯„ä¼°ä¸­æ–‡æ•°æ®
        print(f"\n--- è¯„ä¼°ä¸­æ–‡æ•°æ® (AlphaFin) ---")
        chinese_results = evaluate_dataset(
            eval_data=alphafin_eval,
            retriever=retriever,
            encoder=encoder_ch,
            language='zh',
            dataset_name="AlphaFin"
        )
        
        # è¯„ä¼°è‹±æ–‡æ•°æ®
        print(f"\n--- è¯„ä¼°è‹±æ–‡æ•°æ® (TatQA) ---")
        english_results = evaluate_dataset(
            eval_data=tatqa_eval,
            retriever=retriever,
            encoder=encoder_en,
            language='en',
            dataset_name="TatQA"
        )
        
        # æ±‡æ€»ç»“æœ
        print(f"\n=== è¯„ä¼°ç»“æœæ±‡æ€» ===")
        print(f"ä¸­æ–‡æ•°æ® (AlphaFin):")
        print(f"  MRR: {chinese_results['mrr']:.4f}")
        print(f"  Hit@1: {chinese_results['hit_at_1']:.4f}")
        print(f"  Hit@3: {chinese_results['hit_at_3']:.4f}")
        print(f"  Hit@5: {chinese_results['hit_at_5']:.4f}")
        print(f"  Hit@10: {chinese_results['hit_at_10']:.4f}")
        print(f"  æ€»æ ·æœ¬æ•°: {chinese_results['total_samples']}")
        print(f"  æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬æ•°: {chinese_results['found_samples']}")
        
        print(f"\nè‹±æ–‡æ•°æ® (TatQA):")
        print(f"  MRR: {english_results['mrr']:.4f}")
        print(f"  Hit@1: {english_results['hit_at_1']:.4f}")
        print(f"  Hit@3: {english_results['hit_at_3']:.4f}")
        print(f"  Hit@5: {english_results['hit_at_5']:.4f}")
        print(f"  Hit@10: {english_results['hit_at_10']:.4f}")
        print(f"  æ€»æ ·æœ¬æ•°: {english_results['total_samples']}")
        print(f"  æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬æ•°: {english_results['found_samples']}")
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_samples = chinese_results['total_samples'] + english_results['total_samples']
        total_found = chinese_results['found_samples'] + english_results['found_samples']
        overall_mrr = (chinese_results['mrr'] + english_results['mrr']) / 2
        overall_hit_at_1 = (chinese_results['hit_at_1'] + english_results['hit_at_1']) / 2
        
        print(f"\næ€»ä½“æŒ‡æ ‡:")
        print(f"  æ€»ä½“MRR: {overall_mrr:.4f}")
        print(f"  æ€»ä½“Hit@1: {overall_hit_at_1:.4f}")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  æ€»æ‰¾åˆ°æ•°: {total_found}")
        print(f"  æ€»ä½“å¬å›ç‡: {total_found/total_samples:.4f}")
        
        return {
            'chinese': chinese_results,
            'english': english_results,
            'overall': {
                'mrr': overall_mrr,
                'hit_at_1': overall_hit_at_1,
                'total_samples': total_samples,
                'found_samples': total_found
            }
        }
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def evaluate_dataset(eval_data, retriever, encoder, language, dataset_name):
    """
    è¯„ä¼°å•ä¸ªæ•°æ®é›†çš„æ£€ç´¢è´¨é‡
    
    Args:
        eval_data: è¯„ä¼°æ•°æ®åˆ—è¡¨
        retriever: æ£€ç´¢å™¨
        encoder: ç¼–ç å™¨
        language: è¯­è¨€ ('zh' æˆ– 'en')
        dataset_name: æ•°æ®é›†åç§°
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"å¼€å§‹è¯„ä¼° {dataset_name} æ•°æ®é›† ({len(eval_data)} ä¸ªæ ·æœ¬)...")
    
    mrr_scores = []
    hit_at_1 = 0
    hit_at_3 = 0
    hit_at_5 = 0
    hit_at_10 = 0
    found_samples = 0
    
    for i, sample in enumerate(eval_data):
        if i % 100 == 0:
            print(f"  å¤„ç†è¿›åº¦: {i}/{len(eval_data)}")
        
        query = sample['query']
        context = sample['context']
        
        try:
            # æ£€ç´¢
            retrieved_result = retriever.retrieve(
                text=query, 
                top_k=20, 
                return_scores=True, 
                language=language
            )
            
            if isinstance(retrieved_result, tuple):
                retrieved_docs, scores = retrieved_result
            else:
                retrieved_docs = retrieved_result
                scores = []
            
            # æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å
            found_rank = find_correct_document_rank(
                context=context,
                retrieved_docs=retrieved_docs,
                sample=sample,
                encoder=encoder
            )
            
            if found_rank > 0:
                found_samples += 1
                mrr_score = 1.0 / found_rank
                mrr_scores.append(mrr_score)
                
                if found_rank == 1:
                    hit_at_1 += 1
                if found_rank <= 3:
                    hit_at_3 += 1
                if found_rank <= 5:
                    hit_at_5 += 1
                if found_rank <= 10:
                    hit_at_10 += 1
            else:
                mrr_scores.append(0.0)
                
        except Exception as e:
            print(f"   æ ·æœ¬ {i} å¤„ç†å¤±è´¥: {e}")
            mrr_scores.append(0.0)
    
    # è®¡ç®—æŒ‡æ ‡
    total_samples = len(eval_data)
    mrr = sum(mrr_scores) / total_samples if total_samples > 0 else 0.0
    hit_at_1_rate = hit_at_1 / total_samples if total_samples > 0 else 0.0
    hit_at_3_rate = hit_at_3 / total_samples if total_samples > 0 else 0.0
    hit_at_5_rate = hit_at_5 / total_samples if total_samples > 0 else 0.0
    hit_at_10_rate = hit_at_10 / total_samples if total_samples > 0 else 0.0
    
    print(f"  {dataset_name} è¯„ä¼°å®Œæˆ:")
    print(f"    MRR: {mrr:.4f}")
    print(f"    Hit@1: {hit_at_1_rate:.4f} ({hit_at_1}/{total_samples})")
    print(f"    Hit@3: {hit_at_3_rate:.4f} ({hit_at_3}/{total_samples})")
    print(f"    Hit@5: {hit_at_5_rate:.4f} ({hit_at_5}/{total_samples})")
    print(f"    Hit@10: {hit_at_10_rate:.4f} ({hit_at_10}/{total_samples})")
    print(f"    æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆ: {found_samples}/{total_samples}")
    
    return {
        'mrr': mrr,
        'hit_at_1': hit_at_1_rate,
        'hit_at_3': hit_at_3_rate,
        'hit_at_5': hit_at_5_rate,
        'hit_at_10': hit_at_10_rate,
        'total_samples': total_samples,
        'found_samples': found_samples
    }

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯„ä¼°æ£€ç´¢è´¨é‡")
    parser.add_argument("--include_eval_data", action="store_true", 
                       help="æ˜¯å¦å°†è¯„ä¼°æ•°æ®åŒ…å«åœ¨çŸ¥è¯†åº“ä¸­")
    parser.add_argument("--max_samples", type=int, default=None,
                       help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºè¯„ä¼°æ‰€æœ‰æ ·æœ¬")
    parser.add_argument("--test_mode", action="store_true",
                       help="æµ‹è¯•æ¨¡å¼ï¼Œåªè¯„ä¼°å°‘é‡æ ·æœ¬")
    
    args = parser.parse_args()
    
    if args.test_mode:
        print("=== æµ‹è¯•æ¨¡å¼ ===")
        test_retrieval_with_eval_context(include_eval_data=args.include_eval_data)
    else:
        print("=== å®Œæ•´è¯„ä¼°æ¨¡å¼ ===")
        # é»˜è®¤è¯„ä¼°æ‰€æœ‰æ•°æ®
        max_samples = args.max_samples if args.max_samples else None
        results = evaluate_retrieval_quality(
            include_eval_data=args.include_eval_data,
            max_eval_samples=max_samples
        )
        
        if results:
            print("\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
            print(f"æ€»ä½“MRR: {results['overall']['mrr']:.4f}")
            print(f"æ€»ä½“Hit@1: {results['overall']['hit_at_1']:.4f}")
        else:
            print("âŒ è¯„ä¼°å¤±è´¥") 