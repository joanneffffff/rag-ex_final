from typing import List, Dict, Tuple, Union
import numpy as np
import torch
import faiss
import os
import pickle
import hashlib
from sentence_transformers.util import semantic_search
from langdetect import detect
from tqdm import tqdm

from xlm.components.encoder.finbert import FinbertEncoder
from xlm.components.retriever.retriever import Retriever
from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata

class BilingualRetriever(Retriever):
    def __init__(
        self,
        encoder_en: FinbertEncoder,
        encoder_ch: FinbertEncoder,
        max_context_length: int = 100,
        num_threads: int = 4,
        corpus_documents_en: List[DocumentWithMetadata] = None,
        corpus_documents_ch: List[DocumentWithMetadata] = None,
        use_faiss: bool = False,
        batch_size: int = 32,
        use_gpu: bool = False,
        cache_dir: str = "cache",
        use_existing_embedding_index: bool = False
    ):
        self.encoder_en = encoder_en
        self.encoder_ch = encoder_ch
        self.max_context_length = max_context_length
        self.__num_threads = num_threads
        self.use_faiss = use_faiss
        self.batch_size = batch_size
        self.use_gpu = use_gpu
        self.cache_dir = cache_dir
        self.use_existing_embedding_index = use_existing_embedding_index

        self.corpus_documents_en = corpus_documents_en or []
        self.corpus_embeddings_en = None
        self.index_en = None

        self.corpus_documents_ch = corpus_documents_ch or []
        self.corpus_embeddings_ch = None
        self.index_ch = None

        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(self.cache_dir, exist_ok=True)

        # åˆå§‹åŒ–åµŒå…¥å‘é‡ä¸ºç©ºæ•°ç»„ï¼Œç¡®ä¿å³ä½¿æ–‡æ¡£ä¸ºç©ºä¹Ÿæœ‰æœ‰æ•ˆçŠ¶æ€
        if self.corpus_documents_en is None:
            self.corpus_documents_en = []
        if self.corpus_documents_ch is None:
            self.corpus_documents_ch = []
            
        print(f"åˆå§‹åŒ–çŠ¶æ€: è‹±æ–‡æ–‡æ¡£ {len(self.corpus_documents_en)} ä¸ª, ä¸­æ–‡æ–‡æ¡£ {len(self.corpus_documents_ch)} ä¸ª")
        
        # æ™ºèƒ½ç¼“å­˜åŠ è½½ï¼šä¼˜å…ˆå°è¯•ç¼“å­˜ï¼Œå¤±è´¥æ—¶è‡ªåŠ¨å›é€€åˆ°é‡æ–°è®¡ç®—
        if self.use_existing_embedding_index:
            try:
                print("ğŸ”„ å°è¯•åŠ è½½ç°æœ‰ç¼“å­˜...")
                loaded = self._load_cached_embeddings()
                if loaded:
                    print("âœ… ç¼“å­˜åŠ è½½æˆåŠŸ")
                    
                    # éªŒè¯åŠ è½½çš„åµŒå…¥å‘é‡æ˜¯å¦æœ‰æ•ˆ
                    if self._validate_loaded_embeddings():
                        print("âœ… åµŒå…¥å‘é‡éªŒè¯é€šè¿‡")
                    else:
                        print("âš ï¸ åµŒå…¥å‘é‡éªŒè¯å¤±è´¥ï¼Œé‡æ–°è®¡ç®—...")
                        self._compute_embeddings()
                else:
                    print("âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œé‡æ–°è®¡ç®—embedding...")
                    self._compute_embeddings()
                    
            except Exception as e:
                print(f"âŒ ç¼“å­˜åŠ è½½è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                print("ğŸ”„ è‡ªåŠ¨å›é€€åˆ°é‡æ–°è®¡ç®—embedding...")
                self._compute_embeddings()
        else:
            print("ğŸ”„ å¼ºåˆ¶é‡æ–°è®¡ç®—embedding...")
            self._compute_embeddings()

        # éªŒè¯æ–‡æ¡£ç±»å‹
        if self.corpus_documents_en:
            for i, doc in enumerate(self.corpus_documents_en):
                if not isinstance(doc, DocumentWithMetadata):
                    print(f"è­¦å‘Š: corpus_documents_en[{i}]ä¸æ˜¯DocumentWithMetadataç±»å‹: {type(doc)}")
                elif not hasattr(doc, 'content') or not isinstance(doc.content, str):
                    print(f"è­¦å‘Š: corpus_documents_en[{i}]çš„contentå­—æ®µç±»å‹é”™è¯¯: {type(getattr(doc, 'content', None))}")
                    
        if self.corpus_documents_ch:
            for i, doc in enumerate(self.corpus_documents_ch):
                if not isinstance(doc, DocumentWithMetadata):
                    print(f"è­¦å‘Š: corpus_documents_ch[{i}]ä¸æ˜¯DocumentWithMetadataç±»å‹: {type(doc)}")
                elif not hasattr(doc, 'content') or not isinstance(doc.content, str):
                    print(f"è­¦å‘Š: corpus_documents_ch[{i}]çš„contentå­—æ®µç±»å‹é”™è¯¯: {type(getattr(doc, 'content', None))}")

    def _get_cache_key(self, documents: List[DocumentWithMetadata], encoder_name: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®ï¼ŒåŸºäºæ–‡æ¡£å†…å®¹å’Œç¼–ç å™¨åç§°"""
        # åˆ›å»ºæ–‡æ¡£å†…å®¹çš„å“ˆå¸Œ
        content_hash = hashlib.md5()
        for doc in documents:
            content_hash.update(doc.content.encode('utf-8'))
        
        # åªä½¿ç”¨ç¼–ç å™¨åç§°çš„æœ€åéƒ¨åˆ†ï¼Œé¿å…è·¯å¾„é—®é¢˜
        encoder_basename = os.path.basename(encoder_name)
        
        # ç»“åˆç¼–ç å™¨åç§°å’Œæ–‡æ¡£æ•°é‡
        cache_key = f"{encoder_basename}_{len(documents)}_{content_hash.hexdigest()[:16]}"
        return cache_key

    def _get_cache_path(self, cache_key: str, suffix: str) -> str:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}.{suffix}")

    def _is_cache_valid(self, documents: List[DocumentWithMetadata], cache_key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆæ•°æ®æ˜¯å¦å‘ç”Ÿå˜åŒ–ï¼‰"""
        try:
            # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            embeddings_path = self._get_cache_path(cache_key, "npy")
            index_path = self._get_cache_path(cache_key, "faiss")
            
            if not os.path.exists(embeddings_path) or not os.path.exists(index_path):
                print(f"âš ï¸ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ")
                return False
            
            # æ£€æŸ¥åµŒå…¥å‘é‡ç»´åº¦æ˜¯å¦åŒ¹é…
            if os.path.exists(embeddings_path):
                try:
                    cached_embeddings = np.load(embeddings_path)
                    if cached_embeddings.shape[0] != len(documents):
                        print(f"âš ï¸ æ–‡æ¡£æ•°é‡ä¸åŒ¹é…: ç¼“å­˜={cached_embeddings.shape[0]}, å½“å‰={len(documents)}")
                        return False
                    
                    # æ£€æŸ¥åµŒå…¥å‘é‡æ˜¯å¦ä¸ºç©ºæˆ–æ— æ•ˆ
                    if cached_embeddings.size == 0:
                        print(f"âš ï¸ ç¼“å­˜çš„åµŒå…¥å‘é‡ä¸ºç©º")
                        return False
                        
                except Exception as e:
                    print(f"âš ï¸ åµŒå…¥å‘é‡ç¼“å­˜è¯»å–å¤±è´¥: {e}")
                    return False
            
            # æ£€æŸ¥FAISSç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
            if os.path.exists(index_path):
                try:
                    index = faiss.read_index(index_path)
                    if hasattr(index, 'ntotal') and index.ntotal != len(documents):
                        print(f"âš ï¸ FAISSç´¢å¼•å¤§å°ä¸åŒ¹é…: ç¼“å­˜={index.ntotal}, å½“å‰={len(documents)}")
                        return False
                        
                    # æ£€æŸ¥FAISSç´¢å¼•æ˜¯å¦ä¸ºç©º
                    if hasattr(index, 'ntotal') and index.ntotal == 0:
                        print(f"âš ï¸ FAISSç´¢å¼•ä¸ºç©º")
                        return False
                        
                except Exception as e:
                    print(f"âš ï¸ FAISSç´¢å¼•è¯»å–å¤±è´¥: {e}")
                    return False
            
            print(f"âœ… ç¼“å­˜éªŒè¯é€šè¿‡")
            return True
            
        except Exception as e:
            print(f"âš ï¸ ç¼“å­˜éªŒè¯å¤±è´¥: {e}")
            return False

    def _clear_invalid_cache(self, cache_key: str):
        """æ¸…é™¤æ— æ•ˆçš„ç¼“å­˜æ–‡ä»¶"""
        try:
            embeddings_path = self._get_cache_path(cache_key, "npy")
            index_path = self._get_cache_path(cache_key, "faiss")
            
            if os.path.exists(embeddings_path):
                os.remove(embeddings_path)
                print(f"ğŸ—‘ï¸ åˆ é™¤æ— æ•ˆåµŒå…¥å‘é‡ç¼“å­˜: {embeddings_path}")
            
            if os.path.exists(index_path):
                os.remove(index_path)
                print(f"ğŸ—‘ï¸ åˆ é™¤æ— æ•ˆFAISSç´¢å¼•ç¼“å­˜: {index_path}")
                
        except Exception as e:
            print(f"âš ï¸ æ¸…é™¤ç¼“å­˜å¤±è´¥: {e}")
    
    def _validate_loaded_embeddings(self) -> bool:
        """éªŒè¯åŠ è½½çš„åµŒå…¥å‘é‡æ˜¯å¦æœ‰æ•ˆ"""
        try:
            # éªŒè¯è‹±æ–‡åµŒå…¥å‘é‡
            if self.corpus_documents_en:
                if self.corpus_embeddings_en is None:
                    print("âŒ è‹±æ–‡åµŒå…¥å‘é‡ä¸ºNone")
                    return False
                
                if self.corpus_embeddings_en.size == 0:
                    print("âŒ è‹±æ–‡åµŒå…¥å‘é‡ä¸ºç©º")
                    return False
                
                if self.corpus_embeddings_en.shape[0] != len(self.corpus_documents_en):
                    print(f"âŒ è‹±æ–‡åµŒå…¥å‘é‡ç»´åº¦ä¸åŒ¹é…: {self.corpus_embeddings_en.shape[0]} != {len(self.corpus_documents_en)}")
                    return False
                
                print(f"âœ… è‹±æ–‡åµŒå…¥å‘é‡æœ‰æ•ˆ: {self.corpus_embeddings_en.shape}")
            
            # éªŒè¯ä¸­æ–‡åµŒå…¥å‘é‡
            if self.corpus_documents_ch:
                if self.corpus_embeddings_ch is None:
                    print("âŒ ä¸­æ–‡åµŒå…¥å‘é‡ä¸ºNone")
                    return False
                
                if self.corpus_embeddings_ch.size == 0:
                    print("âŒ ä¸­æ–‡åµŒå…¥å‘é‡ä¸ºç©º")
                    return False
                
                if self.corpus_embeddings_ch.shape[0] != len(self.corpus_documents_ch):
                    print(f"âŒ ä¸­æ–‡åµŒå…¥å‘é‡ç»´åº¦ä¸åŒ¹é…: {self.corpus_embeddings_ch.shape[0]} != {len(self.corpus_documents_ch)}")
                    return False
                
                print(f"âœ… ä¸­æ–‡åµŒå…¥å‘é‡æœ‰æ•ˆ: {self.corpus_embeddings_ch.shape}")
            
            # éªŒè¯FAISSç´¢å¼•
            if self.use_faiss:
                if self.corpus_documents_en and self.index_en:
                    if not hasattr(self.index_en, 'ntotal') or self.index_en.ntotal == 0:
                        print("âŒ è‹±æ–‡FAISSç´¢å¼•ä¸ºç©º")
                        return False
                    print(f"âœ… è‹±æ–‡FAISSç´¢å¼•æœ‰æ•ˆ: {self.index_en.ntotal} ä¸ªæ–‡æ¡£")
                
                if self.corpus_documents_ch and self.index_ch:
                    if not hasattr(self.index_ch, 'ntotal') or self.index_ch.ntotal == 0:
                        print("âŒ ä¸­æ–‡FAISSç´¢å¼•ä¸ºç©º")
                        return False
                    print(f"âœ… ä¸­æ–‡FAISSç´¢å¼•æœ‰æ•ˆ: {self.index_ch.ntotal} ä¸ªæ–‡æ¡£")
            
            return True
            
        except Exception as e:
            print(f"âŒ åµŒå…¥å‘é‡éªŒè¯å¤±è´¥: {e}")
            return False

    def _load_cached_embeddings(self) -> bool:
        """å°è¯•åŠ è½½ç¼“å­˜çš„åµŒå…¥å‘é‡ï¼Œè‡ªåŠ¨æ£€æµ‹æ•°æ®å˜åŒ–"""
        try:
            loaded_any = False
            
            # æ£€æŸ¥è‹±æ–‡æ–‡æ¡£ç¼“å­˜
            if self.corpus_documents_en:
                cache_key_en = self._get_cache_key(self.corpus_documents_en, str(self.encoder_en.model_name))
                print(f"ğŸ” æ£€æŸ¥è‹±æ–‡ç¼“å­˜: {cache_key_en}")
                
                try:
                    if self._is_cache_valid(self.corpus_documents_en, cache_key_en):
                        embeddings_path_en = self._get_cache_path(cache_key_en, "npy")
                        index_path_en = self._get_cache_path(cache_key_en, "faiss")
                        
                        # å°è¯•åŠ è½½åµŒå…¥å‘é‡
                        try:
                            self.corpus_embeddings_en = np.load(embeddings_path_en)
                            print(f"âœ… è‹±æ–‡åµŒå…¥å‘é‡åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {self.corpus_embeddings_en.shape}")
                            loaded_any = True
                        except Exception as e:
                            print(f"âŒ è‹±æ–‡åµŒå…¥å‘é‡åŠ è½½å¤±è´¥: {e}")
                            self.corpus_embeddings_en = np.array([])
                        
                        # å°è¯•åŠ è½½FAISSç´¢å¼•
                        if self.use_faiss and os.path.exists(index_path_en):
                            try:
                                self.index_en = faiss.read_index(index_path_en)
                                print(f"âœ… è‹±æ–‡FAISSç´¢å¼•åŠ è½½æˆåŠŸï¼Œæ–‡æ¡£æ•°: {len(self.corpus_documents_en)}")
                            except Exception as e:
                                print(f"âŒ è‹±æ–‡FAISSç´¢å¼•åŠ è½½å¤±è´¥: {e}")
                                self.index_en = None
                    else:
                        # æ¸…é™¤æ— æ•ˆç¼“å­˜
                        self._clear_invalid_cache(cache_key_en)
                        print(f"ğŸ”„ è‹±æ–‡æ•°æ®å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆç´¢å¼•")
                        self.corpus_embeddings_en = np.array([])
                        self.index_en = None
                        
                except Exception as e:
                    print(f"âŒ è‹±æ–‡ç¼“å­˜éªŒè¯å¤±è´¥: {e}")
                    self._clear_invalid_cache(cache_key_en)
                    self.corpus_embeddings_en = np.array([])
                    self.index_en = None
            else:
                print("âš ï¸ è‹±æ–‡æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œåˆå§‹åŒ–ç©ºåµŒå…¥å‘é‡")
                self.corpus_embeddings_en = np.array([])
                if self.use_faiss:
                    self.index_en = self._init_faiss(self.encoder_en, 0)

            # æ£€æŸ¥ä¸­æ–‡æ–‡æ¡£ç¼“å­˜
            if self.corpus_documents_ch:
                cache_key_ch = self._get_cache_key(self.corpus_documents_ch, str(self.encoder_ch.model_name))
                print(f"ğŸ” æ£€æŸ¥ä¸­æ–‡ç¼“å­˜: {cache_key_ch}")
                
                try:
                    if self._is_cache_valid(self.corpus_documents_ch, cache_key_ch):
                        embeddings_path_ch = self._get_cache_path(cache_key_ch, "npy")
                        index_path_ch = self._get_cache_path(cache_key_ch, "faiss")
                        
                        # å°è¯•åŠ è½½åµŒå…¥å‘é‡
                        try:
                            self.corpus_embeddings_ch = np.load(embeddings_path_ch)
                            print(f"âœ… ä¸­æ–‡åµŒå…¥å‘é‡åŠ è½½æˆåŠŸï¼Œå½¢çŠ¶: {self.corpus_embeddings_ch.shape}")
                            loaded_any = True
                        except Exception as e:
                            print(f"âŒ ä¸­æ–‡åµŒå…¥å‘é‡åŠ è½½å¤±è´¥: {e}")
                            self.corpus_embeddings_ch = np.array([])
                        
                        # å°è¯•åŠ è½½FAISSç´¢å¼•
                        if self.use_faiss and os.path.exists(index_path_ch):
                            try:
                                self.index_ch = faiss.read_index(index_path_ch)
                                print(f"âœ… ä¸­æ–‡FAISSç´¢å¼•åŠ è½½æˆåŠŸï¼Œæ–‡æ¡£æ•°: {len(self.corpus_documents_ch)}")
                            except Exception as e:
                                print(f"âŒ ä¸­æ–‡FAISSç´¢å¼•åŠ è½½å¤±è´¥: {e}")
                                self.index_ch = None
                    else:
                        # æ¸…é™¤æ— æ•ˆç¼“å­˜
                        self._clear_invalid_cache(cache_key_ch)
                        print(f"ğŸ”„ ä¸­æ–‡æ•°æ®å‘ç”Ÿå˜åŒ–ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆç´¢å¼•")
                        self.corpus_embeddings_ch = np.array([])
                        self.index_ch = None
                        
                except Exception as e:
                    print(f"âŒ ä¸­æ–‡ç¼“å­˜éªŒè¯å¤±è´¥: {e}")
                    self._clear_invalid_cache(cache_key_ch)
                    self.corpus_embeddings_ch = np.array([])
                    self.index_ch = None
            else:
                print("âš ï¸ ä¸­æ–‡æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œåˆå§‹åŒ–ç©ºåµŒå…¥å‘é‡")
                self.corpus_embeddings_ch = np.array([])
                if self.use_faiss:
                    self.index_ch = self._init_faiss(self.encoder_ch, 0)

            return loaded_any
        except Exception as e:
            print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            # ç¡®ä¿åµŒå…¥å‘é‡ä¸ä¸ºNone
            if self.corpus_embeddings_en is None:
                self.corpus_embeddings_en = np.array([])
            if self.corpus_embeddings_ch is None:
                self.corpus_embeddings_ch = np.array([])
            return False

    def _save_cached_embeddings(self):
        """ä¿å­˜åµŒå…¥å‘é‡åˆ°ç¼“å­˜"""
        try:
            # ä¿å­˜è‹±æ–‡æ–‡æ¡£åµŒå…¥å‘é‡
            if self.corpus_documents_en and self.corpus_embeddings_en is not None and self.corpus_embeddings_en.size > 0:
                try:
                    cache_key_en = self._get_cache_key(self.corpus_documents_en, str(self.encoder_en.model_name))
                    embeddings_path_en = self._get_cache_path(cache_key_en, "npy")
                    index_path_en = self._get_cache_path(cache_key_en, "faiss")
                    
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(embeddings_path_en), exist_ok=True)
                    os.makedirs(os.path.dirname(index_path_en), exist_ok=True)
                    
                    # ä¿å­˜åµŒå…¥å‘é‡
                    np.save(embeddings_path_en, self.corpus_embeddings_en)
                    print(f"âœ… è‹±æ–‡åµŒå…¥å‘é‡å·²ä¿å­˜: {embeddings_path_en}")
                    
                    # ä¿å­˜FAISSç´¢å¼•
                    if self.use_faiss and self.index_en:
                        faiss.write_index(self.index_en, index_path_en)
                        print(f"âœ… è‹±æ–‡FAISSç´¢å¼•å·²ä¿å­˜: {index_path_en}")
                        
                except Exception as e:
                    print(f"âŒ è‹±æ–‡ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")

            # ä¿å­˜ä¸­æ–‡æ–‡æ¡£åµŒå…¥å‘é‡
            if self.corpus_documents_ch and self.corpus_embeddings_ch is not None and self.corpus_embeddings_ch.size > 0:
                try:
                    cache_key_ch = self._get_cache_key(self.corpus_documents_ch, str(self.encoder_ch.model_name))
                    embeddings_path_ch = self._get_cache_path(cache_key_ch, "npy")
                    index_path_ch = self._get_cache_path(cache_key_ch, "faiss")
                    
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(embeddings_path_ch), exist_ok=True)
                    os.makedirs(os.path.dirname(index_path_ch), exist_ok=True)
                    
                    # ä¿å­˜åµŒå…¥å‘é‡
                    np.save(embeddings_path_ch, self.corpus_embeddings_ch)
                    print(f"âœ… ä¸­æ–‡åµŒå…¥å‘é‡å·²ä¿å­˜: {embeddings_path_ch}")
                    
                    # ä¿å­˜FAISSç´¢å¼•
                    if self.use_faiss and self.index_ch:
                        faiss.write_index(self.index_ch, index_path_ch)
                        print(f"âœ… ä¸­æ–‡FAISSç´¢å¼•å·²ä¿å­˜: {index_path_ch}")
                        
                except Exception as e:
                    print(f"âŒ ä¸­æ–‡ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
                    
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    def _compute_embeddings(self):
        """è®¡ç®—åµŒå…¥å‘é‡"""
        print("=== å¼€å§‹è®¡ç®—åµŒå…¥å‘é‡ ===")
        print(f"use_existing_embedding_index: {self.use_existing_embedding_index}")
        
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆå§‹åŒ–FAISSç´¢å¼•
            if self.use_faiss:
                if self.corpus_documents_en and self.index_en is None:
                    print(f"åˆå§‹åŒ–è‹±æ–‡FAISSç´¢å¼•ï¼Œæ–‡æ¡£æ•°é‡: {len(self.corpus_documents_en)}")
                    self.index_en = self._init_faiss(self.encoder_en, len(self.corpus_documents_en))
                if self.corpus_documents_ch and self.index_ch is None:
                    print(f"åˆå§‹åŒ–ä¸­æ–‡FAISSç´¢å¼•ï¼Œæ–‡æ¡£æ•°é‡: {len(self.corpus_documents_ch)}")
                    self.index_ch = self._init_faiss(self.encoder_ch, len(self.corpus_documents_ch))

            if self.corpus_documents_en:
                print(f"å¼€å§‹ç¼–ç è‹±æ–‡æ–‡æ¡£ï¼Œæ•°é‡: {len(self.corpus_documents_en)}")
                print(f"è‹±æ–‡ç¼–ç å™¨: {self.encoder_en.model_name}")
                print(f"è‹±æ–‡ç¼–ç å™¨è®¾å¤‡: {self.encoder_en.device}")
                
                # æ£€æŸ¥è‹±æ–‡æ–‡æ¡£å†…å®¹
                if self.corpus_documents_en:
                    first_doc = self.corpus_documents_en[0]
                    print(f"ç¬¬ä¸€ä¸ªè‹±æ–‡æ–‡æ¡£å†…å®¹é¢„è§ˆ: {first_doc.content[:100]}...")
                
                self.corpus_embeddings_en = self._batch_encode_corpus(self.corpus_documents_en, self.encoder_en, 'en')
                print(f"è‹±æ–‡åµŒå…¥å‘é‡è®¡ç®—å®Œæˆï¼Œå½¢çŠ¶: {self.corpus_embeddings_en.shape if self.corpus_embeddings_en is not None else 'None'}")
                
                if self.corpus_embeddings_en is None or self.corpus_embeddings_en.shape[0] == 0:
                    print("âŒ è‹±æ–‡åµŒå…¥å‘é‡è®¡ç®—å¤±è´¥ï¼")
                    self.corpus_embeddings_en = np.array([])  # ç¡®ä¿ä¸ä¸ºNone
                else:
                    print("âœ… è‹±æ–‡åµŒå…¥å‘é‡è®¡ç®—æˆåŠŸï¼")
                    
                if self.use_faiss and self.corpus_embeddings_en is not None and self.corpus_embeddings_en.shape[0] > 0:
                    print("å°†è‹±æ–‡åµŒå…¥å‘é‡æ·»åŠ åˆ°FAISSç´¢å¼•")
                    self._add_to_faiss(self.index_en, self.corpus_embeddings_en)
            else:
                print("âš ï¸ è‹±æ–‡æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œåˆå§‹åŒ–ç©ºåµŒå…¥å‘é‡")
                self.corpus_embeddings_en = np.array([])
                if self.use_faiss and self.index_en is None:
                    self.index_en = self._init_faiss(self.encoder_en, 0)

            if self.corpus_documents_ch:
                print(f"å¼€å§‹ç¼–ç ä¸­æ–‡æ–‡æ¡£ï¼Œæ•°é‡: {len(self.corpus_documents_ch)}")
                self.corpus_embeddings_ch = self._batch_encode_corpus(self.corpus_documents_ch, self.encoder_ch, 'zh')
                print(f"ä¸­æ–‡åµŒå…¥å‘é‡è®¡ç®—å®Œæˆï¼Œå½¢çŠ¶: {self.corpus_embeddings_ch.shape if self.corpus_embeddings_ch is not None else 'None'}")
                
                if self.corpus_embeddings_ch is None or self.corpus_embeddings_ch.shape[0] == 0:
                    print("âŒ ä¸­æ–‡åµŒå…¥å‘é‡è®¡ç®—å¤±è´¥ï¼")
                    self.corpus_embeddings_ch = np.array([])  # ç¡®ä¿ä¸ä¸ºNone
                else:
                    print("âœ… ä¸­æ–‡åµŒå…¥å‘é‡è®¡ç®—æˆåŠŸï¼")
                    
                if self.use_faiss and self.corpus_embeddings_ch is not None and self.corpus_embeddings_ch.shape[0] > 0:
                    print("å°†ä¸­æ–‡åµŒå…¥å‘é‡æ·»åŠ åˆ°FAISSç´¢å¼•")
                    self._add_to_faiss(self.index_ch, self.corpus_embeddings_ch)
            else:
                print("âš ï¸ ä¸­æ–‡æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œåˆå§‹åŒ–ç©ºåµŒå…¥å‘é‡")
                self.corpus_embeddings_ch = np.array([])
                if self.use_faiss and self.index_ch is None:
                    self.index_ch = self._init_faiss(self.encoder_ch, 0)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            print("ä¿å­˜åµŒå…¥å‘é‡åˆ°ç¼“å­˜")
            try:
                self._save_cached_embeddings()
            except Exception as e:
                print(f"âš ï¸ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
            
            print("=== åµŒå…¥å‘é‡è®¡ç®—å®Œæˆ ===")
            print(f"æœ€ç»ˆçŠ¶æ€:")
            print(f"  è‹±æ–‡åµŒå…¥å‘é‡: {self.corpus_embeddings_en.shape if self.corpus_embeddings_en is not None else 'None'}")
            print(f"  ä¸­æ–‡åµŒå…¥å‘é‡: {self.corpus_embeddings_ch.shape if self.corpus_embeddings_ch is not None else 'None'}")
            
        except Exception as e:
            print(f"âŒ åµŒå…¥å‘é‡è®¡ç®—è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            # ç¡®ä¿åµŒå…¥å‘é‡ä¸ä¸ºNone
            if self.corpus_embeddings_en is None:
                self.corpus_embeddings_en = np.array([])
            if self.corpus_embeddings_ch is None:
                self.corpus_embeddings_ch = np.array([])
            print("ğŸ”„ å·²é‡ç½®åµŒå…¥å‘é‡ä¸ºç©ºæ•°ç»„")

    def _init_faiss(self, encoder, corpus_size):
        """Initialize FAISS index"""
        dimension = encoder.get_embedding_dimension()
        
        # Create FAISS index
        if self.use_gpu and faiss.get_num_gpus() > 0:
            res = faiss.StandardGpuResources()
            index = faiss.IndexFlatL2(dimension)
            gpu_index = faiss.index_cpu_to_gpu(res, 0, index)
            return gpu_index
        else:
            if corpus_size < 1000:
                return faiss.IndexFlatL2(dimension)
            else:
                nlist = min(max(int(corpus_size / 100), 4), 1024)
                quantizer = faiss.IndexFlatL2(dimension)
                index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
                return index

    def _batch_encode_corpus(self, documents: List[DocumentWithMetadata], encoder: FinbertEncoder, language: str = None) -> np.ndarray:
        """Encode corpus documents in batches with a progress bar."""
        print(f"=== å¼€å§‹æ‰¹é‡ç¼–ç è¯­æ–™åº“ ===")
        print(f"è¯­è¨€: {language}")
        print(f"æ–‡æ¡£æ•°é‡: {len(documents) if documents else 0}")
        print(f"ç¼–ç å™¨: {encoder.model_name}")
        print(f"ç¼–ç å™¨è®¾å¤‡: {encoder.device}")
        
        if not documents:
            print("âš ï¸ æ–‡æ¡£åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºæ•°ç»„")
            return np.array([])
        
        batch_texts = []
        for i, doc in enumerate(documents):
            if language == 'zh' and hasattr(doc.metadata, 'summary') and doc.metadata.summary:
                # ä¸­æ–‡ä½¿ç”¨summaryå­—æ®µè¿›è¡ŒFAISSç´¢å¼•
                batch_texts.append(doc.metadata.summary)
            else:
                # è‹±æ–‡ä½¿ç”¨contentå­—æ®µ
                batch_texts.append(doc.content)
            
            # æ‰“å°å‰å‡ ä¸ªæ–‡æ¡£çš„å†…å®¹é¢„è§ˆ
            if i < 3:
                content_preview = batch_texts[-1][:100] + "..." if len(batch_texts[-1]) > 100 else batch_texts[-1]
                print(f"æ–‡æ¡£ {i+1} å†…å®¹é¢„è§ˆ: {content_preview}")
        
        print(f"å‡†å¤‡ç¼–ç  {len(batch_texts)} ä¸ªæ–‡æœ¬")
        
        # æµ‹è¯•ç¼–ç å™¨æ˜¯å¦æ­£å¸¸å·¥ä½œ
        try:
            print("æµ‹è¯•ç¼–ç å™¨...")
            test_text = batch_texts[0] if batch_texts else "test"
            test_embedding = encoder.encode([test_text])
            print(f"æµ‹è¯•ç¼–ç æˆåŠŸï¼ŒåµŒå…¥ç»´åº¦: {test_embedding.shape}")
        except Exception as e:
            print(f"âŒ ç¼–ç å™¨æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return np.array([])
        
        try:
            print("å¼€å§‹æ‰¹é‡ç¼–ç ...")
            embeddings = encoder.encode(texts=batch_texts, batch_size=self.batch_size, show_progress_bar=True)
            print(f"ç¼–ç å®Œæˆï¼ŒåµŒå…¥å‘é‡å½¢çŠ¶: {embeddings.shape}")
            return embeddings
        except Exception as e:
            print(f"âŒ æ‰¹é‡ç¼–ç è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return np.array([])
    
    def _add_to_faiss(self, index, embeddings: np.ndarray):
        """Add embeddings to FAISS index in batches"""
        if embeddings is None or embeddings.shape[0] == 0:
            return
        
        embeddings_np = embeddings.astype('float32')
        if not index.is_trained:
            index.train(embeddings_np)
        index.add(embeddings_np)

    def retrieve(
        self,
        text: str,
        top_k: int = 3,
        return_scores: bool = False,
        language: str = None,
    ) -> Union[List[DocumentWithMetadata], Tuple[List[DocumentWithMetadata], List[float]]]:
        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        print(f"=== BilingualRetriever.retrieve() è°ƒè¯•ä¿¡æ¯ ===")
        print(f"æŸ¥è¯¢æ–‡æœ¬: {text}")
        print(f"è‹±æ–‡æ–‡æ¡£æ•°é‡: {len(self.corpus_documents_en) if self.corpus_documents_en else 0}")
        print(f"ä¸­æ–‡æ–‡æ¡£æ•°é‡: {len(self.corpus_documents_ch) if self.corpus_documents_ch else 0}")
        print(f"è‹±æ–‡åµŒå…¥å‘é‡å½¢çŠ¶: {self.corpus_embeddings_en.shape if self.corpus_embeddings_en is not None else 'None'}")
        print(f"ä¸­æ–‡åµŒå…¥å‘é‡å½¢çŠ¶: {self.corpus_embeddings_ch.shape if self.corpus_embeddings_ch is not None else 'None'}")
        print(f"è‹±æ–‡FAISSç´¢å¼•: {'å·²åˆå§‹åŒ–' if self.index_en else 'æœªåˆå§‹åŒ–'}")
        print(f"ä¸­æ–‡FAISSç´¢å¼•: {'å·²åˆå§‹åŒ–' if self.index_ch else 'æœªåˆå§‹åŒ–'}")
        
        # æ£€æŸ¥self.corpus_documents_en/chç±»å‹
        if hasattr(self, 'corpus_documents_en') and self.corpus_documents_en is not None:
            for i, doc in enumerate(self.corpus_documents_en):
                if not isinstance(doc, DocumentWithMetadata):
                    print(f"è­¦å‘Š: corpus_documents_en[{i}]ä¸æ˜¯DocumentWithMetadataç±»å‹: {type(doc)}")
                elif not hasattr(doc, 'content') or not isinstance(doc.content, str):
                    print(f"è­¦å‘Š: corpus_documents_en[{i}]çš„contentå­—æ®µç±»å‹é”™è¯¯: {type(getattr(doc, 'content', None))}")
                    
        if hasattr(self, 'corpus_documents_ch') and self.corpus_documents_ch is not None:
            for i, doc in enumerate(self.corpus_documents_ch):
                if not isinstance(doc, DocumentWithMetadata):
                    print(f"è­¦å‘Š: corpus_documents_ch[{i}]ä¸æ˜¯DocumentWithMetadataç±»å‹: {type(doc)}")
                elif not hasattr(doc, 'content') or not isinstance(doc.content, str):
                    print(f"è­¦å‘Š: corpus_documents_ch[{i}]çš„contentå­—æ®µç±»å‹é”™è¯¯: {type(getattr(doc, 'content', None))}")
                    
        if language is None:
            # å¢å¼ºçš„è¯­è¨€æ£€æµ‹é€»è¾‘
            try:
                lang = detect(text)
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
                chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
                total_chars = len([char for char in text if char.isalpha() or '\u4e00' <= char <= '\u9fff'])
                
                # å¦‚æœåŒ…å«ä¸­æ–‡å­—ç¬¦ä¸”ä¸­æ–‡æ¯”ä¾‹è¶…è¿‡30%ï¼Œæˆ–è€…langdetectæ£€æµ‹ä¸ºä¸­æ–‡ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸­æ–‡
                if chinese_chars > 0 and (chinese_chars / total_chars > 0.3 or lang.startswith('zh')):
                    language = 'zh'
                else:
                    language = 'en'
            except:
                # å¦‚æœlangdetectå¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦æ£€æµ‹
                chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
                language = 'zh' if chinese_chars > 0 else 'en'
        
        print(f"æ£€æµ‹åˆ°çš„è¯­è¨€: {language}")
        
        if language == 'zh':
            encoder = self.encoder_ch
            corpus_embeddings = self.corpus_embeddings_ch
            corpus_documents = self.corpus_documents_ch
            index = self.index_ch
        else:
            encoder = self.encoder_en
            corpus_embeddings = self.corpus_embeddings_en
            corpus_documents = self.corpus_documents_en
            index = self.index_en
        
        print(f"é€‰æ‹©çš„ç¼–ç å™¨: {encoder.model_name}")
        print(f"é€‰æ‹©çš„è¯­æ–™åº“æ–‡æ¡£æ•°é‡: {len(corpus_documents) if corpus_documents else 0}")
        print(f"é€‰æ‹©çš„åµŒå…¥å‘é‡å½¢çŠ¶: {corpus_embeddings.shape if corpus_embeddings is not None else 'None'}")
        
        if corpus_embeddings is None or corpus_embeddings.shape[0] == 0:
            print("âŒ åµŒå…¥å‘é‡ä¸ºç©ºæˆ–å½¢çŠ¶ä¸º0ï¼Œè¿”å›ç©ºç»“æœ")
            if return_scores:
                return [], []
            else:
                return []
        
        query_embeddings = encoder.encode([text])
        print(f"æŸ¥è¯¢åµŒå…¥å‘é‡å½¢çŠ¶: {query_embeddings.shape}")
        
        if self.use_faiss and index:
            print("ä½¿ç”¨FAISSç´¢å¼•è¿›è¡Œæ£€ç´¢")
            distances, indices = index.search(query_embeddings.astype('float32'), top_k)
            results = []
            for score, idx in zip(distances[0], indices[0]):
                if idx != -1:
                    results.append({'corpus_id': idx, 'score': 1 - score / 2})
            print(f"FAISSæ£€ç´¢ç»“æœæ•°é‡: {len(results)}")
        else:
            print("ä½¿ç”¨è¯­ä¹‰æœç´¢è¿›è¡Œæ£€ç´¢")
            # ç¡®ä¿tensoråœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            query_tensor = torch.tensor(query_embeddings, device=encoder.device)
            corpus_tensor = torch.tensor(corpus_embeddings, device=encoder.device)
            hits = semantic_search(
                query_tensor,
                corpus_tensor,
                top_k=top_k
            )
            results = hits[0]
            print(f"è¯­ä¹‰æœç´¢ç»“æœæ•°é‡: {len(results)}")
        
        doc_indices = [hit['corpus_id'] for hit in results]
        scores = [hit['score'] for hit in results]
        
        # æ·»åŠ ç´¢å¼•èŒƒå›´æ£€æŸ¥
        print(f"æ£€ç´¢åˆ°çš„ç´¢å¼•: {doc_indices}")
        print(f"è¯­æ–™åº“æ–‡æ¡£æ•°é‡: {len(corpus_documents)}")
        
        # è¿‡æ»¤æ— æ•ˆç´¢å¼•
        valid_indices = []
        valid_scores = []
        for i, (idx, score) in enumerate(zip(doc_indices, scores)):
            if 0 <= idx < len(corpus_documents):
                valid_indices.append(idx)
                valid_scores.append(score)
            else:
                print(f"è­¦å‘Š: è·³è¿‡æ— æ•ˆç´¢å¼• {idx} (è¶…å‡ºèŒƒå›´ [0, {len(corpus_documents)})")
        
        raw_documents = [corpus_documents[i] for i in valid_indices]
        scores = valid_scores
        
        print(f"æœ€ç»ˆè¿”å›æ–‡æ¡£æ•°é‡: {len(raw_documents)}")
        
        # ç¡®ä¿è¿”å›çš„æ˜¯DocumentWithMetadataå¯¹è±¡ï¼Œç»Ÿä¸€ä½¿ç”¨contentå­—æ®µ
        documents = []
        for i, doc in enumerate(raw_documents):
            if isinstance(doc, DocumentWithMetadata):
                # å·²ç»æ˜¯æ­£ç¡®çš„ç±»å‹ï¼Œç›´æ¥ä½¿ç”¨
                documents.append(doc)
            elif isinstance(doc, dict):
                # å¦‚æœæ˜¯å­—å…¸ï¼Œè½¬æ¢ä¸ºDocumentWithMetadata
                content = doc.get('content', doc.get('context', ''))
                if not isinstance(content, str):
                    # å¦‚æœcontentä¸æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•å–contextå­—æ®µæˆ–è½¬ä¸ºå­—ç¬¦ä¸²
                    content = content.get('context', '') if isinstance(content, dict) and 'context' in content else str(content)
                metadata = DocumentMetadata(
                    source=doc.get('source', 'unknown'),
                    created_at=doc.get('created_at', ''),
                    author=doc.get('author', ''),
                    language=language or 'unknown'
                )
                documents.append(DocumentWithMetadata(content=content, metadata=metadata))
            else:
                # å…¶ä»–ç±»å‹ï¼Œå°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                print(f"è­¦å‘Š: æ£€ç´¢ç»“æœ[{i}]ç±»å‹å¼‚å¸¸: {type(doc)}, å°è¯•è½¬æ¢")
                content = str(doc) if doc is not None else ""
                metadata = DocumentMetadata(
                    source="unknown",
                    created_at="",
                    author="",
                    language=language or 'unknown'
                )
                documents.append(DocumentWithMetadata(content=content, metadata=metadata))
        
        if return_scores:
            return documents, scores
        else:
            return documents 