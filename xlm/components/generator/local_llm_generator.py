import os
import json
import re
from typing import List, Optional, Dict, Any

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.utils.quantization_config import BitsAndBytesConfig

from xlm.components.generator.generator import Generator
from config.parameters import Config


class LocalLLMGenerator(Generator):
    def __init__(
        self,
        model_name: Optional[str] = None,
        cache_dir: Optional[str] = None,
        device: Optional[str] = None,
        use_quantization: Optional[bool] = None,
        quantization_type: Optional[str] = None,
        use_flash_attention: bool = False
    ):
        self.config = Config() # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        
        # å¦‚æœæ²¡æœ‰æä¾›model_nameï¼Œä»configè¯»å–
        if model_name is None:
            model_name = self.config.generator.model_name
        
        # å¦‚æœæ²¡æœ‰æä¾›deviceï¼Œä»configè¯»å–
        if device is None:
            device = self.config.generator.device
        
        # å¦‚æœæ²¡æœ‰æä¾›é‡åŒ–å‚æ•°ï¼Œä»configè¯»å–
        if use_quantization is None:
            use_quantization = self.config.generator.use_quantization
        if quantization_type is None:
            quantization_type = self.config.generator.quantization_type
        
        # éªŒè¯é…ç½®å‚æ•°
        self._validate_config(model_name, device or "cpu", use_quantization, quantization_type)
        
        super().__init__(model_name=model_name)
        self.device = device
        self.temperature = self.config.generator.temperature
        self.max_new_tokens = self.config.generator.max_new_tokens
        self.top_p = self.config.generator.top_p
        
        # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        if cache_dir is None:
            cache_dir = self.config.generator.cache_dir 
        
        self.cache_dir = cache_dir  
        self.use_quantization = use_quantization
        self.quantization_type = quantization_type
        self.use_flash_attention = use_flash_attention
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # è®¾ç½®Hugging Faceç¯å¢ƒå˜é‡
        os.environ['HF_HOME'] = self.cache_dir
        os.environ['TRANSFORMERS_CACHE'] = os.path.join(self.cache_dir, 'transformers')
        
        # å†…å­˜ä¼˜åŒ–è®¾ç½®
        self._setup_memory_optimization()
        
        self._load_model_and_tokenizer()
        print(f"LocalLLMGenerator '{model_name}' loaded on {self.device} with quantization: {self.use_quantization} ({self.quantization_type}).")
    
    def _validate_config(self, model_name: str, device: str, use_quantization: bool, quantization_type: str):
        """éªŒè¯é…ç½®å‚æ•°çš„æœ‰æ•ˆæ€§"""
        if not model_name:
            raise ValueError("model_name cannot be empty")
        
        if device and device.startswith('cuda'):
            if not torch.cuda.is_available():
                print("âš ï¸  CUDA requested but not available, falling back to CPU")
                self.device = "cpu"
            else:
                # æ£€æŸ¥CUDAå†…å­˜
                cuda_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
                print(f"CUDA memory available: {cuda_memory:.1f} GB")
                
                if cuda_memory < 4 and use_quantization:
                    print("âš ï¸  CUDA memory < 4GB, enabling quantization")
                    use_quantization = True
                    if quantization_type not in ['4bit', '8bit']:
                        quantization_type = '4bit'
        
        if use_quantization and quantization_type not in ['4bit', '8bit']:
            print(f"âš ï¸  Invalid quantization type: {quantization_type}, falling back to 4bit")
            quantization_type = '4bit'
    
    def _setup_memory_optimization(self):
        """è®¾ç½®å†…å­˜ä¼˜åŒ–"""
        # è®¾ç½®PyTorchå†…å­˜åˆ†é…å™¨
        if torch.cuda.is_available():
            # å¯ç”¨å†…å­˜ç¼“å­˜
            torch.cuda.empty_cache()
            
            # è®¾ç½®æ›´æ¿€è¿›çš„å†…å­˜åˆ†é…ç­–ç•¥
            os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:64'
            
            # å¦‚æœå†…å­˜ä¸è¶³ï¼Œå¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
            if hasattr(self.config.generator, 'use_gradient_checkpointing'):
                if getattr(self.config.generator, 'use_gradient_checkpointing', False):
                    print("Enabling gradient checkpointing for memory optimization")
        
        # è®¾ç½®transformerså†…å­˜ä¼˜åŒ–
        os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # é¿å…tokenizerå¹¶è¡ŒåŒ–é—®é¢˜
    
    def _load_model_and_tokenizer(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name,
            cache_dir=self.cache_dir,
            use_fast=True
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
        # Prepare model loading arguments
        model_kwargs = {
            "cache_dir": self.cache_dir,
            "low_cpu_mem_usage": True,
            "use_cache": False,  # ç¦ç”¨ KV ç¼“å­˜ä»¥èŠ‚çœå†…å­˜
        }

        # æ ¹æ®é…ç½®åº”ç”¨é‡åŒ–
        if self.use_quantization and self.device and self.device.startswith('cuda'):
            print(f"CUDA device detected. Applying {self.quantization_type} quantization...")
            
            if self.quantization_type == "4bit":
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=False,
                )
            elif self.quantization_type == "8bit":
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_threshold=6.0,
                    llm_int8_has_fp16_weight=False,
                )
            else:
                print(f"Unknown quantization type: {self.quantization_type}, falling back to 4bit")
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=False,
                )
            
            model_kwargs["quantization_config"] = quantization_config
            model_kwargs["device_map"] = self.device  # æ˜ç¡®æŒ‡å®šè®¾å¤‡
        else:
            if self.device and self.device.startswith('cuda'):
                print("CUDA device detected but quantization disabled. Loading model without quantization.")
                model_kwargs["device_map"] = self.device  # æ˜ç¡®æŒ‡å®šè®¾å¤‡
                model_kwargs["torch_dtype"] = torch.float16
            else:
                print("CPU device detected. Loading model without quantization.")
                model_kwargs["device_map"] = "cpu"
                model_kwargs["torch_dtype"] = torch.float32

        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            **model_kwargs
        )
    
    def convert_to_json_chat_format(self, text):
        """å°†åŒ…å« ===SYSTEM=== å’Œ ===USER=== æ ‡è®°çš„å­—ç¬¦ä¸²è½¬æ¢ä¸ºJSONèŠå¤©æ ¼å¼"""
        
        # å¦‚æœè¾“å…¥å·²ç»æ˜¯JSONæ ¼å¼ï¼Œç›´æ¥è¿”å›
        if text.strip().startswith('[') and text.strip().endswith(']'):
            try:
                json.loads(text)
                print("Input is already in JSON format")
                return text
            except json.JSONDecodeError:
                pass
        
        # æ£€æµ‹ multi_stage_chinese_template.txt æ ¼å¼
        if "===SYSTEM===" in text and "===USER===" in text:
            print("Detected multi-stage Chinese template format")
            
            # æå– SYSTEM éƒ¨åˆ†
            system_start = text.find("===SYSTEM===")
            user_start = text.find("===USER===")
            
            if system_start != -1 and user_start != -1:
                system_content = text[system_start + 12:user_start].strip()
                user_content = text[user_start + 10:].strip()
                
                # æ„å»ºJSONæ ¼å¼
                chat_data = [
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": user_content}
                ]
                
                return json.dumps(chat_data, ensure_ascii=False, indent=2)
        
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡ç³»ç»ŸæŒ‡ä»¤ï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
        if "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆ" in text:
            print("Detected Chinese system instruction")
            # æå–systeméƒ¨åˆ† - æŸ¥æ‰¾ç³»ç»ŸæŒ‡ä»¤çš„å¼€å§‹å’Œç»“æŸ
            system_start = text.find("ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆ")
            
            # æŸ¥æ‰¾ç³»ç»ŸæŒ‡ä»¤çš„ç»“æŸä½ç½®ï¼ˆé€šå¸¸æ˜¯"ã€å…¬å¸è´¢åŠ¡æŠ¥å‘Šæ‘˜è¦ã€‘"æˆ–"ã€å…¬å¸è´¢åŠ¡æŠ¥å‘Šç‰‡æ®µã€‘"ä¹‹å‰ï¼‰
            context_markers = ["ã€å…¬å¸è´¢åŠ¡æŠ¥å‘Šæ‘˜è¦ã€‘", "ã€å…¬å¸è´¢åŠ¡æŠ¥å‘Šç‰‡æ®µã€‘", "ã€å®Œæ•´å…¬å¸è´¢åŠ¡æŠ¥å‘Šç‰‡æ®µã€‘"]
            context_start = -1
            for marker in context_markers:
                pos = text.find(marker)
                if pos != -1:
                    context_start = pos
                    break
            
            if system_start != -1 and context_start != -1:
                system_content = text[system_start:context_start].strip()
                user_content = text[context_start:].strip()
                
                # æ„å»ºJSONæ ¼å¼
                chat_data = [
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": user_content}
                ]
                
                return json.dumps(chat_data, ensure_ascii=False, indent=2)
        
        # å¦‚æœéƒ½ä¸åŒ¹é…ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºuseræ¶ˆæ¯
        print("No specific format detected, treating as user message")
        chat_data = [
            {"role": "user", "content": text}
        ]
        return json.dumps(chat_data, ensure_ascii=False, indent=2)

    def convert_json_to_model_format(self, json_chat: str) -> str:
        """å°†JSONèŠå¤©æ ¼å¼è½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›çš„æ ¼å¼"""
        try:
            chat_data = json.loads(json_chat)
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©è½¬æ¢æ–¹æ³•
            if "Fin-R1" in self.model_name:
                return self._convert_to_fin_r1_format(chat_data)
            elif "Qwen" in self.model_name:
                return self._convert_to_qwen_format(chat_data)
            else:
                return self._convert_to_default_format(chat_data)
                
        except json.JSONDecodeError as e:
            print(f"JSONè§£æé”™è¯¯: {e}")
            return json_chat  # è¿”å›åŸå§‹æ–‡æœ¬ä½œä¸ºfallback

    def _convert_to_fin_r1_format(self, chat_data: List[Dict]) -> str:
        """è½¬æ¢ä¸ºFin-R1æœŸæœ›çš„ <|im_start|>...<|im_end|> æ ¼å¼"""
        formatted_parts = []
        
        for message in chat_data:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                formatted_parts.append(f"<|im_start|>system\n{content}<|im_end|>")
            elif role == "user":
                formatted_parts.append(f"<|im_start|>user\n{content}<|im_end|>")
            elif role == "assistant":
                formatted_parts.append(f"<|im_start|>assistant\n{content}<|im_end|>")
        
        # æ·»åŠ assistantå¼€å§‹æ ‡è®°
        formatted_parts.append("<|im_start|>assistant\n")
        
        return "\n".join(formatted_parts)

    def _convert_to_qwen_format(self, chat_data: List[Dict]) -> str:
        """è½¬æ¢ä¸ºQwenæœŸæœ›çš„æ ¼å¼"""
        formatted_parts = []
        
        for message in chat_data:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                formatted_parts.append(f"<|im_start|>system\n{content}<|im_end|>")
            elif role == "user":
                formatted_parts.append(f"<|im_start|>user\n{content}<|im_end|>")
            elif role == "assistant":
                formatted_parts.append(f"<|im_start|>assistant\n{content}<|im_end|>")
        
        # æ·»åŠ assistantå¼€å§‹æ ‡è®°
        formatted_parts.append("<|im_start|>assistant\n")
        
        return "\n".join(formatted_parts)

    def _convert_to_default_format(self, chat_data: List[Dict]) -> str:
        """è½¬æ¢ä¸ºé»˜è®¤æ ¼å¼ï¼ˆç›´æ¥æ‹¼æ¥ï¼‰"""
        formatted_parts = []
        
        for message in chat_data:
            role = message.get("role", "user")
            content = message.get("content", "")
            
            if role == "system":
                formatted_parts.append(f"System: {content}")
            elif role == "user":
                formatted_parts.append(f"User: {content}")
            elif role == "assistant":
                formatted_parts.append(f"Assistant: {content}")
        
        return "\n".join(formatted_parts)

    def generate(self, texts: List[str]) -> List[str]:
        """ç”Ÿæˆå›ç­”ï¼ŒåŒ…å«é”™è¯¯å¤„ç†å’Œæ€§èƒ½ä¼˜åŒ–"""
        responses = []
        
        for i, text in enumerate(texts):
            try:
                print(f"å¤„ç†ç¬¬ {i+1}/{len(texts)} ä¸ªè¾“å…¥...")
                
                # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°è®¾å¤‡çŠ¶æ€
                print(f"Generator device: {self.device}")
                model_device = next(self.model.parameters()).device 
                print(f"Model device: {model_device}")
                
                print(f"Input text length: {len(text)} characters")
                
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒèŠå¤©æ ¼å¼å¹¶è¿›è¡Œè½¬æ¢
                processed_text = text
                if any(model_name in self.model_name for model_name in ["Fin-R1", "Qwen"]):
                    print(f"Chat model detected ({self.model_name}), converting to JSON chat format...")
                    json_chat_str = self.convert_to_json_chat_format(text)
                    print(f"JSON chat format length: {len(json_chat_str)} characters")
                    
                    processed_text = self.convert_json_to_model_format(json_chat_str)
                    print(f"Converted to {self.model_name} format, length: {len(processed_text)} characters")
                else:
                    print(f"Non-chat model detected ({self.model_name}), using original format...")
                
                # Tokenizeè¾“å…¥
                inputs = self.tokenizer(
                    processed_text,
                    return_tensors="pt",
                    truncation=False,  
                    padding=False,     
                    add_special_tokens=True
                )
                
                print(f"Tokenized input length: {inputs['input_ids'].shape[1]} tokens")
                
                # ç¡®ä¿æ‰€æœ‰è¾“å…¥tensoréƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if model_device.type == 'cuda':
                    input_ids = inputs["input_ids"].to(model_device)
                    attention_mask = inputs["attention_mask"].to(model_device)
                    print(f"Input tensors moved to: {input_ids.device}")
                else:
                    input_ids = inputs["input_ids"].cpu()
                    attention_mask = inputs["attention_mask"].cpu()
                    print(f"Input tensors moved to: {input_ids.device}")
                
                enable_completion = getattr(self.config.generator, 'enable_sentence_completion', True) 
                
                if enable_completion:
                    response = self._generate_with_completion_check(input_ids, attention_mask)
                else:
                    response = self._generate_simple(input_ids, attention_mask)
                
                # æ¸…ç†ç­”æ¡ˆï¼Œç§»é™¤å¯èƒ½çš„promptæ³¨å…¥å’Œæ ¼å¼æ ‡è®°
                cleaned_response = self._clean_response(response)
                responses.append(cleaned_response.strip())
                
                print(f"âœ… ç¬¬ {i+1} ä¸ªè¾“å…¥å¤„ç†å®Œæˆ")
                
            except Exception as e:
                print(f"âŒ å¤„ç†ç¬¬ {i+1} ä¸ªè¾“å…¥æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                # è¿”å›é”™è¯¯ä¿¡æ¯ï¼Œé¿å…æ•´ä¸ªæ‰¹æ¬¡å¤±è´¥
                responses.append(f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                
        return responses
    
    def _generate_simple(self, input_ids, attention_mask):
        """ç®€å•çš„ç”Ÿæˆæ–¹æ³•ï¼Œä¸åŒ…å«å®Œæ•´æ€§æ£€æŸ¥"""
        import time
        import threading
        
        start_time = time.time()
        
        # è·å–æ¨¡å‹ç‰¹å®šé…ç½®
        model_config = self._get_model_specific_config()
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ”§ ç”Ÿæˆå‚æ•°è°ƒè¯•:")
        print(f"   - max_new_tokens: {self.max_new_tokens}")
        print(f"   - model_type: {model_config['model_type']}")
        
        # æ ¹æ®æ¨¡å‹ç±»å‹æ˜¾ç¤ºç›¸å…³å‚æ•°
        if model_config["model_type"] == "fin_r1":
            print(f"   - do_sample: False (Fin-R1ä½¿ç”¨ç¡®å®šæ€§ç”Ÿæˆ)")
            print(f"   - repetition_penalty: 1.1")
        else:
            print(f"   - temperature: {self.temperature}")
            print(f"   - top_p: {self.top_p}")
            print(f"   - do_sample: {getattr(self.config.generator, 'do_sample', False)}")
        
        # è·å–é…ç½®å‚æ•°
        do_sample = getattr(self.config.generator, 'do_sample', False)
        repetition_penalty = getattr(self.config.generator, 'repetition_penalty', 1.1)
        
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„ç”Ÿæˆå‚æ•°
        if model_config["model_type"] == "fin_r1":
            # Fin-R1 å‚æ•°ï¼šåªä½¿ç”¨æ¨¡å‹æ”¯æŒçš„å‚æ•°ï¼Œé¿å…è­¦å‘Š
            generation_kwargs = {
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "max_new_tokens": self.max_new_tokens,
                "do_sample": False,  # ä½¿ç”¨ç¡®å®šæ€§ç”Ÿæˆ
                "pad_token_id": model_config["pad_token_id"],
                "eos_token_id": model_config["eos_token_id"],
                "repetition_penalty": 1.1  # é˜²æ­¢é‡å¤
            }
        else:
            # å…¶ä»–æ¨¡å‹ï¼šä½¿ç”¨å®Œæ•´çš„ç”Ÿæˆå‚æ•°
            generation_kwargs = {
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "max_new_tokens": self.max_new_tokens,
                "do_sample": do_sample,
                "pad_token_id": model_config["pad_token_id"],
                "eos_token_id": model_config["eos_token_id"],
                "repetition_penalty": repetition_penalty
            }
            
            # æ·»åŠ é‡‡æ ·ç›¸å…³å‚æ•°ï¼ˆä»…å¯¹éFin-R1æ¨¡å‹ï¼‰
            if do_sample:
                generation_kwargs.update({
                    "top_p": self.top_p,
                    "temperature": self.temperature,
                    "no_repeat_ngram_size": 3
                })
        
        print(f"   - æœ€ç»ˆä½¿ç”¨çš„max_new_tokens: {generation_kwargs['max_new_tokens']}")
        print(f"   - ç”Ÿæˆå‚æ•°æ•°é‡: {len(generation_kwargs)}")
        
        # æ·»åŠ è¶…æ—¶æœºåˆ¶
        max_generation_time = getattr(self.config.generator, 'max_generation_time', 30)  # 30ç§’è¶…æ—¶
        print(f"   - ç”Ÿæˆè¶…æ—¶æ—¶é—´: {max_generation_time}ç§’")
        
        # ä½¿ç”¨çº¿ç¨‹å’Œäº‹ä»¶æ¥ç›‘æ§è¶…æ—¶
        generation_completed = threading.Event()
        generation_result: list = [None]
        generation_error: list = [None]
        
        def generate_with_timeout():
            try:
                with torch.no_grad():
                    outputs = self.model.generate(**generation_kwargs)
                generation_result[0] = outputs
                generation_completed.set()
            except Exception as e:
                generation_error[0] = str(e)
                generation_completed.set()
        
        # å¯åŠ¨ç”Ÿæˆçº¿ç¨‹
        generation_thread = threading.Thread(target=generate_with_timeout)
        generation_thread.start()
        
        # ç­‰å¾…ç”Ÿæˆå®Œæˆæˆ–è¶…æ—¶
        if generation_completed.wait(timeout=max_generation_time):
            if generation_error[0]:
                print(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {generation_error[0]}")
                return f"ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯: {generation_error[0]}"
            outputs = generation_result[0]
            if outputs is None:
                return "ç”Ÿæˆå¤±è´¥ï¼Œæ— æ³•è·å–å›ç­”"
        else:
            print("âš ï¸  ç”Ÿæˆè¶…æ—¶ï¼Œè¿”å›éƒ¨åˆ†ç»“æœ...")
            # å°è¯•è·å–éƒ¨åˆ†ç»“æœ
            try:
                with torch.no_grad():
                    # ä½¿ç”¨æ›´å°çš„max_new_tokensé‡è¯•
                    generation_kwargs["max_new_tokens"] = min(50, self.max_new_tokens // 4)
                    outputs = self.model.generate(**generation_kwargs)
            except Exception as e:
                print(f"âŒ é‡è¯•ç”Ÿæˆä¹Ÿå¤±è´¥: {str(e)}")
                return "ç”Ÿæˆè¶…æ—¶ï¼Œæ— æ³•è·å–å›ç­”"
        
        # è®¡ç®—å®é™…ç”Ÿæˆçš„tokenæ•°é‡å’Œè€—æ—¶
        generated_tokens = outputs[0][input_ids.shape[1]:]
        actual_new_tokens = len(generated_tokens)
        end_time = time.time()
        generation_time = end_time - start_time
        
        print(f"   - å®é™…ç”Ÿæˆtokenæ•°: {actual_new_tokens}")
        print(f"   - ç”Ÿæˆè€—æ—¶: {generation_time:.2f}ç§’")
        print(f"   - ç”Ÿæˆé€Ÿåº¦: {actual_new_tokens/generation_time:.1f} tokens/ç§’")
        
        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å¤ªå¤štoken
        if actual_new_tokens >= self.max_new_tokens:
            print(f"âš ï¸  ç”Ÿæˆäº†æœ€å¤§tokenæ•° ({actual_new_tokens})ï¼Œå¯èƒ½è¢«æˆªæ–­")
        
        return self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
    
    def _clean_response(self, response: str) -> str:
        """
        å¼ºåˆ¶åå¤„ç†æ¨¡å—ï¼šæ¸…é™¤æ‰€æœ‰æ±¡æŸ“å†…å®¹
        åŸºäº test_clean.py ä¸­çš„ä¼˜åŒ–ç‰ˆæœ¬
        """
        print("ğŸ§¹ å¼€å§‹å¼ºåˆ¶åå¤„ç†...")
        
        # 0. ä¼˜å…ˆå¤„ç†å…¬å¸åç§°å’Œå¹´ä»½çš„ä¿®æ­£ï¼Œå› ä¸ºå®ƒä»¬å¯èƒ½å½±å“åç»­æ¸…ç†çš„åŒ¹é…
        text = self._fix_company_name_translation(response)
        
        # 1. ç§»é™¤å…ƒè¯„è®ºå’Œè°ƒè¯•ä¿¡æ¯ (æ”¾åœ¨æœ€å‰é¢ï¼Œå¤„ç†å¤§å—å†—ä½™)
        # æ³¨æ„ï¼šæ­£åˆ™é¡ºåºå¾ˆé‡è¦ï¼Œæ›´å®½æ³›çš„æ”¾å‰é¢
        patterns_to_remove = [
            # æœ€å¯èƒ½å‡ºç°çš„å¤§æ®µè¯„ä¼°/æ€è€ƒæ¨¡å¼
            r'æˆ‘éœ€è¦æ£€æŸ¥è¿™ä¸ªå›ç­”æ˜¯å¦ç¬¦åˆè¦æ±‚.*?====', # åŒ¹é…ä»"æˆ‘éœ€è¦æ£€æŸ¥"åˆ°"===="
            r'\*\*æ³¨æ„\*\*:.*?æ”¹è¿›åçš„ç‰ˆæœ¬[:ï¼š]', # åŒ¹é…"**æ³¨æ„**:"åˆ°"æ”¹è¿›åçš„ç‰ˆæœ¬:"
            r'ä¸Šé¢çš„ç­”æ¡ˆè™½ç„¶ç¬¦åˆè¦æ±‚.*?ä»¥ä¸‹æ˜¯æ”¹è¿›åçš„ç‰ˆæœ¬:', # åŒä¸Š
            r'###\s*æ”¹è¿›ç‰ˆç­”æ¡ˆ', # ç§»é™¤ ### æ”¹è¿›ç‰ˆç­”æ¡ˆ æ ‡é¢˜
            r'###\s*å›ç­”', # ç§»é™¤ ### å›ç­” æ ‡é¢˜
            r'å›ç­”å®Œæˆåç«‹å³åœæ­¢ç”Ÿæˆ', # ç§»é™¤promptçš„æœ€åæŒ‡ä»¤
            r'å›ç­”å®Œæˆå¹¶åœæ­¢', # ç§»é™¤promptçš„æœ€åæŒ‡ä»¤
            r'ç¡®ä¿å›ç­”', # ç§»é™¤promptçš„æœ€åæŒ‡ä»¤
            r'ç”¨æˆ·å¯èƒ½', # ç§»é™¤promptçš„æœ€åæŒ‡ä»¤
            r'æ€»ç»“ä¸€ä¸‹', # ç§»é™¤promptçš„æœ€åæŒ‡ä»¤
            r'è¯·ç”¨ç®€æ´', # ç§»é™¤promptçš„æœ€åæŒ‡ä»¤
            r'è¿›ä¸€æ­¥ç®€åŒ–', # ç§»é™¤promptçš„æœ€åæŒ‡ä»¤
            r'å†ç®€åŒ–çš„ç‰ˆæœ¬', # ç§»é™¤promptçš„æœ€åæŒ‡ä»¤
            r'æœ€ç»ˆç­”æ¡ˆå®šç¨¿å¦‚ä¸‹', # ç§»é™¤promptçš„æœ€åæŒ‡ä»¤
            r'è¿™ä¸ªæ€»ç»“å…¨é¢', # ç§»é™¤promptçš„æœ€åæŒ‡ä»¤
            r'æ ¸å¿ƒç‚¹æ€»ç»“[:ï¼š]?', # ç§»é™¤æ ¸å¿ƒç‚¹æ€»ç»“æ ‡é¢˜
            r'ä»¥ä¸Šåˆ†ææ˜¯å¦æ­£ç¡®ï¼Ÿè¿˜æœ‰å“ªäº›æ–¹é¢å¯ä»¥æ”¹è¿›ï¼Ÿ', 
            r'æ‚¨çš„åˆ†æåŸºæœ¬åˆç†ï¼Œä½†åœ¨æŸäº›åœ°æ–¹å¯ä»¥è¿›ä¸€æ­¥å®Œå–„å’Œç»†åŒ–ã€‚ä»¥ä¸‹æ˜¯å‡ ç‚¹æ”¹è¿›å»ºè®®ï¼š',
            r'ï¼ˆå‚é˜…ç¬¬ä¸‰éƒ¨åˆ†ï¼‰',
            r'ï¼ˆè¯¦æƒ…è§ç¬¬â‘¡æ®µï¼‰',
            r'è¿™äº›é—®é¢˜çš„ç­”æ¡ˆéœ€è¦ç»“åˆå…·ä½“çš„ç ”ç©¶æŠ¥å‘Šå†…å®¹è¿›è¡Œè¯¦ç»†åˆ†æã€‚',
            r'ä¸Šè¿°ç­”æ¡ˆæ¶µç›–äº†æŠ¥å‘Šä¸­æåŠçš„å…³é”®å› ç´ ï¼Œå¹¶è¿›è¡Œäº†é€‚å½“å½’çº³ã€‚',
            r'å¦‚æœ‰éœ€è¦è¿›ä¸€æ­¥ç»†åŒ–æŸä¸€æ–¹é¢çš„å†…å®¹ï¼Œè¯·å‘ŠçŸ¥ã€‚',
            r'æ³¨æ„ï¼šä»¥ä¸Šè®ºæ–­å®Œå…¨ä¾èµ–äºå·²å…¬å¼€æŠ«éœ²çš„ä¿¡æ¯èµ„æº ; å¯¹æœªæ¥çš„å…·ä½“å‰æ™¯å°šéœ€ç»“åˆæ›´å¤šå®æ—¶æ•°æ®åŠ ä»¥éªŒè¯å’Œå®Œå–„', 
            r'ï¼ˆæ³¨æ„æ­¤æ®µæ–‡å­—è™½è¯¦ç»†é˜è¿°äº†å‡ æ–¹é¢å› ç´ åŠå…¶ç›¸äº’ä½œç”¨æœºåˆ¶ï¼Œä½†ç”±äºé¢˜å¹²è¦æ±‚é«˜åº¦æµ“ç¼©ä¸ºä¸€å¥è¯å†…å®Œæˆè¡¨è¿°ï¼Œæ•…åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†é€‚å½“ç®€åŒ–å‹ç¼©ï¼‰', 
            r'è¯·æ³¨æ„ï¼Œä»¥ä¸Šå†…å®¹æ˜¯å¯¹.*?å±•æœ›ï¼Œå¹¶éç»å¯¹ç»“è®ºã€‚', 
            r'å®é™…èµ°åŠ¿è¿˜éœ€ç»“åˆå®é™…æƒ…å†µä¸æ–­è¯„ä¼°è°ƒæ•´ã€‚å¸Œæœ›è¿™ä¸ªå›ç­”å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼', 
            r'è¦é¢„æµ‹.*?åšå‡ºåˆ¤æ–­[:ï¼š]?', 
            r'ä»¥ä¸‹æ˜¯å‡ ä¸ªå…³é”®å› ç´ å’Œæ­¥éª¤[:ï¼š]?',
            r'ç»¼ä¸Šæ‰€è¿°[:ï¼š]?', 
            r'æœ€ç»ˆç»“è®º[:ï¼š]?',
            r'ç­”æ¡ˆç¤ºä¾‹[:ï¼š]?',
            r'æœ€ç»ˆç¡®è®¤[:ï¼š]?',
            r'ç­”æ¡ˆå¿ å®åœ°åæ˜ äº†åŸå§‹æ–‡æ¡£çš„å†…å®¹è€Œæ— å¤šä½™æ¨æ–­',
            r'å›ç­”[:ï¼š]\s*$', # ç§»é™¤ç‹¬ç«‹çš„"å›ç­”ï¼š"æˆ–"å›ç­”ï¼š"åœ¨è¡Œå°¾
            r'å›ç­”æ˜¯ï¼š\s*', # ç§»é™¤"å›ç­”æ˜¯ï¼š"
            r'ä»¥ä¸‹æ˜¯åŸå› ï¼š\s*', # ç§»é™¤"ä»¥ä¸‹æ˜¯åŸå› ï¼š"

            # ç§»é™¤ <|æ ‡è®°|> (è¿™äº›åº”è¯¥è¢«skip_special_tokens=Trueå¤„ç†ï¼Œä½†ä½œä¸ºåå¤„ç†å…œåº•)
            r'<\|[^>]+\|>',
            r'\\boxed\{.*?\}', # ç§»é™¤\boxed{}æ ¼å¼
            r'\\text\{.*?\}', # ç§»é™¤LaTeX textæ ¼å¼
            r'\\s*', # ç§»é™¤ä¸€äº› LaTeX ç›¸å…³çš„ç©ºç™½
            r'[\u2460-\u2469]\s*', # ç§»é™¤å¸¦åœˆæ•°å­—ï¼Œå¦‚ â‘ 

            # æ¸…é™¤Promptä¸­å­˜åœ¨çš„ç»“æ„æ€§æ ‡è®°ï¼Œå¦‚æœå®ƒä»¬æ„å¤–å‡ºç°åœ¨ç­”æ¡ˆä¸­
            r'===SYSTEM===[\s\S]*?===USER===', # ç§»é™¤Systeméƒ¨åˆ†
            r'---[\s\S]*?---', # ç§»é™¤USERéƒ¨åˆ†çš„---åˆ†éš”ç¬¦åŠå…¶ä¸­é—´çš„æ‰€æœ‰å†…å®¹ï¼ˆå¦‚æœæ„å¤–å¤åˆ¶ï¼‰
            r'ã€å…¬å¸è´¢åŠ¡æŠ¥å‘Šæ‘˜è¦ã€‘[\s\S]*?ã€å®Œæ•´å…¬å¸è´¢åŠ¡æŠ¥å‘Šç‰‡æ®µã€‘', # ç§»é™¤æ‘˜è¦å’Œç‰‡æ®µæ ‡ç­¾
            r'ã€ç”¨æˆ·é—®é¢˜ã€‘[\s\S]*?ã€å›ç­”ã€‘', # ç§»é™¤é—®é¢˜å’Œå›ç­”æ ‡ç­¾

            r'Based on the provided financial reports and analyses, the main reasons for Desay Battery\'s (000049) continued profit growth in 2021 are:', # è‹±æ–‡å¼€å¤´
            r'Here are the main reasons for Desay Battery\'s (000049) continued profit growth in 2021:', # è‹±æ–‡å¼€å¤´

            r'æ ¹æ®è´¢æŠ¥é¢„æµ‹åŠè¯„è®ºï¼Œå¾·èµ› battery \(00\) çš„20\(21\?\) å¹´åº¦åˆ©æ¶¦å¢æ¶¨ä¸»å› æœ‰ä¸‰:', # ç‰¹å®šå¼€å¤´
            r'æ ¹æ®è´¢æŠ¥é¢„æµ‹ï¼Œå¾·èµ› battery \(00\) çš„20\(21\?\) å¹´åº¦åˆ©æ¶¦å¢æ¶¨ä¸»å› æœ‰ä¸‰:', # ç‰¹å®šå¼€å¤´

            r'ç»¼ä¸Šæ‰€è¿°ï¼ŒAå®¢ æˆ·å¸‚åœºä»½é¢æ‰©å¼  \+ å¤šå…ƒåŒ–åº”ç”¨ç”Ÿæ€ç³»ç»Ÿçš„ååŒæ•ˆåº”å…±åŒæ„æˆäº†20å¹´åº¦ä¹ƒè‡³æ•´ä¸ª21è´¢å¹´å†…ç¨³å¥å¢é•¿çš„åŸºç¡€æ¡ä»¶ \. æ³¨æ„ ï¼šä»¥ä¸Šè®ºæ–­å®Œå…¨ä¾èµ–äºå·²å…¬å¼€æŠ«éœ²çš„ä¿¡æ¯èµ„æº ; å¯¹æœªæ¥çš„å…·ä½“å‰æ™¯å°šéœ€ç»“åˆæ›´å¤šå®æ—¶æ•°æ®åŠ ä»¥éªŒè¯å’Œå®Œå–„', # é’ˆå¯¹ä¸Šæ¬¡æ—¥å¿—çš„ç²¾ç¡®åŒ¹é…

            r'ï¼ˆæ³¨æ„æ­¤æ®µæ–‡å­—è™½è¯¦ç»†é˜è¿°äº†å‡ æ–¹é¢å› ç´ åŠå…¶ç›¸äº’ä½œç”¨æœºåˆ¶ï¼Œä½†ç”±äºé¢˜å¹²è¦æ±‚é«˜åº¦æµ“ç¼©ä¸ºä¸€å¥è¯å†…å®Œæˆè¡¨è¿°ï¼Œæ•…åœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œäº†é€‚å½“ç®€åŒ–å‹ç¼©ï¼‰', # é’ˆå¯¹ä¸Šæ¬¡æ—¥å¿—çš„ç²¾ç¡®åŒ¹é…

            r'å¾·èµ› battery \(00\) çš„ 20 å¹´åº¦è´¢æŠ¥æ˜¾ç¤ºå…¶åˆ©æ¶¦å¤§å¹…è¶…è¶Šé¢„æœŸ , ä¸»è¦ç”±äº iPhone 1\(Pro Max \) æ–°æœºå‹çš„éœ€æ±‚æ—ºç›› å’Œæ–°äº§å“å¸¦æ¥çš„é«˜æ¯›åˆ©ç‡ã€‚å±•æœ›æœªæ¥ä¸€å¹´ , åŸå› æœ‰ä¸‰ :', # å¦ä¸€ä¸ªç‰¹æ®Šå¼€å¤´
        ]
        
        # æ‰¹é‡åº”ç”¨æ¸…ç†æ¨¡å¼
        for pattern in patterns_to_remove:
            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE).strip()
        
        # 2. ç§»é™¤æ‰€æœ‰æ ¼å¼æ ‡è®° (é€šç”¨æ€§æ›´å¼ºçš„æ¸…ç†)
        text = re.sub(r'\*\*(.*?)\*\*', r'\1', text) # ç§»é™¤ **åŠ ç²—**ï¼Œä¿ç•™å†…å®¹
        text = re.sub(r'\*(.*?)\*', r'\1', text)   # ç§»é™¤ *æ–œä½“*ï¼Œä¿ç•™å†…å®¹
        text = text.replace("---", "").replace("===", "") # ç§»é™¤åˆ†éš”ç¬¦
        text = re.sub(r'^\s*[\d]+\.\s*', '', text, flags=re.MULTILINE) # ç§»é™¤è¡Œé¦–æ•°å­—åˆ—è¡¨ "1. "
        text = re.sub(r'^\s*[-*â€¢Â·]\s*', '', text, flags=re.MULTILINE) # ç§»é™¤è¡Œé¦–ç‚¹å·åˆ—è¡¨ "- "
        text = re.sub(r'^\s*\((\w|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å])+\)\s*', '', text, flags=re.MULTILINE) # ç§»é™¤è¡Œé¦– (i), (ä¸€)
        text = re.sub(r'\s*\([^\)]*\)\s*', '', text) # ç§»é™¤æ‰€æœ‰è‹±æ–‡æ‹¬å·åŠå†…å®¹ï¼Œ**æ…ç”¨**
        text = re.sub(r'\s*ï¼ˆ[^ï¼‰]*ï¼‰\s*', '', text) # ç§»é™¤æ‰€æœ‰ä¸­æ–‡æ‹¬å·åŠå†…å®¹ï¼Œ**æ…ç”¨**
        text = re.sub(r'[ï¼Œï¼›,;]$', '', text) # ç§»é™¤ç»“å°¾çš„é€—å·æˆ–åˆ†å·ï¼Œé˜²æ­¢å¥å­è¢«è¯¯åˆ¤ä¸ºå®Œæ•´

        # 3. æ¸…ç†å¤šä½™ç©ºç™½å’Œæ¢è¡Œ
        text = re.sub(r'\n+', ' ', text).strip() # å°†å¤šä¸ªæ¢è¡Œæ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼ï¼Œç„¶åtrim
        text = re.sub(r'\s+', ' ', text).strip() # å°†å¤šä¸ªç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼

        # 4. é™åˆ¶å¥æ•° (ç¡®ä¿å¥å­å®Œæ•´å†æˆªæ–­)
        sentences = re.split(r'(?<=[ã€‚ï¼Ÿï¼ï¼›])\s*', text) # ä½¿ç”¨lookbehindç¡®ä¿åˆ†å‰²ç¬¦ä¿ç•™åœ¨å¥å­æœ«å°¾
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if len(sentences) > 3: # è¿™é‡Œå‡è®¾ä½ æƒ³é™åˆ¶åœ¨3å¥ä»¥å†…
            sentences = sentences[:3]
        
        final_text = ' '.join(sentences) # å…ˆç”¨ç©ºæ ¼è¿æ¥

        # ç¡®ä¿ä»¥å¥æœ«æ ‡ç‚¹ç»“å°¾
        if final_text and not final_text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?')):
            final_text += 'ã€‚'
        
        # æ·»åŠ é•¿åº¦æ§åˆ¶ - é™åˆ¶å›ç­”é•¿åº¦
        enable_length_limit = getattr(self.config.generator, 'enable_response_length_limit', True)
        if enable_length_limit:
            max_chars = getattr(self.config.generator, 'max_response_chars', 800)  # é»˜è®¤800å­—ç¬¦
            if len(final_text) > max_chars:
                # å°è¯•åœ¨å¥å·å¤„æˆªæ–­
                sentences = final_text.split('ã€‚')
                truncated = ""
                for sentence in sentences:
                    if len(truncated + sentence + 'ã€‚') <= max_chars:
                        truncated += sentence + 'ã€‚'
                    else:
                        break
                
                if truncated:
                    final_text = truncated
                else:
                    # å¦‚æœæ— æ³•åœ¨å¥å·å¤„æˆªæ–­ï¼Œç›´æ¥æˆªæ–­
                    final_text = final_text[:max_chars].rstrip('ã€‚') + 'ã€‚'
                
                print(f"ğŸ“ å›ç­”è¿‡é•¿ï¼Œå·²æˆªæ–­åˆ° {len(final_text)} å­—ç¬¦")
        else:
            print(f"ğŸ“ é•¿åº¦é™åˆ¶å·²ç¦ç”¨ï¼Œå½“å‰å›ç­”é•¿åº¦: {len(final_text)} å­—ç¬¦")
        
        # å¦‚æœæ¸…ç†åä¸ºç©ºï¼Œè¿”å›åŸå§‹å“åº”çš„å‰Nä¸ªå­—ç¬¦ä½œä¸ºå…œåº•
        if not final_text.strip():
            return response[:150].strip()
            
        print(f"ğŸ§¹ åå¤„ç†å®Œæˆï¼Œé•¿åº¦: {len(final_text)} å­—ç¬¦")
        return final_text
    
    def _fix_company_name_translation(self, text: str) -> str:
        """ä¿®æ­£å…¬å¸åç§°ç¿»è¯‘é—®é¢˜å’Œå¹´ä»½é—®é¢˜"""
        # å¸¸è§çš„å…¬å¸åç§°ç¿»è¯‘æ˜ å°„å’Œä¸è§„èŒƒè¡¨è¾¾ä¿®æ­£ï¼ˆä¸­æ–‡ -> ä¸­æ–‡æ ‡å‡†ï¼‰
        company_translations = {
            # å¾·èµ›ç”µæ± ç›¸å…³ (ç¡®ä¿åŒ¹é…æ›´å®½æ³›ï¼ŒåŒ…æ‹¬ç©ºæ ¼æˆ–ä¸è§„èŒƒè¡¨è¾¾)
            r'å¾·èµ›\s*battery\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰',
            r'å¾·èµ›\s*Battery\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰',
            r'å¾·èµ›\s*BATTERY\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰',
            r'å¾·èµ›\s*battery': 'å¾·èµ›ç”µæ± ',
            r'å¾·èµ›\s*Battery': 'å¾·èµ›ç”µæ± ',
            r'å¾·èµ›\s*BATTERY': 'å¾·èµ›ç”µæ± ',
            r'å¾·èµ›\s*\(00\)': 'å¾·èµ›ç”µæ± ï¼ˆ000049ï¼‰', 
            r'å¾·å¡ç”µæ± ': 'å¾·èµ›ç”µæ± ', # ä¿®æ­£é”™åˆ«å­—
            
            # äº§å“åä¿®æ­£
            r'iPhone\s*\+\s*ProMax': 'iPhone 12 Pro Max',
            r'iPhon\s*e12ProMax': 'iPhone 12 Pro Max',
            r'iPhone\s*X\s*ç³»åˆ—': 'iPhone 12 Pro Max', 
            r'iPhone\s*1\s*\(Pro\s*Max\s*\)': 'iPhone 12 Pro Max',
            r'iPhone\s*1\s*Pro\s*Max': 'iPhone 12 Pro Max',
            r'iPhone\s*2\s*ProMax': 'iPhone 12 Pro Max', # ä¿®æ­£ä¹‹å‰æ—¥å¿—ä¸­å‡ºç°çš„
        }
        for pattern, replacement in company_translations.items():
            text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
        
        # å¹´ä»½ä¿®æ­£
        text = re.sub(r'20\s*\(\s*\d{2}\?\)\s*å¹´åº¦', r'2021å¹´åº¦', text, flags=re.IGNORECASE) # ä¿®æ­£ 20(21?) å¹´åº¦
        text = text.replace('20XXå¹´', '2021å¹´') # ä¿®æ­£ 20XXå¹´
        text = text.replace('20+', '2021') # ä¿®æ­£ 20+
        text = text.replace('2OI Iå¹´', '2021å¹´') # ä¿®æ­£ 2OI Iå¹´
        text = text.replace('20 I Iå¹´', '2021å¹´') # ä¿®æ­£ 20 I Iå¹´ (æœ‰ç©ºæ ¼çš„)

        return text
    
    def _is_sentence_complete(self, text: str) -> bool:
        """
        æ™ºèƒ½æ£€æµ‹å¥å­æ˜¯å¦å®Œæ•´ã€‚
        ä¼˜åŒ–ï¼šæ›´åŠ å‡†ç¡®åœ°åˆ¤æ–­ä¸­æ–‡å¥å­çš„å®Œæ•´æ€§ã€‚
        """
        if not text.strip():
            return True
        
        text_stripped = text.strip()
        
        # 1. æ£€æŸ¥æ˜¯å¦ä»¥å¥æœ«æ ‡ç‚¹ç»“å°¾
        sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', 'ï¼š', 'â€¦', '...', '.', '!', '?', ';']
        for ending in sentence_endings:
            if text_stripped.endswith(ending):
                return True
        
        # 2. æ£€æŸ¥æ˜¯å¦ä»¥éå¥æœ«æ ‡ç‚¹ç»“å°¾ï¼ˆé€šå¸¸è¡¨ç¤ºä¸å®Œæ•´ï¼‰
        incomplete_endings = ['ï¼Œ', 'ã€', ',', '/', '-', 'ï¼š', ':', 'ï¼›', ';']
        for ending in incomplete_endings:
            if text_stripped.endswith(ending):
                return False
        
        # 3. æ£€æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„å¥å­ç»“æ„
        # å¦‚æœæ–‡æœ¬åŒ…å«å¥å·ä½†ä¸æ˜¯ä»¥å¥å·ç»“å°¾ï¼Œå¯èƒ½ä¸å®Œæ•´
        if 'ã€‚' in text_stripped and not text_stripped.endswith('ã€‚'):
            # æ£€æŸ¥æœ€åä¸€ä¸ªå¥å·åçš„å†…å®¹æ˜¯å¦æ„æˆå®Œæ•´å¥å­
            last_period_pos = text_stripped.rfind('ã€‚')
            after_last_period = text_stripped[last_period_pos + 1:].strip()
            
            if after_last_period:
                # å¦‚æœå¥å·åæœ‰å†…å®¹ï¼Œæ£€æŸ¥æ˜¯å¦ä»¥å¥æœ«æ ‡ç‚¹ç»“å°¾
                for ending in sentence_endings:
                    if after_last_period.endswith(ending):
                        return True
                # å¦‚æœå¥å·åæœ‰å†…å®¹ä½†ä¸ä»¥å¥æœ«æ ‡ç‚¹ç»“å°¾ï¼Œå¯èƒ½ä¸å®Œæ•´
                return False
        
        # 4. æ£€æŸ¥é•¿åº¦å’Œå†…å®¹ç‰¹å¾
        # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼ˆå°‘äº10ä¸ªå­—ç¬¦ï¼‰ï¼Œä¸”ä¸ä»¥å¥æœ«æ ‡ç‚¹ç»“å°¾ï¼Œå¯èƒ½ä¸å®Œæ•´
        if len(text_stripped) < 10 and not any(text_stripped.endswith(ending) for ending in sentence_endings):
            return False
        
        # 5. æ£€æŸ¥æ˜¯å¦ä»¥å¸¸è§çš„ä¸å®Œæ•´æ¨¡å¼ç»“å°¾
        incomplete_patterns = [
            r'ç­‰$',  # ä»¥"ç­‰"ç»“å°¾
            r'ç­‰[ï¼Œã€‚]?$',  # ä»¥"ç­‰ï¼Œ"æˆ–"ç­‰ã€‚"ç»“å°¾
            r'ç­‰ç­‰$',  # ä»¥"ç­‰ç­‰"ç»“å°¾
            r'ç­‰ç­‰[ï¼Œã€‚]?$',  # ä»¥"ç­‰ç­‰ï¼Œ"æˆ–"ç­‰ç­‰ã€‚"ç»“å°¾
            r'å…¶ä¸­$',  # ä»¥"å…¶ä¸­"ç»“å°¾
            r'åŒ…æ‹¬$',  # ä»¥"åŒ…æ‹¬"ç»“å°¾
            r'ä¾‹å¦‚$',  # ä»¥"ä¾‹å¦‚"ç»“å°¾
            r'ä¸»è¦$',  # ä»¥"ä¸»è¦"ç»“å°¾
            r'é‡è¦$',  # ä»¥"é‡è¦"ç»“å°¾
            r'å…³é”®$',  # ä»¥"å…³é”®"ç»“å°¾
            r'æ ¸å¿ƒ$',  # ä»¥"æ ¸å¿ƒ"ç»“å°¾
            r'æ–¹é¢$',  # ä»¥"æ–¹é¢"ç»“å°¾
            r'å› ç´ $',  # ä»¥"å› ç´ "ç»“å°¾
            r'åŸå› $',  # ä»¥"åŸå› "ç»“å°¾
            r'å½±å“$',  # ä»¥"å½±å“"ç»“å°¾
            r'å¯¼è‡´$',  # ä»¥"å¯¼è‡´"ç»“å°¾
            r'é€ æˆ$',  # ä»¥"é€ æˆ"ç»“å°¾
            r'æ¨åŠ¨$',  # ä»¥"æ¨åŠ¨"ç»“å°¾
            r'ä¿ƒè¿›$',  # ä»¥"ä¿ƒè¿›"ç»“å°¾
            r'æå‡$',  # ä»¥"æå‡"ç»“å°¾
            r'å¢é•¿$',  # ä»¥"å¢é•¿"ç»“å°¾
            r'ä¸‹é™$',  # ä»¥"ä¸‹é™"ç»“å°¾
            r'å‡å°‘$',  # ä»¥"å‡å°‘"ç»“å°¾
            r'å¢åŠ $',  # ä»¥"å¢åŠ "ç»“å°¾
            r'æé«˜$',  # ä»¥"æé«˜"ç»“å°¾
            r'æ”¹å–„$',  # ä»¥"æ”¹å–„"ç»“å°¾
            r'ä¼˜åŒ–$',  # ä»¥"ä¼˜åŒ–"ç»“å°¾
            r'è°ƒæ•´$',  # ä»¥"è°ƒæ•´"ç»“å°¾
            r'å˜åŒ–$',  # ä»¥"å˜åŒ–"ç»“å°¾
            r'è¶‹åŠ¿$',  # ä»¥"è¶‹åŠ¿"ç»“å°¾
            r'å‰æ™¯$',  # ä»¥"å‰æ™¯"ç»“å°¾
            r'å±•æœ›$',  # ä»¥"å±•æœ›"ç»“å°¾
            r'é¢„æœŸ$',  # ä»¥"é¢„æœŸ"ç»“å°¾
            r'é¢„è®¡$',  # ä»¥"é¢„è®¡"ç»“å°¾
            r'é¢„æµ‹$',  # ä»¥"é¢„æµ‹"ç»“å°¾
            r'åˆ†æ$',  # ä»¥"åˆ†æ"ç»“å°¾
            r'ç ”ç©¶$',  # ä»¥"ç ”ç©¶"ç»“å°¾
            r'è°ƒæŸ¥$',  # ä»¥"è°ƒæŸ¥"ç»“å°¾
            r'æŠ¥å‘Š$',  # ä»¥"æŠ¥å‘Š"ç»“å°¾
            r'æ•°æ®$',  # ä»¥"æ•°æ®"ç»“å°¾
            r'æŒ‡æ ‡$',  # ä»¥"æŒ‡æ ‡"ç»“å°¾
            r'è¡¨ç°$',  # ä»¥"è¡¨ç°"ç»“å°¾
            r'ä¸šç»©$',  # ä»¥"ä¸šç»©"ç»“å°¾
            r'æ”¶å…¥$',  # ä»¥"æ”¶å…¥"ç»“å°¾
            r'åˆ©æ¶¦$',  # ä»¥"åˆ©æ¶¦"ç»“å°¾
            r'æˆæœ¬$',  # ä»¥"æˆæœ¬"ç»“å°¾
            r'ä»·æ ¼$',  # ä»¥"ä»·æ ¼"ç»“å°¾
            r'é”€é‡$',  # ä»¥"é”€é‡"ç»“å°¾
            r'äº§é‡$',  # ä»¥"äº§é‡"ç»“å°¾
            r'äº§èƒ½$',  # ä»¥"äº§èƒ½"ç»“å°¾
            r'å¸‚åœº$',  # ä»¥"å¸‚åœº"ç»“å°¾
            r'è¡Œä¸š$',  # ä»¥"è¡Œä¸š"ç»“å°¾
            r'å…¬å¸$',  # ä»¥"å…¬å¸"ç»“å°¾
            r'ä¼ä¸š$',  # ä»¥"ä¼ä¸š"ç»“å°¾
            r'äº§å“$',  # ä»¥"äº§å“"ç»“å°¾
            r'æœåŠ¡$',  # ä»¥"æœåŠ¡"ç»“å°¾
            r'æŠ€æœ¯$',  # ä»¥"æŠ€æœ¯"ç»“å°¾
            r'åˆ›æ–°$',  # ä»¥"åˆ›æ–°"ç»“å°¾
            r'å‘å±•$',  # ä»¥"å‘å±•"ç»“å°¾
            r'æˆ˜ç•¥$',  # ä»¥"æˆ˜ç•¥"ç»“å°¾
            r'è®¡åˆ’$',  # ä»¥"è®¡åˆ’"ç»“å°¾
            r'ç›®æ ‡$',  # ä»¥"ç›®æ ‡"ç»“å°¾
            r'æŠ•èµ„$',  # ä»¥"æŠ•èµ„"ç»“å°¾
            r'èèµ„$',  # ä»¥"èèµ„"ç»“å°¾
            r'åˆä½œ$',  # ä»¥"åˆä½œ"ç»“å°¾
            r'ç«äº‰$',  # ä»¥"ç«äº‰"ç»“å°¾
            r'ä¼˜åŠ¿$',  # ä»¥"ä¼˜åŠ¿"ç»“å°¾
            r'åŠ£åŠ¿$',  # ä»¥"åŠ£åŠ¿"ç»“å°¾
            r'æœºä¼š$',  # ä»¥"æœºä¼š"ç»“å°¾
            r'å¨èƒ$',  # ä»¥"å¨èƒ"ç»“å°¾
            r'é£é™©$',  # ä»¥"é£é™©"ç»“å°¾
            r'æŒ‘æˆ˜$',  # ä»¥"æŒ‘æˆ˜"ç»“å°¾
            r'é—®é¢˜$',  # ä»¥"é—®é¢˜"ç»“å°¾
            r'å›°éš¾$',  # ä»¥"å›°éš¾"ç»“å°¾
            r'ç“¶é¢ˆ$',  # ä»¥"ç“¶é¢ˆ"ç»“å°¾
            r'é™åˆ¶$',  # ä»¥"é™åˆ¶"ç»“å°¾
            r'çº¦æŸ$',  # ä»¥"çº¦æŸ"ç»“å°¾
            r'æ¡ä»¶$',  # ä»¥"æ¡ä»¶"ç»“å°¾
            r'è¦æ±‚$',  # ä»¥"è¦æ±‚"ç»“å°¾
            r'æ ‡å‡†$',  # ä»¥"æ ‡å‡†"ç»“å°¾
            r'è§„èŒƒ$',  # ä»¥"è§„èŒƒ"ç»“å°¾
            r'æ”¿ç­–$',  # ä»¥"æ”¿ç­–"ç»“å°¾
            r'æ³•è§„$',  # ä»¥"æ³•è§„"ç»“å°¾
            r'ç›‘ç®¡$',  # ä»¥"ç›‘ç®¡"ç»“å°¾
            r'ç¯å¢ƒ$',  # ä»¥"ç¯å¢ƒ"ç»“å°¾
            r'èƒŒæ™¯$',  # ä»¥"èƒŒæ™¯"ç»“å°¾
            r'æƒ…å†µ$',  # ä»¥"æƒ…å†µ"ç»“å°¾
            r'çŠ¶æ€$',  # ä»¥"çŠ¶æ€"ç»“å°¾
            r'æ°´å¹³$',  # ä»¥"æ°´å¹³"ç»“å°¾
            r'ç¨‹åº¦$',  # ä»¥"ç¨‹åº¦"ç»“å°¾
            r'è§„æ¨¡$',  # ä»¥"è§„æ¨¡"ç»“å°¾
            r'èŒƒå›´$',  # ä»¥"èŒƒå›´"ç»“å°¾
            r'é¢†åŸŸ$',  # ä»¥"é¢†åŸŸ"ç»“å°¾
            r'æ–¹å‘$',  # ä»¥"æ–¹å‘"ç»“å°¾
            r'é‡ç‚¹$',  # ä»¥"é‡ç‚¹"ç»“å°¾
            r'æ ¸å¿ƒ$',  # ä»¥"æ ¸å¿ƒ"ç»“å°¾
            r'å…³é”®$',  # ä»¥"å…³é”®"ç»“å°¾
            r'ä¸»è¦$',  # ä»¥"ä¸»è¦"ç»“å°¾
            r'é‡è¦$',  # ä»¥"é‡è¦"ç»“å°¾
            r'æ˜¾è‘—$',  # ä»¥"æ˜¾è‘—"ç»“å°¾
            r'æ˜æ˜¾$',  # ä»¥"æ˜æ˜¾"ç»“å°¾
            r'çªå‡º$',  # ä»¥"çªå‡º"ç»“å°¾
            r'ä¼˜ç§€$',  # ä»¥"ä¼˜ç§€"ç»“å°¾
            r'è‰¯å¥½$',  # ä»¥"è‰¯å¥½"ç»“å°¾
            r'ç¨³å®š$',  # ä»¥"ç¨³å®š"ç»“å°¾
            r'æŒç»­$',  # ä»¥"æŒç»­"ç»“å°¾
            r'ä¸æ–­$',  # ä»¥"ä¸æ–­"ç»“å°¾
            r'é€æ­¥$',  # ä»¥"é€æ­¥"ç»“å°¾
            r'é€æ¸$',  # ä»¥"é€æ¸"ç»“å°¾
            r'å¿«é€Ÿ$',  # ä»¥"å¿«é€Ÿ"ç»“å°¾
            r'è¿…é€Ÿ$',  # ä»¥"è¿…é€Ÿ"ç»“å°¾
            r'å¤§å¹…$',  # ä»¥"å¤§å¹…"ç»“å°¾
            r'æ˜¾è‘—$',  # ä»¥"æ˜¾è‘—"ç»“å°¾
            r'æ˜æ˜¾$',  # ä»¥"æ˜æ˜¾"ç»“å°¾
            r'çªå‡º$',  # ä»¥"çªå‡º"ç»“å°¾
            r'ä¼˜ç§€$',  # ä»¥"ä¼˜ç§€"ç»“å°¾
            r'è‰¯å¥½$',  # ä»¥"è‰¯å¥½"ç»“å°¾
            r'ç¨³å®š$',  # ä»¥"ç¨³å®š"ç»“å°¾
            r'æŒç»­$',  # ä»¥"æŒç»­"ç»“å°¾
            r'ä¸æ–­$',  # ä»¥"ä¸æ–­"ç»“å°¾
            r'é€æ­¥$',  # ä»¥"é€æ­¥"ç»“å°¾
            r'é€æ¸$',  # ä»¥"é€æ¸"ç»“å°¾
            r'å¿«é€Ÿ$',  # ä»¥"å¿«é€Ÿ"ç»“å°¾
            r'è¿…é€Ÿ$',  # ä»¥"è¿…é€Ÿ"ç»“å°¾
            r'å¤§å¹…$',  # ä»¥"å¤§å¹…"ç»“å°¾
        ]
        
        for pattern in incomplete_patterns:
            if re.search(pattern, text_stripped):
                return False
        
        # 6. æ£€æŸ¥æ˜¯å¦ä»¥æ•°å­—æˆ–å­—æ¯ç»“å°¾ï¼ˆå¯èƒ½è¡¨ç¤ºä¸å®Œæ•´ï¼‰
        if re.search(r'[\dA-Za-z]$', text_stripped):
            return False
        
        # 7. æ£€æŸ¥æ˜¯å¦ä»¥æ‹¬å·æˆ–å¼•å·ç»“å°¾ï¼ˆå¯èƒ½è¡¨ç¤ºä¸å®Œæ•´ï¼‰
        if text_stripped.endswith(('(', 'ï¼ˆ', '[', 'ã€', '"', '"', ''', ''')):
            return False
        
        # 8. é»˜è®¤ï¼šå¦‚æœä»¥ä¸Šæ£€æŸ¥éƒ½é€šè¿‡ï¼Œè®¤ä¸ºå¥å­å®Œæ•´
        return True
    
    def _generate_with_completion_check(self, input_ids, attention_mask):
        """å¸¦å®Œæ•´æ€§æ£€æŸ¥çš„ç”Ÿæˆï¼Œå¦‚æœå¥å­ä¸å®Œæ•´åˆ™é‡è¯•"""
        
        # ä»é…ç½®è·å–å‚æ•°
        max_attempts = getattr(self.config.generator, 'max_completion_attempts', 2)  # å‡å°‘é‡è¯•æ¬¡æ•°
        token_increment = getattr(self.config.generator, 'token_increment', 100)  # å¢åŠ tokenå¢é‡
        max_total_tokens = getattr(self.config.generator, 'max_total_tokens', 1000)  # å¢åŠ æœ€å¤§tokenæ•°
        
        # è·å–æ¨¡å‹ç‰¹å®šé…ç½®
        model_config = self._get_model_specific_config()
        
        for attempt in range(max_attempts):
            # è®¡ç®—å½“å‰å°è¯•çš„tokenæ•°é‡
            current_max_tokens = min(
                self.max_new_tokens + (attempt * token_increment),
                max_total_tokens
            )
            
            # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„ç”Ÿæˆå‚æ•°
            generation_kwargs = {
                "input_ids": input_ids,
                "attention_mask": attention_mask,
                "max_new_tokens": current_max_tokens,
                "do_sample": True,
                "top_p": self.top_p,
                "temperature": self.temperature,
                "pad_token_id": model_config["pad_token_id"],
                "repetition_penalty": 1.3,
                "no_repeat_ngram_size": 3
            }
            
            # åªä¸ºæ”¯æŒçš„æ¨¡å‹æ·»åŠ è¿™äº›å‚æ•°
            if model_config["model_type"] in ["fin_r1", "default"]:
                generation_kwargs.update({
                    "length_penalty": 0.8,
                    "early_stopping": True,
                    "eos_token_id": model_config["eos_token_id"]
                })
            else:
                # Qwenæ¨¡å‹ä¸ä½¿ç”¨è¿™äº›å‚æ•°
                generation_kwargs["eos_token_id"] = model_config["eos_token_id"]
            
            with torch.no_grad():
                outputs = self.model.generate(**generation_kwargs)
            
            # è§£ç å“åº”
            response = self.tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)
            
            # æ£€æŸ¥å¥å­å®Œæ•´æ€§
            if self._is_sentence_complete(response):
                return response
            
            print(f"âš ï¸  ç¬¬{attempt+1}æ¬¡ç”Ÿæˆå¥å­ä¸å®Œæ•´ï¼Œå¢åŠ tokenæ•°é‡é‡è¯•...")
        
        # å¦‚æœæ‰€æœ‰å°è¯•éƒ½å¤±è´¥ï¼Œè¿”å›æœ€åä¸€æ¬¡çš„ç»“æœ
        print("âš ï¸  è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¿”å›å½“å‰ç»“æœ")
        return response

    def _get_model_specific_config(self) -> Dict[str, Any]:
        """è·å–æ¨¡å‹ç‰¹å®šçš„é…ç½®å‚æ•°"""
        config = {}
        
        if "Fin-R1" in self.model_name:
            config.update({
                "eos_token_id": 151645,  # Fin-R1çš„EOS token ID (ä¿®æ­£)
                "pad_token_id": 0,
                "model_type": "fin_r1"
            })
        elif "Qwen" in self.model_name:
            config.update({
                "eos_token_id": 151645,  # Qwen3-8Bçš„EOS token ID
                "pad_token_id": 0,
                "model_type": "qwen"
            })
        else:
            # é»˜è®¤é…ç½®
            config.update({
                "eos_token_id": self.tokenizer.eos_token_id,
                "pad_token_id": self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,
                "model_type": "default"
            })
        
        return config