#!/usr/bin/env python3
"""
Optimized RAG UI with FAISS support
"""

import os
import sys
import re
from pathlib import Path
from typing import List, Optional, Tuple
import gradio as gr
import numpy as np
import torch
import faiss
from langdetect import detect, LangDetectException
import hashlib

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata, RagOutput
from xlm.components.rag_system.rag_system import RagSystem
from xlm.components.generator.generator import Generator
from xlm.components.retriever.retriever import Retriever
from xlm.components.retriever.reranker import QwenReranker
from xlm.components.retriever.bilingual_retriever import BilingualRetriever
from xlm.components.encoder.finbert import FinbertEncoder
from xlm.utils.dual_language_loader import DualLanguageLoader
from xlm.utils.visualizer import Visualizer
from xlm.registry.retriever import load_enhanced_retriever
from xlm.registry.generator import load_generator
from config.parameters import Config, EncoderConfig, RetrieverConfig, ModalityConfig, EMBEDDING_CACHE_DIR, RERANKER_CACHE_DIR
from xlm.components.prompt_templates.template_loader import template_loader
from xlm.utils.stock_info_extractor import extract_stock_info, extract_stock_info_with_mapping, extract_report_date

# å°è¯•å¯¼å…¥å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
try:
    from alphafin_data_process.multi_stage_retrieval_final import MultiStageRetrievalSystem
    MULTI_STAGE_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢")
    MULTI_STAGE_AVAILABLE = False

def try_load_qwen_reranker(model_name, cache_dir=None, device=None):
    """å°è¯•åŠ è½½Qwené‡æ’åºå™¨ï¼Œæ”¯æŒæŒ‡å®šè®¾å¤‡å’Œå›é€€ç­–ç•¥"""
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        
        # ç¡®ä¿cache_diræ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²
        if cache_dir is None:
            cache_dir = RERANKER_CACHE_DIR
        
        print(f"å°è¯•ä½¿ç”¨8bité‡åŒ–åŠ è½½QwenReranker...")
        print(f"åŠ è½½é‡æ’åºå™¨æ¨¡å‹: {model_name}")
        
        # ä½¿ç”¨æŒ‡å®šçš„è®¾å¤‡ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šåˆ™ä½¿ç”¨GPU 0
        if device is None:
            device = "cuda:0"  # é»˜è®¤ä½¿ç”¨GPU 0
        
        print(f"- è®¾å¤‡: {device}")
        print(f"- ç¼“å­˜ç›®å½•: {cache_dir}")
        print(f"- é‡åŒ–: True (8bit)")
        print(f"- Flash Attention: False")
        
        # æ£€æŸ¥è®¾å¤‡ç±»å‹
        if device.startswith("cuda"):
            try:
                # è§£æGPU ID
                gpu_id = int(device.split(":")[1]) if ":" in device else 0
                
                # æ£€æŸ¥GPUå†…å­˜
                gpu_memory = torch.cuda.get_device_properties(gpu_id).total_memory
                allocated_memory = torch.cuda.memory_allocated(gpu_id)
                free_memory = gpu_memory - allocated_memory
                
                print(f"- GPU {gpu_id} æ€»å†…å­˜: {gpu_memory / 1024**3:.1f}GB")
                print(f"- GPU {gpu_id} å·²ç”¨å†…å­˜: {allocated_memory / 1024**3:.1f}GB")
                print(f"- GPU {gpu_id} å¯ç”¨å†…å­˜: {free_memory / 1024**3:.1f}GB")
                
                # å¦‚æœå¯ç”¨å†…å­˜å°‘äº2GBï¼Œå›é€€åˆ°CPU
                if free_memory < 2 * 1024**3:  # 2GB
                    print(f"- GPU {gpu_id} å†…å­˜ä¸è¶³ï¼Œå›é€€åˆ°CPU")
                    device = "cpu"
                else:
                    # å°è¯•åœ¨æŒ‡å®šGPUä¸ŠåŠ è½½
                    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
                    model = AutoModelForSequenceClassification.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        load_in_8bit=True
                    )
                    print("é‡åŒ–æ¨¡å‹å·²è‡ªåŠ¨è®¾ç½®åˆ°è®¾å¤‡ï¼Œè·³è¿‡æ‰‹åŠ¨ç§»åŠ¨")
                    print("é‡æ’åºå™¨æ¨¡å‹åŠ è½½å®Œæˆ")
                    print("é‡åŒ–åŠ è½½æˆåŠŸï¼")
                    return QwenReranker(model_name, device=device, cache_dir=cache_dir)
                    
            except Exception as e:
                print(f"- GPU {gpu_id} åŠ è½½å¤±è´¥: {e}")
                print("- å›é€€åˆ°CPU")
                device = "cpu"
        
        # CPUå›é€€
        device = "cpu"  # ç¡®ä¿deviceå˜é‡æ€»æ˜¯æœ‰å®šä¹‰
        if device == "cpu" or not torch.cuda.is_available():
            print(f"- è®¾å¤‡: {device}")
            print(f"- ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"- é‡åŒ–: False (CPUæ¨¡å¼)")
            
            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
            model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                torch_dtype=torch.float32
            )
            model = model.to(device)
            print("é‡æ’åºå™¨æ¨¡å‹åŠ è½½å®Œæˆ")
            print("CPUåŠ è½½æˆåŠŸï¼")
            return QwenReranker(model_name, device=device, cache_dir=cache_dir)
            
    except Exception as e:
        print(f"åŠ è½½é‡æ’åºå™¨å¤±è´¥: {e}")
        return None

class OptimizedRagUI:
    def __init__(
        self,
        # encoder_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        encoder_model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
        # generator_model_name: str = "facebook/opt-125m",
        # generator_model_name: str = "Qwen/Qwen2-1.5B-Instruct",
        # generator_model_name: str = "SUFE-AIFLM-Lab/Fin-R1",  # ä½¿ç”¨é‡‘èä¸“ç”¨Fin-R1æ¨¡å‹
        cache_dir: Optional[str] = None,
        use_faiss: bool = True,
        enable_reranker: bool = True,
        use_existing_embedding_index: Optional[bool] = None,  # ä»configè¯»å–ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼
        max_alphafin_chunks: Optional[int] = None,  # ä»configè¯»å–ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼
        window_title: str = "RAG System with FAISS",
        title: str = "RAG System with FAISS",
        examples: Optional[List[List[str]]] = None,
    ):
        # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        self.config = Config()
        self.cache_dir = EMBEDDING_CACHE_DIR if (not cache_dir or not isinstance(cache_dir, str)) else cache_dir
        self.encoder_model_name = encoder_model_name
        # ä»configè¯»å–ç”Ÿæˆå™¨æ¨¡å‹åç§°ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
        self.generator_model_name = self.config.generator.model_name
        self.use_faiss = use_faiss
        self.enable_reranker = enable_reranker
        # ä»configè¯»å–å‚æ•°ï¼Œå¦‚æœä¼ å…¥Noneåˆ™ä½¿ç”¨configé»˜è®¤å€¼
        self.use_existing_embedding_index = use_existing_embedding_index if use_existing_embedding_index is not None else self.config.retriever.use_existing_embedding_index
        self.max_alphafin_chunks = max_alphafin_chunks if max_alphafin_chunks is not None else self.config.retriever.max_alphafin_chunks
        self.window_title = window_title
        self.title = title
        self.examples = examples or [
            ["ä»€ä¹ˆæ˜¯è‚¡ç¥¨æŠ•èµ„ï¼Ÿ"],
            ["è¯·è§£é‡Šå€ºåˆ¸çš„åŸºæœ¬æ¦‚å¿µ"],
            ["åŸºé‡‘æŠ•èµ„ä¸è‚¡ç¥¨æŠ•èµ„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"],
            ["What is stock investment?"],
            ["Explain the basic concepts of bonds"],
            ["What are the differences between fund investment and stock investment?"]
        ]
        
        # Set environment variables for model caching
        os.environ['TRANSFORMERS_CACHE'] = os.path.join(self.cache_dir, 'transformers')
        os.environ['HF_HOME'] = self.cache_dir
        os.environ['HF_DATASETS_CACHE'] = os.path.join(self.cache_dir, 'datasets')
        
        # Initialize system components
        self._init_components()
        
        # Create Gradio interface
        self.interface = self._create_interface()
        self.docid2context = self._load_docid2context(self.config.data.chinese_data_path)

    def _load_docid2context(self, data_path):
        import json
        docid2context = {}
        try:
            with open(data_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                for item in data:
                    doc_id = str(item.get("doc_id", ""))
                    # æ¯”è¾ƒoriginal_contentå’Œcontextçš„é•¿åº¦ï¼Œä½¿ç”¨æ›´é•¿çš„é‚£ä¸ª
                    original_content = item.get("original_content", "")
                    context_content = item.get("context", "")
                    context = original_content if len(original_content) > len(context_content) else context_content
                    if doc_id and context:  # åªæ·»åŠ æœ‰æ•ˆçš„æ˜ å°„
                        docid2context[doc_id] = context
            print(f"æˆåŠŸåŠ è½½ {len(docid2context)} ä¸ªdoc_idåˆ°contextçš„æ˜ å°„")
        except Exception as e:
            print(f"åŠ è½½doc_idåˆ°contextæ˜ å°„å¤±è´¥: {e}")
        return docid2context

    def _init_components(self):
        """Initialize RAG system components"""
        print("\nStep 1. Loading bilingual retriever with dual encoders...")
        
        # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        config = Config()
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"GPUå†…å­˜æ¸…ç†å®Œæˆ")
        
        # åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
        print("\nStep 1.0. Initializing Multi-Stage Retrieval System for Chinese queries...")
        if MULTI_STAGE_AVAILABLE:
            try:
                # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„ä¸­æ–‡æ•°æ®è·¯å¾„
                chinese_data_path = Path(config.data.chinese_data_path)
                
                if chinese_data_path.exists():
                    print("âœ… åˆå§‹åŒ–ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
                    self.chinese_retrieval_system = MultiStageRetrievalSystem(
                        data_path=chinese_data_path,
                        dataset_type="chinese",
                        use_existing_config=True
                    )
                    print("âœ… ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"âŒ ä¸­æ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {chinese_data_path}")
                    self.chinese_retrieval_system = None
                
                # è‹±æ–‡æ•°æ®ä½¿ç”¨ä¼ ç»ŸRAGç³»ç»Ÿï¼Œä¸åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢
                print("â„¹ï¸ è‹±æ–‡æ•°æ®ä½¿ç”¨ä¼ ç»ŸRAGç³»ç»Ÿï¼Œè·³è¿‡å¤šé˜¶æ®µæ£€ç´¢åˆå§‹åŒ–")
                self.english_retrieval_system = None
                
            except Exception as e:
                print(f"âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.chinese_retrieval_system = None
                self.english_retrieval_system = None
        
        print("\nStep 1.1. Loading data with optimized chunking...")
        # åŠ è½½åŒè¯­è¨€æ•°æ® - ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è·¯å¾„
        data_loader = DualLanguageLoader()
        
        # åˆ†åˆ«åŠ è½½ä¸­æ–‡å’Œè‹±æ–‡æ•°æ®
        chinese_docs = []
        english_docs = []
        
        # åŠ è½½ä¸­æ–‡æ•°æ®
        if config.data.chinese_data_path:
            print(f"åŠ è½½ä¸­æ–‡æ•°æ®: {config.data.chinese_data_path}")
            if config.data.chinese_data_path.endswith('.json'):
                chinese_docs = data_loader.load_alphafin_data(config.data.chinese_data_path)
            elif config.data.chinese_data_path.endswith('.jsonl'):
                chinese_docs = data_loader.load_jsonl_data(config.data.chinese_data_path, 'chinese')
        
        # åŠ è½½è‹±æ–‡æ•°æ®ï¼ˆä½¿ç”¨context-onlyæ–¹æ³•ï¼‰
        if config.data.english_data_path:
            print(f"åŠ è½½è‹±æ–‡æ•°æ®: {config.data.english_data_path}")
            english_docs = data_loader.load_tatqa_context_only(config.data.english_data_path)
        
        print(f"æ•°æ®åŠ è½½å®Œæˆ: {len(chinese_docs)} ä¸ªä¸­æ–‡æ–‡æ¡£, {len(english_docs)} ä¸ªè‹±æ–‡æ–‡æ¡£")
        
        # æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("\nStep 2. Loading Chinese encoder...")
        print(f"Step 2. Loading Chinese encoder ({config.encoder.chinese_model_path})...")
        self.encoder_ch = FinbertEncoder(
            model_name=config.encoder.chinese_model_path,
            cache_dir=config.encoder.cache_dir,
            device=config.encoder.device  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾å¤‡è®¾ç½®
        )
        
        print("\nStep 3. Loading English encoder...")
        print(f"Step 3. Loading English encoder ({config.encoder.english_model_path})...")
        self.encoder_en = FinbertEncoder(
            model_name=config.encoder.english_model_path,
            cache_dir=config.encoder.cache_dir,
            device=config.encoder.device  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾å¤‡è®¾ç½®
        )
        
        # æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # å¼ºåˆ¶é‡æ–°è®¡ç®—åµŒå…¥
        if self.use_existing_embedding_index is False:
            print("Forcing to recompute embeddings (ignoring existing cache)...")
        else:
            print("Using existing embedding index (if available)...")
        
        print(f"[UI DEBUG] self.use_existing_embedding_index={self.use_existing_embedding_index}")
        
        print("=== BEFORE BilingualRetriever ===")
        self.retriever = BilingualRetriever(
            encoder_en=self.encoder_en,
            encoder_ch=self.encoder_ch,
            corpus_documents_en=english_docs,
            corpus_documents_ch=chinese_docs,
            use_faiss=self.use_faiss,
            use_gpu=True,
            batch_size=32,
            cache_dir=config.encoder.cache_dir,
            use_existing_embedding_index=self.use_existing_embedding_index
        )
        print("=== AFTER BilingualRetriever ===")
        print(f"[UI DEBUG] BilingualRetriever created with use_existing_embedding_index={self.use_existing_embedding_index}")
        
        # æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("\nStep 3.1. Initializing FAISS index...")
        self._init_faiss()
        
        print("\nStep 4. Loading reranker...")
        if self.enable_reranker:
            self.reranker = try_load_qwen_reranker(
                model_name=config.reranker.model_name,
                cache_dir=config.reranker.cache_dir,
                device=config.reranker.device  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾å¤‡è®¾ç½®
            )
            if self.reranker is None:
                print("âš ï¸ é‡æ’åºå™¨åŠ è½½å¤±è´¥ï¼Œå°†ç¦ç”¨é‡æ’åºåŠŸèƒ½")
                self.enable_reranker = False
        else:
            self.reranker = None
        
        # æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("\nStep 5. Loading generator...")
        # ç°åœ¨GPUå†…å­˜å……è¶³ï¼Œä½¿ç”¨GPU 1åŠ è½½ç”Ÿæˆå™¨
        try:
            print("GPUå†…å­˜å……è¶³ï¼Œä½¿ç”¨GPU 1åŠ è½½ç”Ÿæˆå™¨...")
            self.generator = load_generator(
                generator_model_name=config.generator.model_name,
                use_local_llm=True,
                use_gpu=True,  # ä½¿ç”¨GPU
                gpu_device="cuda:1",  # ä½¿ç”¨GPU 1
                cache_dir=config.generator.cache_dir
            )
            print("âœ… ç”Ÿæˆå™¨GPUæ¨¡å¼åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå™¨GPUæ¨¡å¼åŠ è½½å¤±è´¥: {e}")
            print("å›é€€åˆ°CPUæ¨¡å¼...")
            try:
                self.generator = load_generator(
                    generator_model_name=config.generator.model_name,
                    use_local_llm=True,
                    use_gpu=False,  # å›é€€åˆ°CPU
                    cache_dir=config.generator.cache_dir
                )
                print("âœ… ç”Ÿæˆå™¨CPUæ¨¡å¼åŠ è½½æˆåŠŸ")
            except Exception as e2:
                print(f"âŒ ç”Ÿæˆå™¨CPUæ¨¡å¼ä¹Ÿå¤±è´¥: {e2}")
                raise e2
        
        # æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        print("\nStep 6. Initializing RAG system...")
        self.rag_system = RagSystem(
            retriever=self.retriever,
            generator=self.generator,
            retriever_top_k=20
        )
        
        print("\nStep 7. Loading visualizer...")
        self.visualizer = Visualizer(show_mid_features=True)
    
    def _init_faiss(self):
        """Initialize FAISS index"""
        # BilingualRetrieverå·²ç»å¤„ç†äº†FAISSç´¢å¼•ï¼Œè¿™é‡Œä¸éœ€è¦é¢å¤–çš„FAISSåˆå§‹åŒ–
        print("FAISSç´¢å¼•å·²åœ¨BilingualRetrieverä¸­å¤„ç†ï¼Œè·³è¿‡UIå±‚çš„FAISSåˆå§‹åŒ–")
        self.index = None
    
    def _create_interface(self) -> gr.Blocks:
        """Create optimized Gradio interface"""
        with gr.Blocks(
            title=self.window_title
        ) as interface:
            # æ ‡é¢˜
            gr.Markdown(f"# {self.title}")
            
            # è¾“å…¥åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=4):
                    datasource = gr.Radio(
                        choices=["TatQA", "AlphaFin", "Both"],
                        value="Both",
                        label="Data Source"
                    )
                    
            with gr.Row():
                with gr.Column(scale=4):
                    question_input = gr.Textbox(
                        show_label=False,
                        placeholder="Enter your question",
                        label="Question",
                        lines=3
                    )
            
            # æ§åˆ¶æŒ‰é’®åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=1):
                    reranker_checkbox = gr.Checkbox(
                        label="Enable Reranker",
                        value=True,
                        interactive=True
                    )
                with gr.Column(scale=1):
                    submit_btn = gr.Button("Submit")
            
            # ä½¿ç”¨æ ‡ç­¾é¡µåˆ†ç¦»æ˜¾ç¤º
            with gr.Tabs():
                # å›ç­”æ ‡ç­¾é¡µ
                with gr.TabItem("Answer"):
                    answer_output = gr.Textbox(
                        show_label=False,
                        interactive=False,
                        label="Generated Response",
                        lines=5
                    )
                
                # è§£é‡Šæ ‡ç­¾é¡µ
                with gr.TabItem("Explanation"):
                    # ä½¿ç”¨HTMLç»„ä»¶æ¥æ˜¾ç¤ºå¯ç‚¹å‡»çš„ä¸Šä¸‹æ–‡
                    context_html_output = gr.HTML(
                        label="Retrieved Contexts (Click to expand)",
                        value="<p>No contexts retrieved yet.</p>"
                    )
                    
                    # ä¿ç•™åŸæœ‰çš„DataFrameä½œä¸ºå¤‡ç”¨
                    context_output = gr.Dataframe(
                        headers=["Score", "Context"],
                        datatype=["number", "str"],
                        label="Retrieved Contexts (Table View)",
                        interactive=False,
                        visible=False  # é»˜è®¤éšè—
                    )

            # æ·»åŠ ç¤ºä¾‹é—®é¢˜
            gr.Examples(
                examples=self.examples,
                inputs=[question_input],
                label="Example Questions"
            )

            # ç»‘å®šäº‹ä»¶
            submit_btn.click(
                self._process_question,
                inputs=[question_input, datasource, reranker_checkbox],
                outputs=[answer_output, context_html_output]
            )
            
            return interface
    
    def _process_question(
        self,
        question: str,
        datasource: str,
        reranker_checkbox: bool
    ) -> tuple[str, str]:
        if not question.strip():
            return "è¯·è¾“å…¥é—®é¢˜", ""
        
        # æ£€æµ‹è¯­è¨€
        try:
            lang = detect(question)
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡å­—ç¬¦
            chinese_chars = sum(1 for char in question if '\u4e00' <= char <= '\u9fff')
            total_chars = len([char for char in question if char.isalpha() or '\u4e00' <= char <= '\u9fff'])
            
            # å¦‚æœåŒ…å«ä¸­æ–‡å­—ç¬¦ä¸”ä¸­æ–‡æ¯”ä¾‹è¶…è¿‡30%ï¼Œæˆ–è€…langdetectæ£€æµ‹ä¸ºä¸­æ–‡ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸­æ–‡
            if chinese_chars > 0 and (chinese_chars / total_chars > 0.3 or lang.startswith('zh')):
                language = 'zh'
            else:
                language = 'en'
        except:
            # å¦‚æœlangdetectå¤±è´¥ï¼Œä½¿ç”¨å­—ç¬¦æ£€æµ‹
            chinese_chars = sum(1 for char in question if '\u4e00' <= char <= '\u9fff')
            language = 'zh' if chinese_chars > 0 else 'en'
        
        # ç»Ÿä¸€ä½¿ç”¨ç›¸åŒçš„RAGç³»ç»Ÿå¤„ç†
        return self._unified_rag_processing(question, language, reranker_checkbox)
    
    def _unified_rag_processing(self, question: str, language: str, reranker_checkbox: bool) -> tuple[str, str]:
        """
        ç»Ÿä¸€çš„RAGå¤„ç†æµç¨‹ - ä¸­æ–‡å’Œè‹±æ–‡ä½¿ç”¨ç›¸åŒçš„FAISSã€é‡æ’åºå™¨å’Œç”Ÿæˆå™¨
        """
        print(f"å¼€å§‹ç»Ÿä¸€RAGæ£€ç´¢...")
        print(f"æŸ¥è¯¢: {question}")
        print(f"è¯­è¨€: {language}")
        print(f"ä½¿ç”¨FAISS: {self.use_faiss}")
        print(f"å¯ç”¨é‡æ’åºå™¨: {reranker_checkbox}")
        
                # 1. ä¸­æ–‡æŸ¥è¯¢ï¼šå…³é”®è¯æå– -> å…ƒæ•°æ®è¿‡æ»¤ -> FAISSæ£€ç´¢ -> chunké‡æ’åº
        if language == 'zh' and self.chinese_retrieval_system:
            print("æ£€æµ‹åˆ°ä¸­æ–‡æŸ¥è¯¢ï¼Œå°è¯•ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤...")
            try:
                # 1.1 æå–å…³é”®è¯
                company_name, stock_code = extract_stock_info_with_mapping(question)
                report_date = extract_report_date(question)
                if company_name:
                    print(f"æå–åˆ°å…¬å¸åç§°: {company_name}")
                if stock_code:
                    print(f"æå–åˆ°è‚¡ç¥¨ä»£ç : {stock_code}")
                if report_date:
                    print(f"æå–åˆ°æŠ¥å‘Šæ—¥æœŸ: {report_date}")
                
                # 1.2 å…ƒæ•°æ®è¿‡æ»¤
                candidate_indices = self.chinese_retrieval_system.pre_filter(
                    company_name=company_name,
                    stock_code=stock_code,
                    report_date=report_date,
                    max_candidates=1000
                )
                
                if candidate_indices:
                    print(f"å…ƒæ•°æ®è¿‡æ»¤æˆåŠŸï¼Œæ‰¾åˆ° {len(candidate_indices)} ä¸ªå€™é€‰æ–‡æ¡£")
                    
                    # 1.3 ä½¿ç”¨å·²æœ‰çš„FAISSç´¢å¼•åœ¨è¿‡æ»¤åçš„æ–‡æ¡£ä¸­è¿›è¡Œæ£€ç´¢
                    faiss_results = self.chinese_retrieval_system.faiss_search(
                        query=question,
                        candidate_indices=candidate_indices,
                        top_k=self.config.retriever.retrieval_top_k  # ä½¿ç”¨é…ç½®çš„æ£€ç´¢æ•°é‡
                    )
                    
                    if faiss_results:
                        print(f"FAISSæ£€ç´¢æˆåŠŸï¼Œæ‰¾åˆ° {len(faiss_results)} ä¸ªç›¸å…³æ–‡æ¡£")
                        
                        # 1.4 è½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼ï¼ˆcontentæ˜¯chunkï¼‰
                        unique_docs = []
                        for doc_idx, faiss_score in faiss_results:
                            original_doc = self.chinese_retrieval_system.data[doc_idx]
                            chunks = self.chinese_retrieval_system.doc_to_chunks_mapping.get(doc_idx, [])
                            if chunks:
                                content = chunks[0]  # ä½¿ç”¨chunkä½œä¸ºcontent
                                # ä½¿ç”¨åŸå§‹æ•°æ®æ–‡ä»¶çš„doc_idï¼Œè€Œä¸æ˜¯ç´¢å¼•å·
                                original_doc_id = original_doc.get('doc_id', str(doc_idx))
                                doc = DocumentWithMetadata(
                                    content=content,
                                    metadata=DocumentMetadata(
                                        source=str(original_doc.get('company_name', '')),
                                        created_at="",
                                        author="",
                                        language="chinese",
                                        doc_id=str(original_doc_id),
                                        origin_doc_id=str(original_doc_id)  # ç¡®ä¿origin_doc_idä¹Ÿä½¿ç”¨åŸå§‹doc_id
                                    )
                                )
                                unique_docs.append((doc, faiss_score))
                        
                        # 1.5 å¯¹chunkåº”ç”¨é‡æ’åºå™¨
                        if reranker_checkbox and self.reranker:
                            print("å¯¹chunkåº”ç”¨é‡æ’åºå™¨...")
                            reranked_docs = []
                            reranked_scores = []
                            
                            # æå–æ–‡æ¡£å†…å®¹ï¼ˆä¸­æ–‡æ•°æ®ï¼šsummary + contextï¼Œè‹±æ–‡æ•°æ®ï¼šcontextï¼‰
                            doc_texts = []
                            doc_id_to_original_map = {}  # ä½¿ç”¨doc_idè¿›è¡Œæ˜ å°„
                            for doc, _ in unique_docs:
                                # è·å–doc_id
                                doc_id = getattr(doc.metadata, 'doc_id', None)
                                if doc_id is None:
                                    # å¦‚æœæ²¡æœ‰doc_idï¼Œä½¿ç”¨contentçš„hashä½œä¸ºå”¯ä¸€æ ‡è¯†
                                    doc_id = hashlib.md5(doc.content.encode('utf-8')).hexdigest()[:16]
                                
                                if hasattr(doc, 'metadata') and hasattr(doc.metadata, 'language') and doc.metadata.language == 'chinese':
                                    # ä¸­æ–‡æ•°æ®ï¼šå°è¯•ç»„åˆsummaryå’Œcontext
                                    summary = ""
                                    if hasattr(doc.metadata, 'summary') and doc.metadata.summary:
                                        summary = doc.metadata.summary
                                    else:
                                        # å¦‚æœæ²¡æœ‰summaryï¼Œä½¿ç”¨contextçš„å‰200å­—ç¬¦ä½œä¸ºsummary
                                        summary = doc.content[:200] + "..." if len(doc.content) > 200 else doc.content
                                    
                                    # ç»„åˆsummaryå’Œcontextï¼Œé¿å…è¿‡é•¿
                                    combined_text = f"æ‘˜è¦ï¼š{summary}\n\nè¯¦ç»†å†…å®¹ï¼š{doc.content}"
                                    # é™åˆ¶æ€»é•¿åº¦ï¼Œé¿å…è¶…å‡ºé‡æ’åºå™¨çš„tokené™åˆ¶
                                    if len(combined_text) > 4000:  # å‡è®¾é‡æ’åºå™¨é™åˆ¶ä¸º4000å­—ç¬¦
                                        combined_text = f"æ‘˜è¦ï¼š{summary}\n\nè¯¦ç»†å†…å®¹ï¼š{doc.content[:3500]}..."
                                    doc_texts.append(combined_text)
                                    doc_id_to_original_map[doc_id] = doc  # ä½¿ç”¨doc_idæ˜ å°„
                                else:
                                    # è‹±æ–‡æ•°æ®ï¼šåªä½¿ç”¨context
                                    doc_texts.append(doc.content)
                                    doc_id_to_original_map[doc_id] = doc  # ä½¿ç”¨doc_idæ˜ å°„
                            
                            # ä½¿ç”¨QwenRerankerçš„rerankæ–¹æ³•
                            reranked_items = self.reranker.rerank(
                                query=question,
                                documents=doc_texts,
                                batch_size=4
                            )
                            
                            # å°†é‡æ’åºç»“æœæ˜ å°„å›æ–‡æ¡£ï¼ˆä½¿ç”¨ç´¢å¼•ä½ç½®æ˜ å°„ï¼‰
                            for i, (doc_text, rerank_score) in enumerate(reranked_items):
                                if i < len(unique_docs):
                                    # ä½¿ç”¨ç´¢å¼•ä½ç½®è·å–å¯¹åº”çš„doc_id
                                    doc_id = getattr(unique_docs[i][0].metadata, 'doc_id', None)
                                    if doc_id is None:
                                        doc_id = hashlib.md5(unique_docs[i][0].content.encode('utf-8')).hexdigest()[:16]
                                    
                                    if doc_id in doc_id_to_original_map:
                                        reranked_docs.append(doc_id_to_original_map[doc_id])
                                        reranked_scores.append(rerank_score)
                            
                            try:
                                sorted_pairs = sorted(zip(reranked_docs, reranked_scores), key=lambda x: x[1], reverse=True)
                                unique_docs = [(doc, score) for doc, score in sorted_pairs[:self.config.retriever.rerank_top_k]]
                                print(f"chunké‡æ’åºå®Œæˆï¼Œä¿ç•™å‰ {len(unique_docs)} ä¸ªæ–‡æ¡£")
                            except Exception as e:
                                print(f"é‡æ’åºå¼‚å¸¸: {e}")
                                unique_docs = []
                        else:
                            print("è·³è¿‡é‡æ’åºå™¨...")
                            unique_docs = unique_docs[:10]
                        
                        # 1.6 ä½¿ç”¨chunkç”Ÿæˆç­”æ¡ˆ
                        answer = self._generate_answer_with_context(question, unique_docs)
                        return self._format_and_return_result(answer, unique_docs, reranker_checkbox, "ä¸­æ–‡å®Œæ•´æµç¨‹")
                    else:
                        print("FAISSæ£€ç´¢æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå›é€€åˆ°ç»Ÿä¸€FAISSæ£€ç´¢...")
                else:
                    print("å…ƒæ•°æ®è¿‡æ»¤æœªæ‰¾åˆ°å€™é€‰æ–‡æ¡£ï¼Œå›é€€åˆ°ç»Ÿä¸€FAISSæ£€ç´¢...")
                    
            except Exception as e:
                print(f"ä¸­æ–‡å¤„ç†æµç¨‹å¤±è´¥: {e}ï¼Œå›é€€åˆ°ç»Ÿä¸€RAGå¤„ç†")
        
        # 2. ä½¿ç”¨ç»Ÿä¸€çš„æ£€ç´¢å™¨è¿›è¡ŒFAISSæ£€ç´¢
        # ä¸­æ–‡ä½¿ç”¨summaryï¼Œè‹±æ–‡ä½¿ç”¨chunk
        retrieval_result = self.retriever.retrieve(
            text=question, 
            top_k=self.config.retriever.retrieval_top_k,  # ä½¿ç”¨é…ç½®çš„æ£€ç´¢æ•°é‡
            return_scores=True,
            language=language
        )
        
        # å¤„ç†è¿”å›ç»“æœ
        if isinstance(retrieval_result, tuple):
            retrieved_documents, retriever_scores = retrieval_result
        else:
            retrieved_documents = retrieval_result
            retriever_scores = [1.0] * len(retrieved_documents)  # é»˜è®¤åˆ†æ•°
        
        print(f"FAISSå¬å›æ•°é‡: {len(retrieved_documents)}")
        if not retrieved_documents:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£", ""
        
        # 3. å¯é€‰çš„é‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if reranker_checkbox and self.reranker:
            print(f"åº”ç”¨é‡æ’åºå™¨... è¾“å…¥æ•°é‡: {len(retrieved_documents)}")
            reranked_docs = []
            reranked_scores = []
            
            # æ£€æµ‹æŸ¥è¯¢è¯­è¨€
            try:
                from langdetect import detect
                query_language = detect(question)
                is_chinese_query = query_language.startswith('zh')
            except:
                # å¦‚æœè¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œæ ¹æ®æŸ¥è¯¢å†…å®¹åˆ¤æ–­
                is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
            
            # æå–æ–‡æ¡£å†…å®¹ï¼ˆåªæœ‰ä¸­æ–‡æŸ¥è¯¢ä½¿ç”¨æ™ºèƒ½å†…å®¹é€‰æ‹©ï¼‰
            doc_texts = []
            doc_id_to_original_map = {}  # ä½¿ç”¨doc_idè¿›è¡Œæ˜ å°„
            for doc in retrieved_documents:
                # è·å–doc_id
                doc_id = getattr(doc.metadata, 'doc_id', None)
                if doc_id is None:
                    # å¦‚æœæ²¡æœ‰doc_idï¼Œä½¿ç”¨contentçš„hashä½œä¸ºå”¯ä¸€æ ‡è¯†
                    doc_id = hashlib.md5(doc.content.encode('utf-8')).hexdigest()[:16]
                
                if is_chinese_query and hasattr(doc, 'metadata') and hasattr(doc.metadata, 'language') and doc.metadata.language == 'chinese':
                    # ä¸­æ–‡æ•°æ®ï¼šå°è¯•ç»„åˆsummaryå’Œcontext
                    summary = ""
                    if hasattr(doc.metadata, 'summary') and doc.metadata.summary:
                        summary = doc.metadata.summary
                    else:
                        # å¦‚æœæ²¡æœ‰summaryï¼Œä½¿ç”¨contextçš„å‰200å­—ç¬¦ä½œä¸ºsummary
                        summary = doc.content[:200] + "..." if len(doc.content) > 200 else doc.content
                    
                    # ç»„åˆsummaryå’Œcontextï¼Œé¿å…è¿‡é•¿
                    combined_text = f"æ‘˜è¦ï¼š{summary}\n\nè¯¦ç»†å†…å®¹ï¼š{doc.content}"
                    # é™åˆ¶æ€»é•¿åº¦ï¼Œé¿å…è¶…å‡ºé‡æ’åºå™¨çš„tokené™åˆ¶
                    if len(combined_text) > 4000:  # å‡è®¾é‡æ’åºå™¨é™åˆ¶ä¸º4000å­—ç¬¦
                        combined_text = f"æ‘˜è¦ï¼š{summary}\n\nè¯¦ç»†å†…å®¹ï¼š{doc.content[:3500]}..."
                    doc_texts.append(combined_text)
                    doc_id_to_original_map[doc_id] = doc  # ä½¿ç”¨doc_idæ˜ å°„
                else:
                    # è‹±æ–‡æ•°æ®æˆ–éä¸­æ–‡æ•°æ®ï¼šåªä½¿ç”¨context
                    doc_texts.append(doc.content if hasattr(doc, 'content') else str(doc))
                    doc_id_to_original_map[doc_id] = doc  # ä½¿ç”¨doc_idæ˜ å°„
            
            # ä½¿ç”¨QwenRerankerçš„rerank_with_doc_idsæ–¹æ³•
            doc_ids = []
            for doc in retrieved_documents:
                doc_id = getattr(doc.metadata, 'doc_id', None)
                if doc_id is None:
                    doc_id = hashlib.md5(doc.content.encode('utf-8')).hexdigest()[:16]
                doc_ids.append(doc_id)
            
            reranked_items = self.reranker.rerank_with_doc_ids(
                query=question,
                documents=doc_texts,
                doc_ids=doc_ids,
                batch_size=self.config.reranker.batch_size  # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ‰¹å¤„ç†å¤§å°
            )
            
            # å°†é‡æ’åºç»“æœæ˜ å°„å›æ–‡æ¡£ï¼ˆrerankerç›´æ¥è¿”å›doc_idï¼Œæ— éœ€å¤æ‚æ˜ å°„ï¼‰
            for doc_text, rerank_score, doc_id in reranked_items:
                if doc_id in doc_id_to_original_map:
                    reranked_docs.append(doc_id_to_original_map[doc_id])
                    reranked_scores.append(rerank_score)
                    print(f"DEBUG: âœ… æˆåŠŸæ˜ å°„æ–‡æ¡£ (doc_id: {doc_id})ï¼Œé‡æ’åºåˆ†æ•°: {rerank_score:.4f}")
                else:
                    print(f"DEBUG: âŒ doc_idä¸åœ¨æ˜ å°„ä¸­: {doc_id}")
            
            # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
            sorted_pairs = sorted(zip(reranked_docs, reranked_scores), key=lambda x: x[1], reverse=True)
            retrieved_documents = [doc for doc, _ in sorted_pairs[:self.config.retriever.rerank_top_k]]  # ä½¿ç”¨é…ç½®çš„é‡æ’åºtop-k
            retriever_scores = [score for _, score in sorted_pairs[:self.config.retriever.rerank_top_k]]
            print(f"é‡æ’åºåæ•°é‡: {len(retrieved_documents)}")
        else:
            print("è·³è¿‡é‡æ’åºå™¨...")
        
        # 4. å»é‡å¤„ç†
        unique_docs = []
        seen_hashes = set()
        
        for doc, score in zip(retrieved_documents, retriever_scores):
            if hasattr(doc, 'content'):
                content = doc.content
            else:
                content = str(doc)
            h = hashlib.md5(content.encode('utf-8')).hexdigest()
            if h not in seen_hashes:
                unique_docs.append((doc, score))
                seen_hashes.add(h)
            if len(unique_docs) >= self.config.retriever.rerank_top_k:
                break
        
        # 5. ä½¿ç”¨ç»Ÿä¸€çš„ç”Ÿæˆå™¨ç”Ÿæˆç­”æ¡ˆ
        answer = self._generate_answer_with_context(question, unique_docs)
        
        # 6. æ‰“å°ç»“æœå¹¶è¿”å›
        return self._format_and_return_result(answer, unique_docs, reranker_checkbox, "ç»Ÿä¸€RAG")
    
    def _generate_answer_with_context(self, question: str, unique_docs: List[Tuple[DocumentWithMetadata, float]]) -> str:
        """ä½¿ç”¨ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ"""
        # æ„å»ºä¸Šä¸‹æ–‡å’Œæå–æ‘˜è¦
        context_parts = []
        summary_parts = []
        
        # æ ¹æ®æŸ¥è¯¢è¯­è¨€é€‰æ‹©promptæ¨¡æ¿
        try:
            from langdetect import detect
            query_language = detect(question)
            is_chinese_query = query_language.startswith('zh')
        except:
            # å¦‚æœè¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œæ ¹æ®æŸ¥è¯¢å†…å®¹åˆ¤æ–­
            is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
        
        for doc, _ in unique_docs:
            if hasattr(doc, 'content'):
                content = doc.content
            else:
                content = str(doc)
            
            if not isinstance(content, str):
                if isinstance(content, dict):
                    content = content.get('context', content.get('content', str(content)))
                else:
                    content = str(content)
            
            if is_chinese_query:
                # ä¸­æ–‡æŸ¥è¯¢ï¼šä½¿ç”¨æ™ºèƒ½å†…å®¹é€‰æ‹©
                if hasattr(doc, 'metadata') and hasattr(doc.metadata, 'language') and doc.metadata.language == 'chinese':
                    # ä¸­æ–‡æ•°æ®ï¼šå°è¯•ç»„åˆsummaryå’Œcontext
                    summary = ""
                    if hasattr(doc.metadata, 'summary') and doc.metadata.summary:
                        summary = doc.metadata.summary
                    else:
                        # å¦‚æœæ²¡æœ‰summaryï¼Œä½¿ç”¨contextçš„å‰200å­—ç¬¦ä½œä¸ºsummary
                        summary = content[:200] + "..." if len(content) > 200 else content
                    
                    # ç»„åˆsummaryå’Œcontextï¼Œé¿å…è¿‡é•¿
                    combined_text = f"æ‘˜è¦ï¼š{summary}\n\nè¯¦ç»†å†…å®¹ï¼š{content}"
                    # é™åˆ¶æ€»é•¿åº¦ï¼Œé¿å…è¶…å‡ºpromptçš„tokené™åˆ¶
                    if len(combined_text) > 4000:  # å‡è®¾prompté™åˆ¶ä¸º4000å­—ç¬¦
                        combined_text = f"æ‘˜è¦ï¼š{summary}\n\nè¯¦ç»†å†…å®¹ï¼š{content[:3500]}..."
                    
                    context_parts.append(combined_text)
                    summary_parts.append(summary)
                else:
                    # éä¸­æ–‡æ•°æ®ï¼šåªä½¿ç”¨context
                    context_parts.append(content)
            else:
                # è‹±æ–‡æŸ¥è¯¢ï¼šåªä½¿ç”¨context
                context_parts.append(content)
        
        context_str = "\n\n".join(context_parts)
        summary_str = "\n\n".join(summary_parts) if summary_parts else None
        
        # ä½¿ç”¨ç”Ÿæˆå™¨ç”Ÿæˆç­”æ¡ˆ
        print("ä½¿ç”¨ç”Ÿæˆå™¨ç”Ÿæˆç­”æ¡ˆ...")
        
        if is_chinese_query:
            # ä¸­æ–‡æŸ¥è¯¢ï¼šä½¿ç”¨ä¸­æ–‡promptæ¨¡æ¿ï¼ŒåŒæ—¶æä¾›summaryå’Œcontext
            try:
                from xlm.components.prompt_templates.template_loader import template_loader
                prompt = template_loader.format_template(
                    "multi_stage_chinese_template",
                    summary=summary_str if summary_str else "æ— æ‘˜è¦ä¿¡æ¯",
                    context=context_str,
                    query=question
                )
                if prompt is None:
                    # å›é€€åˆ°ç®€å•ä¸­æ–‡prompt
                    if summary_str:
                        prompt = f"æ‘˜è¦ï¼š{summary_str}\n\nå®Œæ•´ä¸Šä¸‹æ–‡ï¼š{context_str}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
                    else:
                        prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context_str}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
            except Exception as e:
                print(f"ä¸­æ–‡æ¨¡æ¿åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•ä¸­æ–‡prompt")
                if summary_str:
                    prompt = f"æ‘˜è¦ï¼š{summary_str}\n\nå®Œæ•´ä¸Šä¸‹æ–‡ï¼š{context_str}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
                else:
                    prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context_str}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
        else:
            # è‹±æ–‡æŸ¥è¯¢ï¼šåªä½¿ç”¨context
            try:
                from xlm.components.prompt_templates.template_loader import template_loader
                prompt = template_loader.format_template(
                    "rag_english_template",
                    context=context_str,
                    question=question
                )
                if prompt is None:
                    # å›é€€åˆ°ç®€å•è‹±æ–‡prompt
                    prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
            except Exception as e:
                print(f"è‹±æ–‡æ¨¡æ¿åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•è‹±æ–‡prompt")
                prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
        
        try:
            # æ ¹æ®è¯­è¨€æ£€æµ‹ç»“æœå†³å®šhybrid_decisionå‚æ•°
            if is_chinese_query:
                hybrid_decision = "multi_stage_chinese"
            else:
                # è‹±æ–‡æŸ¥è¯¢ï¼šä½¿ç”¨æ··åˆå†³ç­–ç®—æ³•
                try:
                    # å¯¼å…¥æ··åˆå†³ç­–å‡½æ•°
                    from comprehensive_evaluation_enhanced import hybrid_decision_enhanced
                    decision_result = hybrid_decision_enhanced(context_str, question)
                    hybrid_decision = decision_result['primary_decision']
                    print(f"ğŸ¤– è‹±æ–‡æ··åˆå†³ç­–: {hybrid_decision} (ç½®ä¿¡åº¦: {decision_result['confidence']:.3f})")
                except Exception as e:
                    print(f"æ··åˆå†³ç­–å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤hybrid")
                    hybrid_decision = "hybrid"
            
            # ä½¿ç”¨generate_hybrid_answeræ–¹æ³•ï¼Œä¼ é€’æ··åˆå†³ç­–å‚æ•°
            answer = self.generator.generate_hybrid_answer(
                question=question,
                table_context="",  # UIä¸­æ²¡æœ‰åˆ†ç¦»çš„ä¸Šä¸‹æ–‡
                text_context=context_str,
                hybrid_decision=hybrid_decision
            )
        except Exception as e:
            print(f"ç”Ÿæˆå™¨è°ƒç”¨å¤±è´¥: {e}")
            answer = "ç”Ÿæˆå™¨è°ƒç”¨å¤±è´¥"
        
        return answer
    
    def _format_and_return_result(self, answer: str, unique_docs: List[Tuple[DocumentWithMetadata, float]], 
                                 reranker_checkbox: bool, method: str) -> tuple[str, str]:
        """æ ¼å¼åŒ–å¹¶è¿”å›ç»“æœ"""
        # æ‰“å°æ£€ç´¢ç»“æœ
        print(f"\n=== æ£€ç´¢åˆ°çš„åŸå§‹ä¸Šä¸‹æ–‡ ({method}) ===")
        print(f"æ£€ç´¢åˆ° {len(unique_docs)} ä¸ªå”¯ä¸€æ–‡æ¡£")
        for i, (doc, score) in enumerate(unique_docs[:5]):
            if hasattr(doc, 'content'):
                content = doc.content
            else:
                content = str(doc)
            
            if not isinstance(content, str):
                if isinstance(content, dict):
                    content = content.get('context', content.get('content', str(content)))
                else:
                    content = str(content)
            
            display_content = content[:800] + "..." if len(content) > 800 else content
            print(f"æ–‡æ¡£ {i+1} (åˆ†æ•°: {score:.4f}): {display_content}")
        
        if len(unique_docs) > 5:
            print(f"... è¿˜æœ‰ {len(unique_docs) - 5} ä¸ªæ–‡æ¡£")
        
        # æ‰“å°LLMå“åº”
        print(f"\n=== LLMå“åº” ===")
        print(f"ç”Ÿæˆç­”æ¡ˆ: {answer}")
        print(f"æ£€ç´¢æ–‡æ¡£æ•°: {len(unique_docs)}")
        
        # æ·»åŠ é‡æ’åºå™¨ä¿¡æ¯
        if reranker_checkbox and self.reranker:
            answer = f"[Reranker: Enabled] {answer}"
        else:
            answer = f"[Reranker: Disabled] {answer}"
        
        # æ„å»ºUIä¸“ç”¨ç»“æ„ï¼ˆåªå½±å“å±•ç¤ºï¼Œä¸å½±å“RAGä¸»æµç¨‹ï¼‰
        ui_docs = []
        seen_ui_hashes = set()  # æ·»åŠ UIçº§åˆ«çš„å»é‡
        seen_table_ids = set()  # æ·»åŠ Table IDå»é‡
        seen_paragraph_ids = set()  # æ·»åŠ Paragraph IDå»é‡
        
        for doc, score in unique_docs:
            if getattr(doc.metadata, 'language', '') == 'chinese':
                doc_id = str(getattr(doc.metadata, 'origin_doc_id', '') or getattr(doc.metadata, 'doc_id', '')).strip()
                raw_context = self.docid2context.get(doc_id, "")
                if not raw_context:
                    raw_context = doc.content
                    print(f"[UI DEBUG] doc_idæœªå‘½ä¸­: {doc_id}ï¼Œä½¿ç”¨æ–‡æ¡£å†…å®¹")
            else:
                raw_context = doc.content
            
            # æ£€æŸ¥å†…å®¹ç±»å‹å¹¶åº”ç”¨ç›¸åº”çš„å»é‡é€»è¾‘
            has_table_id = "Table ID:" in raw_context
            has_paragraph_id = "Paragraph ID:" in raw_context
            
            if has_table_id:
                # è¡¨æ ¼å†…å®¹æˆ–è¡¨æ ¼+æ–‡æœ¬å†…å®¹ï¼šä½¿ç”¨Table IDå»é‡
                import re
                table_id_match = re.search(r'Table ID:\s*([a-f0-9-]+)', raw_context)
                if table_id_match:
                    table_id = table_id_match.group(1)
                    if table_id in seen_table_ids:
                        print(f"[UI DEBUG] è·³è¿‡é‡å¤çš„Table ID: {table_id}ï¼Œå†…å®¹å‰50å­—ç¬¦: {raw_context[:50]}...")
                        continue
                    seen_table_ids.add(table_id)
                    print(f"[UI DEBUG] ä¿ç•™Table ID: {table_id}")
            elif has_paragraph_id:
                # çº¯æ–‡æœ¬å†…å®¹ï¼šä½¿ç”¨Paragraph IDå»é‡
                import re
                paragraph_id_match = re.search(r'Paragraph ID:\s*([a-f0-9-]+)', raw_context)
                if paragraph_id_match:
                    paragraph_id = paragraph_id_match.group(1)
                    if paragraph_id in seen_paragraph_ids:
                        print(f"[UI DEBUG] è·³è¿‡é‡å¤çš„Paragraph ID: {paragraph_id}ï¼Œå†…å®¹å‰50å­—ç¬¦: {raw_context[:50]}...")
                        continue
                    seen_paragraph_ids.add(paragraph_id)
                    print(f"[UI DEBUG] ä¿ç•™Paragraph ID: {paragraph_id}")
            
            # å¯¹raw_contextè¿›è¡Œå»é‡æ£€æŸ¥
            context_hash = hash(raw_context)
            if context_hash in seen_ui_hashes:
                print(f"[UI DEBUG] è·³è¿‡é‡å¤çš„UIæ–‡æ¡£ï¼Œå†…å®¹å‰50å­—ç¬¦: {raw_context[:50]}...")
                continue
            
            seen_ui_hashes.add(context_hash)
            preview_content = raw_context[:200] + "..." if len(raw_context) > 200 else raw_context
            ui_docs.append((doc, score, preview_content, raw_context))
        html_content = self._generate_clickable_context_html(ui_docs)
        
        print(f"=== æŸ¥è¯¢å¤„ç†å®Œæˆ ===\n")
        return answer, html_content

    def _generate_clickable_context_html(self, ui_docs):
        # ui_docs: List[Tuple[DocumentWithMetadata, float, str, str]]
        if not ui_docs:
            return "<p>æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ã€‚</p>"

        # æœ€ç»ˆçš„å»é‡æ£€æŸ¥ï¼Œç¡®ä¿HTMLä¸­ä¸ä¼šæœ‰é‡å¤å†…å®¹
        final_ui_docs = []
        seen_final_hashes = set()
        seen_final_table_ids = set()  # æ·»åŠ Table IDå»é‡
        seen_final_paragraph_ids = set()  # æ·»åŠ Paragraph IDå»é‡
        
        for doc, score, preview_content, raw_context in ui_docs:
            # æ£€æŸ¥å†…å®¹ç±»å‹å¹¶åº”ç”¨ç›¸åº”çš„å»é‡é€»è¾‘
            has_table_id = "Table ID:" in raw_context
            has_paragraph_id = "Paragraph ID:" in raw_context
            
            if has_table_id:
                # è¡¨æ ¼å†…å®¹æˆ–è¡¨æ ¼+æ–‡æœ¬å†…å®¹ï¼šä½¿ç”¨Table IDå»é‡
                import re
                table_id_match = re.search(r'Table ID:\s*([a-f0-9-]+)', raw_context)
                if table_id_match:
                    table_id = table_id_match.group(1)
                    if table_id in seen_final_table_ids:
                        print(f"[HTML DEBUG] è·³è¿‡é‡å¤çš„Table ID: {table_id}ï¼Œå†…å®¹å‰50å­—ç¬¦: {raw_context[:50]}...")
                        continue
                    seen_final_table_ids.add(table_id)
                    print(f"[HTML DEBUG] ä¿ç•™Table ID: {table_id}")
            elif has_paragraph_id:
                # çº¯æ–‡æœ¬å†…å®¹ï¼šä½¿ç”¨Paragraph IDå»é‡
                import re
                paragraph_id_match = re.search(r'Paragraph ID:\s*([a-f0-9-]+)', raw_context)
                if paragraph_id_match:
                    paragraph_id = paragraph_id_match.group(1)
                    if paragraph_id in seen_final_paragraph_ids:
                        print(f"[HTML DEBUG] è·³è¿‡é‡å¤çš„Paragraph ID: {paragraph_id}ï¼Œå†…å®¹å‰50å­—ç¬¦: {raw_context[:50]}...")
                        continue
                    seen_final_paragraph_ids.add(paragraph_id)
                    print(f"[HTML DEBUG] ä¿ç•™Paragraph ID: {paragraph_id}")
            
            # ä½¿ç”¨raw_contextçš„å“ˆå¸Œå€¼è¿›è¡Œæœ€ç»ˆå»é‡
            context_hash = hash(raw_context)
            if context_hash in seen_final_hashes:
                print(f"[HTML DEBUG] è·³è¿‡é‡å¤çš„HTMLæ–‡æ¡£ï¼Œå†…å®¹å‰50å­—ç¬¦: {raw_context[:50]}...")
                continue
            
            seen_final_hashes.add(context_hash)
            final_ui_docs.append((doc, score, preview_content, raw_context))

        html_parts = []
        html_parts.append("""
        <div style='font-family: Arial, sans-serif;'>
        <style>
        .expand-btn, .collapse-btn {
            margin-top: 10px;
            border: none;
            padding: 8px 16px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 12px;
            color: white;
            transition: background-color 0.3s ease;
        }
        .expand-btn { 
            background-color: #4caf50; 
        }
        .expand-btn:hover { 
            background-color: #45a049; 
        }
        .collapse-btn { 
            background-color: #757575; 
        }
        .collapse-btn:hover { 
            background-color: #616161; 
        }
        .content-section { 
            margin-bottom: 20px; 
            border: 1px solid #ddd; 
            border-radius: 8px; 
            padding: 15px; 
            background-color: #f9f9f9; 
            transition: box-shadow 0.3s ease;
        }
        .content-section:hover {
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }
        .header { 
            display: flex; 
            justify-content: space-between; 
            align-items: center; 
            margin-bottom: 10px; 
        }
        .score { 
            background-color: #ff9800; 
            color: #fff; 
            padding: 4px 8px; 
            border-radius: 4px; 
            font-size: 12px; 
            font-weight: bold;
        }
        .short-content, .full-content { 
            margin: 0; 
            line-height: 1.6; 
        }
        .short-content { 
            color: #555; 
        }
        .full-content { 
            color: #333; 
        }
        .full-content p {
            white-space: pre-wrap;
            font-family: 'Courier New', monospace;
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 6px;
            border: 1px solid #e9ecef;
            margin: 10px 0;
            font-size: 13px;
            line-height: 1.5;
            overflow-x: auto;
            max-height: 500px;
            overflow-y: auto;
        }
        </style>
        """)
        
        for i, (doc, score, preview_content, raw_context) in enumerate(final_ui_docs):
            short_content = preview_content
            full_content = raw_context
            def html_escape(text):
                return text.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\n', '<br>').replace('  ', '&nbsp;&nbsp;')
            html_parts.append(f"""
            <div class='content-section'>
                <div class='header'>
                    <strong style='color: #333;'>æ–‡æ¡£ {i+1}</strong>
                    <span class='score'>score: {score:.4f}</span>
                </div>
                <div class='short-content' id='short_{i}'>
                    <p>{html_escape(short_content)}</p>
                    <button class='expand-btn' onclick='document.getElementById(\"short_{i}\").style.display=\"none\"; document.getElementById(\"full_{i}\").style.display=\"block\";'>
                        Read more
                    </button>
                </div>
                <div class='full-content' id='full_{i}' style='display: none;'>
                    <p>{html_escape(full_content)}</p>
                    <button class='collapse-btn' onclick='document.getElementById(\"full_{i}\").style.display=\"none\"; document.getElementById(\"short_{i}\").style.display=\"block\";'>
                        Show less 
                    </button>
                </div>
            </div>
            """)
        html_parts.append("</div>")
        return ''.join(html_parts)
    
    def _detect_data_source(self, question: str, language: str) -> str:
        """æ£€æµ‹æ•°æ®æºç±»å‹"""
        if language == 'zh':
            return "AlphaFin"
        else:
            return "TAT_QA"
    
    def launch(self, share: bool = False):
        """Launch UI interface"""
        self.interface.launch(share=share)
    
    def _chunk_documents(self, documents: List[DocumentWithMetadata], chunk_size: int = 512, overlap: int = 50) -> List[DocumentWithMetadata]:
        """
        å°†æ–‡æ¡£åˆ†å‰²æˆæ›´å°çš„chunks
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            chunk_size: chunkå¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap: é‡å å­—ç¬¦æ•°
            
        Returns:
            åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        chunked_docs = []
        
        for doc in documents:
            content = doc.content
            if len(content) <= chunk_size:
                # æ–‡æ¡£è¾ƒçŸ­ï¼Œä¸éœ€è¦åˆ†å—
                chunked_docs.append(doc)
            else:
                # æ–‡æ¡£è¾ƒé•¿ï¼Œéœ€è¦åˆ†å—
                start = 0
                chunk_id = 0
                
                while start < len(content):
                    end = start + chunk_size
                    
                    # ç¡®ä¿ä¸åœ¨å•è¯ä¸­é—´æˆªæ–­
                    if end < len(content):
                        # å°è¯•åœ¨å¥å·ã€é€—å·æˆ–ç©ºæ ¼å¤„æˆªæ–­
                        for i in range(end, max(start + chunk_size - 100, start), -1):
                            if content[i] in '.ã€‚ï¼Œ, ':
                                end = i + 1
                                break
                    
                    chunk_content = content[start:end].strip()
                    
                    if chunk_content:  # ç¡®ä¿chunkä¸ä¸ºç©º
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å…ƒæ•°æ®
                        chunk_metadata = DocumentMetadata(
                            source=f"{doc.metadata.source}_chunk_{chunk_id}",
                            created_at=doc.metadata.created_at,
                            author=doc.metadata.author,
                            language=doc.metadata.language,
                            origin_doc_id=getattr(doc.metadata, 'doc_id', None) if doc.metadata.language == 'chinese' else None
                        )
                        
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å¯¹è±¡
                        chunk_doc = DocumentWithMetadata(
                            content=chunk_content,
                            metadata=chunk_metadata
                        )
                        
                        chunked_docs.append(chunk_doc)
                        chunk_id += 1
                    
                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªchunkï¼Œè€ƒè™‘é‡å 
                    start = end - overlap
                    if start >= len(content):
                        break
        
        return chunked_docs 
    
    def _chunk_documents_advanced(self, documents: List[DocumentWithMetadata]) -> List[DocumentWithMetadata]:
        """
        ä½¿ç”¨finetune_chinese_encoder.pyä¸­çš„é«˜çº§chunké€»è¾‘å¤„ç†ä¸­æ–‡æ–‡æ¡£
        å¹¶é›†æˆfinetune_encoder.pyä¸­çš„è¡¨æ ¼æ–‡æœ¬åŒ–å¤„ç†
        """
        import re
        import json
        import ast
        
        def extract_unit_from_paragraph(paragraphs):
            """ä»æ®µè½ä¸­æå–æ•°å€¼å•ä½"""
            for para in paragraphs:
                text = para.get("text", "") if isinstance(para, dict) else para
                match = re.search(r'dollars in (millions|billions)|in (millions|billions)', text, re.IGNORECASE)
                if match:
                    unit = match.group(1) or match.group(2)
                    if unit:
                        return unit.lower().replace('s', '') + " USD"
            return ""

        def table_to_natural_text(table_dict, caption="", unit_info=""):
            """å°†è¡¨æ ¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°"""
            rows = table_dict.get("table", [])
            lines = []

            if caption:
                lines.append(f"Table Topic: {caption}.")

            if not rows:
                return ""

            headers = rows[0]
            data_rows = rows[1:]

            for i, row in enumerate(data_rows):
                if not row or all(str(v).strip() == "" for v in row):
                    continue

                if len(row) > 1 and str(row[0]).strip() != "" and all(str(v).strip() == "" for v in row[1:]):
                    lines.append(f"Table Category: {str(row[0]).strip()}.")
                    continue

                row_name = str(row[0]).strip().replace('.', '')

                data_descriptions = []
                for h_idx, v in enumerate(row):
                    if h_idx == 0:
                        continue
                    
                    header = headers[h_idx] if h_idx < len(headers) else f"Column {h_idx+1}"
                    value = str(v).strip()

                    if value:
                        if re.match(r'^-?\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?$|^\(\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?\)$', value): 
                            formatted_value = value.replace('$', '')
                            if unit_info:
                                if formatted_value.startswith('(') and formatted_value.endswith(')'):
                                     formatted_value = f"(${formatted_value[1:-1]} {unit_info})"
                                else:
                                     formatted_value = f"${formatted_value} {unit_info}"
                            else:
                                formatted_value = f"${formatted_value}"
                        else:
                            formatted_value = value
                        
                        data_descriptions.append(f"{header} is {formatted_value}")

                if row_name and data_descriptions:
                    lines.append(f"Details for item {row_name}: {'; '.join(data_descriptions)}.")
                elif data_descriptions:
                    lines.append(f"Other data item: {'; '.join(data_descriptions)}.")
                elif row_name:
                    lines.append(f"Data item: {row_name}.")

            return "\n".join(lines)
        
        def convert_json_context_to_natural_language_chunks(json_str_context, company_name="å…¬å¸"):
            chunks = []
            if not json_str_context or not json_str_context.strip():
                return chunks
            processed_str_context = json_str_context.replace("\\n", "\n")
            cleaned_initial = re.sub(re.escape("ã€é—®é¢˜ã€‘:"), "", processed_str_context)
            cleaned_initial = re.sub(re.escape("ã€ç­”æ¡ˆã€‘:"), "", cleaned_initial).strip()
            cleaned_initial = cleaned_initial.replace('ï¼Œ', ',')
            cleaned_initial = cleaned_initial.replace('ï¼š', ':')
            cleaned_initial = cleaned_initial.replace('ã€', '') 
            cleaned_initial = cleaned_initial.replace('ã€‘', '') 
            cleaned_initial = cleaned_initial.replace('\u3000', ' ')
            cleaned_initial = cleaned_initial.replace('\xa0', ' ').strip()
            cleaned_initial = re.sub(r'\s+', ' ', cleaned_initial).strip()
            
            # å¤„ç†ç ”æŠ¥æ ¼å¼
            report_match = re.match(
                r"è¿™æ˜¯ä»¥(.+?)ä¸ºé¢˜ç›®,åœ¨(\d{4}-\d{2}-\d{2}(?: \d{2}:\d{2}:\d{2})?)æ—¥æœŸå‘å¸ƒçš„ç ”ç©¶æŠ¥å‘Šã€‚ç ”æŠ¥å†…å®¹å¦‚ä¸‹: (.+)", 
                cleaned_initial, 
                re.DOTALL
            )
            if report_match:
                report_title_full = report_match.group(1).strip()
                report_date = report_match.group(2).strip()
                report_raw_content = report_match.group(3).strip() 
                content_after_second_title_match = re.match(r"ç ”æŠ¥é¢˜ç›®æ˜¯:(.+)", report_raw_content, re.DOTALL)
                if content_after_second_title_match:
                    report_content_preview = content_after_second_title_match.group(1).strip()
                else:
                    report_content_preview = report_raw_content 
                report_content_preview = re.sub(re.escape("ã€é—®é¢˜ã€‘:"), "", report_content_preview)
                report_content_preview = re.sub(re.escape("ã€ç­”æ¡ˆã€‘:"), "", report_content_preview).strip()
                report_content_preview = re.sub(r'\s+', ' ', report_content_preview).strip() 
                company_stock_match = re.search(r"(.+?)ï¼ˆ(\d{6}\.\w{2})ï¼‰", report_title_full)
                company_info = ""
                if company_stock_match:
                    report_company_name = company_stock_match.group(1).strip()
                    report_stock_code = company_stock_match.group(2).strip()
                    company_info = f"ï¼Œå…¬å¸åç§°ï¼š{report_company_name}ï¼Œè‚¡ç¥¨ä»£ç ï¼š{report_stock_code}"
                    report_title_main = re.sub(r"ï¼ˆ\d{6}\.\w{2}ï¼‰", "", report_title_full).strip()
                else:
                    report_title_main = report_title_full
                chunk_text = f"ä¸€ä»½å‘å¸ƒæ—¥æœŸä¸º {report_date} çš„ç ”ç©¶æŠ¥å‘Šï¼Œå…¶æ ‡é¢˜æ˜¯ï¼š\"{report_title_main}\"{company_info}ã€‚æŠ¥å‘Šæ‘˜è¦å†…å®¹ï¼š{report_content_preview.rstrip('...') if report_content_preview.endswith('...') else report_content_preview}ã€‚"
                chunks.append(chunk_text)
                return chunks 

            # å¤„ç†å­—å…¸æ ¼å¼
            extracted_dict_str = None
            parsed_data = None 
            temp_dict_search_str = re.sub(r"Timestamp\(['\"](.*?)['\"]\)", r"'\1'", cleaned_initial) 
            all_dict_matches = re.findall(r"(\{.*?\})", temp_dict_search_str, re.DOTALL) 
            for potential_dict_str in all_dict_matches:
                cleaned_potential_dict_str = potential_dict_str.strip()
                json_compatible_str_temp = cleaned_potential_dict_str.replace("'", '"')
                try:
                    parsed_data_temp = json.loads(json_compatible_str_temp)
                    if isinstance(parsed_data_temp, dict):
                        extracted_dict_str = cleaned_potential_dict_str
                        parsed_data = parsed_data_temp
                        break 
                except json.JSONDecodeError:
                    pass 
                fixed_for_ast_eval_temp = re.sub(
                    r"(?<!['\"\w.])\b(0[1-9]\d*)\b(?![\d.]|['\"\w.])", 
                    r"'\1'", 
                    cleaned_potential_dict_str
                )
                try:
                    parsed_data_temp = ast.literal_eval(fixed_for_ast_eval_temp)
                    if isinstance(parsed_data_temp, dict):
                        extracted_dict_str = cleaned_potential_dict_str
                        parsed_data = parsed_data_temp
                        break 
                except (ValueError, SyntaxError):
                    pass 

            if extracted_dict_str is not None and isinstance(parsed_data, dict):
                for metric_name, time_series_data in parsed_data.items():
                    if not isinstance(metric_name, str):
                        metric_name = str(metric_name)
                    cleaned_metric_name = re.sub(r'ï¼ˆ.*?ï¼‰', '', metric_name).strip()
                    if not isinstance(time_series_data, dict):
                        if time_series_data is not None and str(time_series_data).strip():
                            chunks.append(f"{company_name}çš„{cleaned_metric_name}æ•°æ®ä¸ºï¼š{time_series_data}ã€‚")
                        continue
                    if not time_series_data:
                        continue
                    try:
                        sorted_dates = sorted(time_series_data.keys(), key=str)
                    except TypeError:
                        sorted_dates = [str(k) for k in time_series_data.keys()]
                    description_parts = []
                    for date in sorted_dates:
                        value = time_series_data[date]
                        if isinstance(value, (int, float)):
                            formatted_value = f"{value:.4f}".rstrip('0').rstrip('.') if isinstance(value, float) else str(value)
                        else:
                            formatted_value = str(value)
                        description_parts.append(f"åœ¨{date}ä¸º{formatted_value}")
                    if description_parts:
                        if len(description_parts) <= 3:
                            full_description = f"{company_name}çš„{cleaned_metric_name}æ•°æ®: " + "ï¼Œ".join(description_parts) + "ã€‚"
                        else:
                            first_part = "ï¼Œ".join(description_parts[:3])
                            last_part = "ï¼Œ".join(description_parts[-3:])
                            if len(sorted_dates) > 6:
                                full_description = f"{company_name}çš„{cleaned_metric_name}æ•°æ®ä»{sorted_dates[0]}åˆ°{sorted_dates[-1]}ï¼Œä¸»è¦å˜åŒ–ä¸ºï¼š{first_part}ï¼Œ...ï¼Œ{last_part}ã€‚"
                            else:
                                full_description = f"{company_name}çš„{cleaned_metric_name}æ•°æ®: " + "ï¼Œ".join(description_parts) + "ã€‚"
                        chunks.append(full_description)
                return chunks 

            # å¤„ç†çº¯æ–‡æœ¬
            pure_text = cleaned_initial
            pure_text = re.sub(r"^\d{4}-\d{2}-\d{2}( \d{2}:\d{2}:\d{2})?[_;]?", "", pure_text, 1).strip()
            pure_text = re.sub(r"^[\u4e00-\u9fa5]+(?:/[\u4e00-\u9fa5]+)?\d{4}å¹´\d{2}æœˆ\d{2}æ—¥\d{2}:\d{2}:\d{2}(?:æ®[\u4e00-\u9fa5]+?,)?\d{1,2}æœˆ\d{1,2}æ—¥,?", "", pure_text).strip()
            pure_text = re.sub(r"^(?:å¸‚åœºèµ„é‡‘è¿›å‡º)?æˆªè‡³å‘¨[ä¸€äºŒä¸‰å››äº”å…­æ—¥]æ”¶ç›˜,?", "", pure_text).strip()
            pure_text = re.sub(r"^[\u4e00-\u9fa5]+?ä¸­æœŸå‡€åˆ©é¢„å‡\d+%-?\d*%(?:[\u4e00-\u9fa5]+?\d{1,2}æœˆ\d{1,2}æ—¥æ™šé—´å…¬å‘Š,)?", "", pure_text).strip()

            if pure_text: 
                chunks.append(pure_text)
            else:
                chunks.append(f"åŸå§‹æ ¼å¼ï¼Œè§£æå¤±è´¥æˆ–æ— æœ‰æ•ˆç»“æ„ï¼š{json_str_context.strip()[:100]}...")
            return chunks
        
        chunked_docs = []
        for doc in documents:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼æ•°æ®
            content = doc.content
            
            # å°è¯•è§£æä¸ºJSONï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼ç»“æ„
            try:
                parsed_content = json.loads(content)
                if isinstance(parsed_content, dict) and 'tables' in parsed_content:
                    # å¤„ç†åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£
                    paragraphs = parsed_content.get('paragraphs', [])
                    tables = parsed_content.get('tables', [])
                    
                    # æå–å•ä½ä¿¡æ¯
                    unit_info = extract_unit_from_paragraph(paragraphs)
                    
                    # å¤„ç†æ®µè½
                    for p_idx, para in enumerate(paragraphs):
                        para_text = para.get("text", "") if isinstance(para, dict) else para
                        if para_text.strip():
                            chunk_metadata = DocumentMetadata(
                                source=f"{doc.metadata.source}_table_para_{p_idx}",
                                created_at=doc.metadata.created_at,
                                author=doc.metadata.author,
                                language=doc.metadata.language
                            )
                            chunk_doc = DocumentWithMetadata(
                                content=para_text.strip(),
                                metadata=chunk_metadata
                            )
                            chunked_docs.append(chunk_doc)
                    
                    # å¤„ç†è¡¨æ ¼
                    for t_idx, table in enumerate(tables):
                        table_text = table_to_natural_text(table, table.get("caption", ""), unit_info)
                        if table_text.strip():
                            chunk_metadata = DocumentMetadata(
                                source=f"{doc.metadata.source}_table_text_{t_idx}",
                                created_at=doc.metadata.created_at,
                                author=doc.metadata.author,
                                language=doc.metadata.language
                            )
                            chunk_doc = DocumentWithMetadata(
                                content=table_text.strip(),
                                metadata=chunk_metadata
                            )
                            chunked_docs.append(chunk_doc)
                    
                    continue  # å·²å¤„ç†è¡¨æ ¼æ•°æ®ï¼Œè·³è¿‡åç»­å¤„ç†
                    
            except (json.JSONDecodeError, TypeError):
                pass  # ä¸æ˜¯JSONæ ¼å¼ï¼Œç»§ç»­ä½¿ç”¨åŸæœ‰çš„chunké€»è¾‘
            
            # ä½¿ç”¨åŸæœ‰çš„é«˜çº§chunké€»è¾‘å¤„ç†
            chunks = convert_json_context_to_natural_language_chunks(content)
            
            for i, chunk_content in enumerate(chunks):
                if chunk_content.strip():
                    chunk_metadata = DocumentMetadata(
                        source=f"{doc.metadata.source}_advanced_chunk_{i}",
                        created_at=doc.metadata.created_at,
                        author=doc.metadata.author,
                        language=doc.metadata.language,
                        origin_doc_id=getattr(doc.metadata, 'doc_id', None) if doc.metadata.language == 'chinese' else None
                    )
                    
                    chunk_doc = DocumentWithMetadata(
                        content=chunk_content,
                        metadata=chunk_metadata
                    )
                    
                    chunked_docs.append(chunk_doc)
        
        return chunked_docs
    
    def _chunk_documents_simple(self, documents: List[DocumentWithMetadata], chunk_size: int = 512, overlap: int = 50) -> List[DocumentWithMetadata]:
        """
        ç®€å•çš„æ–‡æ¡£åˆ†å—æ–¹æ³•ï¼Œç”¨äºè‹±æ–‡æ–‡æ¡£
        """
        chunked_docs = []
        
        for doc in documents:
            content = doc.content
            if len(content) <= chunk_size:
                # æ–‡æ¡£è¾ƒçŸ­ï¼Œä¸éœ€è¦åˆ†å—
                chunked_docs.append(doc)
            else:
                # æ–‡æ¡£è¾ƒé•¿ï¼Œéœ€è¦åˆ†å—
                start = 0
                chunk_id = 0
                
                while start < len(content):
                    end = start + chunk_size
                    
                    # ç¡®ä¿ä¸åœ¨å•è¯ä¸­é—´æˆªæ–­
                    if end < len(content):
                        # å°è¯•åœ¨å¥å·ã€é€—å·æˆ–ç©ºæ ¼å¤„æˆªæ–­
                        for i in range(end, max(start + chunk_size - 100, start), -1):
                            if content[i] in '.ã€‚ï¼Œ, ':
                                end = i + 1
                                break
                    
                    chunk_content = content[start:end].strip()
                    
                    if chunk_content:  # ç¡®ä¿chunkä¸ä¸ºç©º
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å…ƒæ•°æ®
                        chunk_metadata = DocumentMetadata(
                            source=f"{doc.metadata.source}_simple_chunk_{chunk_id}",
                            created_at=doc.metadata.created_at,
                            author=doc.metadata.author,
                            language=doc.metadata.language
                        )
                        
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å¯¹è±¡
                        chunk_doc = DocumentWithMetadata(
                            content=chunk_content,
                            metadata=chunk_metadata
                        )
                        
                        chunked_docs.append(chunk_doc)
                        chunk_id += 1
                    
                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªchunkï¼Œè€ƒè™‘é‡å 
                    start = end - overlap
                    if start >= len(content):
                        break
        
        return chunked_docs 