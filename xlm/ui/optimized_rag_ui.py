#!/usr/bin/env python3
"""
Optimized RAG UI with FAISS support
"""

import os
import sys
import re
from pathlib import Path
from typing import List, Optional, Tuple
import gradio as gr
import numpy as np
import torch
import faiss
from langdetect import detect, LangDetectException
import hashlib

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata
from xlm.components.rag_system.rag_system import RagSystem
from xlm.components.generator.generator import Generator
from xlm.components.retriever.reranker import QwenReranker
from xlm.utils.visualizer import Visualizer
from xlm.registry.retriever import load_enhanced_retriever
from xlm.registry.generator import load_generator
from config.parameters import Config, EncoderConfig, RetrieverConfig, ModalityConfig, EMBEDDING_CACHE_DIR, RERANKER_CACHE_DIR

# å°è¯•å¯¼å…¥å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
try:
    sys.path.append(str(Path(__file__).parent.parent.parent / "alphafin_data_process"))
    from multi_stage_retrieval_final import MultiStageRetrievalSystem
    MULTI_STAGE_AVAILABLE = True
except ImportError as e:
    print(f"å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}")
    MULTI_STAGE_AVAILABLE = False
    MultiStageRetrievalSystem = None

def try_load_qwen_reranker(model_name, cache_dir=None):
    """å°è¯•åŠ è½½Qwené‡æ’åºå™¨ï¼Œæ”¯æŒGPU 0å’ŒCPUå›é€€"""
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        
        # ç¡®ä¿cache_diræ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²
        if cache_dir is None:
            cache_dir = RERANKER_CACHE_DIR
        
        print(f"å°è¯•ä½¿ç”¨8bité‡åŒ–åŠ è½½QwenReranker...")
        print(f"åŠ è½½é‡æ’åºå™¨æ¨¡å‹: {model_name}")
        
        # é¦–å…ˆå°è¯•GPU 0
        if torch.cuda.is_available() and torch.cuda.device_count() > 0:
            device = "cuda:0"  # æ˜ç¡®æŒ‡å®šGPU 0
            print(f"- è®¾å¤‡: {device}")
            print(f"- ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"- é‡åŒ–: True (8bit)")
            print(f"- Flash Attention: False")
            
            try:
                # æ£€æŸ¥GPU 0çš„å¯ç”¨å†…å­˜
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated(0)
                free_memory = gpu_memory - allocated_memory
                
                print(f"- GPU 0 æ€»å†…å­˜: {gpu_memory / 1024**3:.1f}GB")
                print(f"- GPU 0 å·²ç”¨å†…å­˜: {allocated_memory / 1024**3:.1f}GB")
                print(f"- GPU 0 å¯ç”¨å†…å­˜: {free_memory / 1024**3:.1f}GB")
                
                # å¦‚æœå¯ç”¨å†…å­˜å°‘äº2GBï¼Œå›é€€åˆ°CPU
                if free_memory < 2 * 1024**3:  # 2GB
                    print("- GPU 0 å†…å­˜ä¸è¶³ï¼Œå›é€€åˆ°CPU")
                    device = "cpu"
                else:
                    # å°è¯•åœ¨GPU 0ä¸ŠåŠ è½½
                    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
                    model = AutoModelForSequenceClassification.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        load_in_8bit=True
                    )
                    print("é‡åŒ–æ¨¡å‹å·²è‡ªåŠ¨è®¾ç½®åˆ°è®¾å¤‡ï¼Œè·³è¿‡æ‰‹åŠ¨ç§»åŠ¨")
                    print("é‡æ’åºå™¨æ¨¡å‹åŠ è½½å®Œæˆ")
                    print("é‡åŒ–åŠ è½½æˆåŠŸï¼")
                    return QwenReranker(model_name, device=device, cache_dir=cache_dir)
                    
            except Exception as e:
                print(f"- GPU 0 åŠ è½½å¤±è´¥: {e}")
                print("- å›é€€åˆ°CPU")
                device = "cpu"
        
        # CPUå›é€€
        if device == "cpu" or not torch.cuda.is_available():
            device = "cpu"
            print(f"- è®¾å¤‡: {device}")
            print(f"- ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"- é‡åŒ–: False (CPUæ¨¡å¼)")
            
            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
            model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                torch_dtype=torch.float32
            )
            model = model.to(device)
            print("é‡æ’åºå™¨æ¨¡å‹åŠ è½½å®Œæˆ")
            print("CPUåŠ è½½æˆåŠŸï¼")
            return QwenReranker(model_name, device=device, cache_dir=cache_dir)
            
    except Exception as e:
        print(f"åŠ è½½é‡æ’åºå™¨å¤±è´¥: {e}")
        return None

class OptimizedRagUI:
    def __init__(
        self,
        # encoder_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        encoder_model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
        # generator_model_name: str = "facebook/opt-125m",
        # generator_model_name: str = "Qwen/Qwen2-1.5B-Instruct",
        # generator_model_name: str = "SUFE-AIFLM-Lab/Fin-R1",  # ä½¿ç”¨é‡‘èä¸“ç”¨Fin-R1æ¨¡å‹
        cache_dir: Optional[str] = None,
        use_faiss: bool = True,
        enable_reranker: bool = True,
        use_existing_embedding_index: Optional[bool] = None,  # ä»configè¯»å–ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼
        max_alphafin_chunks: Optional[int] = None,  # ä»configè¯»å–ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼
        window_title: str = "RAG System with FAISS",
        title: str = "RAG System with FAISS",
        examples: Optional[List[List[str]]] = None,
    ):
        # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        config = Config()
        self.cache_dir = EMBEDDING_CACHE_DIR if (not cache_dir or not isinstance(cache_dir, str)) else cache_dir
        self.encoder_model_name = encoder_model_name
        # ä»configè¯»å–ç”Ÿæˆå™¨æ¨¡å‹åç§°ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç 
        self.generator_model_name = config.generator.model_name
        self.use_faiss = use_faiss
        self.enable_reranker = enable_reranker
        # ä»configè¯»å–å‚æ•°ï¼Œå¦‚æœä¼ å…¥Noneåˆ™ä½¿ç”¨configé»˜è®¤å€¼
        self.use_existing_embedding_index = use_existing_embedding_index if use_existing_embedding_index is not None else config.retriever.use_existing_embedding_index
        self.max_alphafin_chunks = max_alphafin_chunks if max_alphafin_chunks is not None else config.retriever.max_alphafin_chunks
        self.window_title = window_title
        self.title = title
        self.examples = examples or [
            ["ä»€ä¹ˆæ˜¯è‚¡ç¥¨æŠ•èµ„ï¼Ÿ"],
            ["è¯·è§£é‡Šå€ºåˆ¸çš„åŸºæœ¬æ¦‚å¿µ"],
            ["åŸºé‡‘æŠ•èµ„ä¸è‚¡ç¥¨æŠ•èµ„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"],
            ["What is stock investment?"],
            ["Explain the basic concepts of bonds"],
            ["What are the differences between fund investment and stock investment?"]
        ]
        
        # Set environment variables for model caching
        os.environ['TRANSFORMERS_CACHE'] = os.path.join(self.cache_dir, 'transformers')
        os.environ['HF_HOME'] = self.cache_dir
        os.environ['HF_DATASETS_CACHE'] = os.path.join(self.cache_dir, 'datasets')
        
        # Initialize system components
        self._init_components()
        
        # Create Gradio interface
        self.interface = self._create_interface()
    
    def _init_components(self):
        """Initialize RAG system components"""
        print("\nStep 1. Loading bilingual retriever with dual encoders...")
        
        # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        config = Config()
        
        # åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿï¼ˆç”¨äºä¸­æ–‡æŸ¥è¯¢ï¼‰
        print("\nStep 1.0. Initializing Multi-Stage Retrieval System for Chinese queries...")
        self.multi_stage_system = None
        if MULTI_STAGE_AVAILABLE and MultiStageRetrievalSystem:
            try:
                # ä¸­æ–‡æ•°æ®è·¯å¾„
                chinese_data_path = Path("data/alphafin/alphafin_merged_generated_qa.json")
                
                if chinese_data_path.exists():
                    print("âœ… åˆå§‹åŒ–ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
                    self.multi_stage_system = MultiStageRetrievalSystem(
                        data_path=chinese_data_path,
                        dataset_type="chinese",
                        use_existing_config=True
                    )
                    print("âœ… ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"âŒ ä¸­æ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {chinese_data_path}")
                    self.multi_stage_system = None
                    
            except Exception as e:
                print(f"âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.multi_stage_system = None
        else:
            print("âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»ŸRAGç³»ç»Ÿ")
            self.multi_stage_system = None
        
        # ä½¿ç”¨ä¼˜åŒ–çš„æ•°æ®åŠ è½½å™¨ï¼Œå®ç°æ–‡æ¡£çº§åˆ«chunking
        print("\nStep 1.1. Loading data with optimized chunking...")
        try:
            from xlm.utils.optimized_data_loader import OptimizedDataLoader
            
            # å¯¹äºAlphaFinæ•°æ®ï¼Œä½¿ç”¨summaryå­—æ®µè€Œä¸æ˜¯chunks
            data_loader = OptimizedDataLoader(
                data_dir="data",
                max_samples=config.data.max_samples,
                chinese_document_level=True,  # ä½¿ç”¨æ–‡æ¡£çº§åˆ«ï¼Œé¿å…è¿‡åº¦chunking
                english_chunk_level=True      # è‹±æ–‡ä¿æŒchunkçº§åˆ«
            )
            
            # è·å–å¤„ç†åçš„æ–‡æ¡£
            chinese_docs = data_loader.chinese_docs
            english_chunks = data_loader.english_docs
            
            # å¯¹äºä¸­æ–‡æ•°æ®ï¼Œæå–summaryå­—æ®µç”¨äºä¼ ç»ŸRAG
            chinese_summaries = []
            for doc in chinese_docs:
                # å°è¯•ä»æ–‡æ¡£å†…å®¹ä¸­æå–summaryä¿¡æ¯
                content = doc.content
                if isinstance(content, str) and len(content) > 0:
                    # å¦‚æœå†…å®¹å¤ªé•¿ï¼Œå–å‰500å­—ç¬¦ä½œä¸ºsummary
                    summary = content[:500] + "..." if len(content) > 500 else content
                    # åˆ›å»ºæ–°çš„DocumentWithMetadataå¯¹è±¡
                    summary_doc = DocumentWithMetadata(
                        content=summary,
                        metadata=DocumentMetadata(
                            source=doc.metadata.source,
                            created_at=doc.metadata.created_at,
                            author=doc.metadata.author,
                            language="chinese"
                        )
                    )
                    chinese_summaries.append(summary_doc)
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            stats = data_loader.get_statistics()
            print(f"âœ… æ–‡æ¡£çº§åˆ«å¤„ç†å®Œæˆ:")
            print(f"   ä¸­æ–‡æ–‡æ¡£æ•°: {stats['chinese_docs']}")
            print(f"   è‹±æ–‡æ–‡æ¡£æ•°: {stats['english_docs']}")
            print(f"   ä¸­æ–‡å¹³å‡é•¿åº¦: {stats['chinese_avg_length']:.2f}")
            print(f"   è‹±æ–‡å¹³å‡é•¿åº¦: {stats['english_avg_length']:.2f}")
            print(f"   ä¸­æ–‡summaryæ•°: {len(chinese_summaries)}")
            
        except Exception as e:
            print(f"âŒ ä¼˜åŒ–æ•°æ®åŠ è½½å™¨å¤±è´¥: {e}")
            print("å›é€€åˆ°ä¼ ç»Ÿæ•°æ®åŠ è½½æ–¹å¼...")
            
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹å¼
            from xlm.utils.dual_language_loader import DualLanguageLoader
            
            data_loader = DualLanguageLoader()
            chinese_docs, english_docs = data_loader.load_dual_language_data(
                chinese_data_path=config.data.chinese_data_path,
                english_data_path=config.data.english_data_path
            )
            
            print(f"Loaded {len(chinese_docs)} Chinese documents")
            print(f"Loaded {len(english_docs)} English documents")
            
            # ä½¿ç”¨ä¼ ç»Ÿchunking
            print("\nStep 1.1. Applying traditional document chunking...")
            chinese_chunks = self._chunk_documents_advanced(chinese_docs)
            english_chunks = self._chunk_documents_simple(english_docs, chunk_size=512, overlap=50)
            
            # é™åˆ¶AlphaFinæ•°æ®chunkæ•°é‡ï¼Œé¿å…200k+ chunkså½±å“æµ‹è¯•
            if len(chinese_chunks) > self.max_alphafin_chunks:
                print(f"Limiting Chinese chunks from {len(chinese_chunks)} to {self.max_alphafin_chunks} for testing...")
                chinese_chunks = chinese_chunks[:self.max_alphafin_chunks]
            
            # å¯¹äºä¼ ç»Ÿæ–¹å¼ï¼Œä¹Ÿæå–summary
            chinese_summaries = []
            for doc in chinese_chunks:
                content = doc.content
                if isinstance(content, str) and len(content) > 0:
                    summary = content[:500] + "..." if len(content) > 500 else content
                    # åˆ›å»ºæ–°çš„DocumentWithMetadataå¯¹è±¡
                    summary_doc = DocumentWithMetadata(
                        content=summary,
                        metadata=DocumentMetadata(
                            source=doc.metadata.source,
                            created_at=doc.metadata.created_at,
                            author=doc.metadata.author,
                            language="chinese"
                        )
                    )
                    chinese_summaries.append(summary_doc)
        
        print(f"Final summary count: {len(chinese_summaries)} Chinese summaries, {len(english_chunks)} English chunks")
        
        # ç›´æ¥åˆ›å»ºBilingualRetrieverï¼ŒembeddingåŸºäºchunk
        from xlm.components.retriever.bilingual_retriever import BilingualRetriever
        from xlm.components.encoder.finbert import FinbertEncoder
        
        # ä½¿ç”¨é…ç½®ä¸­çš„æ¨¡å‹è·¯å¾„ï¼Œå¹¶è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
        chinese_model_path = config.encoder.chinese_model_path
        english_model_path = config.encoder.english_model_path
        
        if not Path(chinese_model_path).is_absolute():
            chinese_model_path = str(Path(__file__).parent.parent.parent / chinese_model_path)
        if not Path(english_model_path).is_absolute():
            english_model_path = str(Path(__file__).parent.parent.parent / english_model_path)
        
        print(f"\nStep 2. Loading Chinese encoder ({chinese_model_path})...")
        encoder_ch = FinbertEncoder(
            model_name=chinese_model_path,
            cache_dir=config.encoder.cache_dir,  # ä½¿ç”¨encoderçš„ç¼“å­˜ç›®å½•
        )
        print(f"Step 3. Loading English encoder ({english_model_path})...")
        encoder_en = FinbertEncoder(
            model_name=english_model_path,
            cache_dir=config.encoder.cache_dir,  # ä½¿ç”¨encoderçš„ç¼“å­˜ç›®å½•
        )
        
        # æ ¹æ®use_existing_embedding_indexå‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨ç°æœ‰ç´¢å¼•
        if self.use_existing_embedding_index:
            print("Using existing embedding index (if available)...")
            cache_dir = config.encoder.cache_dir  # ä½¿ç”¨encoderçš„ç¼“å­˜ç›®å½•
        else:
            print("Forcing to recompute embeddings (ignoring existing cache)...")
            # ä»ç„¶ä½¿ç”¨models/embedding_cacheï¼Œä½†ä¼šé‡æ–°è®¡ç®—åµŒå…¥
            cache_dir = config.encoder.cache_dir
            print(f"Using cache directory: {cache_dir} (will recompute embeddings)")
        
        print(f"[UI DEBUG] self.use_existing_embedding_index={self.use_existing_embedding_index}")
        print("=== BEFORE BilingualRetriever ===", flush=True)
        self.retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,
            corpus_documents_en=english_chunks,
            corpus_documents_ch=chinese_summaries,
            use_faiss=self.use_faiss,
            use_gpu=False,
            batch_size=8,
            cache_dir=cache_dir,
            use_existing_embedding_index=self.use_existing_embedding_index
        )
        print("=== AFTER BilingualRetriever ===", flush=True)
        print(f"[UI DEBUG] BilingualRetriever created with use_existing_embedding_index={self.use_existing_embedding_index}")
        
        if self.use_faiss:
            print("Step 3.1. Initializing FAISS index...")
            self._init_faiss()
            print("FAISS index initialized successfully")
        
        # Initialize reranker if enabled
        if self.enable_reranker:
            print("\nStep 4. Loading reranker...")
            self.reranker = try_load_qwen_reranker(
                model_name="Qwen/Qwen3-Reranker-0.6B",
                cache_dir=config.reranker.cache_dir  # ä½¿ç”¨rerankerçš„ç¼“å­˜ç›®å½•
            )
        else:
            self.reranker = None
        
        print("\nStep 5. Loading generator...")
        self.generator = load_generator(
            generator_model_name=self.generator_model_name,
            use_local_llm=True,
            use_gpu=True,  # å¯ç”¨GPU
            gpu_device="cuda:1",  # ä½¿ç”¨GPU 1
            cache_dir=config.generator.cache_dir  # ä½¿ç”¨generatorçš„ç¼“å­˜ç›®å½•
        )
        
        print("\nStep 6. Initializing RAG system...")
        
        self.rag_system = RagSystem(
            retriever=self.retriever,
            generator=self.generator,
            retriever_top_k=20  # å¢åŠ åˆ°20ï¼Œä»configè¯»å–retriever_top_k
        )
        
        print("\nStep 7. Loading visualizer...")
        self.visualizer = Visualizer(show_mid_features=True)
    
    def _init_faiss(self):
        """Initialize FAISS index"""
        # å¯¹äºåŒç©ºé—´æ£€ç´¢å™¨ï¼Œæˆ‘ä»¬éœ€è¦åˆå¹¶è‹±æ–‡å’Œä¸­æ–‡åµŒå…¥å‘é‡
        if hasattr(self.retriever, 'corpus_embeddings_en') and hasattr(self.retriever, 'corpus_embeddings_ch'):
            # åŒç©ºé—´æ£€ç´¢å™¨
            embeddings_en = self.retriever.corpus_embeddings_en
            embeddings_ch = self.retriever.corpus_embeddings_ch
            
            if embeddings_en is not None and embeddings_ch is not None:
                # åˆå¹¶åµŒå…¥å‘é‡
                all_embeddings = np.vstack([embeddings_en, embeddings_ch])
                self.dimension = all_embeddings.shape[1]
                self.index = faiss.IndexFlatL2(self.dimension)
                # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                embeddings_float32 = all_embeddings.astype('float32')
                self.index.add(embeddings_float32)
                print(f"Step 3.1. Initializing FAISS index with {all_embeddings.shape[0]} embeddings...")
            else:
                print("Warning: No embeddings available for FAISS initialization")
        else:
            # å•ç©ºé—´æ£€ç´¢å™¨ - å¯¹äºBilingualRetrieverï¼Œè·³è¿‡FAISSåˆå§‹åŒ–
            print("Warning: BilingualRetriever detected, skipping FAISS initialization")
            print("FAISS index will not be available for this retriever type")
    
    def _create_interface(self) -> gr.Blocks:
        """Create optimized Gradio interface"""
        with gr.Blocks(
            title=self.window_title
        ) as interface:
            # æ ‡é¢˜
            gr.Markdown(f"# {self.title}")
            
            # è¾“å…¥åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=4):
                    datasource = gr.Radio(
                        choices=["TatQA", "AlphaFin", "Both"],
                        value="Both",
                        label="Data Source"
                    )
                    
            with gr.Row():
                with gr.Column(scale=4):
                    question_input = gr.Textbox(
                        show_label=False,
                        placeholder="Enter your question",
                        label="Question",
                        lines=3
                    )
            
            # æ§åˆ¶æŒ‰é’®åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=1):
                    reranker_checkbox = gr.Checkbox(
                        label="Enable Reranker",
                        value=True,
                        interactive=True
                    )
                with gr.Column(scale=1):
                    submit_btn = gr.Button("Submit")
            
            # ä½¿ç”¨æ ‡ç­¾é¡µåˆ†ç¦»æ˜¾ç¤º
            with gr.Tabs():
                # å›ç­”æ ‡ç­¾é¡µ
                with gr.TabItem("Answer"):
                    answer_output = gr.Textbox(
                        show_label=False,
                        interactive=False,
                        label="Generated Response",
                        lines=5
                    )
                
                # è§£é‡Šæ ‡ç­¾é¡µ
                with gr.TabItem("Explanation"):
                    context_output = gr.Dataframe(
                        headers=["Score", "Context"],
                        datatype=["number", "str"],
                        label="Retrieved Contexts",
                        interactive=False
                    )

            # æ·»åŠ ç¤ºä¾‹é—®é¢˜
            gr.Examples(
                examples=self.examples,
                inputs=[question_input],
                label="Example Questions"
            )

            # ç»‘å®šäº‹ä»¶
            submit_btn.click(
                self._process_question,
                inputs=[question_input, datasource, reranker_checkbox],
                outputs=[answer_output, context_output]
            )
            
            return interface
    
    def _process_question(
        self,
        question: str,
        datasource: str,
        reranker_checkbox: bool
    ) -> tuple[str, List[List[str]], Optional[gr.Plot]]:
        """Process user question and return results"""
        if not question.strip():
            return "Please enter a question.", [], None
        print(f"\nProcessing question: {question}")
        print(f"Data source: {datasource}")
        
        # Detect language and pass to rag_system
        try:
            from langdetect import detect
            lang = detect(question)
            # ä¸­æ–‡å­—ç¬¦å…œåº•
            def is_chinese(text):
                return len([c for c in text if '\u4e00' <= c <= '\u9fff']) > max(1, len(text) // 6)
            if lang.startswith('zh') or (lang == 'ko' and is_chinese(question)) or is_chinese(question):
                language = 'zh'
            else:
                language = 'en'
        except Exception as e:
            print(f"Language detection failed: {e}, fallback to English.")
            language = 'en'
        print(f"Detected language: {language}")
        
        # æ ¹æ®è¯­è¨€é€‰æ‹©æ£€ç´¢ç³»ç»Ÿ
        if language == 'zh' and self.multi_stage_system:
            print("ğŸ” ä½¿ç”¨ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
            return self._process_chinese_with_multi_stage(question, reranker_checkbox)
        else:
            print("ğŸ” ä½¿ç”¨ä¼ ç»ŸRAGç³»ç»Ÿ...")
            return self._process_with_traditional_rag(question, language, reranker_checkbox)
    
    def _process_chinese_with_multi_stage(self, question: str, reranker_checkbox: bool) -> tuple[str, List[List[str]], Optional[gr.Plot]]:
        """ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå¤„ç†ä¸­æ–‡æŸ¥è¯¢"""
        if not self.multi_stage_system:
            print("âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ£€ç´¢")
            return self._process_with_traditional_rag(question, 'zh', reranker_checkbox)
        try:
            # å°è¯•æå–å…¬å¸åç§°å’Œè‚¡ç¥¨ä»£ç ç”¨äºå…ƒæ•°æ®è¿‡æ»¤
            company_name = None
            stock_code = None
            
            # ç®€å•çš„å®ä½“æå–
            import re
            # æå–è‚¡ç¥¨ä»£ç 
            stock_match = re.search(r'\((\d{6})\)', question)
            if stock_match:
                stock_code = stock_match.group(1)
            
            # æå–å…¬å¸åç§°ï¼ˆç®€å•å®ç°ï¼‰
            company_patterns = [
                r'([^ï¼Œã€‚ï¼Ÿ\s]+(?:è‚¡ä»½|é›†å›¢|å…¬å¸|æœ‰é™|ç§‘æŠ€|ç½‘ç»œ|é“¶è¡Œ|è¯åˆ¸|ä¿é™©))',
                r'([^ï¼Œã€‚ï¼Ÿ\s]+(?:è‚¡ä»½|é›†å›¢|å…¬å¸|æœ‰é™|ç§‘æŠ€|ç½‘ç»œ|é“¶è¡Œ|è¯åˆ¸|ä¿é™©)[^ï¼Œã€‚ï¼Ÿ\s]*)'
            ]
            
            for pattern in company_patterns:
                company_match = re.search(pattern, question)
                if company_match:
                    company_name = company_match.group(1)
                    break
            
            # æ‰§è¡Œå¤šé˜¶æ®µæ£€ç´¢
            results = self.multi_stage_system.search(
                query=question,
                company_name=company_name,
                stock_code=stock_code,
                top_k=20
            )
            
            # è½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼
            retrieved_documents = []
            retriever_scores = []
            
            # æ£€æŸ¥resultsçš„æ ¼å¼
            if isinstance(results, dict) and 'retrieved_documents' in results:
                documents = results['retrieved_documents']
                llm_answer = results.get('llm_answer', '')
                for result in documents:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
                print(f"âœ… å¤šé˜¶æ®µæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(retrieved_documents)} æ¡ç»“æœ")
                # === æ–°å¢ï¼šoriginal_contextå»é‡ ===
                unique_contexts = {}
                for doc, score in zip(retrieved_documents, retriever_scores):
                    context = doc.content
                    h = hashlib.md5(context.encode('utf-8')).hexdigest()
                    if h not in unique_contexts or score > unique_contexts[h][1]:
                        unique_contexts[h] = (doc, score)
                # åªä¿ç•™å»é‡åçš„å†…å®¹ï¼ŒæŒ‰åˆ†æ•°æ’åº
                dedup_docs = sorted(unique_contexts.values(), key=lambda x: -x[1])
                # å¦‚æœå¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå·²ç»ç”Ÿæˆäº†ç­”æ¡ˆï¼Œç›´æ¥ä½¿ç”¨
                if llm_answer:
                    context_data = []
                    for doc, score in dedup_docs[:20]:
                        context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                    answer = f"[Multi-Stage Retrieval: ZH] {llm_answer}"
                    return answer, context_data, None
            else:
                for result in results:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
            if retrieved_documents:
                # === æ–°å¢ï¼šoriginal_contextå»é‡ ===
                unique_contexts = {}
                for doc, score in zip(retrieved_documents, retriever_scores):
                    context = doc.content
                    h = hashlib.md5(context.encode('utf-8')).hexdigest()
                    if h not in unique_contexts or score > unique_contexts[h][1]:
                        unique_contexts[h] = (doc, score)
                dedup_docs = sorted(unique_contexts.values(), key=lambda x: -x[1])
                context_str = "\n\n".join([doc.content for doc, _ in dedup_docs[:10]])
                from xlm.components.rag_system.rag_system import PROMPT_TEMPLATE_ZH
                prompt = PROMPT_TEMPLATE_ZH.format(context=context_str, question=question)
                generated_responses = self.rag_system.generator.generate(texts=[prompt])
                answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                context_data = []
                for doc, score in dedup_docs[:20]:
                    context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                answer = f"[Multi-Stage Retrieval: ZH] {answer}"
                return answer, context_data, None
            else:
                return "No relevant documents found.", [], None
        except Exception as e:
            print(f"âŒ å¤šé˜¶æ®µæ£€ç´¢å¤±è´¥: {e}")
            print("å›é€€åˆ°ä¼ ç»Ÿæ£€ç´¢...")
            return self._process_with_traditional_rag(question, 'zh', reranker_checkbox)
    
    def _process_with_traditional_rag(self, question: str, language: str, reranker_checkbox: bool) -> tuple[str, List[List[str]], Optional[gr.Plot]]:
        """ä½¿ç”¨ä¼ ç»ŸRAGç³»ç»Ÿå¤„ç†æŸ¥è¯¢"""
        # æ ¹æ®æ•°æ®æºé€‰æ‹©å†³å®šæ˜¯å¦ä½¿ç”¨é‡æ’åºå™¨
        use_reranker = reranker_checkbox and self.enable_reranker and self.reranker is not None
        
        # === æ–°å¢ï¼šä»…å¯¹ä¸­æ–‡queryå¯ç”¨ä¼˜åŒ–æ£€ç´¢ ===
        rag_output = None
        if language == 'zh':
            try:
                from simple_query_test import SimpleQueryOptimizer
            except ImportError:
                print("simple_query_test.pyæœªæ‰¾åˆ°æˆ–å¯¼å…¥å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ£€ç´¢ã€‚")
                try:
                    rag_output = self.rag_system.run(user_input=question, language=language)
                except Exception as e:
                    print(f"ä¼ ç»ŸRAGç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
                    # å¦‚æœä¼ ç»ŸRAGä¹Ÿå¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                    return f"ç³»ç»Ÿé”™è¯¯: {str(e)}", [], None
            else:
                try:
                    optimizer = SimpleQueryOptimizer()
                    entity = optimizer.extract_entities(question)
                    query_variants = optimizer.create_optimized_queries(question, entity)
                    all_docs = []
                    all_scores = []
                    seen_doc_ids = set()
                    for q in query_variants:
                        docs, scores = self.rag_system.retriever.retrieve(
                            text=q, top_k=self.rag_system.retriever_top_k, return_scores=True, language=language
                        )
                        for doc, score in zip(docs, scores):
                            doc_id = getattr(doc, 'id', None)
                            if doc_id not in seen_doc_ids:
                                all_docs.append(doc)
                                all_scores.append(score)
                                seen_doc_ids.add(doc_id)
                        if len(all_docs) >= self.rag_system.retriever_top_k:
                            break
                    # æ„é€ RagOutputå‰ï¼Œå¢åŠ å®ä½“è¿‡æ»¤
                    def filter_by_entity(docs, scores, entity):
                        filtered_docs = []
                        filtered_scores = []
                        for doc, score in zip(docs, scores):
                            if entity.stock_code and entity.stock_code in doc.content:
                                filtered_docs.append(doc)
                                filtered_scores.append(score)
                            elif entity.company_name and entity.company_name in doc.content:
                                filtered_docs.append(doc)
                                filtered_scores.append(score)
                        if filtered_docs:
                            return filtered_docs, filtered_scores
                        else:
                            return docs, scores
                    all_docs, all_scores = filter_by_entity(all_docs, all_scores, entity)
                    # æ„é€ RagOutput
                    if not all_docs:
                        rag_output = self.rag_system.run(user_input=question, language=language)
                    else:
                        # åªç”Ÿæˆä¸€æ¬¡promptå’Œç­”æ¡ˆ
                        context_str = "\n\n".join([doc.content for doc in all_docs[:self.rag_system.retriever_top_k]])
                        # ä¿®å¤ï¼šä»rag_systemæ¨¡å—å¯¼å…¥promptæ¨¡æ¿
                        from xlm.components.rag_system.rag_system import PROMPT_TEMPLATE_ZH
                        try:
                            prompt = PROMPT_TEMPLATE_ZH.format(context=context_str, question=question)
                        except Exception as e:
                            print(f"Promptæ ¼å¼åŒ–å¤±è´¥: {e}")
                            # ä½¿ç”¨ç®€å•çš„promptä½œä¸ºå›é€€
                            prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context_str}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
                        
                        generated_responses = self.rag_output.generator.generate(texts=[prompt])
                        # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„RagOutputç±»å‹
                        from xlm.dto.dto import RagOutput
                        rag_output = RagOutput(
                            retrieved_documents=all_docs[:self.rag_system.retriever_top_k],
                            retriever_scores=all_scores[:self.rag_system.retriever_top_k],
                            prompt=prompt,
                            generated_responses=generated_responses,
                            metadata=dict(
                                # ä¿®å¤ï¼šå®‰å…¨è®¿é—®encoder_chå±æ€§
                                retriever_model_name=getattr(getattr(self.rag_system.retriever, 'encoder_ch', None), 'model_name', 'unknown') if hasattr(self.rag_system.retriever, 'encoder_ch') else 'unknown',
                                top_k=self.rag_system.retriever_top_k,
                                generator_model_name=self.rag_system.generator.model_name,
                                prompt_template="Golden-ZH",
                                question_language="zh"
                            ),
                        )
                except Exception as e:
                    print(f"ä¼˜åŒ–æ£€ç´¢å¤±è´¥: {e}")
                    # å›é€€åˆ°ä¼ ç»ŸRAG
                    try:
                        rag_output = self.rag_system.run(user_input=question, language=language)
                    except Exception as e2:
                        print(f"ä¼ ç»ŸRAGä¹Ÿå¤±è´¥: {e2}")
                        return f"ç³»ç»Ÿé”™è¯¯: {str(e2)}", [], None
        else:
            try:
                rag_output = self.rag_system.run(user_input=question, language=language)
            except Exception as e:
                print(f"ä¼ ç»ŸRAGç³»ç»Ÿè¿è¡Œå¤±è´¥: {e}")
                return f"ç³»ç»Ÿé”™è¯¯: {str(e)}", [], None
        
        # æ£€æµ‹é—®é¢˜è¯­è¨€å¹¶æ‰“å°æ•°æ®æºä¿¡æ¯
        def is_chinese(text):
            return len(re.findall(r'[\u4e00-\u9fff]', text)) > max(1, len(text) // 6)
        try:
            from langdetect import detect
            lang = detect(question)
            # ä¿®æ­£ï¼šå¦‚æœæ£€æµ‹ä¸ºéŸ©æ–‡ä½†å†…å®¹æ˜æ˜¾æ˜¯ä¸­æ–‡ï¼Œå¼ºåˆ¶è®¾ä¸ºä¸­æ–‡
            if lang == 'ko' and is_chinese(question):
                lang = 'zh-cn'
            is_chinese_q = lang.startswith('zh')
            detected_data_source = "AlphaFin" if is_chinese_q else "TAT_QA"
            print(f"Detected language: {lang} -> Using data source: {detected_data_source}")
        except LangDetectException:
            print("Language detection failed, defaulting to English -> TAT_QA")
            detected_data_source = "TAT_QA"
        except Exception as e:
            print(f"Language detection error: {e}")
            detected_data_source = "TAT_QA"
        
        # Apply reranker if enabled
        if use_reranker and rag_output.retrieved_documents and self.reranker is not None:
            print("Applying reranker...")
            docs_for_rerank = [(doc.content, doc.metadata.source) for doc in rag_output.retrieved_documents]
            reranked_docs = self.reranker.rerank(
                query=question,
                documents=[doc[0] for doc in docs_for_rerank]
            )
            if reranked_docs:
                reranked_documents = []
                reranked_scores = []
                for doc_content, score in reranked_docs:
                    for orig_doc in rag_output.retrieved_documents:
                        if orig_doc.content == doc_content:
                            reranked_documents.append(orig_doc)
                            reranked_scores.append(score)
                            break
                rag_output.retrieved_documents = reranked_documents
                rag_output.retriever_scores = reranked_scores
        
        # æ£€ç´¢ç»“æœå»é‡ï¼Œåªæ˜¾ç¤ºå‰20æ¡chunk
        unique_docs = []
        seen_hashes = set()
        for doc, score in zip(rag_output.retrieved_documents, rag_output.retriever_scores):
            content = doc.content
            h = hashlib.md5(content.encode('utf-8')).hexdigest()
            if h not in seen_hashes:
                unique_docs.append((doc, score))
                seen_hashes.add(h)
            if len(unique_docs) >= 20:
                break
        
        # Prepare answer
        answer = rag_output.generated_responses[0] if rag_output.generated_responses else "Unable to generate answer"
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°promptå’Œè¯­è¨€ä¿¡æ¯
        print(f"\n=== DEBUG INFO ===")
        print(f"Question language: {lang}")
        print(f"Data source: {detected_data_source}")
        print(f"Prompt template used: {rag_output.metadata.get('prompt_template', 'Unknown')}")
        print(f"Generated response: {answer[:200]}...")  # åªæ‰“å°å‰200ä¸ªå­—ç¬¦
        print(f"=== END DEBUG ===\n")
        
        # Add reranker info to answer if used
        if use_reranker:
            answer = f"[Reranker: Enabled] {answer}"
        else:
            answer = f"[Reranker: Disabled] {answer}"
        
        # Prepare context data
        context_data = []
        for doc, score in unique_docs:
            content = doc.content
            context_data.append([f"{score:.4f}", content])
        
        return answer, context_data, None
    
    def launch(self, share: bool = False):
        """Launch UI interface"""
        self.interface.launch(share=share)
    
    def _chunk_documents(self, documents: List[DocumentWithMetadata], chunk_size: int = 512, overlap: int = 50) -> List[DocumentWithMetadata]:
        """
        å°†æ–‡æ¡£åˆ†å‰²æˆæ›´å°çš„chunks
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            chunk_size: chunkå¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap: é‡å å­—ç¬¦æ•°
            
        Returns:
            åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        chunked_docs = []
        
        for doc in documents:
            content = doc.content
            if len(content) <= chunk_size:
                # æ–‡æ¡£è¾ƒçŸ­ï¼Œä¸éœ€è¦åˆ†å—
                chunked_docs.append(doc)
            else:
                # æ–‡æ¡£è¾ƒé•¿ï¼Œéœ€è¦åˆ†å—
                start = 0
                chunk_id = 0
                
                while start < len(content):
                    end = start + chunk_size
                    
                    # ç¡®ä¿ä¸åœ¨å•è¯ä¸­é—´æˆªæ–­
                    if end < len(content):
                        # å°è¯•åœ¨å¥å·ã€é€—å·æˆ–ç©ºæ ¼å¤„æˆªæ–­
                        for i in range(end, max(start + chunk_size - 100, start), -1):
                            if content[i] in '.ã€‚ï¼Œ, ':
                                end = i + 1
                                break
                    
                    chunk_content = content[start:end].strip()
                    
                    if chunk_content:  # ç¡®ä¿chunkä¸ä¸ºç©º
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å…ƒæ•°æ®
                        chunk_metadata = DocumentMetadata(
                            source=f"{doc.metadata.source}_chunk_{chunk_id}",
                            created_at=doc.metadata.created_at,
                            author=doc.metadata.author,
                            language=doc.metadata.language
                        )
                        
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å¯¹è±¡
                        chunk_doc = DocumentWithMetadata(
                            content=chunk_content,
                            metadata=chunk_metadata
                        )
                        
                        chunked_docs.append(chunk_doc)
                        chunk_id += 1
                    
                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªchunkï¼Œè€ƒè™‘é‡å 
                    start = end - overlap
                    if start >= len(content):
                        break
        
        return chunked_docs 
    
    def _chunk_documents_advanced(self, documents: List[DocumentWithMetadata]) -> List[DocumentWithMetadata]:
        """
        ä½¿ç”¨finetune_chinese_encoder.pyä¸­çš„é«˜çº§chunké€»è¾‘å¤„ç†ä¸­æ–‡æ–‡æ¡£
        å¹¶é›†æˆfinetune_encoder.pyä¸­çš„è¡¨æ ¼æ–‡æœ¬åŒ–å¤„ç†
        """
        import re
        import json
        import ast
        
        def extract_unit_from_paragraph(paragraphs):
            """ä»æ®µè½ä¸­æå–æ•°å€¼å•ä½"""
            for para in paragraphs:
                text = para.get("text", "") if isinstance(para, dict) else para
                match = re.search(r'dollars in (millions|billions)|in (millions|billions)', text, re.IGNORECASE)
                if match:
                    unit = match.group(1) or match.group(2)
                    if unit:
                        return unit.lower().replace('s', '') + " USD"
            return ""

        def table_to_natural_text(table_dict, caption="", unit_info=""):
            """å°†è¡¨æ ¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°"""
            rows = table_dict.get("table", [])
            lines = []

            if caption:
                lines.append(f"Table Topic: {caption}.")

            if not rows:
                return ""

            headers = rows[0]
            data_rows = rows[1:]

            for i, row in enumerate(data_rows):
                if not row or all(str(v).strip() == "" for v in row):
                    continue

                if len(row) > 1 and str(row[0]).strip() != "" and all(str(v).strip() == "" for v in row[1:]):
                    lines.append(f"Table Category: {str(row[0]).strip()}.")
                    continue

                row_name = str(row[0]).strip().replace('.', '')

                data_descriptions = []
                for h_idx, v in enumerate(row):
                    if h_idx == 0:
                        continue
                    
                    header = headers[h_idx] if h_idx < len(headers) else f"Column {h_idx+1}"
                    value = str(v).strip()

                    if value:
                        if re.match(r'^-?\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?$|^\(\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?\)$', value): 
                            formatted_value = value.replace('$', '')
                            if unit_info:
                                if formatted_value.startswith('(') and formatted_value.endswith(')'):
                                     formatted_value = f"(${formatted_value[1:-1]} {unit_info})"
                                else:
                                     formatted_value = f"${formatted_value} {unit_info}"
                            else:
                                formatted_value = f"${formatted_value}"
                        else:
                            formatted_value = value
                        
                        data_descriptions.append(f"{header} is {formatted_value}")

                if row_name and data_descriptions:
                    lines.append(f"Details for item {row_name}: {'; '.join(data_descriptions)}.")
                elif data_descriptions:
                    lines.append(f"Other data item: {'; '.join(data_descriptions)}.")
                elif row_name:
                    lines.append(f"Data item: {row_name}.")

            return "\n".join(lines)
        
        def convert_json_context_to_natural_language_chunks(json_str_context, company_name="å…¬å¸"):
            chunks = []
            if not json_str_context or not json_str_context.strip():
                return chunks
            processed_str_context = json_str_context.replace("\\n", "\n")
            cleaned_initial = re.sub(re.escape("ã€é—®é¢˜ã€‘:"), "", processed_str_context)
            cleaned_initial = re.sub(re.escape("ã€ç­”æ¡ˆã€‘:"), "", cleaned_initial).strip()
            cleaned_initial = cleaned_initial.replace('ï¼Œ', ',')
            cleaned_initial = cleaned_initial.replace('ï¼š', ':')
            cleaned_initial = cleaned_initial.replace('ã€', '') 
            cleaned_initial = cleaned_initial.replace('ã€‘', '') 
            cleaned_initial = cleaned_initial.replace('\u3000', ' ')
            cleaned_initial = cleaned_initial.replace('\xa0', ' ').strip()
            cleaned_initial = re.sub(r'\s+', ' ', cleaned_initial).strip()
            
            # å¤„ç†ç ”æŠ¥æ ¼å¼
            report_match = re.match(
                r"è¿™æ˜¯ä»¥(.+?)ä¸ºé¢˜ç›®,åœ¨(\d{4}-\d{2}-\d{2}(?: \d{2}:\d{2}:\d{2})?)æ—¥æœŸå‘å¸ƒçš„ç ”ç©¶æŠ¥å‘Šã€‚ç ”æŠ¥å†…å®¹å¦‚ä¸‹: (.+)", 
                cleaned_initial, 
                re.DOTALL
            )
            if report_match:
                report_title_full = report_match.group(1).strip()
                report_date = report_match.group(2).strip()
                report_raw_content = report_match.group(3).strip() 
                content_after_second_title_match = re.match(r"ç ”æŠ¥é¢˜ç›®æ˜¯:(.+)", report_raw_content, re.DOTALL)
                if content_after_second_title_match:
                    report_content_preview = content_after_second_title_match.group(1).strip()
                else:
                    report_content_preview = report_raw_content 
                report_content_preview = re.sub(re.escape("ã€é—®é¢˜ã€‘:"), "", report_content_preview)
                report_content_preview = re.sub(re.escape("ã€ç­”æ¡ˆã€‘:"), "", report_content_preview).strip()
                report_content_preview = re.sub(r'\s+', ' ', report_content_preview).strip() 
                company_stock_match = re.search(r"(.+?)ï¼ˆ(\d{6}\.\w{2})ï¼‰", report_title_full)
                company_info = ""
                if company_stock_match:
                    report_company_name = company_stock_match.group(1).strip()
                    report_stock_code = company_stock_match.group(2).strip()
                    company_info = f"ï¼Œå…¬å¸åç§°ï¼š{report_company_name}ï¼Œè‚¡ç¥¨ä»£ç ï¼š{report_stock_code}"
                    report_title_main = re.sub(r"ï¼ˆ\d{6}\.\w{2}ï¼‰", "", report_title_full).strip()
                else:
                    report_title_main = report_title_full
                chunk_text = f"ä¸€ä»½å‘å¸ƒæ—¥æœŸä¸º {report_date} çš„ç ”ç©¶æŠ¥å‘Šï¼Œå…¶æ ‡é¢˜æ˜¯ï¼š\"{report_title_main}\"{company_info}ã€‚æŠ¥å‘Šæ‘˜è¦å†…å®¹ï¼š{report_content_preview.rstrip('...') if report_content_preview.endswith('...') else report_content_preview}ã€‚"
                chunks.append(chunk_text)
                return chunks 

            # å¤„ç†å­—å…¸æ ¼å¼
            extracted_dict_str = None
            parsed_data = None 
            temp_dict_search_str = re.sub(r"Timestamp\(['\"](.*?)['\"]\)", r"'\1'", cleaned_initial) 
            all_dict_matches = re.findall(r"(\{.*?\})", temp_dict_search_str, re.DOTALL) 
            for potential_dict_str in all_dict_matches:
                cleaned_potential_dict_str = potential_dict_str.strip()
                json_compatible_str_temp = cleaned_potential_dict_str.replace("'", '"')
                try:
                    parsed_data_temp = json.loads(json_compatible_str_temp)
                    if isinstance(parsed_data_temp, dict):
                        extracted_dict_str = cleaned_potential_dict_str
                        parsed_data = parsed_data_temp
                        break 
                except json.JSONDecodeError:
                    pass 
                fixed_for_ast_eval_temp = re.sub(
                    r"(?<!['\"\w.])\b(0[1-9]\d*)\b(?![\d.]|['\"\w.])", 
                    r"'\1'", 
                    cleaned_potential_dict_str
                )
                try:
                    parsed_data_temp = ast.literal_eval(fixed_for_ast_eval_temp)
                    if isinstance(parsed_data_temp, dict):
                        extracted_dict_str = cleaned_potential_dict_str
                        parsed_data = parsed_data_temp
                        break 
                except (ValueError, SyntaxError):
                    pass 

            if extracted_dict_str is not None and isinstance(parsed_data, dict):
                for metric_name, time_series_data in parsed_data.items():
                    if not isinstance(metric_name, str):
                        metric_name = str(metric_name)
                    cleaned_metric_name = re.sub(r'ï¼ˆ.*?ï¼‰', '', metric_name).strip()
                    if not isinstance(time_series_data, dict):
                        if time_series_data is not None and str(time_series_data).strip():
                            chunks.append(f"{company_name}çš„{cleaned_metric_name}æ•°æ®ä¸ºï¼š{time_series_data}ã€‚")
                        continue
                    if not time_series_data:
                        continue
                    try:
                        sorted_dates = sorted(time_series_data.keys(), key=str)
                    except TypeError:
                        sorted_dates = [str(k) for k in time_series_data.keys()]
                    description_parts = []
                    for date in sorted_dates:
                        value = time_series_data[date]
                        if isinstance(value, (int, float)):
                            formatted_value = f"{value:.4f}".rstrip('0').rstrip('.') if isinstance(value, float) else str(value)
                        else:
                            formatted_value = str(value)
                        description_parts.append(f"åœ¨{date}ä¸º{formatted_value}")
                    if description_parts:
                        if len(description_parts) <= 3:
                            full_description = f"{company_name}çš„{cleaned_metric_name}æ•°æ®: " + "ï¼Œ".join(description_parts) + "ã€‚"
                        else:
                            first_part = "ï¼Œ".join(description_parts[:3])
                            last_part = "ï¼Œ".join(description_parts[-3:])
                            if len(sorted_dates) > 6:
                                full_description = f"{company_name}çš„{cleaned_metric_name}æ•°æ®ä»{sorted_dates[0]}åˆ°{sorted_dates[-1]}ï¼Œä¸»è¦å˜åŒ–ä¸ºï¼š{first_part}ï¼Œ...ï¼Œ{last_part}ã€‚"
                            else:
                                full_description = f"{company_name}çš„{cleaned_metric_name}æ•°æ®: " + "ï¼Œ".join(description_parts) + "ã€‚"
                        chunks.append(full_description)
                return chunks 

            # å¤„ç†çº¯æ–‡æœ¬
            pure_text = cleaned_initial
            pure_text = re.sub(r"^\d{4}-\d{2}-\d{2}( \d{2}:\d{2}:\d{2})?[_;]?", "", pure_text, 1).strip()
            pure_text = re.sub(r"^[\u4e00-\u9fa5]+(?:/[\u4e00-\u9fa5]+)?\d{4}å¹´\d{2}æœˆ\d{2}æ—¥\d{2}:\d{2}:\d{2}(?:æ®[\u4e00-\u9fa5]+?,)?\d{1,2}æœˆ\d{1,2}æ—¥,?", "", pure_text).strip()
            pure_text = re.sub(r"^(?:å¸‚åœºèµ„é‡‘è¿›å‡º)?æˆªè‡³å‘¨[ä¸€äºŒä¸‰å››äº”å…­æ—¥]æ”¶ç›˜,?", "", pure_text).strip()
            pure_text = re.sub(r"^[\u4e00-\u9fa5]+?ä¸­æœŸå‡€åˆ©é¢„å‡\d+%-?\d*%(?:[\u4e00-\u9fa5]+?\d{1,2}æœˆ\d{1,2}æ—¥æ™šé—´å…¬å‘Š,)?", "", pure_text).strip()

            if pure_text: 
                chunks.append(pure_text)
            else:
                chunks.append(f"åŸå§‹æ ¼å¼ï¼Œè§£æå¤±è´¥æˆ–æ— æœ‰æ•ˆç»“æ„ï¼š{json_str_context.strip()[:100]}...")
            return chunks
        
        chunked_docs = []
        for doc in documents:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼æ•°æ®
            content = doc.content
            
            # å°è¯•è§£æä¸ºJSONï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼ç»“æ„
            try:
                parsed_content = json.loads(content)
                if isinstance(parsed_content, dict) and 'tables' in parsed_content:
                    # å¤„ç†åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£
                    paragraphs = parsed_content.get('paragraphs', [])
                    tables = parsed_content.get('tables', [])
                    
                    # æå–å•ä½ä¿¡æ¯
                    unit_info = extract_unit_from_paragraph(paragraphs)
                    
                    # å¤„ç†æ®µè½
                    for p_idx, para in enumerate(paragraphs):
                        para_text = para.get("text", "") if isinstance(para, dict) else para
                        if para_text.strip():
                            chunk_metadata = DocumentMetadata(
                                source=f"{doc.metadata.source}_table_para_{p_idx}",
                                created_at=doc.metadata.created_at,
                                author=doc.metadata.author,
                                language=doc.metadata.language
                            )
                            chunk_doc = DocumentWithMetadata(
                                content=para_text.strip(),
                                metadata=chunk_metadata
                            )
                            chunked_docs.append(chunk_doc)
                    
                    # å¤„ç†è¡¨æ ¼
                    for t_idx, table in enumerate(tables):
                        table_text = table_to_natural_text(table, table.get("caption", ""), unit_info)
                        if table_text.strip():
                            chunk_metadata = DocumentMetadata(
                                source=f"{doc.metadata.source}_table_text_{t_idx}",
                                created_at=doc.metadata.created_at,
                                author=doc.metadata.author,
                                language=doc.metadata.language
                            )
                            chunk_doc = DocumentWithMetadata(
                                content=table_text.strip(),
                                metadata=chunk_metadata
                            )
                            chunked_docs.append(chunk_doc)
                    
                    continue  # å·²å¤„ç†è¡¨æ ¼æ•°æ®ï¼Œè·³è¿‡åç»­å¤„ç†
                    
            except (json.JSONDecodeError, TypeError):
                pass  # ä¸æ˜¯JSONæ ¼å¼ï¼Œç»§ç»­ä½¿ç”¨åŸæœ‰çš„chunké€»è¾‘
            
            # ä½¿ç”¨åŸæœ‰çš„é«˜çº§chunké€»è¾‘å¤„ç†
            chunks = convert_json_context_to_natural_language_chunks(content)
            
            for i, chunk_content in enumerate(chunks):
                if chunk_content.strip():
                    chunk_metadata = DocumentMetadata(
                        source=f"{doc.metadata.source}_advanced_chunk_{i}",
                        created_at=doc.metadata.created_at,
                        author=doc.metadata.author,
                        language=doc.metadata.language
                    )
                    
                    chunk_doc = DocumentWithMetadata(
                        content=chunk_content,
                        metadata=chunk_metadata
                    )
                    
                    chunked_docs.append(chunk_doc)
        
        return chunked_docs
    
    def _chunk_documents_simple(self, documents: List[DocumentWithMetadata], chunk_size: int = 512, overlap: int = 50) -> List[DocumentWithMetadata]:
        """
        ç®€å•çš„æ–‡æ¡£åˆ†å—æ–¹æ³•ï¼Œç”¨äºè‹±æ–‡æ–‡æ¡£
        """
        chunked_docs = []
        
        for doc in documents:
            content = doc.content
            if len(content) <= chunk_size:
                # æ–‡æ¡£è¾ƒçŸ­ï¼Œä¸éœ€è¦åˆ†å—
                chunked_docs.append(doc)
            else:
                # æ–‡æ¡£è¾ƒé•¿ï¼Œéœ€è¦åˆ†å—
                start = 0
                chunk_id = 0
                
                while start < len(content):
                    end = start + chunk_size
                    
                    # ç¡®ä¿ä¸åœ¨å•è¯ä¸­é—´æˆªæ–­
                    if end < len(content):
                        # å°è¯•åœ¨å¥å·ã€é€—å·æˆ–ç©ºæ ¼å¤„æˆªæ–­
                        for i in range(end, max(start + chunk_size - 100, start), -1):
                            if content[i] in '.ã€‚ï¼Œ, ':
                                end = i + 1
                                break
                    
                    chunk_content = content[start:end].strip()
                    
                    if chunk_content:  # ç¡®ä¿chunkä¸ä¸ºç©º
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å…ƒæ•°æ®
                        chunk_metadata = DocumentMetadata(
                            source=f"{doc.metadata.source}_simple_chunk_{chunk_id}",
                            created_at=doc.metadata.created_at,
                            author=doc.metadata.author,
                            language=doc.metadata.language
                        )
                        
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å¯¹è±¡
                        chunk_doc = DocumentWithMetadata(
                            content=chunk_content,
                            metadata=chunk_metadata
                        )
                        
                        chunked_docs.append(chunk_doc)
                        chunk_id += 1
                    
                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªchunkï¼Œè€ƒè™‘é‡å 
                    start = end - overlap
                    if start >= len(content):
                        break
        
        return chunked_docs 