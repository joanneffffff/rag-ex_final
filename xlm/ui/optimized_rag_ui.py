import os
import gradio as gr
from typing import List, Optional
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
from gradio.components import Markdown
import torch
from langdetect import detect, LangDetectException
import re

from xlm.registry.generator import load_generator
from xlm.registry.retriever import load_enhanced_retriever
from xlm.components.rag_system.rag_system import RagSystem
from xlm.components.retriever.reranker import QwenReranker
from xlm.utils.visualizer import Visualizer
from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata
from config.parameters import Config, EncoderConfig, RetrieverConfig, ModalityConfig, EMBEDDING_CACHE_DIR, RERANKER_CACHE_DIR

def try_load_qwen_reranker(model_name, cache_dir=None):
    """
    ä¼˜å…ˆå°è¯•é‡åŒ–åŠ è½½QwenRerankerï¼Œå¤±è´¥åˆ™è‡ªåŠ¨å›é€€ä¸ºéé‡åŒ–ã€‚
    """
    try:
        import bitsandbytes as bnb
        if torch.cuda.is_available():
            print("å°è¯•ä½¿ç”¨8bité‡åŒ–åŠ è½½QwenReranker...")
            reranker = QwenReranker(
                model_name=model_name,
                device="cuda",
                cache_dir=cache_dir,
                use_quantization=True,
                quantization_type="8bit",
                use_flash_attention=False
            )
            print("é‡åŒ–åŠ è½½æˆåŠŸï¼")
            return reranker
        else:
            print("æœªæ£€æµ‹åˆ°CUDAï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºCPUéé‡åŒ–åŠ è½½ã€‚")
            raise ImportError("No CUDA for quantization")
    except Exception as e:
        print(f"é‡åŒ–åŠ è½½å¤±è´¥ï¼Œè‡ªåŠ¨å›é€€ä¸ºéé‡åŒ–åŠ è½½ã€‚åŸå› : {e}")
        reranker = QwenReranker(
            model_name=model_name,
            device="cpu",
            cache_dir=cache_dir,
            use_quantization=False,
            use_flash_attention=False
        )
        print("éé‡åŒ–åŠ è½½æˆåŠŸã€‚")
        return reranker

class OptimizedRagUI:
    def __init__(
        self,
        # encoder_model_name: str = "sentence-transformers/all-MiniLM-L6-v2",
        encoder_model_name: str = "paraphrase-multilingual-MiniLM-L12-v2",
        # generator_model_name: str = "facebook/opt-125m",
        generator_model_name: str = "Qwen/Qwen2-1.5B-Instruct",
        cache_dir: str = None,
        data_path: str = "data/rise_of_ai.txt",
        use_faiss: bool = True,
        enable_reranker: bool = True,
        use_existing_embedding_index: bool = None,  # ä»configè¯»å–ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼
        max_alphafin_chunks: int = None,  # ä»configè¯»å–ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨é»˜è®¤å€¼
        window_title: str = "RAG System with FAISS",
        title: str = "RAG System with FAISS",
        examples: Optional[List[List[str]]] = None,
    ):
        # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        config = Config()
        self.cache_dir = EMBEDDING_CACHE_DIR if (not cache_dir or not isinstance(cache_dir, str)) else cache_dir
        self.encoder_model_name = encoder_model_name
        self.generator_model_name = generator_model_name
        self.data_path = data_path
        self.use_faiss = use_faiss
        self.enable_reranker = enable_reranker
        # ä»configè¯»å–å‚æ•°ï¼Œå¦‚æœä¼ å…¥Noneåˆ™ä½¿ç”¨configé»˜è®¤å€¼
        self.use_existing_embedding_index = use_existing_embedding_index if use_existing_embedding_index is not None else config.retriever.use_existing_embedding_index
        self.max_alphafin_chunks = max_alphafin_chunks if max_alphafin_chunks is not None else config.retriever.max_alphafin_chunks
        self.window_title = window_title
        self.title = title
        self.examples = examples or [
            ["ä»€ä¹ˆæ˜¯è‚¡ç¥¨æŠ•èµ„ï¼Ÿ"],
            ["è¯·è§£é‡Šå€ºåˆ¸çš„åŸºæœ¬æ¦‚å¿µ"],
            ["åŸºé‡‘æŠ•èµ„ä¸è‚¡ç¥¨æŠ•èµ„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"],
            ["What is stock investment?"],
            ["Explain the basic concepts of bonds"],
            ["What are the differences between fund investment and stock investment?"]
        ]
        
        # Set environment variables for model caching
        os.environ['TRANSFORMERS_CACHE'] = os.path.join(self.cache_dir, 'transformers')
        os.environ['HF_HOME'] = self.cache_dir
        os.environ['HF_DATASETS_CACHE'] = os.path.join(self.cache_dir, 'datasets')
        
        # Initialize system components
        self._init_components()
        
        # Create Gradio interface
        self.interface = self._create_interface()
    
    def _init_components(self):
        """Initialize RAG system components"""
        print("\nStep 1. Loading bilingual retriever with dual encoders...")
        
        # å¯¼å…¥åŒè¯­è¨€æ•°æ®åŠ è½½å™¨
        from xlm.utils.dual_language_loader import DualLanguageLoader
        
        # åŠ è½½åŒè¯­è¨€æ•°æ®
        data_loader = DualLanguageLoader()
        chinese_docs, english_docs = data_loader.load_dual_language_data(
            chinese_data_path="evaluate_mrr/alphafin_train_qc.jsonl",
            english_data_path="evaluate_mrr/tatqa_train_qc.jsonl"
        )
        
        print(f"Loaded {len(chinese_docs)} Chinese documents")
        print(f"Loaded {len(english_docs)} English documents")
        
        # æ·»åŠ æ–‡æ¡£åˆ†å—åŠŸèƒ½
        print("\nStep 1.1. Applying document chunking...")
        chinese_chunks = self._chunk_documents_advanced(chinese_docs)
        english_chunks = self._chunk_documents_simple(english_docs, chunk_size=512, overlap=50)
        
        # é™åˆ¶AlphaFinæ•°æ®chunkæ•°é‡ï¼Œé¿å…200k+ chunkså½±å“æµ‹è¯•
        if len(chinese_chunks) > self.max_alphafin_chunks:
            print(f"Limiting Chinese chunks from {len(chinese_chunks)} to {self.max_alphafin_chunks} for testing...")
            chinese_chunks = chinese_chunks[:self.max_alphafin_chunks]
        
        print(f"After chunking: {len(chinese_chunks)} Chinese chunks, {len(english_chunks)} English chunks")
        
        # ç›´æ¥åˆ›å»ºBilingualRetrieverï¼ŒembeddingåŸºäºchunk
        from xlm.components.retriever.bilingual_retriever import BilingualRetriever
        from xlm.components.encoder.finbert import FinbertEncoder
        
        print("\nStep 2. Loading Chinese encoder (models/finetuned_alphafin_zh)...")
        encoder_ch = FinbertEncoder(
            model_name="models/finetuned_alphafin_zh",
            cache_dir=self.cache_dir,
        )
        print("Step 3. Loading English encoder (models/finetuned_finbert_tatqa)...")
        encoder_en = FinbertEncoder(
            model_name="models/finetuned_finbert_tatqa",
            cache_dir=self.cache_dir,
        )
        
        # æ ¹æ®use_existing_embedding_indexå‚æ•°å†³å®šæ˜¯å¦ä½¿ç”¨ç°æœ‰ç´¢å¼•
        if self.use_existing_embedding_index:
            print("Using existing embedding index (if available)...")
            cache_dir = self.cache_dir
        else:
            print("Forcing to recompute embeddings (ignoring existing cache)...")
            import tempfile
            cache_dir = tempfile.mkdtemp(prefix="rag_temp_cache_")
            print(f"Using temporary cache directory: {cache_dir}")
        
        self.retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,
            corpus_documents_en=english_chunks,
            corpus_documents_ch=chinese_chunks,
            use_faiss=self.use_faiss,
            use_gpu=False,
            batch_size=8,
            cache_dir=cache_dir
        )
        
        if self.use_faiss:
            print("Step 3.1. Initializing FAISS index...")
            self._init_faiss()
            print("FAISS index initialized successfully")
        
        # Initialize reranker if enabled
        if self.enable_reranker:
            print("\nStep 4. Loading reranker...")
            self.reranker = try_load_qwen_reranker(
                model_name="Qwen/Qwen3-Reranker-0.6B",
                cache_dir=self.cache_dir
            )
        else:
            self.reranker = None
        
        print("\nStep 5. Loading generator...")
        self.generator = load_generator(
            generator_model_name=self.generator_model_name,
            use_local_llm=True,
            cache_dir=self.cache_dir
        )
        
        print("\nStep 6. Initializing RAG system...")
        # é‡æ–°å¯¼å…¥Configç¡®ä¿å¯ç”¨
        config = Config()
        
        self.rag_system = RagSystem(
            retriever=self.retriever,
            generator=self.generator,
            retriever_top_k=config.retriever.retrieval_top_k  # ä»configè¯»å–retriever_top_k
        )
        
        print("\nStep 7. Loading visualizer...")
        self.visualizer = Visualizer(show_mid_features=True)
    
    def _init_faiss(self):
        """Initialize FAISS index"""
        # å¯¹äºåŒç©ºé—´æ£€ç´¢å™¨ï¼Œæˆ‘ä»¬éœ€è¦åˆå¹¶è‹±æ–‡å’Œä¸­æ–‡åµŒå…¥å‘é‡
        if hasattr(self.retriever, 'corpus_embeddings_en') and hasattr(self.retriever, 'corpus_embeddings_ch'):
            # åŒç©ºé—´æ£€ç´¢å™¨
            embeddings_en = self.retriever.corpus_embeddings_en
            embeddings_ch = self.retriever.corpus_embeddings_ch
            
            if embeddings_en is not None and embeddings_ch is not None:
                # åˆå¹¶åµŒå…¥å‘é‡
                all_embeddings = np.vstack([embeddings_en, embeddings_ch])
                self.dimension = all_embeddings.shape[1]
                self.index = faiss.IndexFlatL2(self.dimension)
                self.index.add(all_embeddings.astype('float32'))
                print(f"Step 3.1. Initializing FAISS index with {all_embeddings.shape[0]} embeddings...")
            else:
                print("Warning: No embeddings available for FAISS initialization")
        else:
            # å•ç©ºé—´æ£€ç´¢å™¨
            corpus_embeddings = self.retriever.corpus_embeddings
            self.dimension = len(corpus_embeddings[0])
            self.index = faiss.IndexFlatL2(self.dimension)
            self.index.add(np.array(corpus_embeddings).astype('float32'))
    
    def _create_interface(self) -> gr.Blocks:
        """Create optimized Gradio interface"""
        with gr.Blocks(
            theme=gr.themes.Monochrome().set(
                button_primary_background_fill="#009374",
                button_primary_background_fill_hover="#009374C4",
                checkbox_label_background_fill_selected="#028A6EFF",
            ),
            title=self.window_title
        ) as interface:
            # Title and data source display
            with gr.Row():
                with gr.Column(scale=1):
                    Markdown(
                        f'<p style="text-align: center; font-size:200%; font-weight: bold">{self.title}</p>'
                    )
                with gr.Column(scale=1):
                    data_source_display = gr.Textbox(
                        value="Auto-detect",
                        label="Data Source",
                        interactive=False
                    )
            # Input area
            with gr.Row():
                with gr.Column(scale=1):
                    question_input = gr.Textbox(
                        placeholder="Type your question here and press Enter.",
                        label="Question",
                        lines=3
                    )
            # Control buttons
            with gr.Row():
                with gr.Column(scale=1):
                    reranker_checkbox = gr.Checkbox(
                        label="Enable Reranker",
                        value=True,
                        interactive=True
                    )
                with gr.Column(scale=1):
                    submit_btn = gr.Button(
                        value="ğŸ” Ask",
                        variant="secondary",
                        elem_id="button"
                    )
            # Output area with tabs
            with gr.Row():
                with gr.Tabs():
                    with gr.TabItem("Answer"):
                        answer_output = gr.Textbox(
                            label="Generated Response",
                            lines=5,
                            interactive=False
                        )
                    with gr.TabItem("Explanation"):
                        context_output = gr.Dataframe(
                            headers=["Score", "Context"],
                            datatype=["number", "str"],
                            label="Retrieved Contexts",
                            interactive=False
                        )
            # ç»‘å®šäº‹ä»¶
            submit_btn.click(
                self._process_question,
                inputs=[question_input, reranker_checkbox],
                outputs=[answer_output, context_output]
            )
            # ç¤ºä¾‹é—®é¢˜
            gr.Examples(
                examples=self.examples,
                inputs=[question_input],
            )
            return interface
    
    def _process_question(
        self,
        question: str,
        reranker_checkbox: bool
    ) -> tuple[str, List[List[str]], Optional[gr.Plot]]:
        """Process user question and return results"""
        if not question.strip():
            return "Please enter a question.", [], None
        print(f"\nProcessing question: {question}")
        print(f"Reranker enabled: {reranker_checkbox}")
        # Detect language and pass to rag_system
        try:
            from langdetect import detect
            lang = detect(question)
            # ä¸­æ–‡å­—ç¬¦å…œåº•
            def is_chinese(text):
                return len([c for c in text if '\u4e00' <= c <= '\u9fff']) > max(1, len(text) // 6)
            if lang.startswith('zh') or (lang == 'ko' and is_chinese(question)) or is_chinese(question):
                language = 'zh'
            else:
                language = 'en'
        except Exception as e:
            print(f"Language detection failed: {e}, fallback to English.")
            language = 'en'
        print(f"Detected language: {language}")
        use_reranker = reranker_checkbox and self.enable_reranker and self.reranker
        try:
            rag_output = self.rag_system.run(user_input=question, language=language)
            
            # æ£€æµ‹é—®é¢˜è¯­è¨€å¹¶æ‰“å°æ•°æ®æºä¿¡æ¯
            def is_chinese(text):
                return len(re.findall(r'[\u4e00-\u9fff]', text)) > max(1, len(text) // 6)
            try:
                lang = detect(question)
                # ä¿®æ­£ï¼šå¦‚æœæ£€æµ‹ä¸ºéŸ©æ–‡ä½†å†…å®¹æ˜æ˜¾æ˜¯ä¸­æ–‡ï¼Œå¼ºåˆ¶è®¾ä¸ºä¸­æ–‡
                if lang == 'ko' and is_chinese(question):
                    lang = 'zh-cn'
                is_chinese_q = lang.startswith('zh')
                data_source = "AlphaFin" if is_chinese_q else "TAT_QA"
                print(f"Detected language: {lang} -> Using data source: {data_source}")
            except LangDetectException:
                print("Language detection failed, defaulting to English -> TAT_QA")
                data_source = "TAT_QA"
            
            # Apply reranker if enabled
            if use_reranker and rag_output.retrieved_documents:
                print("Applying reranker...")
                # Prepare documents for reranking
                docs_for_rerank = [(doc.content, doc.metadata.source) for doc in rag_output.retrieved_documents]
                
                # Rerank documents
                reranked_docs = self.reranker.rerank(
                    query=question,
                    documents=[doc[0] for doc in docs_for_rerank]  # åªä¼ å…¥æ–‡æ¡£å†…å®¹ï¼Œä¸ä¼ å…¥å…ƒæ•°æ®
                )
                
                # Update retrieved documents with reranked results
                if reranked_docs:
                    # Create new document objects with reranked order
                    reranked_documents = []
                    reranked_scores = []
                    
                    for doc_content, score in reranked_docs:
                        # Find original document
                        for orig_doc in rag_output.retrieved_documents:
                            if orig_doc.content == doc_content:
                                reranked_documents.append(orig_doc)
                                reranked_scores.append(score)
                                break
                    
                    rag_output.retrieved_documents = reranked_documents
                    rag_output.retriever_scores = reranked_scores
            
            # æ£€ç´¢ç»“æœå»é‡ï¼Œåªæ˜¾ç¤ºå‰5æ¡chunk
            unique_docs = []
            seen_contents = set()
            for doc, score in zip(rag_output.retrieved_documents, rag_output.retriever_scores):
                if doc.content not in seen_contents:
                    unique_docs.append((doc, score))
                    seen_contents.add(doc.content)
                if len(unique_docs) >= 5:
                    break
            
            # Prepare answer
            answer = rag_output.generated_responses[0] if rag_output.generated_responses else "Unable to generate answer"
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæ‰“å°promptå’Œè¯­è¨€ä¿¡æ¯
            print(f"\n=== DEBUG INFO ===")
            print(f"Question language: {lang}")
            print(f"Data source: {data_source}")
            print(f"Prompt template used: {rag_output.metadata.get('prompt_template', 'Unknown')}")
            print(f"Generated response: {answer[:200]}...")  # åªæ‰“å°å‰200ä¸ªå­—ç¬¦
            print(f"=== END DEBUG ===\n")
            
            # Add reranker info to answer if used
            if use_reranker:
                answer = f"[Reranker: Enabled] {answer}"
            else:
                answer = f"[Reranker: Disabled] {answer}"
            
            # Prepare context data
            context_data = []
            for doc, score in unique_docs:
                context_data.append([f"{score:.4f}", doc.content])
            
            return answer, context_data, None
            
        except Exception as e:
            error_msg = f"Error processing question: {str(e)}"
            print(error_msg)
            return error_msg, [], None
    
    def launch(self, share: bool = False):
        """Launch UI interface"""
        self.interface.launch(share=share)
    
    def _chunk_documents(self, documents: List[DocumentWithMetadata], chunk_size: int = 512, overlap: int = 50) -> List[DocumentWithMetadata]:
        """
        å°†æ–‡æ¡£åˆ†å‰²æˆæ›´å°çš„chunks
        
        Args:
            documents: åŸå§‹æ–‡æ¡£åˆ—è¡¨
            chunk_size: chunkå¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
            overlap: é‡å å­—ç¬¦æ•°
            
        Returns:
            åˆ†å—åçš„æ–‡æ¡£åˆ—è¡¨
        """
        chunked_docs = []
        
        for doc in documents:
            content = doc.content
            if len(content) <= chunk_size:
                # æ–‡æ¡£è¾ƒçŸ­ï¼Œä¸éœ€è¦åˆ†å—
                chunked_docs.append(doc)
            else:
                # æ–‡æ¡£è¾ƒé•¿ï¼Œéœ€è¦åˆ†å—
                start = 0
                chunk_id = 0
                
                while start < len(content):
                    end = start + chunk_size
                    
                    # ç¡®ä¿ä¸åœ¨å•è¯ä¸­é—´æˆªæ–­
                    if end < len(content):
                        # å°è¯•åœ¨å¥å·ã€é€—å·æˆ–ç©ºæ ¼å¤„æˆªæ–­
                        for i in range(end, max(start + chunk_size - 100, start), -1):
                            if content[i] in '.ã€‚ï¼Œ, ':
                                end = i + 1
                                break
                    
                    chunk_content = content[start:end].strip()
                    
                    if chunk_content:  # ç¡®ä¿chunkä¸ä¸ºç©º
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å…ƒæ•°æ®
                        chunk_metadata = DocumentMetadata(
                            source=f"{doc.metadata.source}_chunk_{chunk_id}",
                            created_at=doc.metadata.created_at,
                            author=doc.metadata.author,
                            language=doc.metadata.language
                        )
                        
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å¯¹è±¡
                        chunk_doc = DocumentWithMetadata(
                            content=chunk_content,
                            metadata=chunk_metadata
                        )
                        
                        chunked_docs.append(chunk_doc)
                        chunk_id += 1
                    
                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªchunkï¼Œè€ƒè™‘é‡å 
                    start = end - overlap
                    if start >= len(content):
                        break
        
        return chunked_docs 
    
    def _chunk_documents_advanced(self, documents: List[DocumentWithMetadata]) -> List[DocumentWithMetadata]:
        """
        ä½¿ç”¨finetune_chinese_encoder.pyä¸­çš„é«˜çº§chunké€»è¾‘å¤„ç†ä¸­æ–‡æ–‡æ¡£
        å¹¶é›†æˆfinetune_encoder.pyä¸­çš„è¡¨æ ¼æ–‡æœ¬åŒ–å¤„ç†
        """
        import re
        import json
        import ast
        
        def extract_unit_from_paragraph(paragraphs):
            """ä»æ®µè½ä¸­æå–æ•°å€¼å•ä½"""
            for para in paragraphs:
                text = para.get("text", "") if isinstance(para, dict) else para
                match = re.search(r'dollars in (millions|billions)|in (millions|billions)', text, re.IGNORECASE)
                if match:
                    unit = match.group(1) or match.group(2)
                    if unit:
                        return unit.lower().replace('s', '') + " USD"
            return ""

        def table_to_natural_text(table_dict, caption="", unit_info=""):
            """å°†è¡¨æ ¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æè¿°"""
            rows = table_dict.get("table", [])
            lines = []

            if caption:
                lines.append(f"Table Topic: {caption}.")

            if not rows:
                return ""

            headers = rows[0]
            data_rows = rows[1:]

            for i, row in enumerate(data_rows):
                if not row or all(str(v).strip() == "" for v in row):
                    continue

                if len(row) > 1 and str(row[0]).strip() != "" and all(str(v).strip() == "" for v in row[1:]):
                    lines.append(f"Table Category: {str(row[0]).strip()}.")
                    continue

                row_name = str(row[0]).strip().replace('.', '')

                data_descriptions = []
                for h_idx, v in enumerate(row):
                    if h_idx == 0:
                        continue
                    
                    header = headers[h_idx] if h_idx < len(headers) else f"Column {h_idx+1}"
                    value = str(v).strip()

                    if value:
                        if re.match(r'^-?\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?$|^\(\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?\)$', value): 
                            formatted_value = value.replace('$', '')
                            if unit_info:
                                if formatted_value.startswith('(') and formatted_value.endswith(')'):
                                     formatted_value = f"(${formatted_value[1:-1]} {unit_info})"
                                else:
                                     formatted_value = f"${formatted_value} {unit_info}"
                            else:
                                formatted_value = f"${formatted_value}"
                        else:
                            formatted_value = value
                        
                        data_descriptions.append(f"{header} is {formatted_value}")

                if row_name and data_descriptions:
                    lines.append(f"Details for item {row_name}: {'; '.join(data_descriptions)}.")
                elif data_descriptions:
                    lines.append(f"Other data item: {'; '.join(data_descriptions)}.")
                elif row_name:
                    lines.append(f"Data item: {row_name}.")

            return "\n".join(lines)
        
        def convert_json_context_to_natural_language_chunks(json_str_context, company_name="å…¬å¸"):
            chunks = []
            if not json_str_context or not json_str_context.strip():
                return chunks
            processed_str_context = json_str_context.replace("\\n", "\n")
            cleaned_initial = re.sub(re.escape("ã€é—®é¢˜ã€‘:"), "", processed_str_context)
            cleaned_initial = re.sub(re.escape("ã€ç­”æ¡ˆã€‘:"), "", cleaned_initial).strip()
            cleaned_initial = cleaned_initial.replace('ï¼Œ', ',')
            cleaned_initial = cleaned_initial.replace('ï¼š', ':')
            cleaned_initial = cleaned_initial.replace('ã€', '') 
            cleaned_initial = cleaned_initial.replace('ã€‘', '') 
            cleaned_initial = cleaned_initial.replace('\u3000', ' ')
            cleaned_initial = cleaned_initial.replace('\xa0', ' ').strip()
            cleaned_initial = re.sub(r'\s+', ' ', cleaned_initial).strip()
            
            # å¤„ç†ç ”æŠ¥æ ¼å¼
            report_match = re.match(
                r"è¿™æ˜¯ä»¥(.+?)ä¸ºé¢˜ç›®,åœ¨(\d{4}-\d{2}-\d{2}(?: \d{2}:\d{2}:\d{2})?)æ—¥æœŸå‘å¸ƒçš„ç ”ç©¶æŠ¥å‘Šã€‚ç ”æŠ¥å†…å®¹å¦‚ä¸‹: (.+)", 
                cleaned_initial, 
                re.DOTALL
            )
            if report_match:
                report_title_full = report_match.group(1).strip()
                report_date = report_match.group(2).strip()
                report_raw_content = report_match.group(3).strip() 
                content_after_second_title_match = re.match(r"ç ”æŠ¥é¢˜ç›®æ˜¯:(.+)", report_raw_content, re.DOTALL)
                if content_after_second_title_match:
                    report_content_preview = content_after_second_title_match.group(1).strip()
                else:
                    report_content_preview = report_raw_content 
                report_content_preview = re.sub(re.escape("ã€é—®é¢˜ã€‘:"), "", report_content_preview)
                report_content_preview = re.sub(re.escape("ã€ç­”æ¡ˆã€‘:"), "", report_content_preview).strip()
                report_content_preview = re.sub(r'\s+', ' ', report_content_preview).strip() 
                company_stock_match = re.search(r"(.+?)ï¼ˆ(\d{6}\.\w{2})ï¼‰", report_title_full)
                company_info = ""
                if company_stock_match:
                    report_company_name = company_stock_match.group(1).strip()
                    report_stock_code = company_stock_match.group(2).strip()
                    company_info = f"ï¼Œå…¬å¸åç§°ï¼š{report_company_name}ï¼Œè‚¡ç¥¨ä»£ç ï¼š{report_stock_code}"
                    report_title_main = re.sub(r"ï¼ˆ\d{6}\.\w{2}ï¼‰", "", report_title_full).strip()
                else:
                    report_title_main = report_title_full
                chunk_text = f"ä¸€ä»½å‘å¸ƒæ—¥æœŸä¸º {report_date} çš„ç ”ç©¶æŠ¥å‘Šï¼Œå…¶æ ‡é¢˜æ˜¯ï¼š\"{report_title_main}\"{company_info}ã€‚æŠ¥å‘Šæ‘˜è¦å†…å®¹ï¼š{report_content_preview.rstrip('...') if report_content_preview.endswith('...') else report_content_preview}ã€‚"
                chunks.append(chunk_text)
                return chunks 

            # å¤„ç†å­—å…¸æ ¼å¼
            extracted_dict_str = None
            parsed_data = None 
            temp_dict_search_str = re.sub(r"Timestamp\(['\"](.*?)['\"]\)", r"'\1'", cleaned_initial) 
            all_dict_matches = re.findall(r"(\{.*?\})", temp_dict_search_str, re.DOTALL) 
            for potential_dict_str in all_dict_matches:
                cleaned_potential_dict_str = potential_dict_str.strip()
                json_compatible_str_temp = cleaned_potential_dict_str.replace("'", '"')
                try:
                    parsed_data_temp = json.loads(json_compatible_str_temp)
                    if isinstance(parsed_data_temp, dict):
                        extracted_dict_str = cleaned_potential_dict_str
                        parsed_data = parsed_data_temp
                        break 
                except json.JSONDecodeError:
                    pass 
                fixed_for_ast_eval_temp = re.sub(
                    r"(?<!['\"\w.])\b(0[1-9]\d*)\b(?![\d.]|['\"\w.])", 
                    r"'\1'", 
                    cleaned_potential_dict_str
                )
                try:
                    parsed_data_temp = ast.literal_eval(fixed_for_ast_eval_temp)
                    if isinstance(parsed_data_temp, dict):
                        extracted_dict_str = cleaned_potential_dict_str
                        parsed_data = parsed_data_temp
                        break 
                except (ValueError, SyntaxError):
                    pass 

            if extracted_dict_str is not None and isinstance(parsed_data, dict):
                for metric_name, time_series_data in parsed_data.items():
                    if not isinstance(metric_name, str):
                        metric_name = str(metric_name)
                    cleaned_metric_name = re.sub(r'ï¼ˆ.*?ï¼‰', '', metric_name).strip()
                    if not isinstance(time_series_data, dict):
                        if time_series_data is not None and str(time_series_data).strip():
                            chunks.append(f"{company_name}çš„{cleaned_metric_name}æ•°æ®ä¸ºï¼š{time_series_data}ã€‚")
                        continue
                    if not time_series_data:
                        continue
                    try:
                        sorted_dates = sorted(time_series_data.keys(), key=str)
                    except TypeError:
                        sorted_dates = [str(k) for k in time_series_data.keys()]
                    description_parts = []
                    for date in sorted_dates:
                        value = time_series_data[date]
                        if isinstance(value, (int, float)):
                            formatted_value = f"{value:.4f}".rstrip('0').rstrip('.') if isinstance(value, float) else str(value)
                        else:
                            formatted_value = str(value)
                        description_parts.append(f"åœ¨{date}ä¸º{formatted_value}")
                    if description_parts:
                        if len(description_parts) <= 3:
                            full_description = f"{company_name}çš„{cleaned_metric_name}æ•°æ®: " + "ï¼Œ".join(description_parts) + "ã€‚"
                        else:
                            first_part = "ï¼Œ".join(description_parts[:3])
                            last_part = "ï¼Œ".join(description_parts[-3:])
                            if len(sorted_dates) > 6:
                                full_description = f"{company_name}çš„{cleaned_metric_name}æ•°æ®ä»{sorted_dates[0]}åˆ°{sorted_dates[-1]}ï¼Œä¸»è¦å˜åŒ–ä¸ºï¼š{first_part}ï¼Œ...ï¼Œ{last_part}ã€‚"
                            else:
                                full_description = f"{company_name}çš„{cleaned_metric_name}æ•°æ®: " + "ï¼Œ".join(description_parts) + "ã€‚"
                        chunks.append(full_description)
                return chunks 

            # å¤„ç†çº¯æ–‡æœ¬
            pure_text = cleaned_initial
            pure_text = re.sub(r"^\d{4}-\d{2}-\d{2}( \d{2}:\d{2}:\d{2})?[_;]?", "", pure_text, 1).strip()
            pure_text = re.sub(r"^[\u4e00-\u9fa5]+(?:/[\u4e00-\u9fa5]+)?\d{4}å¹´\d{2}æœˆ\d{2}æ—¥\d{2}:\d{2}:\d{2}(?:æ®[\u4e00-\u9fa5]+?,)?\d{1,2}æœˆ\d{1,2}æ—¥,?", "", pure_text).strip()
            pure_text = re.sub(r"^(?:å¸‚åœºèµ„é‡‘è¿›å‡º)?æˆªè‡³å‘¨[ä¸€äºŒä¸‰å››äº”å…­æ—¥]æ”¶ç›˜,?", "", pure_text).strip()
            pure_text = re.sub(r"^[\u4e00-\u9fa5]+?ä¸­æœŸå‡€åˆ©é¢„å‡\d+%-?\d*%(?:[\u4e00-\u9fa5]+?\d{1,2}æœˆ\d{1,2}æ—¥æ™šé—´å…¬å‘Š,)?", "", pure_text).strip()

            if pure_text: 
                chunks.append(pure_text)
            else:
                chunks.append(f"åŸå§‹æ ¼å¼ï¼Œè§£æå¤±è´¥æˆ–æ— æœ‰æ•ˆç»“æ„ï¼š{json_str_context.strip()[:100]}...")
            return chunks
        
        chunked_docs = []
        for doc in documents:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼æ•°æ®
            content = doc.content
            
            # å°è¯•è§£æä¸ºJSONï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼ç»“æ„
            try:
                parsed_content = json.loads(content)
                if isinstance(parsed_content, dict) and 'tables' in parsed_content:
                    # å¤„ç†åŒ…å«è¡¨æ ¼çš„æ–‡æ¡£
                    paragraphs = parsed_content.get('paragraphs', [])
                    tables = parsed_content.get('tables', [])
                    
                    # æå–å•ä½ä¿¡æ¯
                    unit_info = extract_unit_from_paragraph(paragraphs)
                    
                    # å¤„ç†æ®µè½
                    for p_idx, para in enumerate(paragraphs):
                        para_text = para.get("text", "") if isinstance(para, dict) else para
                        if para_text.strip():
                            chunk_metadata = DocumentMetadata(
                                source=f"{doc.metadata.source}_table_para_{p_idx}",
                                created_at=doc.metadata.created_at,
                                author=doc.metadata.author,
                                language=doc.metadata.language
                            )
                            chunk_doc = DocumentWithMetadata(
                                content=para_text.strip(),
                                metadata=chunk_metadata
                            )
                            chunked_docs.append(chunk_doc)
                    
                    # å¤„ç†è¡¨æ ¼
                    for t_idx, table in enumerate(tables):
                        table_text = table_to_natural_text(table, table.get("caption", ""), unit_info)
                        if table_text.strip():
                            chunk_metadata = DocumentMetadata(
                                source=f"{doc.metadata.source}_table_text_{t_idx}",
                                created_at=doc.metadata.created_at,
                                author=doc.metadata.author,
                                language=doc.metadata.language
                            )
                            chunk_doc = DocumentWithMetadata(
                                content=table_text.strip(),
                                metadata=chunk_metadata
                            )
                            chunked_docs.append(chunk_doc)
                    
                    continue  # å·²å¤„ç†è¡¨æ ¼æ•°æ®ï¼Œè·³è¿‡åç»­å¤„ç†
                    
            except (json.JSONDecodeError, TypeError):
                pass  # ä¸æ˜¯JSONæ ¼å¼ï¼Œç»§ç»­ä½¿ç”¨åŸæœ‰çš„chunké€»è¾‘
            
            # ä½¿ç”¨åŸæœ‰çš„é«˜çº§chunké€»è¾‘å¤„ç†
            chunks = convert_json_context_to_natural_language_chunks(content)
            
            for i, chunk_content in enumerate(chunks):
                if chunk_content.strip():
                    chunk_metadata = DocumentMetadata(
                        source=f"{doc.metadata.source}_advanced_chunk_{i}",
                        created_at=doc.metadata.created_at,
                        author=doc.metadata.author,
                        language=doc.metadata.language
                    )
                    
                    chunk_doc = DocumentWithMetadata(
                        content=chunk_content,
                        metadata=chunk_metadata
                    )
                    
                    chunked_docs.append(chunk_doc)
        
        return chunked_docs
    
    def _chunk_documents_simple(self, documents: List[DocumentWithMetadata], chunk_size: int = 512, overlap: int = 50) -> List[DocumentWithMetadata]:
        """
        ç®€å•çš„æ–‡æ¡£åˆ†å—æ–¹æ³•ï¼Œç”¨äºè‹±æ–‡æ–‡æ¡£
        """
        chunked_docs = []
        
        for doc in documents:
            content = doc.content
            if len(content) <= chunk_size:
                # æ–‡æ¡£è¾ƒçŸ­ï¼Œä¸éœ€è¦åˆ†å—
                chunked_docs.append(doc)
            else:
                # æ–‡æ¡£è¾ƒé•¿ï¼Œéœ€è¦åˆ†å—
                start = 0
                chunk_id = 0
                
                while start < len(content):
                    end = start + chunk_size
                    
                    # ç¡®ä¿ä¸åœ¨å•è¯ä¸­é—´æˆªæ–­
                    if end < len(content):
                        # å°è¯•åœ¨å¥å·ã€é€—å·æˆ–ç©ºæ ¼å¤„æˆªæ–­
                        for i in range(end, max(start + chunk_size - 100, start), -1):
                            if content[i] in '.ã€‚ï¼Œ, ':
                                end = i + 1
                                break
                    
                    chunk_content = content[start:end].strip()
                    
                    if chunk_content:  # ç¡®ä¿chunkä¸ä¸ºç©º
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å…ƒæ•°æ®
                        chunk_metadata = DocumentMetadata(
                            source=f"{doc.metadata.source}_simple_chunk_{chunk_id}",
                            created_at=doc.metadata.created_at,
                            author=doc.metadata.author,
                            language=doc.metadata.language
                        )
                        
                        # åˆ›å»ºæ–°çš„æ–‡æ¡£å¯¹è±¡
                        chunk_doc = DocumentWithMetadata(
                            content=chunk_content,
                            metadata=chunk_metadata
                        )
                        
                        chunked_docs.append(chunk_doc)
                        chunk_id += 1
                    
                    # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªchunkï¼Œè€ƒè™‘é‡å 
                    start = end - overlap
                    if start >= len(content):
                        break
        
        return chunked_docs 