#!/usr/bin/env python3
"""
Optimized RAG UI with Multi-Stage Retrieval System Integration
"""

import os
import sys
import re
import hashlib
from pathlib import Path
from typing import List, Optional, Tuple
import gradio as gr
import numpy as np
import torch
import faiss
from langdetect import detect, LangDetectException

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata, RagOutput
from xlm.components.rag_system.rag_system import RagSystem
from xlm.components.generator.generator import Generator
from xlm.components.retriever.retriever import Retriever
from xlm.components.retriever.reranker import QwenReranker
from xlm.utils.visualizer import Visualizer
from xlm.registry.retriever import load_enhanced_retriever
from xlm.registry.generator import load_generator
from config.parameters import Config, EncoderConfig, RetrieverConfig, ModalityConfig, EMBEDDING_CACHE_DIR, RERANKER_CACHE_DIR
from xlm.components.prompt_templates.template_loader import template_loader
from xlm.utils.stock_info_extractor import extract_stock_info, extract_report_date

# å°è¯•å¯¼å…¥å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
try:
    from alphafin_data_process.multi_stage_retrieval_final import MultiStageRetrievalSystem
    MULTI_STAGE_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢")
    MULTI_STAGE_AVAILABLE = False

def try_load_qwen_reranker(model_name, cache_dir=None):
    """å°è¯•åŠ è½½Qwené‡æ’åºå™¨ï¼Œæ”¯æŒGPU 0å’ŒCPUå›é€€"""
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        
        # ç¡®ä¿cache_diræ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²
        if cache_dir is None:
            cache_dir = RERANKER_CACHE_DIR
        
        print(f"å°è¯•ä½¿ç”¨8bité‡åŒ–åŠ è½½QwenReranker...")
        print(f"åŠ è½½é‡æ’åºå™¨æ¨¡å‹: {model_name}")
        
        # é¦–å…ˆå°è¯•GPU 0
        if torch.cuda.is_available() and torch.cuda.device_count() > 0:
            device = "cuda:0"  # æ˜ç¡®æŒ‡å®šGPU 0
            print(f"- è®¾å¤‡: {device}")
            print(f"- ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"- é‡åŒ–: True (8bit)")
            print(f"- Flash Attention: False")
            
            try:
                # æ£€æŸ¥GPU 0çš„å¯ç”¨å†…å­˜
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated(0)
                free_memory = gpu_memory - allocated_memory
                
                print(f"- GPU 0 æ€»å†…å­˜: {gpu_memory / 1024**3:.1f}GB")
                print(f"- GPU 0 å·²ç”¨å†…å­˜: {allocated_memory / 1024**3:.1f}GB")
                print(f"- GPU 0 å¯ç”¨å†…å­˜: {free_memory / 1024**3:.1f}GB")
                
                # å¦‚æœå¯ç”¨å†…å­˜å°‘äº2GBï¼Œå›é€€åˆ°CPU
                if free_memory < 2 * 1024**3:  # 2GB
                    print("- GPU 0 å†…å­˜ä¸è¶³ï¼Œå›é€€åˆ°CPU")
                    device = "cpu"
                else:
                    # å°è¯•åœ¨GPU 0ä¸ŠåŠ è½½
                    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
                    model = AutoModelForSequenceClassification.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        load_in_8bit=True
                    )
                    print("é‡åŒ–æ¨¡å‹å·²è‡ªåŠ¨è®¾ç½®åˆ°è®¾å¤‡ï¼Œè·³è¿‡æ‰‹åŠ¨ç§»åŠ¨")
                    print("é‡æ’åºå™¨æ¨¡å‹åŠ è½½å®Œæˆ")
                    print("é‡åŒ–åŠ è½½æˆåŠŸï¼")
                    return QwenReranker(model_name, device=device, cache_dir=cache_dir)
                    
            except Exception as e:
                print(f"- GPU 0 åŠ è½½å¤±è´¥: {e}")
                print("- å›é€€åˆ°CPU")
                device = "cpu"
        
        # CPUå›é€€
        if device == "cpu" or not torch.cuda.is_available():
            device = "cpu"
            print(f"- è®¾å¤‡: {device}")
            print(f"- ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"- é‡åŒ–: False (CPUæ¨¡å¼)")
            
            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
            model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                torch_dtype=torch.float32
            )
            model = model.to(device)
            print("é‡æ’åºå™¨æ¨¡å‹åŠ è½½å®Œæˆ")
            print("CPUåŠ è½½æˆåŠŸï¼")
            return QwenReranker(model_name, device=device, cache_dir=cache_dir)
            
    except Exception as e:
        print(f"åŠ è½½é‡æ’åºå™¨å¤±è´¥: {e}")
        return None

class OptimizedRagUIWithMultiStage:
    def __init__(
        self,
        cache_dir: Optional[str] = None,
        use_faiss: bool = True,
        enable_reranker: bool = True,
        use_existing_embedding_index: Optional[bool] = None,
        max_alphafin_chunks: Optional[int] = None,
        window_title: str = "Financial Explainable RAG System with Multi-Stage Retrieval",
        title: str = "Financial Explainable RAG System with Multi-Stage Retrieval",
        examples: Optional[List[List[str]]] = None,
    ):
        # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        self.config = Config()
        self.cache_dir = EMBEDDING_CACHE_DIR if (not cache_dir or not isinstance(cache_dir, str)) else cache_dir
        self.use_faiss = use_faiss
        self.enable_reranker = enable_reranker
        self.use_existing_embedding_index = use_existing_embedding_index if use_existing_embedding_index is not None else self.config.retriever.use_existing_embedding_index
        self.max_alphafin_chunks = max_alphafin_chunks if max_alphafin_chunks is not None else self.config.retriever.max_alphafin_chunks
        self.window_title = window_title
        self.title = title
        self.examples = examples or [
            ["å¾·èµ›ç”µæ± (000049)çš„ä¸‹ä¸€å­£åº¦æ”¶ç›Šé¢„æµ‹å¦‚ä½•ï¼Ÿ"],
            ["ç”¨å‹ç½‘ç»œ2019å¹´çš„æ¯è‚¡ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢æ˜¯å¤šå°‘ï¼Ÿ"],
            ["ä¸‹æœˆè‚¡ä»·èƒ½å¦ä¸Šæ¶¨?"],
            ["How was internally developed software capitalised?"],
            ["Why did the Operating revenues decreased from 2018 to 2019?"],
            ["Why did the Operating costs decreased from 2018 to 2019?"]
        ]
        
        # Set environment variables for model caching
        os.environ['TRANSFORMERS_CACHE'] = os.path.join(self.cache_dir, 'transformers')
        os.environ['HF_HOME'] = self.cache_dir
        os.environ['HF_DATASETS_CACHE'] = os.path.join(self.cache_dir, 'datasets')
        
        # Initialize system components
        self._init_components()
        
        # Create Gradio interface
        self.interface = self._create_interface()
    
    def _init_components(self):
        """Initialize RAG system components with multi-stage retrieval"""
        print("\nStep 1. Initializing Multi-Stage Retrieval System...")
        
        # åˆå§‹åŒ–ä¼ ç»ŸRAGç³»ç»Ÿä½œä¸ºå›é€€
        print("Step 2. Initializing Traditional RAG System as fallback...")
        try:
            # åŠ è½½æ£€ç´¢å™¨
            self.retriever = load_enhanced_retriever(
                config=self.config
            )
            
            # åŠ è½½ç”Ÿæˆå™¨
            self.generator = load_generator(
                generator_model_name=self.config.generator.model_name,
                use_local_llm=True,
                use_gpu=True,
                gpu_device="cuda:1",
                cache_dir=self.config.generator.cache_dir
            )
            
            # åˆå§‹åŒ–RAGç³»ç»Ÿ
            self.rag_system = RagSystem(
                retriever=self.retriever,
                generator=self.generator,
                retriever_top_k=self.config.retriever.retrieval_top_k  # ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
            )
            print("âœ… ä¼ ç»ŸRAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ ä¼ ç»ŸRAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.rag_system = None
        
        # åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
        if MULTI_STAGE_AVAILABLE:
            try:
                # ä¸­æ–‡æ•°æ®è·¯å¾„
                chinese_data_path = Path("data/alphafin/alphafin_merged_generated_qa.json")
                
                if chinese_data_path.exists():
                    print("âœ… åˆå§‹åŒ–ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
                    self.chinese_retrieval_system = MultiStageRetrievalSystem(
                        data_path=chinese_data_path,
                        dataset_type="chinese",
                        use_existing_config=True
                    )
                    print("âœ… ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"âŒ ä¸­æ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {chinese_data_path}")
                    self.chinese_retrieval_system = None
                
                # è‹±æ–‡æ•°æ®è·¯å¾„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                english_data_path = Path("data/tatqa/processed_data.json")  # éœ€è¦é¢„å¤„ç†
                if english_data_path.exists():
                    print("âœ… åˆå§‹åŒ–è‹±æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
                    self.english_retrieval_system = MultiStageRetrievalSystem(
                        data_path=english_data_path,
                        dataset_type="english",
                        use_existing_config=True
                    )
                    print("âœ… è‹±æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"âš ï¸ è‹±æ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {english_data_path}")
                    self.english_retrieval_system = None
                
            except Exception as e:
                print(f"âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.chinese_retrieval_system = None
                self.english_retrieval_system = None
        else:
            print("âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ£€ç´¢")
            self.chinese_retrieval_system = None
            self.english_retrieval_system = None
        
        print("\nStep 3. Loading visualizer...")
        self.visualizer = Visualizer(show_mid_features=True)
        
        print("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def _create_interface(self) -> gr.Blocks:
        """Create optimized Gradio interface"""
        with gr.Blocks(
            title=self.window_title
        ) as interface:
            # æ ‡é¢˜
            gr.Markdown(f"# {self.title}")
            
            # è¾“å…¥åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=4):
                    datasource = gr.Radio(
                        choices=["TatQA", "AlphaFin", "Both"],
                        value="Both",
                        label="Data Source"
                    )
                    
            with gr.Row():
                with gr.Column(scale=4):
                    question_input = gr.Textbox(
                        show_label=False,
                        placeholder="Enter your question",
                        label="Question",
                        lines=3
                    )
            
            # æ§åˆ¶æŒ‰é’®åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=1):
                    reranker_checkbox = gr.Checkbox(
                        label="Enable Reranker",
                        value=True,
                        interactive=True
                    )
                with gr.Column(scale=1):
                    submit_btn = gr.Button("Submit")
            
            # ä½¿ç”¨æ ‡ç­¾é¡µåˆ†ç¦»æ˜¾ç¤º
            with gr.Tabs():
                # å›ç­”æ ‡ç­¾é¡µ
                with gr.TabItem("Answer"):
                    answer_output = gr.Textbox(
                        show_label=False,
                        interactive=False,
                        label="Generated Response",
                        lines=5
                    )
                
                # è§£é‡Šæ ‡ç­¾é¡µ
                with gr.TabItem("Explanation"):
                    context_output = gr.Dataframe(
                        headers=["Score", "Context"],
                        datatype=["number", "str"],
                        label="Retrieved Contexts",
                        interactive=False
                    )

            # æ·»åŠ ç¤ºä¾‹é—®é¢˜
            gr.Examples(
                examples=self.examples,
                inputs=[question_input],
                label="Example Questions"
            )

            # ç»‘å®šäº‹ä»¶
            submit_btn.click(
                self._process_question,
                inputs=[question_input, datasource, reranker_checkbox],
                outputs=[answer_output, context_output]
            )
            
            return interface
    
    def _process_question(
        self,
        question: str,
        datasource: str,
        reranker_checkbox: bool
    ) -> tuple[str, List[List[str]]]:
        if not question.strip():
            return "è¯·è¾“å…¥é—®é¢˜", []
        
        # æ£€æµ‹è¯­è¨€
        try:
            lang = detect(question)
            language = 'zh' if lang.startswith('zh') else 'en'
        except:
            language = 'en'
        
        # æ ¹æ®è¯­è¨€é€‰æ‹©æ£€ç´¢ç³»ç»Ÿ
        if language == 'zh' and self.chinese_retrieval_system:
            return self._process_chinese_with_multi_stage(question, reranker_checkbox)
        elif language == 'en' and self.english_retrieval_system:
            return self._process_english_with_multi_stage(question, reranker_checkbox)
        else:
            return self._fallback_retrieval(question, language)
    
    def _process_chinese_with_multi_stage(self, question: str, reranker_checkbox: bool) -> tuple[str, List[List[str]]]:
        """ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå¤„ç†ä¸­æ–‡æŸ¥è¯¢"""
        if not self.chinese_retrieval_system:
            return self._fallback_retrieval(question, 'zh')
        
        try:
            print(f"ğŸ” å¼€å§‹ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢...")
            print(f"ğŸ“‹ æŸ¥è¯¢: {question}")
            company_name, stock_code = extract_stock_info(question)
            report_date = extract_report_date(question)
            print(f"ğŸ¢ å…¬å¸åç§°: {company_name}")
            print(f"ğŸ“ˆ è‚¡ç¥¨ä»£ç : {stock_code}")
            print(f"ğŸ“… æŠ¥å‘Šæ—¥æœŸ: {report_date}")
            print(f"âš™ï¸ é…ç½®å‚æ•°: retrieval_top_k={self.config.retriever.retrieval_top_k}, rerank_top_k={self.config.retriever.rerank_top_k}")
            
            results = self.chinese_retrieval_system.search(
                query=question,
                company_name=company_name,
                stock_code=stock_code,
                report_date=report_date,
                top_k=self.config.retriever.rerank_top_k  # ä½¿ç”¨é…ç½®ä¸­çš„é‡æ’åºtop-k
            )
            
            # è½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼
            retrieved_documents = []
            retriever_scores = []
            
            # æ£€æŸ¥resultsçš„æ ¼å¼
            print(f"ğŸ“Š æ£€ç´¢ç»“æœç±»å‹: {type(results)}")
            if isinstance(results, dict) and 'retrieved_documents' in results:
                documents = results['retrieved_documents']
                llm_answer = results.get('llm_answer', '')
                print(f"ğŸ“„ æ£€ç´¢åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
                print(f"ğŸ¤– LLMç­”æ¡ˆ: {'å·²ç”Ÿæˆ' if llm_answer else 'æœªç”Ÿæˆ'}")
                for result in documents:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
                
                # å¦‚æœå¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå·²ç»ç”Ÿæˆäº†ç­”æ¡ˆï¼Œç›´æ¥ä½¿ç”¨
                if llm_answer:
                    context_data = []
                    for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                        context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                    answer = f"[Multi-Stage Retrieval: ZH] {llm_answer}"
                    return answer, context_data
            else:
                for result in results:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
            
            if retrieved_documents:
                context_str = "\n\n".join([doc.content for doc in retrieved_documents[:10]])
                
                # æ ¹æ®æŸ¥è¯¢è¯­è¨€åŠ¨æ€é€‰æ‹©promptæ¨¡æ¿
                try:
                    from langdetect import detect
                    query_language = detect(question)
                    is_chinese_query = query_language.startswith('zh')
                except:
                    # å¦‚æœè¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œæ ¹æ®æŸ¥è¯¢å†…å®¹åˆ¤æ–­
                    is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
                
                if is_chinese_query:
                    # ä¸­æ–‡æŸ¥è¯¢ä½¿ç”¨ä¸­æ–‡promptæ¨¡æ¿
                    summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                    prompt = template_loader.format_template(
                        "multi_stage_chinese_template",
                        summary=summary,
                        context=context_str,
                        query=question
                    )
                    if prompt is None:
                        # å›é€€åˆ°ç®€å•ä¸­æ–‡prompt
                        prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context_str}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
                else:
                    # è‹±æ–‡æŸ¥è¯¢ä½¿ç”¨è‹±æ–‡promptæ¨¡æ¿
                    prompt = template_loader.format_template(
                        "rag_english_template",
                        context=context_str,
                        question=question
                    )
                    if prompt is None:
                        # å›é€€åˆ°ç®€å•è‹±æ–‡prompt
                        prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
                
                generated_responses = self.generator.generate(texts=[prompt])
                answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                context_data = []
                for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                    context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                answer = f"[Multi-Stage Retrieval: ZH] {answer}"
                return answer, context_data
            else:
                return "No relevant documents found.", []
                
        except Exception as e:
            return self._fallback_retrieval(question, 'zh')
    
    def _process_english_with_multi_stage(self, question: str, reranker_checkbox: bool) -> tuple[str, List[List[str]]]:
        """ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå¤„ç†è‹±æ–‡æŸ¥è¯¢"""
        if not self.english_retrieval_system:
            return self._fallback_retrieval(question, 'en')
        
        try:
            print(f"ğŸ” å¼€å§‹è‹±æ–‡å¤šé˜¶æ®µæ£€ç´¢...")
            print(f"ğŸ“‹ æŸ¥è¯¢: {question}")
            print(f"âš™ï¸ é…ç½®å‚æ•°: retrieval_top_k={self.config.retriever.retrieval_top_k}, rerank_top_k={self.config.retriever.rerank_top_k}")
            
            # æ‰§è¡Œå¤šé˜¶æ®µæ£€ç´¢
            results = self.english_retrieval_system.search(
                query=question,
                top_k=self.config.retriever.rerank_top_k  # ä½¿ç”¨é…ç½®ä¸­çš„é‡æ’åºtop-k
            )
            
            # è½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼
            retrieved_documents = []
            retriever_scores = []
            
            for result in results:
                doc = DocumentWithMetadata(
                    content=result.get('context', result.get('content', '')),
                    metadata=DocumentMetadata(
                        source=result.get('source', 'Unknown'),
                        created_at="",
                        author="",
                        language="english"
                    )
                )
                retrieved_documents.append(doc)
                retriever_scores.append(result.get('combined_score', 0.0))
            
            if retrieved_documents:
                context_str = "\n\n".join([doc.content for doc in retrieved_documents[:10]])
                
                # æ ¹æ®æŸ¥è¯¢è¯­è¨€åŠ¨æ€é€‰æ‹©promptæ¨¡æ¿
                try:
                    from langdetect import detect
                    query_language = detect(question)
                    is_chinese_query = query_language.startswith('zh')
                except:
                    # å¦‚æœè¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œæ ¹æ®æŸ¥è¯¢å†…å®¹åˆ¤æ–­
                    is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
                
                if is_chinese_query:
                    # ä¸­æ–‡æŸ¥è¯¢ä½¿ç”¨ä¸­æ–‡promptæ¨¡æ¿
                    summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                    prompt = template_loader.format_template(
                        "multi_stage_chinese_template",
                        summary=summary,
                        context=context_str,
                        query=question
                    )
                    if prompt is None:
                        # å›é€€åˆ°ç®€å•ä¸­æ–‡prompt
                        prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context_str}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
                else:
                    # è‹±æ–‡æŸ¥è¯¢ä½¿ç”¨è‹±æ–‡promptæ¨¡æ¿
                    prompt = template_loader.format_template(
                        "rag_english_template",
                        context=context_str, 
                        question=question
                    )
                    if prompt is None:
                        # å›é€€åˆ°ç®€å•è‹±æ–‡prompt
                        prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
                
                generated_responses = self.generator.generate(texts=[prompt])
                answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                context_data = []
                for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                    context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                answer = f"[Multi-Stage Retrieval: EN] {answer}"
                return answer, context_data
            else:
                return "No relevant documents found.", []
                
        except Exception as e:
            return self._fallback_retrieval(question, 'en')
    
    def _fallback_retrieval(self, question: str, language: str) -> tuple[str, List[List[str]]]:
        """å›é€€åˆ°ä¼ ç»Ÿæ£€ç´¢"""
        if self.rag_system is None:
            return "ä¼ ç»ŸRAGç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ— æ³•å¤„ç†æŸ¥è¯¢", []
        
        try:
            # è¿è¡ŒRAGç³»ç»Ÿ
            rag_output = self.rag_system.run(user_input=question, language=language)
            
            # ç”Ÿæˆç­”æ¡ˆ
            if rag_output.retrieved_documents:
                # æ„å»ºä¸Šä¸‹æ–‡
                context_str = "\n\n".join([doc.content for doc in rag_output.retrieved_documents[:10]])
                
                # æ ¹æ®æŸ¥è¯¢è¯­è¨€åŠ¨æ€é€‰æ‹©promptæ¨¡æ¿
                try:
                    from langdetect import detect
                    query_language = detect(question)
                    is_chinese_query = query_language.startswith('zh')
                except:
                    # å¦‚æœè¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œæ ¹æ®æŸ¥è¯¢å†…å®¹åˆ¤æ–­
                    is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
                
                if is_chinese_query:
                    # ä¸­æ–‡æŸ¥è¯¢ä½¿ç”¨ä¸­æ–‡promptæ¨¡æ¿
                    summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                    prompt = template_loader.format_template(
                        "multi_stage_chinese_template",
                        summary=summary,
                        context=context_str,
                        query=question
                    )
                    if prompt is None:
                        # å›é€€åˆ°ç®€å•ä¸­æ–‡prompt
                        prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context_str}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
                else:
                    # è‹±æ–‡æŸ¥è¯¢ä½¿ç”¨è‹±æ–‡promptæ¨¡æ¿
                    prompt = template_loader.format_template(
                        "rag_english_template",
                        context=context_str, 
                        question=question
                    )
                    if prompt is None:
                        # å›é€€åˆ°ç®€å•è‹±æ–‡prompt
                        prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
                
                # ç”Ÿæˆç­”æ¡ˆ
                generated_responses = self.generator.generate(texts=[prompt])
                answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                
                # å‡†å¤‡ä¸Šä¸‹æ–‡æ•°æ®
                context_data = []
                for doc, score in zip(rag_output.retrieved_documents[:self.config.retriever.rerank_top_k], rag_output.retriever_scores[:self.config.retriever.rerank_top_k]):
                    # ç»Ÿä¸€åªæ˜¾ç¤ºcontentå­—æ®µï¼Œä¸æ˜¾ç¤ºquestionå’Œanswer
                    content = doc.content
                    # ç¡®ä¿contentæ˜¯å­—ç¬¦ä¸²ç±»å‹
                    if not isinstance(content, str):
                        if isinstance(content, dict):
                            # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•æå–contextæˆ–contentå­—æ®µ
                            content = content.get('context', content.get('content', str(content)))
                        else:
                            content = str(content)
                    
                    # æˆªæ–­è¿‡é•¿çš„å†…å®¹
                    display_content = content[:500] + "..." if len(content) > 500 else content
                    context_data.append([f"{score:.4f}", display_content])
                
                # æ·»åŠ æ£€ç´¢ç³»ç»Ÿä¿¡æ¯
                answer = f"[Multi-Stage Retrieval: {language.upper()}] {answer}"
                
                return answer, context_data
            else:
                return "No relevant documents found.", []
                
        except Exception as e:
            return f"æ£€ç´¢å¤±è´¥: {str(e)}", []
    
    def launch(self, share: bool = False):
        """Launch UI interface"""
        self.interface.launch(share=share) 