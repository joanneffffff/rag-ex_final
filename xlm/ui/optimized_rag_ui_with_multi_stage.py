#!/usr/bin/env python3
"""
Optimized RAG UI with Multi-Stage Retrieval System Integration
"""

import os
import sys
import re
import hashlib
import json
import logging
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any
import gradio as gr
import numpy as np
import torch
import faiss
from langdetect import detect, LangDetectException

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata, RagOutput
from xlm.components.rag_system.rag_system import RagSystem
from xlm.components.generator.generator import Generator
from xlm.components.retriever.retriever import Retriever
from xlm.components.retriever.reranker import QwenReranker
from xlm.utils.visualizer import Visualizer
from xlm.registry.retriever import load_enhanced_retriever
from xlm.registry.generator import load_generator
from config.parameters import Config, EncoderConfig, RetrieverConfig, ModalityConfig, EMBEDDING_CACHE_DIR, RERANKER_CACHE_DIR, config
from xlm.components.prompt_templates.template_loader import template_loader
from xlm.utils.stock_info_extractor import extract_stock_info, extract_stock_info_with_mapping, extract_report_date

# å°è¯•å¯¼å…¥å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
try:
    from alphafin_data_process.multi_stage_retrieval_final import MultiStageRetrievalSystem
    MULTI_STAGE_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢")
    MULTI_STAGE_AVAILABLE = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def build_smart_context(summary: str, context: str, query: str) -> str:
    """
    æ™ºèƒ½æ„å»ºcontextï¼Œä½¿ç”¨ä¸chinese_llm_evaluation.pyç›¸åŒçš„é€»è¾‘
    è¿™ä¸ªå‡½æ•°è´Ÿè´£å°†åŸå§‹çš„ `context` å­—ç¬¦ä¸²è¿›è¡Œå¤„ç†ï¼Œé¿å…è¿‡åº¦æˆªæ–­
    """
    processed_context = context
    try:
        # å°è¯•å°† context è§£æä¸ºå­—å…¸ï¼Œå¦‚æœæ˜¯åˆ™æ ¼å¼åŒ–ä¸ºå¯è¯»çš„JSON
        # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ json.loads() ä»£æ›¿ eval() æ›´å®‰å…¨ï¼Œä½†éœ€è¦å…ˆæ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·
        context_data = json.loads(context.replace("'", '"')) 
        if isinstance(context_data, dict):
            processed_context = json.dumps(context_data, ensure_ascii=False, indent=2)
            logger.debug("âœ… Contextè¯†åˆ«ä¸ºå­—å…¸å­—ç¬¦ä¸²å¹¶å·²æ ¼å¼åŒ–ä¸ºJSONã€‚")
    except (json.JSONDecodeError, TypeError):
        logger.debug("âš ï¸ ContextéJSONå­—ç¬¦ä¸²æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹contextã€‚")
        pass

    # ä½¿ç”¨ä¸chinese_llm_evaluation.pyç›¸åŒçš„é•¿åº¦é™åˆ¶ï¼š3500å­—ç¬¦
    max_processed_context_length = 3500 # å­—ç¬¦é•¿åº¦ï¼Œä½œä¸ºç²—ç•¥é™åˆ¶
    if len(processed_context) > max_processed_context_length:
        logger.warning(f"âš ï¸ å¤„ç†åçš„Contexté•¿åº¦è¿‡é•¿ ({len(processed_context)}å­—ç¬¦)ï¼Œè¿›è¡Œæˆªæ–­ã€‚")
        processed_context = processed_context[:max_processed_context_length] + "..."

    return processed_context

def _load_template_content_from_file(template_file_name: str) -> str:
    """
    ä»æ–‡ä»¶åŠ è½½æ¨¡æ¿å†…å®¹
    """
    template_path = Path("data/prompt_templates") / template_file_name
    if not template_path.exists():
        logger.error(f"âŒ æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
        return ""
    
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f"âŒ è¯»å–æ¨¡æ¿æ–‡ä»¶å¤±è´¥: {e}")
        return ""

def get_messages_for_test(summary: str, context: str, query: str, 
                          template_file_name: str = "multi_stage_chinese_template_with_fewshot.txt") -> List[Dict[str, str]]:
    """
    æ„å»ºç”¨äºæµ‹è¯•çš„ messages åˆ—è¡¨ï¼Œä»æŒ‡å®šæ¨¡æ¿æ–‡ä»¶åŠ è½½å†…å®¹ï¼Œå¹¶å°† item_instruction èå…¥ Promptã€‚
    Args:
        summary (str): LLM Qwen2-7B ç”Ÿæˆçš„æ‘˜è¦ã€‚
        context (str): å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå·²åŒ…å«æ‘˜è¦ï¼‰ã€‚
        query (str): ç”¨æˆ·é—®é¢˜ã€‚
        template_file_name (str): è¦åŠ è½½çš„æ¨¡æ¿æ–‡ä»¶åã€‚
    Returns:
        List[Dict[str, str]]: æ„å»ºå¥½çš„ messages åˆ—è¡¨ã€‚
    """
    template_full_string = _load_template_content_from_file(template_file_name)

    messages = []
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†å‰²æ‰€æœ‰éƒ¨åˆ†ï¼Œå¹¶ä¿ç•™åˆ†éš”ç¬¦å†…å®¹
    parts = re.split(r'(===SYSTEM===|===USER===|===ASSISTANT===)', template_full_string, flags=re.DOTALL)

    # ç§»é™¤ç¬¬ä¸€ä¸ªç©ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœå­˜åœ¨ï¼‰å’Œå¤šä½™çš„ç©ºç™½
    parts = [p.strip() for p in parts if p.strip()]

    current_role = None
    current_content = []

    for part in parts:
        if part in ["===SYSTEM===", "===USER===", "===ASSISTANT==="]:
            if current_role is not None:
                messages.append({"role": current_role.lower().replace("===", ""), "content": "\n".join(current_content).strip()})
            current_role = part
            current_content = []
        else:
            current_content.append(part)

    # æ·»åŠ æœ€åä¸€ä¸ªéƒ¨åˆ†çš„ message
    if current_role is not None:
        messages.append({"role": current_role.lower().replace("===", ""), "content": "\n".join(current_content).strip()})

    # æ›¿æ¢å ä½ç¬¦
    for message in messages:
        if message["role"] == "user":
            modified_content = message["content"]
            modified_content = modified_content.replace('{query}', query)
            modified_content = modified_content.replace('{summary}', summary)
            modified_content = modified_content.replace('{context}', context)
            message["content"] = modified_content

    logger.debug(f"æ„å»ºçš„ messages: {messages}")
    return messages

def _convert_messages_to_chatml(messages: List[Dict[str, str]]) -> str:
    """
    å°† messages åˆ—è¡¨è½¬æ¢ä¸º Fin-R1 (Qwen2.5 based) æœŸæœ›çš„ChatMLæ ¼å¼å­—ç¬¦ä¸²ã€‚
    Qwenç³»åˆ—æ ‡å‡†åº”è¯¥æ˜¯ `im_end`
    """
    if not messages:
        return ""

    formatted_prompt = ""
    for message in messages:
        role = message.get("role", "")
        content = message.get("content", "")

        if role == "system":
            formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n"
        elif role == "user":
            formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n"
        elif role == "assistant":
            # è¿™é‡Œçš„ assistant è§’è‰²é€šå¸¸æ˜¯ Few-shot ç¤ºä¾‹çš„ä¸€éƒ¨åˆ†
            formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n"

    # åœ¨æœ€åè¿½åŠ ä¸€ä¸ª <|im_start|>assistant\nï¼Œè¡¨ç¤ºå¸Œæœ›æ¨¡å‹å¼€å§‹ç”Ÿæˆæ–°çš„ assistant å›å¤
    formatted_prompt += "<|im_start|>assistant\n"

    logger.debug(f"è½¬æ¢åçš„ ChatML Prompt (å‰500å­—ç¬¦):\n{formatted_prompt[:500]}...")
    return formatted_prompt

def try_load_qwen_reranker(model_name, cache_dir=None):
    """å°è¯•åŠ è½½Qwené‡æ’åºå™¨ï¼Œæ”¯æŒGPU 0å’ŒCPUå›é€€"""
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        
        # ç¡®ä¿cache_diræ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²
        if cache_dir is None:
            cache_dir = RERANKER_CACHE_DIR
        
        print(f"å°è¯•ä½¿ç”¨8bité‡åŒ–åŠ è½½QwenReranker...")
        print(f"åŠ è½½é‡æ’åºå™¨æ¨¡å‹: {model_name}")
        
        # é¦–å…ˆå°è¯•GPU 0
        if torch.cuda.is_available() and torch.cuda.device_count() > 0:
            device = "cuda:0"  # æ˜ç¡®æŒ‡å®šGPU 0
            print(f"- è®¾å¤‡: {device}")
            print(f"- ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"- é‡åŒ–: True (8bit)")
            print(f"- Flash Attention: False")
            
            try:
                # æ£€æŸ¥GPU 0çš„å¯ç”¨å†…å­˜
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated(0)
                free_memory = gpu_memory - allocated_memory
                
                print(f"- GPU 0 æ€»å†…å­˜: {gpu_memory / 1024**3:.1f}GB")
                print(f"- GPU 0 å·²ç”¨å†…å­˜: {allocated_memory / 1024**3:.1f}GB")
                print(f"- GPU 0 å¯ç”¨å†…å­˜: {free_memory / 1024**3:.1f}GB")
                
                # å¦‚æœå¯ç”¨å†…å­˜å°‘äº2GBï¼Œå›é€€åˆ°CPU
                if free_memory < 2 * 1024**3:  # 2GB
                    print("- GPU 0 å†…å­˜ä¸è¶³ï¼Œå›é€€åˆ°CPU")
                    device = "cpu"
                else:
                    # å°è¯•åœ¨GPU 0ä¸ŠåŠ è½½
                    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
                    model = AutoModelForSequenceClassification.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        load_in_8bit=True
                    )
                    print("é‡åŒ–æ¨¡å‹å·²è‡ªåŠ¨è®¾ç½®åˆ°è®¾å¤‡ï¼Œè·³è¿‡æ‰‹åŠ¨ç§»åŠ¨")
                    print("é‡æ’åºå™¨æ¨¡å‹åŠ è½½å®Œæˆ")
                    print("é‡åŒ–åŠ è½½æˆåŠŸï¼")
                    return QwenReranker(model_name, device=device, cache_dir=cache_dir)
                    
            except Exception as e:
                print(f"- GPU 0 åŠ è½½å¤±è´¥: {e}")
                print("- å›é€€åˆ°CPU")
                device = "cpu"
        
        # CPUå›é€€
        if device == "cpu" or not torch.cuda.is_available():
            device = "cpu"
            print(f"- è®¾å¤‡: {device}")
            print(f"- ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"- é‡åŒ–: False (CPUæ¨¡å¼)")
            
            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
            model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                torch_dtype=torch.float32
            )
            model = model.to(device)
            print("é‡æ’åºå™¨æ¨¡å‹åŠ è½½å®Œæˆ")
            print("CPUåŠ è½½æˆåŠŸï¼")
            return QwenReranker(model_name, device=device, cache_dir=cache_dir)
            
    except Exception as e:
        print(f"åŠ è½½é‡æ’åºå™¨å¤±è´¥: {e}")
        return None

class OptimizedRagUIWithMultiStage:
    def __init__(
        self,
        cache_dir: Optional[str] = None,
        use_faiss: bool = True,
        enable_reranker: bool = True,
        use_existing_embedding_index: Optional[bool] = None,
        max_alphafin_chunks: Optional[int] = None,
        window_title: str = "Financial Explainable RAG System with Multi-Stage Retrieval",
        title: str = "Financial Explainable RAG System with Multi-Stage Retrieval",
        examples: Optional[List[List[str]]] = None,
    ):
        # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        self.config = Config()
        self.cache_dir = EMBEDDING_CACHE_DIR if (not cache_dir or not isinstance(cache_dir, str)) else cache_dir
        self.use_faiss = use_faiss
        self.enable_reranker = enable_reranker
        self.use_existing_embedding_index = use_existing_embedding_index if use_existing_embedding_index is not None else self.config.retriever.use_existing_embedding_index
        self.max_alphafin_chunks = max_alphafin_chunks if max_alphafin_chunks is not None else self.config.retriever.max_alphafin_chunks
        self.window_title = window_title
        self.title = title
        self.examples = examples or [
            ["å¾·èµ›ç”µæ± (000049)çš„ä¸‹ä¸€å­£åº¦æ”¶ç›Šé¢„æµ‹å¦‚ä½•ï¼Ÿ"],
            ["ç”¨å‹ç½‘ç»œ2019å¹´çš„æ¯è‚¡ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢æ˜¯å¤šå°‘ï¼Ÿ"],
            ["ä¸‹æœˆè‚¡ä»·èƒ½å¦ä¸Šæ¶¨?"],
            ["How was internally developed software capitalised?"],
            ["Why did the Operating revenues decreased from 2018 to 2019?"],
            ["Why did the Operating costs decreased from 2018 to 2019?"]
        ]
        
        # Set environment variables for model caching
        os.environ['TRANSFORMERS_CACHE'] = os.path.join(self.cache_dir, 'transformers')
        os.environ['HF_HOME'] = self.cache_dir
        os.environ['HF_DATASETS_CACHE'] = os.path.join(self.cache_dir, 'datasets')
        
        # Initialize system components
        self._init_components()
        
        # Create Gradio interface
        self.interface = self._create_interface()
    
    def _init_components(self):
        """Initialize RAG system components with multi-stage retrieval"""
        print("\nStep 1. Initializing Multi-Stage Retrieval System...")
        
        # åˆå§‹åŒ–ä¼ ç»ŸRAGç³»ç»Ÿä½œä¸ºå›é€€
        print("Step 2. Initializing Traditional RAG System as fallback...")
        try:
            # åŠ è½½æ£€ç´¢å™¨
            self.retriever = load_enhanced_retriever(
                config=self.config
            )
            
            # åŠ è½½ç”Ÿæˆå™¨
            self.generator = load_generator(
                generator_model_name=self.config.generator.model_name,
                use_local_llm=True,
                use_gpu=True,
                gpu_device="cuda:1",
                cache_dir=self.config.generator.cache_dir
            )
            
            # åˆå§‹åŒ–RAGç³»ç»Ÿ
            self.rag_system = RagSystem(
                retriever=self.retriever,
                generator=self.generator,
                retriever_top_k=self.config.retriever.retrieval_top_k  # ä½¿ç”¨é…ç½®ä¸­çš„è®¾ç½®
            )
            print("âœ… ä¼ ç»ŸRAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ ä¼ ç»ŸRAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.rag_system = None
        
        # åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
        if MULTI_STAGE_AVAILABLE:
            try:
                # ä¸­æ–‡æ•°æ®è·¯å¾„
                chinese_data_path = Path(config.data.chinese_data_path)
                
                if chinese_data_path.exists():
                    print("âœ… åˆå§‹åŒ–ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
                    self.chinese_retrieval_system = MultiStageRetrievalSystem(
                        data_path=chinese_data_path,
                        dataset_type="chinese",
                        use_existing_config=True
                    )
                    print("âœ… ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"âŒ ä¸­æ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {chinese_data_path}")
                    self.chinese_retrieval_system = None
                
                # è‹±æ–‡æ•°æ®è·¯å¾„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                english_data_path = Path("data/tatqa/processed_data.json")  # éœ€è¦é¢„å¤„ç†
                if english_data_path.exists():
                    print("âœ… åˆå§‹åŒ–è‹±æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
                    self.english_retrieval_system = MultiStageRetrievalSystem(
                        data_path=english_data_path,
                        dataset_type="english",
                        use_existing_config=True
                    )
                    print("âœ… è‹±æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"âš ï¸ è‹±æ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {english_data_path}")
                    self.english_retrieval_system = None
                
            except Exception as e:
                print(f"âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.chinese_retrieval_system = None
                self.english_retrieval_system = None
        else:
            print("âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ£€ç´¢")
            self.chinese_retrieval_system = None
            self.english_retrieval_system = None
        
        print("\nStep 3. Loading visualizer...")
        self.visualizer = Visualizer(show_mid_features=True)
        
        print("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def _create_interface(self) -> gr.Blocks:
        """Create optimized Gradio interface"""
        with gr.Blocks(
            title=self.window_title
        ) as interface:
            # æ ‡é¢˜
            gr.Markdown(f"# {self.title}")
            
            # è¾“å…¥åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=4):
                    datasource = gr.Radio(
                        choices=["TatQA", "AlphaFin", "Both"],
                        value="Both",
                        label="Data Source"
                    )
                    
            with gr.Row():
                with gr.Column(scale=4):
                    question_input = gr.Textbox(
                        show_label=False,
                        placeholder="Enter your question",
                        label="Question",
                        lines=3
                    )
            
            # æ§åˆ¶æŒ‰é’®åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=1):
                    reranker_checkbox = gr.Checkbox(
                        label="Enable Reranker",
                        value=True,
                        interactive=True
                    )
                with gr.Column(scale=1):
                    stock_prediction_checkbox = gr.Checkbox(
                        label="stock prediction (only for chinese query)",
                        value=False,
                        interactive=True
                    )
                with gr.Column(scale=1):
                    submit_btn = gr.Button("Submit")
            
            # ä½¿ç”¨æ ‡ç­¾é¡µåˆ†ç¦»æ˜¾ç¤º
            with gr.Tabs():
                # å›ç­”æ ‡ç­¾é¡µ
                with gr.TabItem("Answer"):
                    answer_output = gr.Textbox(
                        show_label=False,
                        interactive=False,
                        label="Generated Response",
                        lines=5
                    )
                
                # è§£é‡Šæ ‡ç­¾é¡µ
                with gr.TabItem("Explanation"):
                    context_output = gr.Dataframe(
                        headers=["Score", "Context"],
                        datatype=["number", "str"],
                        label="Retrieved Contexts",
                        interactive=False
                    )

            # æ·»åŠ ç¤ºä¾‹é—®é¢˜
            gr.Examples(
                examples=self.examples,
                inputs=[question_input],
                label="Example Questions"
            )

            # ç»‘å®šäº‹ä»¶
            submit_btn.click(
                self._process_question,
                inputs=[question_input, datasource, reranker_checkbox, stock_prediction_checkbox],
                outputs=[answer_output, context_output]
            )
            
            return interface
    
    def _process_question(
        self,
        question: str,
        datasource: str,
        reranker_checkbox: bool,
        stock_prediction_checkbox: bool
    ) -> tuple[str, List[List[str]]]:
        if not question.strip():
            return "please input your question", []
        
        # æ£€æµ‹è¯­è¨€
        try:
            lang = detect(question)
            language = 'zh' if lang.startswith('zh') else 'en'
        except:
            language = 'en'
        
        # è‚¡ç¥¨é¢„æµ‹æ¨¡å¼å¤„ç†
        if stock_prediction_checkbox and language == 'zh':
            return self._process_stock_prediction(question, reranker_checkbox)
        
        # æ ¹æ®è¯­è¨€é€‰æ‹©æ£€ç´¢ç³»ç»Ÿ
        if language == 'zh' and self.chinese_retrieval_system:
            return self._process_chinese_with_multi_stage(question, reranker_checkbox)
        elif language == 'en' and self.english_retrieval_system:
            return self._process_english_with_multi_stage(question, reranker_checkbox)
        else:
            return self._fallback_retrieval(question, language)
    
    def _process_stock_prediction(self, question: str, reranker_checkbox: bool) -> tuple[str, List[List[str]]]:
        """
        å¤„ç†è‚¡ç¥¨é¢„æµ‹æ¨¡å¼ - ä»…é€‚ç”¨äºä¸­æ–‡æŸ¥è¯¢
        æ£€ç´¢ä½¿ç”¨åŸå§‹queryï¼Œç”Ÿæˆä½¿ç”¨instruction
        """
        print(f"ğŸ” [è‚¡ç¥¨é¢„æµ‹æ¨¡å¼] å¼€å§‹å¤„ç†...")
        print(f"ğŸ“ [è‚¡ç¥¨é¢„æµ‹æ¨¡å¼] åŸå§‹æŸ¥è¯¢: '{question}'")
        
        # æ„å»ºè‚¡ç¥¨é¢„æµ‹çš„instruction
        instruction = self._build_stock_prediction_instruction(question)
        print(f"ğŸ“‹ [è‚¡ç¥¨é¢„æµ‹æ¨¡å¼] ç”Ÿæˆçš„instruction:")
        print(f"   {instruction}")
        print(f"ğŸ”„ [è‚¡ç¥¨é¢„æµ‹æ¨¡å¼] æŸ¥è¯¢è½¬æ¢å®Œæˆ:")
        print(f"   - æ£€ç´¢ä½¿ç”¨: '{question}'")
        print(f"   - ç”Ÿæˆä½¿ç”¨: '{instruction[:100]}{'...' if len(instruction) > 100 else ''}'")
        
        # ä½¿ç”¨åŸå§‹queryè¿›è¡Œæ£€ç´¢ï¼Œinstructionç”¨äºç”Ÿæˆ
        return self._process_chinese_with_multi_stage_with_instruction(question, instruction, reranker_checkbox)
    
    def _process_chinese_with_multi_stage_with_instruction(self, query: str, instruction: str, reranker_checkbox: bool) -> tuple[str, List[List[str]]]:
        """
        ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå¤„ç†ä¸­æ–‡æŸ¥è¯¢ï¼ˆåˆ†ç¦»æ¨¡å¼ï¼‰
        ä½¿ç”¨queryè¿›è¡Œæ£€ç´¢ï¼Œinstructionè¿›è¡Œç”Ÿæˆ
        """
        print(f"ğŸš€ [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] å¼€å§‹å¤„ç†...")
        print(f"ğŸ” [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] æ£€ç´¢æŸ¥è¯¢: '{query}'")
        print(f"ğŸ“ [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] ç”Ÿæˆinstruction: '{instruction[:100]}{'...' if len(instruction) > 100 else ''}'")
        print(f"ğŸ“Š [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] å¤„ç†ç­–ç•¥:")
        print(f"   - æ£€ç´¢é˜¶æ®µ: ä½¿ç”¨åŸå§‹query '{query}' è¿›è¡Œæ–‡æ¡£æ£€ç´¢")
        print(f"   - ç”Ÿæˆé˜¶æ®µ: ä½¿ç”¨instructionè¿›è¡Œç­”æ¡ˆç”Ÿæˆ")
        
        if not self.chinese_retrieval_system:
            return self._fallback_retrieval(query, 'zh')
        
        try:
            print(f"ğŸ” [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] å¼€å§‹ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢...")
            company_name, stock_code = extract_stock_info_with_mapping(query)
            report_date = extract_report_date(query)
            print(f"ğŸ¢ [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] å…¬å¸åç§°: {company_name}")
            print(f"ğŸ“ˆ [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] è‚¡ç¥¨ä»£ç : {stock_code}")
            print(f"ğŸ“… [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] æŠ¥å‘Šæ—¥æœŸ: {report_date}")
            
            # ä½¿ç”¨queryè¿›è¡Œæ£€ç´¢
            results = self.chinese_retrieval_system.search(
                query=query,
                company_name=company_name,
                stock_code=stock_code,
                report_date=report_date,
                top_k=self.config.retriever.rerank_top_k
            )
            
            # è½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼
            retrieved_documents = []
            retriever_scores = []
            
            # æ£€æŸ¥resultsçš„æ ¼å¼
            print(f"ğŸ“Š [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] æ£€ç´¢ç»“æœç±»å‹: {type(results)}")
            if isinstance(results, dict) and 'retrieved_documents' in results:
                documents = results['retrieved_documents']
                print(f"ğŸ“„ [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] æ£€ç´¢åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
                for result in documents:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
            else:
                for result in results:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
            
            if retrieved_documents:
                # ä½¿ç”¨build_smart_contextå¤„ç†ä¸Šä¸‹æ–‡ï¼Œé¿å…è¿‡åº¦æˆªæ–­
                context_parts = []
                summary_parts = []
                
                for doc in retrieved_documents[:10]:
                    content = doc.content
                    if not isinstance(content, str):
                        if isinstance(content, dict):
                            content = content.get('context', content.get('content', str(content)))
                        else:
                            content = str(content)
                    
                    # ä½¿ç”¨build_smart_contextå¤„ç†ä¸Šä¸‹æ–‡
                    processed_context = build_smart_context("", content, instruction)
                    context_parts.append(processed_context)
                
                context_str = "\n\n".join(context_parts)
                summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                
                # ä½¿ç”¨ä¸chinese_llm_evaluation.pyç›¸åŒçš„promptç”Ÿæˆé€»è¾‘
                chinese_template = getattr(self.config.data, 'chinese_prompt_template', 'multi_stage_chinese_template_with_fewshot.txt')
                print(f"ä½¿ç”¨é…ç½®çš„ä¸­æ–‡æ¨¡æ¿: {chinese_template}")
                
                # ä½¿ç”¨get_messages_for_testå’Œ_convert_messages_to_chatml
                messages = get_messages_for_test(summary, context_str, instruction, chinese_template)
                prompt = _convert_messages_to_chatml(messages)
                
                print(f"ğŸ¤– [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] ä½¿ç”¨instructionç”Ÿæˆç­”æ¡ˆ...")
                
                # ç”Ÿæˆç­”æ¡ˆ
                try:
                    generated_responses = self.generator.generate(texts=[prompt])
                    answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                    print(f"âœ… [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
                except Exception as e:
                    print(f"âŒ [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}")
                    answer = f"[å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {e}"
                
                # æ¸…ç†è‚¡ç¥¨é¢„æµ‹ç­”æ¡ˆï¼Œç§»é™¤"æ³¨æ„ï¼š"åŠå…¶åé¢çš„æ–‡å­—
                answer = self._clean_stock_prediction_answer(answer)
                
                # å‡†å¤‡ä¸Šä¸‹æ–‡æ•°æ®
                context_data = []
                for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                    context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                
                return answer, context_data
            else:
                print(f"âŒ [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£", []
                
        except Exception as e:
            print(f"âŒ [å¤šé˜¶æ®µåˆ†ç¦»æ¨¡å¼] å¤„ç†å¤±è´¥: {e}")
            return self._fallback_retrieval(query, 'zh')
    
    def _build_stock_prediction_instruction(self, question: str) -> str:
        """
        æ„å»ºè‚¡ç¥¨é¢„æµ‹çš„instruction
        """
        # ä½¿ç”¨ä¸chinese_llm_evaluation.pyç›¸åŒçš„instructionæ ¼å¼
        return f"è¯·æ ¹æ®ä¸‹æ–¹æä¾›çš„è¯¥è‚¡ç¥¨ç›¸å…³ç ”æŠ¥ä¸æ•°æ®ï¼Œå¯¹è¯¥è‚¡ç¥¨çš„ä¸‹ä¸ªæœˆçš„æ¶¨è·Œï¼Œè¿›è¡Œé¢„æµ‹ï¼Œè¯·ç»™å‡ºæ˜ç¡®çš„ç­”æ¡ˆï¼Œ\"æ¶¨\" æˆ–è€… \"è·Œ\"ã€‚åŒæ—¶ç»™å‡ºè¿™ä¸ªè‚¡ç¥¨ä¸‹æœˆçš„æ¶¨è·Œæ¦‚ç‡ï¼Œåˆ†åˆ«æ˜¯:æå¤§ï¼Œè¾ƒå¤§ï¼Œä¸­ä¸Šï¼Œä¸€èˆ¬ã€‚\n\né—®é¢˜ï¼š{question}"
    
    def _clean_stock_prediction_answer(self, answer: str) -> str:
        """
        æ¸…ç†è‚¡ç¥¨é¢„æµ‹ç­”æ¡ˆï¼Œç§»é™¤"æ³¨æ„ï¼š"åŠå…¶åé¢çš„æ–‡å­—
        """
        if not answer:
            return answer
        
        # æŸ¥æ‰¾"æ³¨æ„ï¼š"çš„ä½ç½®
        notice_index = answer.find("æ³¨æ„ï¼š")
        if notice_index != -1:
            # ç§»é™¤"æ³¨æ„ï¼š"åŠå…¶åé¢çš„æ‰€æœ‰æ–‡å­—
            cleaned_answer = answer[:notice_index].strip()
            print(f"ğŸ”§ æ¸…ç†è‚¡ç¥¨é¢„æµ‹ç­”æ¡ˆ:")
            print(f"   åŸå§‹ç­”æ¡ˆ: {answer}")
            print(f"   æ¸…ç†åç­”æ¡ˆ: {cleaned_answer}")
            return cleaned_answer
        
        return answer
    
    def _process_chinese_with_multi_stage(self, question: str, reranker_checkbox: bool) -> tuple[str, List[List[str]]]:
        """ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå¤„ç†ä¸­æ–‡æŸ¥è¯¢"""
        if not self.chinese_retrieval_system:
            return self._fallback_retrieval(question, 'zh')
        
        try:
            print(f"ğŸ” å¼€å§‹ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢...")
            print(f"ğŸ“‹ æŸ¥è¯¢: {question}")
            company_name, stock_code = extract_stock_info_with_mapping(question)
            report_date = extract_report_date(question)
            print(f"ğŸ¢ å…¬å¸åç§°: {company_name}")
            print(f"ğŸ“ˆ è‚¡ç¥¨ä»£ç : {stock_code}")
            print(f"ğŸ“… æŠ¥å‘Šæ—¥æœŸ: {report_date}")
            print(f"âš™ï¸ é…ç½®å‚æ•°: retrieval_top_k={self.config.retriever.retrieval_top_k}, rerank_top_k={self.config.retriever.rerank_top_k}")
            
            results = self.chinese_retrieval_system.search(
                query=question,
                company_name=company_name,
                stock_code=stock_code,
                report_date=report_date,
                top_k=self.config.retriever.rerank_top_k  # ä½¿ç”¨é…ç½®ä¸­çš„é‡æ’åºtop-k
            )
            
            # è½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼
            retrieved_documents = []
            retriever_scores = []
            
            # æ£€æŸ¥resultsçš„æ ¼å¼
            print(f"ğŸ“Š æ£€ç´¢ç»“æœç±»å‹: {type(results)}")
            if isinstance(results, dict) and 'retrieved_documents' in results:
                documents = results['retrieved_documents']
                llm_answer = results.get('llm_answer', '')
                print(f"ğŸ“„ æ£€ç´¢åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
                print(f"ğŸ¤– LLMç­”æ¡ˆ: {'å·²ç”Ÿæˆ' if llm_answer else 'æœªç”Ÿæˆ'}")
                for result in documents:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
                
                # å¦‚æœå¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå·²ç»ç”Ÿæˆäº†ç­”æ¡ˆï¼Œç›´æ¥ä½¿ç”¨
                if llm_answer:
                    context_data = []
                    for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                        context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                    answer = f"[Multi-Stage Retrieval: ZH] {llm_answer}"
                    return answer, context_data
            else:
                for result in results:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
            
            if retrieved_documents:
                # ä½¿ç”¨build_smart_contextå¤„ç†ä¸Šä¸‹æ–‡ï¼Œé¿å…è¿‡åº¦æˆªæ–­
                context_parts = []
                summary_parts = []
                
                for doc in retrieved_documents[:10]:
                    content = doc.content
                    if not isinstance(content, str):
                        if isinstance(content, dict):
                            content = content.get('context', content.get('content', str(content)))
                        else:
                            content = str(content)
                    
                    # ä½¿ç”¨build_smart_contextå¤„ç†ä¸Šä¸‹æ–‡
                    processed_context = build_smart_context("", content, question)
                    context_parts.append(processed_context)
                
                context_str = "\n\n".join(context_parts)
                
                # æ ¹æ®æŸ¥è¯¢è¯­è¨€åŠ¨æ€é€‰æ‹©promptæ¨¡æ¿
                try:
                    from langdetect import detect
                    query_language = detect(question)
                    is_chinese_query = query_language.startswith('zh')
                except:
                    # å¦‚æœè¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œæ ¹æ®æŸ¥è¯¢å†…å®¹åˆ¤æ–­
                    is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
                
                if is_chinese_query:
                    # ä¸­æ–‡æŸ¥è¯¢ä½¿ç”¨ä¸chinese_llm_evaluation.pyç›¸åŒçš„promptç”Ÿæˆé€»è¾‘
                    summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                    chinese_template = getattr(self.config.data, 'chinese_prompt_template', 'multi_stage_chinese_template_with_fewshot.txt')
                    print(f"ä½¿ç”¨é…ç½®çš„ä¸­æ–‡æ¨¡æ¿: {chinese_template}")
                    
                    # ä½¿ç”¨get_messages_for_testå’Œ_convert_messages_to_chatml
                    messages = get_messages_for_test(summary, context_str, question, chinese_template)
                    prompt = _convert_messages_to_chatml(messages)
                else:
                    # è‹±æ–‡æŸ¥è¯¢ï¼šä½¿ç”¨é…ç½®çš„è‹±æ–‡æ¨¡æ¿
                    try:
                        # å¯¼å…¥RAGç³»ç»Ÿçš„è‹±æ–‡promptå¤„ç†å‡½æ•°
                        from xlm.components.rag_system.rag_system import get_final_prompt_messages_english, _convert_messages_to_chatml
                        
                        # ä½¿ç”¨é…ç½®çš„è‹±æ–‡æ¨¡æ¿
                        english_template = getattr(self.config.data, 'english_prompt_template', 'unified_english_template_no_think.txt')
                        messages = get_final_prompt_messages_english(context_str, question, english_template)
                        prompt = _convert_messages_to_chatml(messages)
                        print(f"ä½¿ç”¨é…ç½®çš„è‹±æ–‡æ¨¡æ¿: {english_template}")
                    except Exception as e:
                        print(f"è‹±æ–‡æ¨¡æ¿åŠ è½½å¤±è´¥: {e}ï¼Œä½¿ç”¨ç®€å•è‹±æ–‡prompt")
                        prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
                
                generated_responses = self.generator.generate(texts=[prompt])
                answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                context_data = []
                for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                    context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                answer = f"[Multi-Stage Retrieval: ZH] {answer}"
                return answer, context_data
            else:
                return "No relevant documents found.", []
                
        except Exception as e:
            return self._fallback_retrieval(question, 'zh')
    
    def _process_english_with_multi_stage(self, question: str, reranker_checkbox: bool) -> tuple[str, List[List[str]]]:
        """ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå¤„ç†è‹±æ–‡æŸ¥è¯¢"""
        if not self.english_retrieval_system:
            return self._fallback_retrieval(question, 'en')
        
        try:
            print(f"ğŸ” å¼€å§‹è‹±æ–‡å¤šé˜¶æ®µæ£€ç´¢...")
            print(f"ğŸ“‹ æŸ¥è¯¢: {question}")
            print(f"âš™ï¸ é…ç½®å‚æ•°: retrieval_top_k={self.config.retriever.retrieval_top_k}, rerank_top_k={self.config.retriever.rerank_top_k}")
            
            # æ‰§è¡Œå¤šé˜¶æ®µæ£€ç´¢
            results = self.english_retrieval_system.search(
                query=question,
                top_k=self.config.retriever.rerank_top_k  # ä½¿ç”¨é…ç½®ä¸­çš„é‡æ’åºtop-k
            )
            
            # è½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼
            retrieved_documents = []
            retriever_scores = []
            
            for result in results:
                doc = DocumentWithMetadata(
                    content=result.get('context', result.get('content', '')),
                    metadata=DocumentMetadata(
                        source=result.get('source', 'Unknown'),
                        created_at="",
                        author="",
                        language="english"
                    )
                )
                retrieved_documents.append(doc)
                retriever_scores.append(result.get('combined_score', 0.0))
            
            if retrieved_documents:
                context_str = "\n\n".join([doc.content for doc in retrieved_documents[:10]])
                
                # æ ¹æ®æŸ¥è¯¢è¯­è¨€åŠ¨æ€é€‰æ‹©promptæ¨¡æ¿
                try:
                    from langdetect import detect
                    query_language = detect(question)
                    is_chinese_query = query_language.startswith('zh')
                except:
                    # å¦‚æœè¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œæ ¹æ®æŸ¥è¯¢å†…å®¹åˆ¤æ–­
                    is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
                
                if is_chinese_query:
                    # ä¸­æ–‡æŸ¥è¯¢ä½¿ç”¨ä¸­æ–‡promptæ¨¡æ¿
                    summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                    prompt = template_loader.format_template(
                        "multi_stage_chinese_template",
                        summary=summary,
                        context=context_str,
                        query=question
                    )
                    if prompt is None:
                        # å›é€€åˆ°ç®€å•ä¸­æ–‡prompt
                        prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context_str}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
                else:
                    # è‹±æ–‡æŸ¥è¯¢ä½¿ç”¨è‹±æ–‡promptæ¨¡æ¿
                    prompt = template_loader.format_template(
                        "rag_english_template",
                        context=context_str, 
                        question=question
                    )
                    if prompt is None:
                        # å›é€€åˆ°ç®€å•è‹±æ–‡prompt
                        prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
                
                generated_responses = self.generator.generate(texts=[prompt])
                answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                context_data = []
                for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                    context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                answer = f"[Multi-Stage Retrieval: EN] {answer}"
                return answer, context_data
            else:
                return "No relevant documents found.", []
                
        except Exception as e:
            return self._fallback_retrieval(question, 'en')
    
    def _fallback_retrieval(self, question: str, language: str) -> tuple[str, List[List[str]]]:
        """å›é€€åˆ°ä¼ ç»Ÿæ£€ç´¢"""
        if self.rag_system is None:
            return "ä¼ ç»ŸRAGç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œæ— æ³•å¤„ç†æŸ¥è¯¢", []
        
        try:
            # è¿è¡ŒRAGç³»ç»Ÿ
            rag_output = self.rag_system.run(user_input=question, language=language)
            
            # ç”Ÿæˆç­”æ¡ˆ
            if rag_output.retrieved_documents:
                # æ„å»ºä¸Šä¸‹æ–‡
                context_str = "\n\n".join([doc.content for doc in rag_output.retrieved_documents[:10]])
                
                # æ ¹æ®æŸ¥è¯¢è¯­è¨€åŠ¨æ€é€‰æ‹©promptæ¨¡æ¿
                try:
                    from langdetect import detect
                    query_language = detect(question)
                    is_chinese_query = query_language.startswith('zh')
                except:
                    # å¦‚æœè¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œæ ¹æ®æŸ¥è¯¢å†…å®¹åˆ¤æ–­
                    is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
                
                if is_chinese_query:
                    # ä¸­æ–‡æŸ¥è¯¢ä½¿ç”¨ä¸­æ–‡promptæ¨¡æ¿
                    summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                    prompt = template_loader.format_template(
                        "multi_stage_chinese_template",
                        summary=summary,
                        context=context_str,
                        query=question
                    )
                    if prompt is None:
                        # å›é€€åˆ°ç®€å•ä¸­æ–‡prompt
                        prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context_str}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
                else:
                    # è‹±æ–‡æŸ¥è¯¢ä½¿ç”¨è‹±æ–‡promptæ¨¡æ¿
                    prompt = template_loader.format_template(
                        "rag_english_template",
                        context=context_str, 
                        question=question
                    )
                    if prompt is None:
                        # å›é€€åˆ°ç®€å•è‹±æ–‡prompt
                        prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
                
                # ç”Ÿæˆç­”æ¡ˆ
                generated_responses = self.generator.generate(texts=[prompt])
                answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                
                # å‡†å¤‡ä¸Šä¸‹æ–‡æ•°æ®
                context_data = []
                for doc, score in zip(rag_output.retrieved_documents[:self.config.retriever.rerank_top_k], rag_output.retriever_scores[:self.config.retriever.rerank_top_k]):
                    # ç»Ÿä¸€åªæ˜¾ç¤ºcontentå­—æ®µï¼Œä¸æ˜¾ç¤ºquestionå’Œanswer
                    content = doc.content
                    # ç¡®ä¿contentæ˜¯å­—ç¬¦ä¸²ç±»å‹
                    if not isinstance(content, str):
                        if isinstance(content, dict):
                            # å¦‚æœæ˜¯å­—å…¸ï¼Œå°è¯•æå–contextæˆ–contentå­—æ®µ
                            content = content.get('context', content.get('content', str(content)))
                        else:
                            content = str(content)
                    
                    # æˆªæ–­è¿‡é•¿çš„å†…å®¹
                    display_content = content[:500] + "..." if len(content) > 500 else content
                    context_data.append([f"{score:.4f}", display_content])
                
                # æ·»åŠ æ£€ç´¢ç³»ç»Ÿä¿¡æ¯
                answer = f"[Multi-Stage Retrieval: {language.upper()}] {answer}"
                
                return answer, context_data
            else:
                return "No relevant documents found.", []
                
        except Exception as e:
            return f"æ£€ç´¢å¤±è´¥: {str(e)}", []
    
    def launch(self, share: bool = False):
        """Launch UI interface"""
        self.interface.launch(share=share) 