#!/usr/bin/env python3
"""
Optimized RAG UI with Multi-Stage Retrieval System Integration
"""

import os
import sys
import re
import hashlib
import json
import logging
from pathlib import Path
from typing import List, Optional, Tuple, Dict, Any
import gradio as gr
import numpy as np
import torch
import faiss
from langdetect import detect, LangDetectException

# Add project root directory to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata, RagOutput
from xlm.components.rag_system.rag_system import RagSystem
from xlm.components.generator.generator import Generator
from xlm.components.retriever.retriever import Retriever
from xlm.components.retriever.reranker import QwenReranker
from xlm.utils.visualizer import Visualizer
from xlm.registry.retriever import load_enhanced_retriever
from xlm.registry.generator import load_generator
from config.parameters import Config, EncoderConfig, RetrieverConfig, ModalityConfig, EMBEDDING_CACHE_DIR, RERANKER_CACHE_DIR, config
from xlm.components.prompt_templates.template_loader import template_loader
from xlm.utils.stock_info_extractor import extract_stock_info, extract_stock_info_with_mapping, extract_report_date

# Try to import multi-stage retrieval system
try:
    from alphafin_data_process.multi_stage_retrieval_final import MultiStageRetrievalSystem
    MULTI_STAGE_AVAILABLE = True
except ImportError:
    print("Warning: Multi-stage retrieval system is not available, falling back to traditional retrieval")
    MULTI_STAGE_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def build_smart_context(summary: str, context: str, query: str) -> str:
    """
    Smartly build context, using the same logic as chinese_llm_evaluation.py
    This function is responsible for processing the original `context` string to avoid excessive truncation
    """
    processed_context = context
    try:
        # Try to parse context as a dictionary, if it is then format it as readable JSON
        # Note: Use json.loads() instead of eval() for more security, but need to replace single quotes with double quotes first
        context_data = json.loads(context.replace("'", '"')) 
        if isinstance(context_data, dict):
            processed_context = json.dumps(context_data, ensure_ascii=False, indent=2)
            logger.debug("✅ Context identified as a dictionary string and formatted as JSON.")
    except (json.JSONDecodeError, TypeError):
        logger.debug("⚠️ Context is not a JSON string format, using original context directly.")
        pass

    # Use the same length limit as chinese_llm_evaluation.py: 3500 characters
    max_processed_context_length = 3500 # Character length, as a rough limit
    if len(processed_context) > max_processed_context_length:
        logger.warning(f"⚠️ Processed context length is too long ({len(processed_context)} characters), truncating.")
        processed_context = processed_context[:max_processed_context_length] + "..."

    return processed_context

def _load_template_content_from_file(template_file_name: str) -> str:
    """
    Load template content from file
    """
    template_path = Path("data/prompt_templates") / template_file_name
    if not template_path.exists():
        logger.error(f"❌ Template file does not exist: {template_path}")
        return ""
    
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        logger.error(f"❌ Failed to read template file: {e}")
        return ""

def get_messages_for_test(summary: str, context: str, query: str, 
                          template_file_name: str = "multi_stage_chinese_template_with_fewshot.txt") -> List[Dict[str, str]]:
    """
    Build a list of messages for testing, load content from a specified template file, and incorporate item_instruction into the Prompt.
    Args:
        summary (str): Summary generated by LLM Qwen2-7B.
        context (str): Full context (already includes summary).
        query (str): User question.
        template_file_name (str): Name of the template file to load.
    Returns:
        List[Dict[str, str]]: List of built messages.
    """
    template_full_string = _load_template_content_from_file(template_file_name)

    messages = []
    # Use regex to split all parts and keep delimiter content
    parts = re.split(r'(===SYSTEM===|===USER===|===ASSISTANT===)', template_full_string, flags=re.DOTALL)

    # Remove the first empty string (if it exists) and extra whitespace
    parts = [p.strip() for p in parts if p.strip()]

    current_role = None
    current_content = []

    for part in parts:
        if part in ["===SYSTEM===", "===USER===", "===ASSISTANT==="]:
            if current_role is not None:
                messages.append({"role": current_role.lower().replace("===", ""), "content": "\n".join(current_content).strip()})
            current_role = part
            current_content = []
        else:
            current_content.append(part)

    # Add the message for the last part
    if current_role is not None:
        messages.append({"role": current_role.lower().replace("===", ""), "content": "\n".join(current_content).strip()})

    # Replace placeholders
    for message in messages:
        if message["role"] == "user":
            modified_content = message["content"]
            modified_content = modified_content.replace('{query}', query)
            modified_content = modified_content.replace('{summary}', summary)
            modified_content = modified_content.replace('{context}', context)
            message["content"] = modified_content

    logger.debug(f"Built messages: {messages}")
    return messages

def _convert_messages_to_chatml(messages: List[Dict[str, str]]) -> str:
    """
    Convert messages list to Fin-R1 (Qwen2.5 based) expected ChatML format string.
    Qwen series standard should be `im_end`
    """
    if not messages:
        return ""

    formatted_prompt = ""
    for message in messages:
        role = message.get("role", "")
        content = message.get("content", "")

        if role == "system":
            formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n"
        elif role == "user":
            formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n"
        elif role == "assistant":
            # The assistant role is usually part of Few-shot examples here
            formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n"

    # Append one <|im_start|>assistant\n at the end to indicate that we hope the model to start generating a new assistant reply
    formatted_prompt += "<|im_start|>assistant\n"

    logger.debug(f"Converted ChatML Prompt (first 500 characters):\n{formatted_prompt[:500]}...")
    return formatted_prompt

def try_load_qwen_reranker(model_name, cache_dir=None):
    """Try to load Qwen reranker, supporting GPU 0 and CPU fallback"""
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        
        # Ensure cache_dir is a valid string
        if cache_dir is None:
            cache_dir = RERANKER_CACHE_DIR
        
        print(f"Attempting to load QwenReranker with 8-bit quantization...")
        print(f"Loading reranker model: {model_name}")
        
        # First try GPU 0
        if torch.cuda.is_available() and torch.cuda.device_count() > 0:
            device = "cuda:0"  # Explicitly specify GPU 0
            print(f"- Device: {device}")
            print(f"- Cache directory: {cache_dir}")
            print(f"- Quantization: True (8-bit)")
            print(f"- Flash Attention: False")
            
            try:
                # Check available memory on GPU 0
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated(0)
                free_memory = gpu_memory - allocated_memory
                
                print(f"- GPU 0 Total Memory: {gpu_memory / 1024**3:.1f}GB")
                print(f"- GPU 0 Allocated Memory: {allocated_memory / 1024**3:.1f}GB")
                print(f"- GPU 0 Available Memory: {free_memory / 1024**3:.1f}GB")
                
                # If available memory is less than 2GB, fallback to CPU
                if free_memory < 2 * 1024**3:  # 2GB
                    print("- GPU 0 Memory insufficient, falling back to CPU")
                    device = "cpu"
                else:
                    # Try to load on GPU 0
                    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
                    model = AutoModelForSequenceClassification.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        load_in_8bit=True
                    )
                    print("Quantized model automatically set to device, skipping manual move")
                    print("Reranker model loaded")
                    print("Quantization successful!")
                    return QwenReranker(model_name, device=device, cache_dir=cache_dir)
                    
            except Exception as e:
                print(f"- GPU 0 loading failed: {e}")
                print("- Falling back to CPU")
                device = "cpu"
        
        # CPU fallback
        if device == "cpu" or not torch.cuda.is_available():
            device = "cpu"
            print(f"- Device: {device}")
            print(f"- Cache directory: {cache_dir}")
            print(f"- Quantization: False (CPU mode)")
            
            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
            model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                torch_dtype=torch.float32
            )
            model = model.to(device)
            print("Reranker model loaded")
            print("CPU loading successful!")
            return QwenReranker(model_name, device=device, cache_dir=cache_dir)
            
    except Exception as e:
        print(f"Reranker loading failed: {e}")
        return None

class OptimizedRagUIWithMultiStage:
    def __init__(
        self,
        cache_dir: Optional[str] = None,
        use_faiss: bool = True,
        enable_reranker: bool = True,
        use_existing_embedding_index: Optional[bool] = None,
        max_alphafin_chunks: Optional[int] = None,
        window_title: str = "Financial Explainable RAG System with Multi-Stage Retrieval",
        title: str = "Financial Explainable RAG System with Multi-Stage Retrieval",
        examples: Optional[List[List[str]]] = None,
    ):
        # Use platform-aware configuration from config
        self.config = Config()
        self.cache_dir = EMBEDDING_CACHE_DIR if (not cache_dir or not isinstance(cache_dir, str)) else cache_dir
        self.use_faiss = use_faiss
        self.enable_reranker = enable_reranker
        self.use_existing_embedding_index = use_existing_embedding_index if use_existing_embedding_index is not None else self.config.retriever.use_existing_embedding_index
        self.max_alphafin_chunks = max_alphafin_chunks if max_alphafin_chunks is not None else self.config.retriever.max_alphafin_chunks
        self.window_title = window_title
        self.title = title
        self.examples = examples or [
            ["德赛电池(000049)的下一季度收益预测如何？"],
            ["用友网络2019年的每股经营活动产生的现金流量净额是多少？"],
            ["下月股价能否上涨?"],
            ["How was internally developed software capitalised?"],
            ["Why did the Operating revenues decreased from 2018 to 2019?"],
            ["Why did the Operating costs decreased from 2018 to 2019?"]
        ]
        
        # Set environment variables for model caching
        os.environ['TRANSFORMERS_CACHE'] = os.path.join(self.cache_dir, 'transformers')
        os.environ['HF_HOME'] = self.cache_dir
        os.environ['HF_DATASETS_CACHE'] = os.path.join(self.cache_dir, 'datasets')
        
        # Initialize system components
        self._init_components()
        
        # Create Gradio interface
        self.interface = self._create_interface()
    
    def _init_components(self):
        """Initialize RAG system components with multi-stage retrieval"""
        print("\nStep 1. Initializing Multi-Stage Retrieval System...")
        
        # Initialize traditional RAG system as fallback
        print("Step 2. Initializing Traditional RAG System as fallback...")
        try:
            # Load retriever
            self.retriever = load_enhanced_retriever(
                config=self.config
            )
            
            # Load generator
            self.generator = load_generator(
                generator_model_name=self.config.generator.model_name,
                use_local_llm=True,
                use_gpu=True,
                gpu_device="cuda:1",
                cache_dir=self.config.generator.cache_dir
            )
            
            # Initialize RAG system
            self.rag_system = RagSystem(
                retriever=self.retriever,
                generator=self.generator,
                retriever_top_k=self.config.retriever.retrieval_top_k  # Use settings from config
            )
            print("✅ Traditional RAG system initialized")
        except Exception as e:
            print(f"❌ Traditional RAG system initialization failed: {e}")
            self.rag_system = None
        
        # Initialize multi-stage retrieval system
        if MULTI_STAGE_AVAILABLE:
            try:
                # Chinese data path
                chinese_data_path = Path(config.data.chinese_data_path)
                
                if chinese_data_path.exists():
                    print("✅ Initializing Chinese multi-stage retrieval system...")
                    self.chinese_retrieval_system = MultiStageRetrievalSystem(
                        data_path=chinese_data_path,
                        dataset_type="chinese",
                        use_existing_config=True
                    )
                    print("✅ Chinese multi-stage retrieval system initialized")
                else:
                    print(f"❌ Chinese data file does not exist: {chinese_data_path}")
                    self.chinese_retrieval_system = None
                
                # English data path (if available)
                english_data_path = Path("data/tatqa/processed_data.json")  # Requires preprocessing
                if english_data_path.exists():
                    print("✅ Initializing English multi-stage retrieval system...")
                    self.english_retrieval_system = MultiStageRetrievalSystem(
                        data_path=english_data_path,
                        dataset_type="english",
                        use_existing_config=True
                    )
                    print("✅ English multi-stage retrieval system initialized")
                else:
                    print(f"⚠️ English data file does not exist: {english_data_path}")
                    self.english_retrieval_system = None
                
            except Exception as e:
                print(f"❌ Multi-stage retrieval system initialization failed: {e}")
                self.chinese_retrieval_system = None
                self.english_retrieval_system = None
        else:
            print("❌ Multi-stage retrieval system is not available, falling back to traditional retrieval")
            self.chinese_retrieval_system = None
            self.english_retrieval_system = None
        
        print("\nStep 3. Loading visualizer...")
        self.visualizer = Visualizer(show_mid_features=True)
        
        print("✅ All components initialized")
    
    def _create_interface(self) -> gr.Blocks:
        """Create optimized Gradio interface"""
        with gr.Blocks(
            title=self.window_title
        ) as interface:
            # Title
            gr.Markdown(f"# {self.title}")
            
            # Input area
            with gr.Row():
                with gr.Column(scale=4):
                    datasource = gr.Radio(
                        choices=["TatQA", "AlphaFin", "Both"],
                        value="Both",
                        label="Data Source"
                    )
                    
            with gr.Row():
                with gr.Column(scale=4):
                    question_input = gr.Textbox(
                        show_label=False,
                        placeholder="Enter your question",
                        label="Question",
                        lines=3
                    )
            
            # Control button area
            with gr.Row():
                with gr.Column(scale=1):
                    reranker_checkbox = gr.Checkbox(
                        label="Enable Reranker",
                        value=True,
                        interactive=True
                    )
                with gr.Column(scale=1):
                    stock_prediction_checkbox = gr.Checkbox(
                        label="stock prediction (only for chinese query)",
                        value=False,
                        interactive=True
                    )
                with gr.Column(scale=1):
                    submit_btn = gr.Button("Submit")
            
            # Use tabs to separate display
            with gr.Tabs():
                # Answer tab
                with gr.TabItem("Answer"):
                    answer_output = gr.Textbox(
                        show_label=False,
                        interactive=False,
                        label="Generated Response",
                        lines=5
                    )
                
                # Explanation tab
                with gr.TabItem("Explanation"):
                    context_output = gr.Dataframe(
                        headers=["Score", "Context"],
                        datatype=["number", "str"],
                        label="Retrieved Contexts",
                        interactive=False
                    )

            # Add example questions
            gr.Examples(
                examples=self.examples,
                inputs=[question_input],
                label="Example Questions"
            )

            # Bind events
            submit_btn.click(
                self._process_question,
                inputs=[question_input, datasource, reranker_checkbox, stock_prediction_checkbox],
                outputs=[answer_output, context_output]
            )
            
            return interface
    
    def _process_question(
        self,
        question: str,
        datasource: str,
        reranker_checkbox: bool,
        stock_prediction_checkbox: bool
    ) -> tuple[str, List[List[str]]]:
        if not question.strip():
            return "please input your question", []
        
        # Detect language
        try:
            lang = detect(question)
            language = 'zh' if lang.startswith('zh') else 'en'
        except:
            language = 'en'
        
        # Stock prediction mode handling
        if stock_prediction_checkbox and language == 'zh':
            return self._process_stock_prediction(question, reranker_checkbox)
        
        # Select retrieval system based on language
        if language == 'zh' and self.chinese_retrieval_system:
            return self._process_chinese_with_multi_stage(question, reranker_checkbox)
        elif language == 'en' and self.english_retrieval_system:
            return self._process_english_with_multi_stage(question, reranker_checkbox)
        else:
            return self._fallback_retrieval(question, language)
    
    def _process_stock_prediction(self, question: str, reranker_checkbox: bool) -> tuple[str, List[List[str]]]:
        """
        Process stock prediction mode - only applicable for Chinese queries
        Retrieval uses the original query, generation uses the instruction
        """
        print(f"🔍 [Stock Prediction Mode] Starting processing...")
        print(f"📝 [Stock Prediction Mode] Original query: '{question}'")
        
        # Build stock prediction instruction
        instruction = self._build_stock_prediction_instruction(question)
        print(f"📋 [Stock Prediction Mode] Generated instruction:")
        print(f"   {instruction}")
        print(f"🔄 [Stock Prediction Mode] Query conversion complete:")
        print(f"   - Retrieval uses: '{question}'")
        print(f"   - Generation uses: '{instruction[:100]}{'...' if len(instruction) > 100 else ''}'")
        
        # Use original query for retrieval, instruction for generation
        return self._process_chinese_with_multi_stage_with_instruction(question, instruction, reranker_checkbox)
    
    def _process_chinese_with_multi_stage_with_instruction(self, query: str, instruction: str, reranker_checkbox: bool) -> tuple[str, List[List[str]]]:
        """
        Process Chinese query using multi-stage retrieval system (separate mode)
        Use query for retrieval, instruction for generation
        """
        print(f"🚀 [Multi-stage Separate Mode] Starting processing...")
        print(f"🔍 [Multi-stage Separate Mode] Retrieval query: '{query}'")
        print(f"📝 [Multi-stage Separate Mode] Generation instruction: '{instruction[:100]}{'...' if len(instruction) > 100 else ''}'")
        print(f"📊 [Multi-stage Separate Mode] Processing strategy:")
        print(f"   - Retrieval stage: Use original query '{query}' for document retrieval")
        print(f"   - Generation stage: Use instruction for answer generation")
        
        if not self.chinese_retrieval_system:
            return self._fallback_retrieval(query, 'zh')
        
        try:
            print(f"🔍 [Multi-stage Separate Mode] Starting Chinese multi-stage retrieval...")
            company_name, stock_code = extract_stock_info_with_mapping(query)
            report_date = extract_report_date(query)
            print(f"🏢 [Multi-stage Separate Mode] Company name: {company_name}")
            print(f"📈 [Multi-stage Separate Mode] Stock code: {stock_code}")
            print(f"📅 [Multi-stage Separate Mode] Report date: {report_date}")
            
            # Use query for retrieval
            results = self.chinese_retrieval_system.search(
                query=query,
                company_name=company_name,
                stock_code=stock_code,
                report_date=report_date,
                top_k=self.config.retriever.rerank_top_k
            )
            
            # Convert to DocumentWithMetadata format
            retrieved_documents = []
            retriever_scores = []
            
            # Check results format
            print(f"📊 [Multi-stage Separate Mode] Retrieval result type: {type(results)}")
            if isinstance(results, dict) and 'retrieved_documents' in results:
                documents = results['retrieved_documents']
                print(f"📄 [Multi-stage Separate Mode] Retrieved {len(documents)} documents")
                for result in documents:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
            else:
                for result in results:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
            
            if retrieved_documents:
                # Use build_smart_context to process context, avoiding excessive truncation
                context_parts = []
                summary_parts = []
                
                for doc in retrieved_documents[:10]:
                    content = doc.content
                    if not isinstance(content, str):
                        if isinstance(content, dict):
                            content = content.get('context', content.get('content', str(content)))
                        else:
                            content = str(content)
                    
                    # Use build_smart_context to process context
                    processed_context = build_smart_context("", content, instruction)
                    context_parts.append(processed_context)
                
                context_str = "\n\n".join(context_parts)
                summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                
                # Use the same prompt generation logic as chinese_llm_evaluation.py
                chinese_template = getattr(self.config.data, 'chinese_prompt_template', 'multi_stage_chinese_template_with_fewshot.txt')
                print(f"Using Chinese template from config: {chinese_template}")
                
                # Use get_messages_for_test and _convert_messages_to_chatml
                messages = get_messages_for_test(summary, context_str, instruction, chinese_template)
                prompt = _convert_messages_to_chatml(messages)
                
                print(f"🤖 [Multi-stage Separate Mode] Generating answer using instruction...")
                
                # Generate answer
                try:
                    generated_responses = self.generator.generate(texts=[prompt])
                    answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                    print(f"✅ [Multi-stage Separate Mode] Answer generation complete")
                except Exception as e:
                    print(f"❌ [Multi-stage Separate Mode] Answer generation failed: {e}")
                    answer = f"[Multi-stage Separate Mode] Answer generation failed: {e}"
                
                # Clean up stock prediction answer, remove "Note:" and everything after it
                answer = self._clean_stock_prediction_answer(answer)
                
                # Prepare context data
                context_data = []
                for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                    context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                
                return answer, context_data
            else:
                print(f"❌ [Multi-stage Separate Mode] No relevant documents found")
                return "No relevant documents found", []
                
        except Exception as e:
            print(f"❌ [Multi-stage Separate Mode] Processing failed: {e}")
            return self._fallback_retrieval(query, 'zh')
    
    def _build_stock_prediction_instruction(self, question: str) -> str:
        """
        Build stock prediction instruction
        """
        # Use the same instruction format as chinese_llm_evaluation.py
        return f"Please predict the stock price movement for the next month based on the following research reports and data for this stock, and provide a clear answer, \"Up\" or \"Down\". Also provide the stock's probability of price movement for the next month, which are: extremely high, high, medium-high, general. \n\nQuestion: {question}"
    
    def _clean_stock_prediction_answer(self, answer: str) -> str:
        """
        Clean up stock prediction answer, remove "Note:" and everything after it
        """
        if not answer:
            return answer
        
        # Find the position of "Note:"
        notice_index = answer.find("Note:")
        if notice_index != -1:
            # Remove "Note:" and everything after it
            cleaned_answer = answer[:notice_index].strip()
            print(f"🔧 Cleaning stock prediction answer:")
            print(f"   Original answer: {answer}")
            print(f"   Cleaned answer: {cleaned_answer}")
            return cleaned_answer
        
        return answer
    
    def _process_chinese_with_multi_stage(self, question: str, reranker_checkbox: bool) -> tuple[str, List[List[str]]]:
        """Use multi-stage retrieval system to process Chinese query"""
        if not self.chinese_retrieval_system:
            return self._fallback_retrieval(question, 'zh')
        
        try:
            print(f"🔍 Starting Chinese multi-stage retrieval...")
            print(f"📋 Query: {question}")
            company_name, stock_code = extract_stock_info_with_mapping(question)
            report_date = extract_report_date(question)
            print(f"🏢 Company name: {company_name}")
            print(f"📈 Stock code: {stock_code}")
            print(f"📅 Report date: {report_date}")
            print(f"⚙️ Configuration parameters: retrieval_top_k={self.config.retriever.retrieval_top_k}, rerank_top_k={self.config.retriever.rerank_top_k}")
            
            results = self.chinese_retrieval_system.search(
                query=question,
                company_name=company_name,
                stock_code=stock_code,
                report_date=report_date,
                top_k=self.config.retriever.rerank_top_k  # Use rerank_top_k from config
            )
            
            # Convert to DocumentWithMetadata format
            retrieved_documents = []
            retriever_scores = []
            
            # Check results format
            print(f"📊 Retrieval result type: {type(results)}")
            if isinstance(results, dict) and 'retrieved_documents' in results:
                documents = results['retrieved_documents']
                llm_answer = results.get('llm_answer', '')
                print(f"📄 Retrieved {len(documents)} documents")
                print(f"🤖 LLM answer: {'Generated' if llm_answer else 'Not generated'}")
                for result in documents:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
                
                # If multi-stage retrieval system already generated an answer, use it directly
                if llm_answer:
                    context_data = []
                    for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                        context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                    answer = f"[Multi-Stage Retrieval: ZH] {llm_answer}"
                    return answer, context_data
            else:
                for result in results:
                    doc = DocumentWithMetadata(
                        content=result.get('original_context', result.get('summary', '')),
                        metadata=DocumentMetadata(
                            source=result.get('company_name', 'Unknown'),
                            created_at="",
                            author="",
                            language="chinese"
                        )
                    )
                    retrieved_documents.append(doc)
                    retriever_scores.append(result.get('combined_score', 0.0))
            
            if retrieved_documents:
                # Use build_smart_context to process context, avoiding excessive truncation
                context_parts = []
                summary_parts = []
                
                for doc in retrieved_documents[:10]:
                    content = doc.content
                    if not isinstance(content, str):
                        if isinstance(content, dict):
                            content = content.get('context', content.get('content', str(content)))
                        else:
                            content = str(content)
                    
                    # Use build_smart_context to process context
                    processed_context = build_smart_context("", content, question)
                    context_parts.append(processed_context)
                
                context_str = "\n\n".join(context_parts)
                
                # Dynamically select prompt template based on query language
                try:
                    from langdetect import detect
                    query_language = detect(question)
                    is_chinese_query = query_language.startswith('zh')
                except:
                    # If language detection fails, determine based on query content
                    is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
                
                if is_chinese_query:
                    # Chinese query uses the same prompt generation logic as chinese_llm_evaluation.py
                    summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                    chinese_template = getattr(self.config.data, 'chinese_prompt_template', 'multi_stage_chinese_template_with_fewshot.txt')
                    print(f"Using Chinese template from config: {chinese_template}")
                    
                    # Use get_messages_for_test and _convert_messages_to_chatml
                    messages = get_messages_for_test(summary, context_str, question, chinese_template)
                    prompt = _convert_messages_to_chatml(messages)
                else:
                    # English query: use English prompt template
                    try:
                        # Import English prompt handling function from RAG system
                        from xlm.components.rag_system.rag_system import get_final_prompt_messages_english, _convert_messages_to_chatml
                        
                        # Use English template from config
                        english_template = getattr(self.config.data, 'english_prompt_template', 'unified_english_template_no_think.txt')
                        messages = get_final_prompt_messages_english(context_str, question, english_template)
                        prompt = _convert_messages_to_chatml(messages)
                        print(f"Using English template from config: {english_template}")
                    except Exception as e:
                        print(f"English template loading failed: {e}, using simple English prompt")
                        prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
                
                generated_responses = self.generator.generate(texts=[prompt])
                answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                context_data = []
                for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                    context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                answer = f"[Multi-Stage Retrieval: ZH] {answer}"
                return answer, context_data
            else:
                return "No relevant documents found.", []
                
        except Exception as e:
            return self._fallback_retrieval(question, 'zh')
    
    def _process_english_with_multi_stage(self, question: str, reranker_checkbox: bool) -> tuple[str, List[List[str]]]:
        """Use multi-stage retrieval system to process English query"""
        if not self.english_retrieval_system:
            return self._fallback_retrieval(question, 'en')
        
        try:
            print(f"🔍 Starting English multi-stage retrieval...")
            print(f"📋 Query: {question}")
            print(f"⚙️ Configuration parameters: retrieval_top_k={self.config.retriever.retrieval_top_k}, rerank_top_k={self.config.retriever.rerank_top_k}")
            
            # Perform multi-stage retrieval
            results = self.english_retrieval_system.search(
                query=question,
                top_k=self.config.retriever.rerank_top_k  # Use rerank_top_k from config
            )
            
            # Convert to DocumentWithMetadata format
            retrieved_documents = []
            retriever_scores = []
            
            for result in results:
                doc = DocumentWithMetadata(
                    content=result.get('context', result.get('content', '')),
                    metadata=DocumentMetadata(
                        source=result.get('source', 'Unknown'),
                        created_at="",
                        author="",
                        language="english"
                    )
                )
                retrieved_documents.append(doc)
                retriever_scores.append(result.get('combined_score', 0.0))
            
            if retrieved_documents:
                context_str = "\n\n".join([doc.content for doc in retrieved_documents[:10]])
                
                # Dynamically select prompt template based on query language
                try:
                    from langdetect import detect
                    query_language = detect(question)
                    is_chinese_query = query_language.startswith('zh')
                except:
                    # If language detection fails, determine based on query content
                    is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
                
                if is_chinese_query:
                    # Chinese query uses Chinese prompt template
                    summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                    prompt = template_loader.format_template(
                        "multi_stage_chinese_template",
                        summary=summary,
                        context=context_str,
                        query=question
                    )
                    if prompt is None:
                        # Fallback to simple Chinese prompt
                        prompt = f"Based on the following context, answer the question:\n\n{context_str}\n\nQuestion: {question}\n\nAnswer:"
                else:
                    # English query uses English prompt template
                    prompt = template_loader.format_template(
                        "rag_english_template",
                        context=context_str, 
                        question=question
                    )
                    if prompt is None:
                        # Fallback to simple English prompt
                        prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
                
                generated_responses = self.generator.generate(texts=[prompt])
                answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                context_data = []
                for doc, score in zip(retrieved_documents[:self.config.retriever.rerank_top_k], retriever_scores[:self.config.retriever.rerank_top_k]):
                    context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                answer = f"[Multi-Stage Retrieval: EN] {answer}"
                return answer, context_data
            else:
                return "No relevant documents found.", []
                
        except Exception as e:
            return self._fallback_retrieval(question, 'en')
    
    def _fallback_retrieval(self, question: str, language: str) -> tuple[str, List[List[str]]]:
        """Fallback to traditional retrieval"""
        if self.rag_system is None:
            return "Traditional RAG system not initialized, cannot process query", []
        
        try:
            # Run RAG system
            rag_output = self.rag_system.run(user_input=question, language=language)
            
            # Generate answer
            if rag_output.retrieved_documents:
                # Build context
                context_str = "\n\n".join([doc.content for doc in rag_output.retrieved_documents[:10]])
                
                # Dynamically select prompt template based on query language
                try:
                    from langdetect import detect
                    query_language = detect(question)
                    is_chinese_query = query_language.startswith('zh')
                except:
                    # If language detection fails, determine based on query content
                    is_chinese_query = any('\u4e00' <= char <= '\u9fff' for char in question)
                
                if is_chinese_query:
                    # Chinese query uses Chinese prompt template
                    summary = context_str[:200] + "..." if len(context_str) > 200 else context_str
                    prompt = template_loader.format_template(
                        "multi_stage_chinese_template",
                        summary=summary,
                        context=context_str,
                        query=question
                    )
                    if prompt is None:
                        # Fallback to simple Chinese prompt
                        prompt = f"Based on the following context, answer the question:\n\n{context_str}\n\nQuestion: {question}\n\nAnswer:"
                else:
                    # English query uses English prompt template
                    prompt = template_loader.format_template(
                        "rag_english_template",
                        context=context_str, 
                        question=question
                    )
                    if prompt is None:
                        # Fallback to simple English prompt
                        prompt = f"Context: {context_str}\nQuestion: {question}\nAnswer:"
                
                # Generate answer
                generated_responses = self.generator.generate(texts=[prompt])
                answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                
                # Prepare context data
                context_data = []
                for doc, score in zip(rag_output.retrieved_documents[:self.config.retriever.rerank_top_k], rag_output.retriever_scores[:self.config.retriever.rerank_top_k]):
                    # Uniformly display only the content field, not question and answer
                    content = doc.content
                    # Ensure content is string type
                    if not isinstance(content, str):
                        if isinstance(content, dict):
                            # If it's a dictionary, try to extract context or content field
                            content = content.get('context', content.get('content', str(content)))
                        else:
                            content = str(content)
                    
                    # Truncate long content
                    display_content = content[:500] + "..." if len(content) > 500 else content
                    context_data.append([f"{score:.4f}", display_content])
                
                # Add retrieval system information
                answer = f"[Multi-Stage Retrieval: {language.upper()}] {answer}"
                
                return answer, context_data
            else:
                return "No relevant documents found.", []
                
        except Exception as e:
            return f"Retrieval failed: {str(e)}", []
    
    def launch(self, share: bool = False):
        """Launch UI interface"""
        self.interface.launch(share=share) 