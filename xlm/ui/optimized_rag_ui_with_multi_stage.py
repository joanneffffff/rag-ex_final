#!/usr/bin/env python3
"""
Optimized RAG UI with Multi-Stage Retrieval System Integration
"""

import os
import sys
import re
from pathlib import Path
from typing import List, Optional, Tuple
import gradio as gr
import numpy as np
import torch
import faiss
from langdetect import detect, LangDetectException

# Add parent directory to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata, RagOutput
from xlm.components.rag_system.rag_system import RagSystem
from xlm.components.generator.generator import Generator
from xlm.components.retriever.reranker import QwenReranker
from xlm.utils.visualizer import Visualizer
from xlm.registry.retriever import load_enhanced_retriever
from xlm.registry.generator import load_generator
from config.parameters import Config, EncoderConfig, RetrieverConfig, ModalityConfig, EMBEDDING_CACHE_DIR, RERANKER_CACHE_DIR

# å¯¼å…¥å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
try:
    sys.path.append(str(Path(__file__).parent.parent.parent / "alphafin_data_process"))
    from multi_stage_retrieval_final import MultiStageRetrievalSystem
    MULTI_STAGE_AVAILABLE = True
except ImportError as e:
    print(f"å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}")
    MULTI_STAGE_AVAILABLE = False

def try_load_qwen_reranker(model_name, cache_dir=None):
    """å°è¯•åŠ è½½Qwené‡æ’åºå™¨ï¼Œæ”¯æŒGPU 0å’ŒCPUå›é€€"""
    try:
        import torch
        from transformers import AutoTokenizer, AutoModelForSequenceClassification
        
        # ç¡®ä¿cache_diræ˜¯æœ‰æ•ˆçš„å­—ç¬¦ä¸²
        if cache_dir is None:
            cache_dir = RERANKER_CACHE_DIR
        
        print(f"å°è¯•ä½¿ç”¨8bité‡åŒ–åŠ è½½QwenReranker...")
        print(f"åŠ è½½é‡æ’åºå™¨æ¨¡å‹: {model_name}")
        
        # é¦–å…ˆå°è¯•GPU 0
        if torch.cuda.is_available() and torch.cuda.device_count() > 0:
            device = "cuda:0"  # æ˜ç¡®æŒ‡å®šGPU 0
            print(f"- è®¾å¤‡: {device}")
            print(f"- ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"- é‡åŒ–: True (8bit)")
            print(f"- Flash Attention: False")
            
            try:
                # æ£€æŸ¥GPU 0çš„å¯ç”¨å†…å­˜
                gpu_memory = torch.cuda.get_device_properties(0).total_memory
                allocated_memory = torch.cuda.memory_allocated(0)
                free_memory = gpu_memory - allocated_memory
                
                print(f"- GPU 0 æ€»å†…å­˜: {gpu_memory / 1024**3:.1f}GB")
                print(f"- GPU 0 å·²ç”¨å†…å­˜: {allocated_memory / 1024**3:.1f}GB")
                print(f"- GPU 0 å¯ç”¨å†…å­˜: {free_memory / 1024**3:.1f}GB")
                
                # å¦‚æœå¯ç”¨å†…å­˜å°‘äº2GBï¼Œå›é€€åˆ°CPU
                if free_memory < 2 * 1024**3:  # 2GB
                    print("- GPU 0 å†…å­˜ä¸è¶³ï¼Œå›é€€åˆ°CPU")
                    device = "cpu"
                else:
                    # å°è¯•åœ¨GPU 0ä¸ŠåŠ è½½
                    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
                    model = AutoModelForSequenceClassification.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        torch_dtype=torch.float16,
                        device_map="auto",
                        load_in_8bit=True
                    )
                    print("é‡åŒ–æ¨¡å‹å·²è‡ªåŠ¨è®¾ç½®åˆ°è®¾å¤‡ï¼Œè·³è¿‡æ‰‹åŠ¨ç§»åŠ¨")
                    print("é‡æ’åºå™¨æ¨¡å‹åŠ è½½å®Œæˆ")
                    print("é‡åŒ–åŠ è½½æˆåŠŸï¼")
                    return QwenReranker(model_name, device=device, cache_dir=cache_dir)
                    
            except Exception as e:
                print(f"- GPU 0 åŠ è½½å¤±è´¥: {e}")
                print("- å›é€€åˆ°CPU")
                device = "cpu"
        
        # CPUå›é€€
        if device == "cpu" or not torch.cuda.is_available():
            device = "cpu"
            print(f"- è®¾å¤‡: {device}")
            print(f"- ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"- é‡åŒ–: False (CPUæ¨¡å¼)")
            
            tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)
            model = AutoModelForSequenceClassification.from_pretrained(
                model_name,
                cache_dir=cache_dir,
                torch_dtype=torch.float32
            )
            model = model.to(device)
            print("é‡æ’åºå™¨æ¨¡å‹åŠ è½½å®Œæˆ")
            print("CPUåŠ è½½æˆåŠŸï¼")
            return QwenReranker(model_name, device=device, cache_dir=cache_dir)
            
    except Exception as e:
        print(f"åŠ è½½é‡æ’åºå™¨å¤±è´¥: {e}")
        return None

class OptimizedRagUIWithMultiStage:
    def __init__(
        self,
        cache_dir: Optional[str] = None,
        use_faiss: bool = True,
        enable_reranker: bool = True,
        use_existing_embedding_index: Optional[bool] = None,
        max_alphafin_chunks: Optional[int] = None,
        window_title: str = "Financial Explainable RAG System with Multi-Stage Retrieval",
        title: str = "Financial Explainable RAG System with Multi-Stage Retrieval",
        examples: Optional[List[List[str]]] = None,
    ):
        # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        config = Config()
        self.cache_dir = EMBEDDING_CACHE_DIR if (not cache_dir or not isinstance(cache_dir, str)) else cache_dir
        self.use_faiss = use_faiss
        self.enable_reranker = enable_reranker
        self.use_existing_embedding_index = use_existing_embedding_index if use_existing_embedding_index is not None else config.retriever.use_existing_embedding_index
        self.max_alphafin_chunks = max_alphafin_chunks if max_alphafin_chunks is not None else config.retriever.max_alphafin_chunks
        self.window_title = window_title
        self.title = title
        self.examples = examples or [
            ["å¾·èµ›ç”µæ± (000049)çš„ä¸‹ä¸€å­£åº¦æ”¶ç›Šé¢„æµ‹å¦‚ä½•ï¼Ÿ"],
            ["ç”¨å‹ç½‘ç»œ2019å¹´çš„æ¯è‚¡ç»è¥æ´»åŠ¨äº§ç”Ÿçš„ç°é‡‘æµé‡å‡€é¢æ˜¯å¤šå°‘ï¼Ÿ"],
            ["ä¸‹æœˆè‚¡ä»·èƒ½å¦ä¸Šæ¶¨?"],
            ["How was internally developed software capitalised?"],
            ["Why did the Operating revenues decreased from 2018 to 2019?"],
            ["Why did the Operating costs decreased from 2018 to 2019?"]
        ]
        
        # Set environment variables for model caching
        os.environ['TRANSFORMERS_CACHE'] = os.path.join(self.cache_dir, 'transformers')
        os.environ['HF_HOME'] = self.cache_dir
        os.environ['HF_DATASETS_CACHE'] = os.path.join(self.cache_dir, 'datasets')
        
        # Initialize system components
        self._init_components()
        
        # Create Gradio interface
        self.interface = self._create_interface()
    
    def _init_components(self):
        """Initialize RAG system components with multi-stage retrieval"""
        print("\nStep 1. Initializing Multi-Stage Retrieval System...")
        
        # ä½¿ç”¨configä¸­çš„å¹³å°æ„ŸçŸ¥é…ç½®
        config = Config()
        
        # åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
        if MULTI_STAGE_AVAILABLE:
            try:
                # ä¸­æ–‡æ•°æ®è·¯å¾„
                chinese_data_path = Path("data/alphafin/alphafin_merged_generated_qa.json")
                
                if chinese_data_path.exists():
                    print("âœ… åˆå§‹åŒ–ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
                    self.chinese_retrieval_system = MultiStageRetrievalSystem(
                        data_path=chinese_data_path,
                        dataset_type="chinese",
                        use_existing_config=True
                    )
                    print("âœ… ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"âŒ ä¸­æ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {chinese_data_path}")
                    self.chinese_retrieval_system = None
                
                # è‹±æ–‡æ•°æ®è·¯å¾„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                english_data_path = Path("data/tatqa/processed_data.json")  # éœ€è¦é¢„å¤„ç†
                if english_data_path.exists():
                    print("âœ… åˆå§‹åŒ–è‹±æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
                    self.english_retrieval_system = MultiStageRetrievalSystem(
                        data_path=english_data_path,
                        dataset_type="english",
                        use_existing_config=True
                    )
                    print("âœ… è‹±æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
                else:
                    print(f"âš ï¸ è‹±æ–‡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {english_data_path}")
                    self.english_retrieval_system = None
                
            except Exception as e:
                print(f"âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                self.chinese_retrieval_system = None
                self.english_retrieval_system = None
        else:
            print("âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ£€ç´¢")
            self.chinese_retrieval_system = None
            self.english_retrieval_system = None
        
        print("\nStep 2. Loading generator...")
        self.generator = load_generator(
            generator_model_name=config.generator.model_name,
            use_local_llm=True,
            use_gpu=True,
            gpu_device="cuda:1",
            cache_dir=config.generator.cache_dir
        )
        
        print("\nStep 3. Loading visualizer...")
        self.visualizer = Visualizer(show_mid_features=True)
        
        print("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    def _create_interface(self) -> gr.Blocks:
        """Create optimized Gradio interface"""
        with gr.Blocks(
            title=self.window_title
        ) as interface:
            # æ ‡é¢˜
            gr.Markdown(f"# {self.title}")
            
            # è¾“å…¥åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=4):
                    datasource = gr.Radio(
                        choices=["TatQA", "AlphaFin", "Both"],
                        value="Both",
                        label="Data Source"
                    )
                    
            with gr.Row():
                with gr.Column(scale=4):
                    question_input = gr.Textbox(
                        show_label=False,
                        placeholder="Enter your question",
                        label="Question",
                        lines=3
                    )
            
            # æ§åˆ¶æŒ‰é’®åŒºåŸŸ
            with gr.Row():
                with gr.Column(scale=1):
                    reranker_checkbox = gr.Checkbox(
                        label="Enable Reranker",
                        value=True,
                        interactive=True
                    )
                with gr.Column(scale=1):
                    submit_btn = gr.Button("Submit")
            
            # ä½¿ç”¨æ ‡ç­¾é¡µåˆ†ç¦»æ˜¾ç¤º
            with gr.Tabs():
                # å›ç­”æ ‡ç­¾é¡µ
                with gr.TabItem("Answer"):
                    answer_output = gr.Textbox(
                        show_label=False,
                        interactive=False,
                        label="Generated Response",
                        lines=5
                    )
                
                # è§£é‡Šæ ‡ç­¾é¡µ
                with gr.TabItem("Explanation"):
                    context_output = gr.Dataframe(
                        headers=["Score", "Context"],
                        datatype=["number", "str"],
                        label="Retrieved Contexts",
                        interactive=False
                    )

            # æ·»åŠ ç¤ºä¾‹é—®é¢˜
            gr.Examples(
                examples=self.examples,
                inputs=[question_input],
                label="Example Questions"
            )

            # ç»‘å®šäº‹ä»¶
            submit_btn.click(
                self._process_question,
                inputs=[question_input, datasource, reranker_checkbox],
                outputs=[answer_output, context_output]
            )
            
            return interface
    
    def _process_question(
        self,
        question: str,
        datasource: str,
        reranker_checkbox: bool
    ) -> tuple[str, List[List[str]]]:
        """Process user question using multi-stage retrieval"""
        if not question.strip():
            return "Please enter a question.", []
        
        print(f"\nProcessing question: {question}")
        print(f"Data source: {datasource}")
        
        # Detect language
        try:
            from langdetect import detect
            lang = detect(question)
            # ä¸­æ–‡å­—ç¬¦å…œåº•
            def is_chinese(text):
                return len([c for c in text if '\u4e00' <= c <= '\u9fff']) > max(1, len(text) // 6)
            if lang.startswith('zh') or (lang == 'ko' and is_chinese(question)) or is_chinese(question):
                language = 'zh'
            else:
                language = 'en'
        except Exception as e:
            print(f"Language detection failed: {e}, fallback to English.")
            language = 'en'
        
        print(f"Detected language: {language}")
        
        # ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
        if MULTI_STAGE_AVAILABLE:
            try:
                # æ ¹æ®è¯­è¨€é€‰æ‹©æ£€ç´¢ç³»ç»Ÿ
                if language == 'zh' and self.chinese_retrieval_system:
                    print("ğŸ” ä½¿ç”¨ä¸­æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
                    retrieval_system = self.chinese_retrieval_system
                    
                    # å°è¯•æå–å…¬å¸åç§°å’Œè‚¡ç¥¨ä»£ç ç”¨äºå…ƒæ•°æ®è¿‡æ»¤
                    company_name = None
                    stock_code = None
                    
                    # ç®€å•çš„å®ä½“æå–ï¼ˆå¯ä»¥æ”¹è¿›ï¼‰
                    import re
                    # æå–è‚¡ç¥¨ä»£ç 
                    stock_match = re.search(r'\((\d{6})\)', question)
                    if stock_match:
                        stock_code = stock_match.group(1)
                    
                    # æå–å…¬å¸åç§°ï¼ˆç®€å•å®ç°ï¼‰
                    company_patterns = [
                        r'([^ï¼Œã€‚ï¼Ÿ\s]+(?:è‚¡ä»½|é›†å›¢|å…¬å¸|æœ‰é™|ç§‘æŠ€|ç½‘ç»œ|é“¶è¡Œ|è¯åˆ¸|ä¿é™©))',
                        r'([^ï¼Œã€‚ï¼Ÿ\s]+(?:è‚¡ä»½|é›†å›¢|å…¬å¸|æœ‰é™|ç§‘æŠ€|ç½‘ç»œ|é“¶è¡Œ|è¯åˆ¸|ä¿é™©)[^ï¼Œã€‚ï¼Ÿ\s]*)'
                    ]
                    
                    for pattern in company_patterns:
                        company_match = re.search(pattern, question)
                        if company_match:
                            company_name = company_match.group(1)
                            break
                    
                    # æ‰§è¡Œå¤šé˜¶æ®µæ£€ç´¢
                    results = retrieval_system.search(
                        query=question,
                        company_name=company_name,
                        stock_code=stock_code,
                        top_k=20
                    )
                    
                    # è½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼
                    retrieved_documents = []
                    retriever_scores = []
                    
                    for result in results:
                        # åˆ›å»ºDocumentWithMetadataå¯¹è±¡
                        doc = DocumentWithMetadata(
                            content=result.get('original_context', result.get('summary', '')),
                            metadata=DocumentMetadata(
                                source=result.get('company_name', 'Unknown'),
                                created_at="",
                                author="",
                                language="chinese"
                            )
                        )
                        retrieved_documents.append(doc)
                        retriever_scores.append(result.get('combined_score', 0.0))
                    
                    print(f"âœ… å¤šé˜¶æ®µæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(retrieved_documents)} æ¡ç»“æœ")
                    
                elif language == 'en' and self.english_retrieval_system:
                    print("ğŸ” ä½¿ç”¨è‹±æ–‡å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ...")
                    retrieval_system = self.english_retrieval_system
                    
                    # è‹±æ–‡æ•°æ®ä¸æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤
                    results = retrieval_system.search(
                        query=question,
                        top_k=20
                    )
                    
                    # è½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼
                    retrieved_documents = []
                    retriever_scores = []
                    
                    for result in results:
                        doc = DocumentWithMetadata(
                            content=result.get('context', result.get('content', '')),
                            metadata=DocumentMetadata(
                                source=result.get('source', 'Unknown'),
                                created_at="",
                                author="",
                                language="english"
                            )
                        )
                        retrieved_documents.append(doc)
                        retriever_scores.append(result.get('combined_score', 0.0))
                    
                    print(f"âœ… å¤šé˜¶æ®µæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(retrieved_documents)} æ¡ç»“æœ")
                    
                else:
                    print("âš ï¸ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢")
                    return self._fallback_retrieval(question, language)
                
                # ç”Ÿæˆç­”æ¡ˆ
                if retrieved_documents:
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context_str = "\n\n".join([doc.content for doc in retrieved_documents[:10]])
                    
                    # æ ¹æ®è¯­è¨€é€‰æ‹©promptæ¨¡æ¿
                    if language == 'zh':
                        from xlm.components.rag_system.rag_system import PROMPT_TEMPLATE_ZH
                        prompt = PROMPT_TEMPLATE_ZH.format(context=context_str, question=question)
                    else:
                        from xlm.components.rag_system.rag_system import PROMPT_TEMPLATE_EN
                        prompt = PROMPT_TEMPLATE_EN.format(context=context_str, question=question)
                    
                    # ç”Ÿæˆç­”æ¡ˆ
                    generated_responses = self.generator.generate(texts=[prompt])
                    answer = generated_responses[0] if generated_responses else "Unable to generate answer"
                    
                    # å‡†å¤‡ä¸Šä¸‹æ–‡æ•°æ®
                    context_data = []
                    for doc, score in zip(retrieved_documents[:20], retriever_scores[:20]):
                        context_data.append([f"{score:.4f}", doc.content[:500] + "..." if len(doc.content) > 500 else doc.content])
                    
                    # æ·»åŠ æ£€ç´¢ç³»ç»Ÿä¿¡æ¯
                    answer = f"[Multi-Stage Retrieval: {language.upper()}] {answer}"
                    
                    return answer, context_data
                else:
                    return "No relevant documents found.", []
                    
            except Exception as e:
                print(f"âŒ å¤šé˜¶æ®µæ£€ç´¢å¤±è´¥: {e}")
                return self._fallback_retrieval(question, language)
        else:
            print("âŒ å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ£€ç´¢")
            return self._fallback_retrieval(question, language)
    
    def _fallback_retrieval(self, question: str, language: str) -> tuple[str, List[List[str]]]:
        """ä¼ ç»Ÿæ£€ç´¢å›é€€æ–¹æ³•"""
        # è¿™é‡Œå¯ä»¥å®ç°ä¼ ç»Ÿçš„æ£€ç´¢é€»è¾‘
        return f"[Fallback Retrieval: {language.upper()}] Traditional retrieval not implemented yet.", []
    
    def launch(self, share: bool = False):
        """Launch UI interface"""
        self.interface.launch(share=share) 