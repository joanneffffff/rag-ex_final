#!/usr/bin/env python3
"""
æµ‹è¯•å¢å¼ºçš„Promptæ¨¡æ¿æ•ˆæœ
- å¤§å¹…å¢åŠ max_new_tokensåˆ°600
- æ·»åŠ æ›´å¼ºç¡¬çš„è´Ÿé¢çº¦æŸ
- éªŒè¯Few-Shot Promptçš„æ•ˆæœ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from xlm.components.generator.local_llm_generator import LocalLLMGenerator
from config.parameters import Config

def test_enhanced_prompts():
    """æµ‹è¯•å¢å¼ºçš„Promptæ¨¡æ¿æ•ˆæœ"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºçš„Promptæ¨¡æ¿æ•ˆæœ...")
    
    # åŠ è½½é…ç½®
    config = Config()
    print(f"ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å‹: {config.generator.model_name}")
    print(f"é‡åŒ–ç±»å‹: {config.generator.quantization_type}")
    print(f"max_new_tokens: {config.generator.max_new_tokens}")
    print()
    
    # æµ‹è¯•é—®é¢˜
    test_question = "é¦–é’¢è‚¡ä»½åœ¨2023å¹´ä¸ŠåŠå¹´çš„ä¸šç»©è¡¨ç°å¦‚ä½•ï¼Ÿ"
    test_context = """é¦–é’¢è‚¡ä»½2023å¹´ä¸ŠåŠå¹´å®ç°è¥ä¸šæ”¶å…¥1,234.56äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™21.7%ï¼›å‡€åˆ©æ¶¦ä¸º-3.17äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™77.14%ï¼Œæ¯è‚¡äºæŸ0.06å…ƒã€‚å…¬å¸è¡¨ç¤ºå°†é€šè¿‡æ³¨å…¥ä¼˜è´¨èµ„äº§æ¥æ”¹å–„é•¿æœŸç›ˆåˆ©èƒ½åŠ›ï¼Œå¹¶æ‰¿è¯ºå›é¦ˆè‚¡ä¸œã€‚"""
    
    print("ğŸ”§ åŠ è½½ç”Ÿæˆå™¨...")
    generator = LocalLLMGenerator()
    
    print("=" * 60)
    print("ğŸ”§ æµ‹è¯•å¢å¼ºçš„Few-Shot Prompt")
    print("=" * 60)
    
    # æ‰‹åŠ¨æ„å»ºpromptæ¥æµ‹è¯•
    from xlm.components.rag_system.rag_system import PROMPT_TEMPLATE_ZH
    
    prompt = PROMPT_TEMPLATE_ZH.format(context=test_context, question=test_question)
    print(f"Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
    print()
    print("ğŸ“ Promptå†…å®¹:")
    print("-" * 40)
    print(prompt)
    print("-" * 40)
    print()
    
    print("ğŸ¤– ç”Ÿæˆå›ç­”ä¸­...")
    response = generator.generate([prompt])[0]
    
    print("âœ… å¢å¼ºçš„Few-Shot å›ç­”:")
    print("-" * 40)
    print(response)
    print("-" * 40)
    
    # åˆ†æå›ç­”è´¨é‡
    print()
    print("ğŸ“Š å›ç­”åˆ†æ:")
    print(f"å­—ç¬¦æ•°: {len(response)}")
    print(f"è¯æ•°: {len(response.split())}")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å…ƒè¯„è®º
    meta_indicators = [
        "æ ¹æ®ä¸Šè¿°", "æ ¹æ®ä»¥ä¸Š", "ç»¼ä¸Šæ‰€è¿°", "æ€»ç»“", "æ³¨æ„", "è¯„åˆ†", 
        "ä¿®æ”¹å", "ç‰ˆæœ¬", "è§„å®š", "ç­”æ¡ˆåº”", "ä¸å¾—è¶…è¿‡", "è¯·æ ¹æ®",
        "ä¸Šè¿°å†…å®¹", "ä¸Šè¿°è§„åˆ™", "ä¸Šè¿°ç¤ºä¾‹", "ä¸Šè¿°åˆ†æ"
    ]
    
    has_meta = any(indicator in response for indicator in meta_indicators)
    print(f"åŒ…å«å…ƒè¯„è®º: {'æ˜¯' if has_meta else 'å¦'}")
    
    # æ£€æŸ¥æ˜¯å¦å®Œæ•´
    is_complete = not response.endswith("...") and len(response) > 50
    print(f"å›ç­”å®Œæ•´: {'æ˜¯' if is_complete else 'å¦'}")
    
    # è´¨é‡è¯„åˆ†
    quality_score = 0
    if is_complete:
        quality_score += 2
    if not has_meta:
        quality_score += 2
    if len(response) < 200:  # ç®€æ´
        quality_score += 1
    if "é¦–é’¢è‚¡ä»½" in response and "2023å¹´" in response:
        quality_score += 1
    
    print(f"è´¨é‡è¯„åˆ†: {quality_score}/6")
    
    if quality_score >= 4:
        print("ğŸ‰ å›ç­”è´¨é‡è‰¯å¥½ï¼")
    elif quality_score >= 2:
        print("âš ï¸ å›ç­”è´¨é‡ä¸€èˆ¬ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print("âŒ å›ç­”è´¨é‡è¾ƒå·®ï¼Œéœ€è¦å¤§å¹…æ”¹è¿›")
    
    print()
    print("=" * 60)
    print("ğŸ† æµ‹è¯•æ€»ç»“")
    print("=" * 60)
    print(f"âœ… max_new_tokenså·²å¢åŠ åˆ°: {config.generator.max_new_tokens}")
    print(f"âœ… å·²æ·»åŠ å¼ºç¡¬è´Ÿé¢çº¦æŸ")
    print(f"âœ… ä½¿ç”¨Few-Shot Promptæ¨¡æ¿")
    print(f"âœ… å›ç­”å­—ç¬¦æ•°: {len(response)}")
    print(f"âœ… è´¨é‡è¯„åˆ†: {quality_score}/6")
    
    if quality_score >= 4:
        print("ğŸ‰ å¢å¼ºçš„Promptæ¨¡æ¿æ•ˆæœè‰¯å¥½ï¼")
    else:
        print("ğŸ’¡ å»ºè®®è¿›ä¸€æ­¥è°ƒæ•´Promptæ¨¡æ¿æˆ–ç”Ÿæˆå‚æ•°")

if __name__ == "__main__":
    test_enhanced_prompts() 