#!/bin/bash

# é€šç”¨ç¼–ç å™¨å¾®è°ƒè„šæœ¬
# æ”¯æŒAlphaFinä¸­æ–‡å’ŒTAT-QAè‹±æ–‡æ•°æ®é›†

echo "=========================================="
echo "é€šç”¨ç¼–ç å™¨å¾®è°ƒè„šæœ¬"
echo "æ”¯æŒ: AlphaFin(ä¸­æ–‡) | TAT-QA(è‹±æ–‡)"
echo "=========================================="

# æ£€æŸ¥å‚æ•°
if [ $# -eq 0 ]; then
    echo "ç”¨æ³•: $0 [alphafin|tatqa] [å¯é€‰: å¿«é€Ÿæµ‹è¯•æ¨¡å¼]"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0 alphafin          # AlphaFinå®Œæ•´å¾®è°ƒ"
    echo "  $0 tatqa             # TAT-QAå®Œæ•´å¾®è°ƒ"
    echo "  $0 alphafin quick    # AlphaFinå¿«é€Ÿæµ‹è¯•"
    echo "  $0 tatqa quick       # TAT-QAå¿«é€Ÿæµ‹è¯•"
    exit 1
fi

DATASET=$1
QUICK_MODE=$2

# è®¾ç½®ç¯å¢ƒå˜é‡
export CUDA_VISIBLE_DEVICES=0
export TOKENIZERS_PARALLELISM=false

# æ ¹æ®æ•°æ®é›†é…ç½®å‚æ•°
if [ "$DATASET" = "alphafin" ]; then
    echo "ğŸ¯ é…ç½®AlphaFinä¸­æ–‡æ•°æ®é›†..."
    
    # æ•°æ®è·¯å¾„
    TRAIN_DATA="evaluate_mrr/alphafin_train_qc.jsonl"
    EVAL_DATA="evaluate_mrr/alphafin_eval.jsonl"
    
    # æ¨¡å‹é…ç½®
    BASE_MODEL="Langboat/mengzi-bert-base-fin"
    OUTPUT_MODEL_PATH="./models/finetuned_alphafin_encoder_summary"
    
    # è®­ç»ƒå‚æ•°
    BATCH_SIZE=16
    EPOCHS=5
    LEARNING_RATE=2e-5
    MAX_SEQ_LENGTH=512
    
    # å¾®è°ƒè„šæœ¬
    FINETUNE_SCRIPT="encoder_finetune_evaluate/finetune_encoder.py"
    
elif [ "$DATASET" = "tatqa" ]; then
    echo "ğŸ¯ é…ç½®TAT-QAè‹±æ–‡æ•°æ®é›†..."
    
    # æ•°æ®è·¯å¾„
    TRAIN_DATA="evaluate_mrr/tatqa_train_qc.jsonl"
    EVAL_DATA="evaluate_mrr/tatqa_eval.jsonl"
    RAW_DATA="evaluate_mrr/tatqa_knowledge_base.jsonl"
    
    # æ¨¡å‹é…ç½®
    # BASE_MODEL="ProsusAI/finbert"
    BASE_MODEL="/users/sgjfei3/data/manually_downloaded_models/finbert"
    OUTPUT_MODEL_PATH="./models/finetuned_tatqa_encoder_table_textualized"
    
    # è®­ç»ƒå‚æ•°
    BATCH_SIZE=32
    EPOCHS=5
    LEARNING_RATE=2e-5
    MAX_SEQ_LENGTH=512
    
    # å¾®è°ƒè„šæœ¬
    FINETUNE_SCRIPT="encoder_finetune_evaluate/finetune_encoder.py"
    
else
    echo "âŒ é”™è¯¯: ä¸æ”¯æŒçš„æ•°æ®é›† '$DATASET'"
    echo "æ”¯æŒçš„æ•°æ®é›†: alphafin, tatqa"
    exit 1
fi

# å¿«é€Ÿæµ‹è¯•æ¨¡å¼é…ç½®
if [ "$QUICK_MODE" = "quick" ]; then
    echo "âš¡ å¿«é€Ÿæµ‹è¯•æ¨¡å¼"
    LIMIT_TRAIN=1000
    LIMIT_EVAL=50
    EPOCHS=1
    BATCH_SIZE=8
else
    echo "ğŸš€ å®Œæ•´è®­ç»ƒæ¨¡å¼"
    LIMIT_TRAIN=0  # ä½¿ç”¨å…¨éƒ¨æ•°æ®
    LIMIT_EVAL=100
fi

EVAL_TOP_K=100

echo ""
echo "é…ç½®ä¿¡æ¯ï¼š"
echo "  æ•°æ®é›†: $DATASET"
echo "  è®­ç»ƒæ•°æ®: $TRAIN_DATA"
echo "  è¯„ä¼°æ•°æ®: $EVAL_DATA"

echo "  åŸºç¡€æ¨¡å‹: $BASE_MODEL"
echo "  è¾“å‡ºè·¯å¾„: $OUTPUT_MODEL_PATH"
echo "  å¾®è°ƒè„šæœ¬: $FINETUNE_SCRIPT"
echo "  æ‰¹æ¬¡å¤§å°: $BATCH_SIZE"
echo "  è®­ç»ƒè½®æ•°: $EPOCHS"
echo "  å­¦ä¹ ç‡: $LEARNING_RATE"
echo "  æœ€å¤§åºåˆ—é•¿åº¦: $MAX_SEQ_LENGTH"
echo "  è¯„ä¼°Top-K: $EVAL_TOP_K"
echo "  è®­ç»ƒæ•°æ®é™åˆ¶: $LIMIT_TRAIN"
echo "  è¯„ä¼°æ•°æ®é™åˆ¶: $LIMIT_EVAL"
echo ""

# æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
echo "æ£€æŸ¥æ•°æ®æ–‡ä»¶..."
if [ ! -f "$TRAIN_DATA" ]; then
    echo "âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: $TRAIN_DATA"
    exit 1
fi

if [ ! -f "$EVAL_DATA" ]; then
    echo "âŒ è¯„ä¼°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: $EVAL_DATA"
    exit 1
fi

echo "âœ… æ‰€æœ‰æ•°æ®æ–‡ä»¶æ£€æŸ¥é€šè¿‡"
echo ""

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$OUTPUT_MODEL_PATH"

# å¼€å§‹å¾®è°ƒ
echo "å¼€å§‹${DATASET^^}ç¼–ç å™¨å¾®è°ƒ..."
echo "æ—¶é—´: $(date)"
echo ""

if [ "$DATASET" = "alphafin" ]; then
    # AlphaFinä½¿ç”¨ç®€åŒ–å¾®è°ƒè„šæœ¬
    python "$FINETUNE_SCRIPT" \
        --model_name "$BASE_MODEL" \
        --train_jsonl "$TRAIN_DATA" \
        --eval_jsonl "$EVAL_DATA" \
        --output_dir "$OUTPUT_MODEL_PATH" \
        --batch_size $BATCH_SIZE \
        --epochs $EPOCHS \
        --learning_rate $LEARNING_RATE \
        --max_seq_length $MAX_SEQ_LENGTH \
        --max_samples $LIMIT_TRAIN
else
    # TAT-QAä½¿ç”¨è‹±æ–‡å¾®è°ƒè„šæœ¬
    python "$FINETUNE_SCRIPT" \
        --model_name "$BASE_MODEL" \
        --train_jsonl "$TRAIN_DATA" \
        --eval_jsonl "$EVAL_DATA" \
        --output_dir "$OUTPUT_MODEL_PATH" \
        --batch_size $BATCH_SIZE \
        --epochs $EPOCHS \
        --max_samples $LIMIT_TRAIN \
        --eval_steps 500
fi

# æ£€æŸ¥å¾®è°ƒæ˜¯å¦æˆåŠŸ
if [ $? -eq 0 ]; then
    echo ""
    echo "âœ… ${DATASET^^}ç¼–ç å™¨å¾®è°ƒå®Œæˆï¼"
    echo "æ¨¡å‹ä¿å­˜è·¯å¾„: $OUTPUT_MODEL_PATH"
    echo "å®Œæˆæ—¶é—´: $(date)"
    echo ""
    
    # ==================== æ·»åŠ è¯„ä¼°æ­¥éª¤ ====================
    echo "ğŸ” å¼€å§‹æ¨¡å‹è¯„ä¼°..."
    echo "æ—¶é—´: $(date)"
    echo ""
    
    # åˆ›å»ºè¯„ä¼°ç»“æœç›®å½•
    EVAL_OUTPUT_DIR="evaluation_results/${DATASET}_encoder_$(date +%Y%m%d_%H%M%S)"
    mkdir -p "$EVAL_OUTPUT_DIR"
    
    if [ "$DATASET" = "alphafin" ]; then
        echo "ğŸ“Š è¿è¡ŒAlphaFinç¼–ç å™¨è¯„ä¼°..."
        
        # 1. è¿è¡ŒMRRè¯„ä¼°
        echo "1. è¿è¡ŒMRRè¯„ä¼°..."
        python encoder_finetune_evaluate/evaluate_chinese_encoder_reranker_mrr.py \
            --encoder_model_name "$OUTPUT_MODEL_PATH" \
            --eval_jsonl "$EVAL_DATA" \
            --base_raw_data_path "data/alphafin/alphafin_final_clean.json" \
            --output_dir "$EVAL_OUTPUT_DIR" \
            --max_samples $LIMIT_EVAL
        
        # 2. è¿è¡Œæ£€ç´¢è¯„ä¼°
        echo "2. è¿è¡Œæ£€ç´¢è¯„ä¼°..."
        python alphafin_data_process/run_retrieval_evaluation_background.py \
            --eval_data_path "$EVAL_DATA" \
            --output_dir "$EVAL_OUTPUT_DIR/retrieval_eval" \
            --modes baseline prefilter reranker \
            --max_samples $LIMIT_EVAL
        
    else
        echo "ğŸ“Š è¿è¡ŒTAT-QAç¼–ç å™¨è¯„ä¼°..."
        
        # 1. è¿è¡Œç¼–ç å™¨è¯„ä¼°
        echo "1. è¿è¡Œç¼–ç å™¨è¯„ä¼°..."
        python encoder_finetune_evaluate/run_encoder_eval.py \
            --model_name "$OUTPUT_MODEL_PATH" \
            --eval_jsonl "$EVAL_DATA" \
            --max_samples $LIMIT_EVAL \
            --output_dir "$EVAL_OUTPUT_DIR"
        
        # 2. è¿è¡ŒTAT-QAæ£€ç´¢è¯„ä¼°
        echo "2. è¿è¡ŒTAT-QAæ£€ç´¢è¯„ä¼°..."
        python alphafin_data_process/run_tatqa_retrieval_evaluation.py \
            --mode reranker \
            --encoder_model_path "$OUTPUT_MODEL_PATH" \
            --output_dir "$EVAL_OUTPUT_DIR/tatqa_eval" \
            --max_samples $LIMIT_EVAL
    fi
    
    echo ""
    echo "âœ… è¯„ä¼°å®Œæˆï¼"
    echo "è¯„ä¼°ç»“æœä¿å­˜åœ¨: $EVAL_OUTPUT_DIR"
    echo "å®Œæˆæ—¶é—´: $(date)"
    echo ""
    
    # ==================== æ˜¾ç¤ºä¸‹ä¸€æ­¥å»ºè®® ====================
    echo "ä¸‹ä¸€æ­¥å»ºè®®ï¼š"
    echo ""
    
    if [ "$DATASET" = "alphafin" ]; then
        echo "1. æŸ¥çœ‹è¯„ä¼°ç»“æœï¼š"
        echo "   ls -la $EVAL_OUTPUT_DIR"
        echo ""
        echo "2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡ŒRAGç³»ç»Ÿæµ‹è¯•ï¼š"
        echo "   python run_optimized_ui.py"
        echo ""
        echo "3. è¿è¡Œå®Œæ•´æ£€ç´¢è¯„ä¼°ï¼š"
        echo "   python alphafin_data_process/run_retrieval_evaluation_background.py \\"
        echo "       --eval_data_path data/alphafin/eval_data_100_from_corpus.jsonl \\"
        echo "       --output_dir alphafin_data_process/evaluation_results \\"
        echo "       --modes baseline prefilter reranker"
    else
        echo "1. æŸ¥çœ‹è¯„ä¼°ç»“æœï¼š"
        echo "   ls -la $EVAL_OUTPUT_DIR"
        echo ""
        echo "2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡ŒRAGç³»ç»Ÿæµ‹è¯•ï¼š"
        echo "   python run_optimized_ui.py"
        echo ""
        echo "3. è¿è¡ŒTAT-QAå®Œæ•´è¯„ä¼°ï¼š"
        echo "   python alphafin_data_process/run_tatqa_retrieval_evaluation.py \\"
        echo "       --mode reranker \\"
        echo "       --encoder_model_path $OUTPUT_MODEL_PATH"
    fi
    
else
    echo ""
    echo "âŒ ${DATASET^^}ç¼–ç å™¨å¾®è°ƒå¤±è´¥ï¼"
    echo "è¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯å¹¶é‡è¯•ã€‚"
    exit 1
fi 