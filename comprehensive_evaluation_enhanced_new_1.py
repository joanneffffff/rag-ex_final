#!/usr/bin/env python3
"""
æœ€ç»ˆç‰ˆå…¨é¢è¯„ä¼°è„šæœ¬ - ä¿®å¤ç‰ˆæœ¬2
ä¿®å¤Promptæ¨¡æ¿ä¸ç­”æ¡ˆæå–é€»è¾‘çš„ä¸åŒ¹é…é—®é¢˜ï¼Œç»Ÿä¸€ä½¿ç”¨<answer>...</answer>æ ‡ç­¾æ ¼å¼
ç›®æ ‡ï¼šä½¿ç”Ÿæˆå™¨(Fin-R1)çš„F1åˆ†æ•°æ¢å¤åˆ°å¹¶ç¨³å®šåœ¨0.4ä»¥ä¸Š
"""

import warnings
import logging
import os
import json
import re
import torch
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Union
import time
import argparse
from collections import Counter
from difflib import SequenceMatcher
import sys
import gc
import signal
import atexit

# ç¯å¢ƒè®¾ç½®
warnings.filterwarnings("ignore")
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ['TRANSFORMERS_VERBOSITY'] = 'error'

try:
    from tqdm import tqdm
except ImportError:
    print("âŒ tqdmæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install tqdm")
    sys.exit(1)

sys.path.append(str(Path(__file__).parent))

try:
    from xlm.components.generator.local_llm_generator import LocalLLMGenerator
    USE_RAG_GENERATOR = True
    print("âœ… ä½¿ç”¨RAGç³»ç»Ÿçš„LocalLLMGenerator")
except ImportError:
    USE_RAG_GENERATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥RAGç³»ç»Ÿçš„LocalLLMGeneratorï¼Œè„šæœ¬å°†æ— æ³•è¿è¡Œã€‚")
    sys.exit(1)

try:
    from xlm.utils.context_separator import context_separator
    USE_CONTEXT_SEPARATOR = True
    print("âœ… ä½¿ç”¨ä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½")
except ImportError:
    USE_CONTEXT_SEPARATOR = False
    print("âš ï¸ æ— æ³•å¯¼å…¥ä¸Šä¸‹æ–‡åˆ†ç¦»å™¨ï¼Œå°†ä½¿ç”¨åŸå§‹ä¸Šä¸‹æ–‡å¤„ç†æ–¹å¼")

# ===================================================================
# èµ„æºæ¸…ç†æœºåˆ¶
# ===================================================================

class ResourceManager:
    """èµ„æºç®¡ç†å™¨ï¼Œç¡®ä¿ç¨‹åºç»“æŸæ—¶æ­£ç¡®æ¸…ç†èµ„æº"""
    
    def __init__(self):
        self.generator = None
        self.cleanup_registered = False
        self._register_cleanup()
    
    def _register_cleanup(self):
        """æ³¨å†Œæ¸…ç†å‡½æ•°"""
        if not self.cleanup_registered:
            atexit.register(self.cleanup_resources)
            signal.signal(signal.SIGINT, self._signal_handler)
            signal.signal(signal.SIGTERM, self._signal_handler)
            self.cleanup_registered = True
    
    def _signal_handler(self, signum, frame):
        """ä¿¡å·å¤„ç†å™¨"""
        print(f"\nğŸ›‘ æ”¶åˆ°ä¿¡å· {signum}ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
        self.cleanup_resources()
        sys.exit(0)
    
    def set_generator(self, generator):
        """è®¾ç½®ç”Ÿæˆå™¨å¼•ç”¨"""
        self.generator = generator
    
    def cleanup_resources(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        print("ğŸ§¹ å¼€å§‹æ¸…ç†èµ„æº...")
        
        try:
            # 1. æ¸…ç†ç”Ÿæˆå™¨
            if self.generator:
                print("ğŸ—‘ï¸ æ¸…ç†ç”Ÿæˆå™¨...")
                if hasattr(self.generator, 'model'):
                    del self.generator.model
                if hasattr(self.generator, 'tokenizer'):
                    del self.generator.tokenizer
                self.generator = None
            
            # 2. æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                print("ğŸ—‘ï¸ æ¸…ç†GPUå†…å­˜...")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
                # æ˜¾ç¤ºæ¸…ç†åçš„å†…å­˜çŠ¶æ€
                for i in range(torch.cuda.device_count()):
                    allocated = torch.cuda.memory_allocated(i) / 1024**3
                    cached = torch.cuda.memory_reserved(i) / 1024**3
                    print(f"   GPU {i}: å·²åˆ†é… {allocated:.2f}GB, ç¼“å­˜ {cached:.2f}GB")
            
            # 3. å¼ºåˆ¶åƒåœ¾å›æ”¶
            print("ğŸ—‘ï¸ å¼ºåˆ¶åƒåœ¾å›æ”¶...")
            gc.collect()
            
            print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âš ï¸ èµ„æºæ¸…ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

# å…¨å±€èµ„æºç®¡ç†å™¨
resource_manager = ResourceManager()

# ===================================================================
# æ ¸å¿ƒè¾…åŠ©å‡½æ•° - ä¿®å¤ç‰ˆæœ¬
# ===================================================================

def _shared_text_standardizer(text: str) -> str:
    """
    Helper function to standardize text for both answer extraction and F1 score calculation.
    Ensures commas are removed, negative numbers in parentheses are handled,
    percentage signs are handled, common introductory phrases are removed,
    trailing punctuation is removed, and currency symbols/unit words are removed.
    """
    text = text.strip()
    # Remove commas from numbers
    text = text.replace(',', '')
    # Handle negative numbers in parentheses (e.g., "(33)" -> "-33")
    if text.startswith('(') and text.endswith(')'):
        text = '-' + text[1:-1]
    
    # Remove common introductory phrases (should be less frequent with optimized prompt)
    # This list should be aligned with phrases you *don't* want in the final answer.
    text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
    
    # Remove trailing punctuation (e.g., periods, commas, semicolons, but ensure percentage sign is removed if numeric)
    # This regex is made more aggressive to ensure any trailing punctuation OR a standalone % is removed.
    text = re.sub(r'[\.ã€‚;,]$', '', text).strip() # General trailing punctuation
    
    # <<< NEW ADDITION / REVISION >>>: Explicitly remove percentage sign at the end of a numeric string
    # This helps when expected_answer is "0.2" but generated is "0.2%"
    if text.endswith('%'):
        # Check if the part before % is numeric (allows for negative, decimal numbers)
        numeric_part_match = re.fullmatch(r'([-+]?[\d.]+)', text[:-1].strip())
        if numeric_part_match:
            text = numeric_part_match.group(1) # Keep only the numeric part
        else:
            text = text[:-1].strip() # If not purely numeric, just strip the %
    
    # Remove common currency symbols and unit words
    text = re.sub(r'(\$|million|billion|usd|eur|pounds|Â£)', '', text, flags=re.IGNORECASE).strip()

    return text

def extract_final_answer_with_rescue(raw_output: str) -> str:
    """
    Extracts the final answer from the model's raw output.
    It exclusively looks for the <answer> tag. If not found or empty, it returns a specific phrase.
    This version implements the "I cannot find the answer" explicit fallback.
    """
    cleaned_output = raw_output.strip()
    # Define the specific phrase for "answer not found" in English
    NOT_FOUND_REPLY_ENGLISH = "I cannot find the answer in the provided context."

    # 1. ç²¾ç¡®å¯»æ‰¾ <answer>...</answer> æ ‡ç­¾
    # Use non-greedy matching .*? to capture content inside the tag
    match = re.search(r'<answer>(.*?)</answer>', cleaned_output, re.DOTALL)
    
    if match:
        content = match.group(1).strip()
        # Ensure extracted content is not empty or an empty tag itself (e.g., <answer></answer>)
        if content and content.lower() not in ['<final></final>', '<answer></answer>', '<final-answer></final-answer>']:
            return _shared_text_standardizer(content)
    
    # If no valid <answer> structure is found or content is invalid,
    # return the specific "not found" phrase.
    return NOT_FOUND_REPLY_ENGLISH

def calculate_f1_score(prediction: str, ground_truth: str) -> float:
    # Define the specific phrase for "answer not found" (standardized lowercase form)
    NOT_FOUND_ANSWER_PHRASE = "i cannot find the answer in the provided context."

    # Standardize both prediction and ground truth texts
    normalized_prediction = _shared_text_standardizer(prediction).lower()
    normalized_ground_truth = _shared_text_standardizer(ground_truth).lower()

    # 1. Handle cases where the model explicitly states "I cannot find the answer..."
    if normalized_prediction == NOT_FOUND_ANSWER_PHRASE:
        # If the ground truth is also "I cannot find the answer...", it's a correct match
        if normalized_ground_truth == NOT_FOUND_ANSWER_PHRASE:
            return 1.0
        # Otherwise, the model said "I cannot find..." but the answer exists, so it's an error
        else:
            return 0.0
    
    # 2. Handle cases where the ground truth is "I cannot find the answer...", but the model gave a factual answer (which is an error)
    if normalized_ground_truth == NOT_FOUND_ANSWER_PHRASE:
        return 0.0

    # 3. Standard F1 score calculation for factual answers
    prediction_tokens = normalized_prediction.split()
    ground_truth_tokens = normalized_ground_truth.split()

    if not ground_truth_tokens: 
        return 1.0 if not prediction_tokens else 0.0 # If ground truth is empty, predict empty for 1.0 F1
    if not prediction_tokens: 
        return 0.0 # If prediction is empty, but ground truth is not, 0.0 F1

    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0: 
        return 0.0

    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def _parse_template_string_to_messages(template_full_string: str, query: str, context: str = "", table_context: str = "", text_context: str = "") -> List[Dict[str, str]]:
    """
    è§£ææ¨¡æ¿å­—ç¬¦ä¸²å¹¶æ ¼å¼åŒ–ä¸ºæ¶ˆæ¯åˆ—è¡¨ã€‚
    æ ¹æ®å½“å‰æ¨¡æ¿è®¾è®¡ï¼Œå¤„ç†åˆ†ç¦»çš„ä¸Šä¸‹æ–‡ï¼Œå¹¶ç²¾ç¡®è§£æ SYSTEM å’Œ USER å—ã€‚
    """
    # æ›¿æ¢æ¨¡æ¿ä¸­çš„å ä½ç¬¦
    formatted_template = template_full_string.replace("{query}", query)
    formatted_template = formatted_template.replace("{table_context}", table_context)
    formatted_template = formatted_template.replace("{text_context}", text_context)
    
    # --- å…³é”®çš„æ­£åˆ™è¡¨è¾¾å¼è°ƒæ•´ ---
    # SYSTEM å—ï¼šä» ===SYSTEM=== ååˆ°ä¸‹ä¸€ä¸ª ===USER=== æˆ–å­—ç¬¦ä¸²æœ«å°¾
    # ä½¿ç”¨ \s* åŒ¹é…å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼æˆ–æ¢è¡Œç¬¦
    system_match = re.search(r'===SYSTEM===\s*\n(.*?)(?=\n===USER===|\Z)', formatted_template, re.DOTALL)
    # USER å—ï¼šä» ===USER=== ååˆ°å­—ç¬¦ä¸²æœ«å°¾
    # ä½¿ç”¨ \s* åŒ¹é…å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼æˆ–æ¢è¡Œç¬¦
    user_match = re.search(r'===USER===\s*\n(.*?)\Z', formatted_template, re.DOTALL) 
    
    messages = []
    
    if system_match:
        system_content = system_match.group(1).strip()
        if system_content:
            messages.append({"role": "system", "content": system_content})
    
    if user_match:
        user_content = user_match.group(1).strip()
        if user_content:
            messages.append({"role": "user", "content": user_content})
    
    return messages

def load_and_format_template(template_name: str, context: str, query: str) -> List[Dict[str, str]]:
    """
    åŠ è½½å¹¶æ ¼å¼åŒ–æŒ‡å®šçš„promptæ¨¡æ¿ï¼ˆç»Ÿä¸€ä¸Šä¸‹æ–‡ï¼‰
    æ³¨æ„ï¼šæ­¤å‡½æ•°åœ¨æ–°æµç¨‹ä¸­å¯èƒ½ä¸ç›´æ¥ä½¿ç”¨ï¼Œä½†å…¶é»˜è®¤æ¨¡æ¿å·²æ›´æ–°ã€‚
    """
    template_path = Path("data/prompt_templates") / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template_full_string = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}")
        # ä½¿ç”¨ä¸æ–°ç­–ç•¥ä¸€è‡´çš„é»˜è®¤æ¨¡æ¿
        template_full_string = """===SYSTEM===
You are a helpful assistant that answers questions based on the provided context.
Your ONLY output MUST be the final, direct, and concise answer enclosed STRICTLY within an <answer> tag. You MUST NOT include any thinking process, intermediate steps, or conversational filler outside this tag.

===USER===
Context: {context_content}

Question: {query}
<answer>""" # ç¡®ä¿è¿™é‡Œæ˜¯ <answer>
    
    return _parse_template_string_to_messages(template_full_string, query, context=context)

def load_and_format_template_with_separated_context(template_name: str, table_context: str, text_context: str, query: str) -> List[Dict[str, str]]:
    """
    åŠ è½½å¹¶æ ¼å¼åŒ–æŒ‡å®šçš„promptæ¨¡æ¿ï¼ˆåˆ†ç¦»çš„ä¸Šä¸‹æ–‡ï¼‰
    """
    template_path = Path("data/prompt_templates") / template_name
    try:
        with open(template_path, 'r', encoding='utf-8') as f:
            template_full_string = f.read().strip()
    except FileNotFoundError:
        print(f"âŒ æ¨¡æ¿æ–‡ä»¶æœªæ‰¾åˆ°: {template_path}")
        # ä½¿ç”¨ä¸æ–°ç­–ç•¥ä¸€è‡´çš„é»˜è®¤æ¨¡æ¿
        template_full_string = """===SYSTEM===
You are a helpful assistant that answers questions based on the provided context.
Your ONLY output MUST be the final, direct, and concise answer enclosed STRICTLY within an <answer> tag. You MUST NOT include any thinking process, intermediate steps, or conversational filler outside this tag.

===USER===
Table Context: {table_context}

Text Context: {text_context}

Question: {query}
<answer>""" # ç¡®ä¿è¿™é‡Œæ˜¯ <answer>
    
    return _parse_template_string_to_messages(template_full_string, query, table_context=table_context, text_context=text_context)

def get_final_prompt(context: str, query: str) -> List[Dict[str, str]]:
    """ä½¿ç”¨ç»Ÿä¸€çš„æ¨¡æ¿ï¼ŒåŒ…å«contextåˆ†ç¦»åŠŸèƒ½"""
    # ä½¿ç”¨æˆ‘ä»¬æœ€æ–°ç¡®å®šçš„ Prompt æ¨¡æ¿æ–‡ä»¶å
    template_file = 'unified_english_template_no_think.txt' # **è¯·åŠ¡å¿…ç¡®ä¿è¿™ä¸ªæ–‡ä»¶åä¸æ‚¨ä¿å­˜çš„æ¨¡æ¿æ–‡ä»¶åä¸€è‡´**
    
    # å¼ºåˆ¶ä½¿ç”¨ä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½
    if not USE_CONTEXT_SEPARATOR:
        print("âŒ æœªå¯ç”¨ä¸Šä¸‹æ–‡åˆ†ç¦»åŠŸèƒ½ï¼Œä½†å½“å‰Promptæ¨¡æ¿è¦æ±‚åˆ†ç¦»ä¸Šä¸‹æ–‡ã€‚è„šæœ¬å°†æ— æ³•ç»§ç»­ã€‚")
        raise NotImplementedError("å½“å‰Promptæ¨¡æ¿è¦æ±‚ä¸Šä¸‹æ–‡åˆ†ç¦»ï¼Œä½†åŠŸèƒ½æœªå¯ç”¨ã€‚è¯·æ£€æŸ¥ USE_CONTEXT_SEPARATOR é…ç½®ã€‚")

    try:
        # åˆ†ç¦»ä¸Šä¸‹æ–‡
        separated = context_separator.separate_context(context)
        
        # æ ¼å¼åŒ– prompt å‚æ•°
        prompt_params = context_separator.format_for_prompt(separated, query)
        
        # ä½¿ç”¨åˆ†ç¦»åçš„ä¸Šä¸‹æ–‡æ ¼å¼åŒ–æ¨¡æ¿
        return load_and_format_template_with_separated_context(
            template_file, 
            prompt_params["table_context"], 
            prompt_params["text_context"], 
            query
        )
    except Exception as e:
        # å¦‚æœä¸Šä¸‹æ–‡åˆ†ç¦»å¤±è´¥ï¼Œæ— æ³•æ„é€ å¸¦æœ‰ table_context/text_context çš„ promptï¼Œ
        # å¹¶ä¸”æ–°æ¨¡æ¿ä¸å…¼å®¹ç»Ÿä¸€ä¸Šä¸‹æ–‡ï¼Œåˆ™ç›´æ¥æŠ›å‡ºé”™è¯¯ã€‚
        print(f"âŒ ä¸Šä¸‹æ–‡åˆ†ç¦»å¤±è´¥ï¼Œä¸”æ— æ³•ä½¿ç”¨å…¼å®¹çš„Promptæ¨¡æ¿ã€‚é”™è¯¯: {e}", file=sys.stderr)
        raise # å¼ºåˆ¶æŠ›å‡ºé”™è¯¯ï¼Œå› ä¸ºæ— æ³•æ­£ç¡®æ„é€  Prompt

# ===================================================================
# æ ¸å¿ƒè¯„ä¼°ç±»
# ===================================================================

class ComprehensiveEvaluator:
    def __init__(self, model_name: str, device: str):
        self.model_name = model_name
        self.device = device
        self.max_new_tokens = 4096
        print("ğŸ”„ åŠ è½½æ¨¡å‹...")
        self.generator = LocalLLMGenerator(model_name=self.model_name, device=self.device)
        
        # æ³¨å†Œç”Ÿæˆå™¨åˆ°èµ„æºç®¡ç†å™¨
        resource_manager.set_generator(self.generator)
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")

    def run_evaluation(self, eval_data: List[Dict[str, Any]]) -> Dict[str, Any]:
        results = []
        start_time = time.time()
        pbar = tqdm(eval_data, desc="ğŸ” è¯„ä¼°æ ·æœ¬", unit="ä¸ª")

        try:
            for sample in pbar:
                result = self._evaluate_single_sample(sample)
                results.append(result)
                
                # å®šæœŸæ¸…ç†GPUå†…å­˜
                if len(results) % 5 == 0:
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œå¼€å§‹æ¸…ç†èµ„æº...")
            resource_manager.cleanup_resources()
            raise
        except Exception as e:
            print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            resource_manager.cleanup_resources()
            raise
        finally:
            total_time = time.time() - start_time
            print(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"ğŸ“Š å¤„ç†äº† {len(results)} ä¸ªç»“æœ")
        
        analysis = self.analyze_results(results)
        analysis['performance'] = {'total_time': total_time, 'avg_time_per_sample': total_time / len(results) if results else 0}
        return {"results": results, "analysis": analysis}

    def _evaluate_single_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        try:
            table_len_ratio, text_len_ratio = 0.0, 0.0 # åˆå§‹åŒ–ä¸º0

            # ç¡®ä¿åœ¨è°ƒç”¨ get_final_prompt ä¹‹å‰è·å–è¿™äº›æ¯”ä¾‹ï¼Œå¦‚æœ context_separator æ”¯æŒ
            if USE_CONTEXT_SEPARATOR:
                try:
                    # å‡è®¾ context_separator.separate_context èƒ½å¤Ÿè¿”å›åŸå§‹çš„ table/text é•¿åº¦
                    # è¿™éœ€è¦ context_separator æ¨¡å—çš„æ”¯æŒ
                    separated_data = context_separator.separate_context(sample["context"])
                    # å‡è®¾ separated_data åŒ…å« 'table_content_length' å’Œ 'text_content_length'
                    # å¦‚æœ context_separator ä¸æä¾›ï¼Œåˆ™æ­¤éƒ¨åˆ†è·³è¿‡æˆ–è‡ªè¡Œè®¡ç®—
                    total_context_length = len(sample["context"])
                    if total_context_length > 0:
                        # è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹ï¼Œæ‚¨éœ€è¦æ ¹æ® context_separator çš„å®é™…è¿”å›æ¥è·å–é•¿åº¦
                        # ä¾‹å¦‚ï¼štable_len = len(separated_data.get('table_context_raw', ''))
                        # text_len = len(separated_data.get('text_context_raw', ''))
                        # table_len_ratio = table_len / total_context_length
                        # text_len_ratio = text_len / total_context_length
                        pass # å¦‚æœ context_separator æ— æ³•æä¾›ï¼Œåˆ™ä¿æŒä¸º0
                except Exception as e:
                    # åˆ†ç¦»å¤±è´¥ä¼šåœ¨ get_final_prompt ä¸­å¤„ç†ï¼Œè¿™é‡Œæ•è·æ˜¯ä¸ºäº†é¿å…é‡å¤é”™è¯¯ä¿¡æ¯
                    pass

            messages = get_final_prompt(sample["context"], sample["query"])
            
            # è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
            prompt_text = self._convert_messages_to_text(messages)

            gen_start_time = time.time()
            # generator.generate æœŸæœ› List[str]ï¼Œæ‰€ä»¥ç”¨ [prompt_text] åŒ…è£¹
            generation_result = self.generator.generate([prompt_text])[0]
            gen_time = time.time() - gen_start_time
            
            final_answer_to_evaluate = extract_final_answer_with_rescue(generation_result)
            evaluation = self._evaluate_quality(final_answer_to_evaluate, sample["answer"])
            
            return {
                "query": sample["query"],
                "expected_answer": sample["answer"],
                "generated_answer": generation_result,      # åŸå§‹æ¨¡å‹è¾“å‡º
                "extracted_answer": final_answer_to_evaluate, # ç»è¿‡ extract_final_answer_with_rescue å¤„ç†åçš„ç­”æ¡ˆ
                "evaluation": evaluation,
                "answer_from": sample.get("answer_from", "unknown"), 
                "predicted_answer_from": "separated_context_answer_only",
                "decision_confidence": 1.0,
                "is_difficult_decision": False,
                "context_type": "separated_context",
                "content_ratio": {"table_ratio": table_len_ratio, "text_ratio": text_len_ratio, "mixed_ratio": table_len_ratio + text_len_ratio}, # mixed_ratio å¯ä»¥æ˜¯ä¸¤è€…ä¹‹å’Œ
                "generation_time": gen_time
            }
        except Exception as e:
            # è¯¦ç»†æ‰“å°é”™è¯¯ä¿¡æ¯ï¼ŒåŒ…æ‹¬å‘ç”Ÿé”™è¯¯çš„æ ·æœ¬IDæˆ–æŸ¥è¯¢
            sample_id = sample.get("id", "N/A") # å¦‚æœä½ çš„æ ·æœ¬æœ‰ID
            print(f"\nâŒ å¤„ç†æ ·æœ¬å¤±è´¥ (ID: {sample_id}, Query: '{sample.get('query', 'N/A')[:50]}...', Error: {e})", file=sys.stderr)
            return {
                "query": sample["query"], 
                "expected_answer": sample["answer"], 
                "error": str(e),
                "context": sample.get("context", "N/A"), # åŒ…å«ä¸Šä¸‹æ–‡ä»¥ä¾›è°ƒè¯•
                "evaluation": {"exact_match": False, "f1_score": 0.0},
                "generated_answer": "", # ç¡®ä¿æœ‰æ­¤å­—æ®µï¼Œå³ä½¿æ˜¯é”™è¯¯æ ·æœ¬
                "extracted_answer": ""  # ç¡®ä¿æœ‰æ­¤å­—æ®µï¼Œå³ä½¿æ˜¯é”™è¯¯æ ·æœ¬
            }

    def _evaluate_quality(self, generated: str, expected: str) -> Dict[str, Any]:
        exact_match = generated.strip().lower() == expected.strip().lower()
        f1 = calculate_f1_score(generated, expected)
        return {"exact_match": exact_match, "f1_score": f1}

    def _convert_messages_to_text(self, messages: List[Dict[str, str]]) -> str:
        """
        å°† messages åˆ—è¡¨è½¬æ¢ä¸ºFin-R1ï¼ˆQwen2.5 basedï¼‰æœŸæœ›çš„ChatMLæ ¼å¼å­—ç¬¦ä¸²ã€‚
        """
        if not messages:
            return ""
        
        # Qwen2.5 ä½¿ç”¨ ChatML æ ¼å¼
        formatted_prompt = ""
        for message in messages:
            role = message.get("role", "")
            content = message.get("content", "")
            
            if role == "system":
                formatted_prompt += f"<|im_start|>system\n{content.strip()}<|im_end|>\n"
            elif role == "user":
                formatted_prompt += f"<|im_start|>user\n{content.strip()}<|im_end|>\n"
            elif role == "assistant": # ç¤ºä¾‹ä¸­å¯èƒ½ä¼šæœ‰ assistant è½®æ¬¡
                formatted_prompt += f"<|im_start|>assistant\n{content.strip()}<|im_end|>\n"
        
        # <<< å…³é”®ä¿®æ”¹ >>>
        # ç§»é™¤æˆ–æ³¨é‡Šæ‰è¿™ä¸€è¡Œï¼Œå› ä¸ºPromptæ¨¡æ¿çš„æœ«å°¾æ˜¯ç”¨æˆ·æ¶ˆæ¯çš„ä¸€éƒ¨åˆ†ï¼ˆä»¥ <think> ç»“å°¾ï¼‰ï¼Œ
        # æ¨¡å‹ä¼šæ ¹æ®ChatMLçš„è§„åˆ™è‡ªåŠ¨åœ¨ç”¨æˆ·æ¶ˆæ¯åç”ŸæˆåŠ©æ‰‹å›åº”ï¼Œæ— éœ€é¢å¤–æ·»åŠ  <|im_start|>assistantã€‚
        # formatted_prompt += "<|im_start|>assistant\n" 
        
        return formatted_prompt

    def analyze_results(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè¯„ä¼°ç»“æœ"""
        if not results:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆç»“æœå¯åˆ†æ"}
        
        # è¿‡æ»¤æ‰æœ‰é”™è¯¯çš„æ ·æœ¬
        valid_results = [r for r in results if "error" not in r]
        error_count = len(results) - len(valid_results)
        
        if not valid_results:
            return {"error": "æ‰€æœ‰æ ·æœ¬éƒ½æœ‰é”™è¯¯", "error_count": error_count}
        
        # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
        exact_matches = sum(1 for r in valid_results if r["evaluation"]["exact_match"])
        f1_scores = [r["evaluation"]["f1_score"] for r in valid_results]
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        avg_f1 = np.mean(f1_scores)
        std_f1 = np.std(f1_scores)
        median_f1 = np.median(f1_scores)
        
        # è®¡ç®—æ—¶é—´ç»Ÿè®¡
        generation_times = [r.get("generation_time", 0) for r in valid_results]
        avg_gen_time = np.mean(generation_times) if generation_times else 0
        
        return {
            "total_samples": len(results),
            "valid_samples": len(valid_results),
            "error_samples": error_count,
            "exact_match_count": exact_matches,
            "exact_match_rate": exact_matches / len(valid_results),
            "avg_f1_score": avg_f1,
            "std_f1_score": std_f1,
            "median_f1_score": median_f1,
            "min_f1_score": min(f1_scores),
            "max_f1_score": max(f1_scores),
            "avg_generation_time": avg_gen_time,
            "performance": {
                "total_time": sum(generation_times),
                "avg_time_per_sample": avg_gen_time
            }
        }

    def print_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*60)
        
        if "error" in analysis:
            print(f"âŒ åˆ†æå¤±è´¥: {analysis['error']}")
            return
        
        print(f"ğŸ“ˆ æ ·æœ¬ç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {analysis['total_samples']}")
        print(f"   æœ‰æ•ˆæ ·æœ¬: {analysis['valid_samples']}")
        print(f"   é”™è¯¯æ ·æœ¬: {analysis['error_samples']}")
        
        print(f"\nğŸ¯ å‡†ç¡®ç‡æŒ‡æ ‡:")
        print(f"   ç²¾ç¡®åŒ¹é…æ•°: {analysis['exact_match_count']}")
        print(f"   ç²¾ç¡®åŒ¹é…ç‡: {analysis['exact_match_rate']:.4f} ({analysis['exact_match_rate']*100:.2f}%)")
        
        print(f"\nğŸ“Š F1åˆ†æ•°ç»Ÿè®¡:")
        print(f"   å¹³å‡F1: {analysis['avg_f1_score']:.4f}")
        print(f"   æ ‡å‡†å·®: {analysis['std_f1_score']:.4f}")
        print(f"   ä¸­ä½æ•°: {analysis['median_f1_score']:.4f}")
        print(f"   æœ€å°å€¼: {analysis['min_f1_score']:.4f}")
        print(f"   æœ€å¤§å€¼: {analysis['max_f1_score']:.4f}")
        
        print(f"\nâ±ï¸ æ€§èƒ½ç»Ÿè®¡:")
        print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {analysis['avg_generation_time']:.3f}ç§’")
        if 'performance' in analysis:
            print(f"   æ€»å¤„ç†æ—¶é—´: {analysis['performance']['total_time']:.2f}ç§’")
            print(f"   å¹³å‡æ ·æœ¬æ—¶é—´: {analysis['performance']['avg_time_per_sample']:.3f}ç§’")
        
        print("="*60)

def load_evaluation_data(data_path: str, sample_size: Optional[int] = None) -> List[Dict[str, Any]]:
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    print(f"ğŸ“– æ­£åœ¨ä» {data_path} åŠ è½½æ•°æ®...")
    
    try:
        if data_path.endswith('.jsonl'):
            data = []
            with open(data_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip(): # é¿å…ç©ºè¡Œ
                        data.append(json.loads(line))
            print(f"âœ… æˆåŠŸåŠ è½½ä¸ºJSONLï¼Œæ ·æœ¬æ•°: {len(data)}")
        else: # å‡è®¾æ˜¯ .json æ–‡ä»¶
            with open(data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼ (å…¼å®¹ TATQA å¸¸è§çš„ JSON æ ¼å¼)
            if isinstance(data, dict):
                if "data" in data: # TATQAæ•°æ®é›†é€šå¸¸æ˜¯ {"data": [...]}
                    data = data["data"]
                elif "samples" in data:
                    data = data["samples"]
                else: # å°è¯•æ‰¾åˆ°åŒ…å«åˆ—è¡¨çš„é”®
                    found_list = False
                    for key, value in data.items():
                        if isinstance(value, list):
                            data = value
                            found_list = True
                            break
                    if not found_list:
                        raise ValueError("æ— æ³•åœ¨JSONæ–‡ä»¶ä¸­æ‰¾åˆ°æ ·æœ¬æ•°æ®åˆ—è¡¨")
            if not isinstance(data, list):
                raise ValueError("æ•°æ®å¿…é¡»æ˜¯æ ·æœ¬åˆ—è¡¨æ ¼å¼")
            print(f"âœ… æˆåŠŸåŠ è½½ä¸ºJSONæ•°ç»„ï¼Œæ ·æœ¬æ•°: {len(data)}")
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if sample_size and sample_size < len(data):
            eval_data = data[:sample_size] # ä½¿ç”¨åˆ‡ç‰‡
            print(f"âœ… é™åˆ¶ä¸ºå‰ {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
        else:
            eval_data = data # å¤åˆ¶ä¸€ä»½ï¼Œé¿å…åŸæ•°æ®è¢«ä¿®æ”¹çš„é£é™©
            print(f"âœ… åŠ è½½æ‰€æœ‰ {len(eval_data)} ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°ã€‚")
        
        return eval_data
        
    except FileNotFoundError:
        print(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {data_path}")
        sys.exit(1)
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æé”™è¯¯: {e}")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½é”™è¯¯: {e}")
        sys.exit(1)

def main():
    parser = argparse.ArgumentParser(description="å…¨é¢è¯„ä¼°è„šæœ¬ - ä¿®å¤ç‰ˆæœ¬2")
    parser.add_argument("--model", type=str, default=None, help="è¦è¯„ä¼°çš„LLMåç§°ï¼ˆé»˜è®¤ä½¿ç”¨config/parameters.pyä¸­çš„è®¾ç½®ï¼‰")
    parser.add_argument("--data_path", type=str, required=True, help="è¯„ä¼°æ•°æ®é›†æ–‡ä»¶è·¯å¾„ (jsonl æˆ– json)")
    parser.add_argument("--sample_size", type=int, default=None, help="è¦è¯„ä¼°çš„éšæœºæ ·æœ¬æ•°é‡ (Noneè¡¨ç¤ºå…¨éƒ¨)")
    parser.add_argument("--device", type=str, default=None, help="è®¾å¤‡ (cuda:0/cuda:1/cpu/autoï¼Œé»˜è®¤ä½¿ç”¨config/parameters.pyä¸­çš„è®¾ç½®ï¼‰")
    
    args = parser.parse_args()
    
    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
    try:
        from config.parameters import config
        print("ğŸ“– åŠ è½½é…ç½®æ–‡ä»¶è®¾ç½®...")
        
        # è®¾ç½®æ¨¡å‹åç§°
        if args.model is None:
            args.model = config.generator.model_name
            print(f"ğŸ“– ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„æ¨¡å‹: {args.model}")
        else:
            print(f"ğŸ“– ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æ¨¡å‹: {args.model}")
        
        # è®¾ç½®è®¾å¤‡
        if args.device is None:
            args.device = config.generator.device
            print(f"ğŸ“– ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾å¤‡: {args.device}")
        else:
            print(f"ğŸ“– ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„è®¾å¤‡: {args.device}")
            
    except ImportError:
        print("âš ï¸ æ— æ³•å¯¼å…¥config/parameters.pyï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®")
        if args.model is None:
            args.model = "SUFE-AIFLM-Lab/Fin-R1"
        if args.device is None:
            args.device = "cuda:0"
    
    # è®¾å¤‡è®¾ç½®
    device = args.device or "cuda:0"  # ç¡®ä¿deviceä¸ä¸ºNone
    if device == "auto":
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
    elif device and device.startswith("cuda"): # æ£€æŸ¥æ˜¯å¦ä»¥cudaå¼€å¤´ï¼Œå…è®¸ cuda:0, cuda:1 ç­‰
        try:
            if not torch.cuda.is_available():
                print("âŒ CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°CPU")
                device = "cpu"
            else:
                # å°è¯•è§£æå…·ä½“çš„GPU ID
                device_id = int(device.split(':')[1]) if ':' in device else 0
                if device_id >= torch.cuda.device_count():
                    print(f"âŒ GPU ID {device_id} ä¸å¯ç”¨ï¼Œæœ€å¤§IDä¸º {torch.cuda.device_count() - 1}ã€‚å›é€€åˆ°cuda:0")
                    device = "cuda:0"
                else:
                    device = f"cuda:{device_id}"
                    print(f"âœ… ä½¿ç”¨GPU: {torch.cuda.get_device_name(device_id)}")
                    print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(device_id).total_memory / 1024**3:.1f} GB")
        except (ValueError, IndexError): # å¤„ç† cuda:bad_id çš„æƒ…å†µ
            print(f"âŒ æ— æ•ˆçš„CUDAè®¾å¤‡å‚æ•° '{args.device}'ã€‚å›é€€åˆ°cuda:0")
            device = "cuda:0"
    else:
        print(f"âš ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    eval_data = load_evaluation_data(args.data_path, args.sample_size)
    
    # ç¡®ä¿æ¨¡å‹åç§°ä¸ä¸ºNone
    model_name = args.model or "SUFE-AIFLM-Lab/Fin-R1"
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ComprehensiveEvaluator(model_name, device)
    
    try:
        # è¿è¡Œè¯„ä¼°
        results = evaluator.run_evaluation(eval_data)
        
        # æ‰“å°æ‘˜è¦
        evaluator.print_summary(results["analysis"])
        
        # ä¿å­˜ç»“æœ
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_file = f"comprehensive_evaluation_results_fixed_v2_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­è¯„ä¼°")
    except Exception as e:
        print(f"\nâŒ è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        raise
    finally:
        # ç¡®ä¿èµ„æºæ¸…ç†
        resource_manager.cleanup_resources()

if __name__ == "__main__":
    main()