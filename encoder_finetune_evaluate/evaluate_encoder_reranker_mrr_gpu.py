#!/usr/bin/env python3
"""
GPUç‰ˆæœ¬çš„TatQAæ•°æ®é›†MRRè¯„ä¼°è„šæœ¬
æ”¯æŒEncoder + FAISS + Rerankerç»„åˆè¯„ä¼°
åªæµ‹è¯•è‹±æ–‡æ•°æ®é›†ï¼Œä¸­æ–‡éƒ¨åˆ†é€šè¿‡å‚æ•°æŽ§åˆ¶ä¸è¿è¡Œ
"""

import json
import argparse
from pathlib import Path
from tqdm import tqdm
from sentence_transformers import SentenceTransformer, util
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))
from xlm.components.retriever.reranker import QwenReranker
from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata
import torch
import numpy as np
import os
import re
from typing import List, Dict, Any, Optional
from enhanced_evaluation_functions import find_correct_document_rank_enhanced

def get_question_or_query(item_dict):
    """å…¼å®¹æ€§å‡½æ•°ï¼šèŽ·å–questionæˆ–queryå­—æ®µ"""
    if "question" in item_dict:
        return item_dict["question"]
    elif "query" in item_dict:
        return item_dict["query"]
    return None

def extract_unit_from_paragraph(paragraphs):
    """ä»Žæ®µè½ä¸­æå–æ•°å€¼å•ä½"""
    for para in paragraphs:
        text = para.get("text", "") if isinstance(para, dict) else para
        match = re.search(r'dollars in (millions|billions)|in (millions|billions)', text, re.IGNORECASE)
        if match:
            unit = match.group(1) or match.group(2)
            if unit:
                return unit.lower().replace('s', '') + " USD"
    return ""

def table_to_natural_text(table_dict, caption="", unit_info=""):
    """å°†è¡¨æ ¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æ–‡æœ¬"""
    rows = table_dict.get("table", [])
    lines = []

    if caption:
        lines.append(f"Table Topic: {caption}.")

    if not rows:
        return ""

    headers = rows[0]
    data_rows = rows[1:]

    for i, row in enumerate(data_rows):
        if not row or all(str(v).strip() == "" for v in row):
            continue

        if len(row) > 1 and str(row[0]).strip() != "" and all(str(v).strip() == "" for v in row[1:]):
            lines.append(f"Table Category: {str(row[0]).strip()}.")
            continue

        row_name = str(row[0]).strip().replace('.', '')

        data_descriptions = []
        for h_idx, v in enumerate(row):
            if h_idx == 0:
                continue
            header = headers[h_idx] if h_idx < len(headers) else f"Column {h_idx+1}"
            value = str(v).strip()
            if value:
                if re.match(r'^-?\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?$|^\(\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?\)$', value):
                    formatted_value = value.replace('$', '')
                    if unit_info:
                        if formatted_value.startswith('(') and formatted_value.endswith(')'):
                             formatted_value = f"(${formatted_value[1:-1]} {unit_info})"
                        else:
                             formatted_value = f"${formatted_value} {unit_info}"
                    else:
                        formatted_value = f"${formatted_value}"
                else:
                    formatted_value = value
                data_descriptions.append(f"{header} is {formatted_value}")
        if row_name and data_descriptions:
            lines.append(f"Details for item {row_name}: {'; '.join(data_descriptions)}.")
        elif data_descriptions:
            lines.append(f"Other data item: {'; '.join(data_descriptions)}.")
        elif row_name:
            lines.append(f"Data item: {row_name}.")
    return "\n".join(lines)

def process_tatqa_to_qca_for_corpus(input_paths):
    """å¤„ç†TatQAåŽŸå§‹æ•°æ®ï¼Œæž„å»ºæ£€ç´¢åº“"""
    all_chunks = []
    global_doc_counter = 0

    for input_path in input_paths:
        if not Path(input_path).exists():
            print(f"è­¦å‘Šï¼šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {input_path}")
            continue
            
        with open(input_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            print(f"è­¦å‘Šï¼šæ–‡ä»¶ {Path(input_path).name} çš„é¡¶å±‚ç»“æž„ä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•ä½œä¸ºå•ä¸ªæ–‡æ¡£å¤„ç†ã€‚")
            data = [data]

        for i, item in tqdm(enumerate(data), desc=f"Processing docs from {Path(input_path).name} for corpus"):
            if not isinstance(item, dict):
                print(f"è­¦å‘Šï¼šæ–‡ä»¶ {Path(input_path).name} ä¸­å‘çŽ°éžå­—å…¸é¡¹ï¼Œè·³è¿‡ã€‚é¡¹å†…å®¹ï¼š{item}")
                continue
            
            doc_id = item.get("doc_id")
            if doc_id is None:
                doc_id = f"generated_doc_{global_doc_counter}_{Path(input_path).stem}_{i}"
                global_doc_counter += 1

            paragraphs = item.get("paragraphs", [])
            tables = item.get("tables", [])

            unit_info = extract_unit_from_paragraph(paragraphs)

            for p_idx, para in enumerate(paragraphs):
                para_text = para.get("text", "") if isinstance(para, dict) else para
                if para_text.strip():
                    all_chunks.append({
                        "doc_id": doc_id,
                        "chunk_id": f"para_{p_idx}",
                        "text": para_text.strip(),
                        "source_type": "paragraph"
                    })
            
            for t_idx, table in enumerate(tables):
                table_text = table_to_natural_text(table, table.get("caption", ""), unit_info)
                if table_text.strip():
                    all_chunks.append({
                        "doc_id": doc_id,
                        "chunk_id": f"table_{t_idx}",
                        "text": table_text.strip(),
                        "source_type": "table"
                    })
    return all_chunks

def compute_mrr_with_reranker_gpu(encoder_model, reranker_model, eval_data, corpus_chunks, 
                                 top_k_retrieval=100, top_k_rerank=10, batch_size=32):
    """
    GPUç‰ˆæœ¬çš„MRRè®¡ç®—å‡½æ•°ï¼Œä½¿ç”¨Encoder + Rerankerç»„åˆ
    """
    device = next(encoder_model.parameters()).device
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    query_texts = [get_question_or_query(item) for item in eval_data]
    correct_chunk_contents = [item["context"] for item in eval_data]

    print("ç¼–ç è¯„ä¼°æŸ¥è¯¢ (Queries) with Encoder...")
    query_embeddings = encoder_model.encode(
        query_texts, 
        show_progress_bar=True, 
        convert_to_tensor=True,
        batch_size=batch_size
    )
    
    print("ç¼–ç æ£€ç´¢åº“ä¸Šä¸‹æ–‡ (Chunks) with Encoder...")
    corpus_texts = [chunk["text"] for chunk in corpus_chunks]
    corpus_embeddings = encoder_model.encode(
        corpus_texts, 
        show_progress_bar=True, 
        convert_to_tensor=True,
        batch_size=batch_size
    )

    mrr_scores = []
    hit_at_1 = 0
    hit_at_3 = 0
    hit_at_5 = 0
    hit_at_10 = 0
    
    print(f"Evaluating MRR with Reranker (Retrieval Top-{top_k_retrieval}, Rerank Top-{top_k_rerank}):")
    for i, query_emb in tqdm(enumerate(query_embeddings), total=len(query_embeddings)):
        current_query_text = query_texts[i]
        correct_chunk_content = correct_chunk_contents[i]

        # 1. åˆæ­¥æ£€ç´¢ (Encoder)
        cos_scores = util.cos_sim(query_emb, corpus_embeddings)[0]
        top_retrieved_indices = torch.topk(cos_scores, k=min(top_k_retrieval, len(corpus_chunks)))[1].tolist()
        
        # å‡†å¤‡ Reranker çš„è¾“å…¥
        reranker_input_pairs = []
        retrieved_chunks = []
        for idx in top_retrieved_indices:
            chunk = corpus_chunks[idx]
            reranker_input_pairs.append([current_query_text, chunk["text"]])
            retrieved_chunks.append(chunk)

        if not reranker_input_pairs:
            mrr_scores.append(0)
            continue

        # 2. é‡æŽ’åº (Reranker) - åˆ†æ‰¹å¤„ç†ä»¥èŠ‚çœGPUå†…å­˜
        reranker_scores = []
        reranker_batch_size = 4  # è¾ƒå°çš„batch sizeä»¥é€‚åº”GPUå†…å­˜
        
        # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨ç”¨äºŽrerank
        docs_for_rerank = [pair[1] for pair in reranker_input_pairs]  # æå–æ–‡æ¡£å†…å®¹
        
        try:
            reranked_results = reranker_model.rerank(
                query=current_query_text,
                documents=docs_for_rerank,
                batch_size=reranker_batch_size
            )
            # æå–åˆ†æ•°
            reranker_scores = [score for doc, score in reranked_results]
        except Exception as e:
            print(f"é‡æŽ’åºå¤±è´¥: {e}")
            # å›žé€€åˆ°åŽŸå§‹åˆ†æ•°
            reranker_scores = [1.0] * len(docs_for_rerank)
        
        # å°† reranker scores ä¸Žå¯¹åº”çš„ chunk å…³è”èµ·æ¥ï¼Œå¹¶åˆ›å»ºDocumentWithMetadataå¯¹è±¡
        scored_retrieved_chunks_with_scores = []
        for j, score in enumerate(reranker_scores):
            chunk = retrieved_chunks[j]
            scored_retrieved_chunks_with_scores.append({
                "chunk": chunk, 
                "score": score
            })

        # æŒ‰ Reranker åˆ†æ•°é™åºæŽ’åº
        scored_retrieved_chunks_with_scores.sort(key=lambda x: x["score"], reverse=True)
        
        # æ ¹æ®æŽ’åºåŽçš„åˆ†æ•°åˆ›å»ºDocumentWithMetadataåˆ—è¡¨
        final_retrieved_docs_for_ranking: List[DocumentWithMetadata] = []
        for item in scored_retrieved_chunks_with_scores[:top_k_rerank]:
            chunk = item["chunk"]
            final_retrieved_docs_for_ranking.append(DocumentWithMetadata(
                content=chunk['text'],
                metadata=DocumentMetadata(
                    doc_id=chunk.get('doc_id'),
                    source=chunk.get('source_type', 'unknown'),
                    created_at="",
                    author="",
                    language="english"
                )
            ))

        # 3. æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æŽ’åï¼ˆä½¿ç”¨å¢žå¼ºç‰ˆå‡½æ•°ï¼‰
        found_rank = find_correct_document_rank_enhanced(
            context=correct_chunk_content, # Gold Context (æ­£ç¡®ç­”æ¡ˆçš„ä¸Šä¸‹æ–‡å†…å®¹)
            retrieved_docs=final_retrieved_docs_for_ranking, # ä½¿ç”¨é‡æŽ’åºåŽçš„DocumentWithMetadataåˆ—è¡¨
            sample=eval_data[i], # ä¼ å…¥æ•´ä¸ªæ ·æœ¬ï¼Œå› ä¸º find_correct_document_rank_enhanced å¯èƒ½éœ€è¦ relevant_doc_ids
            encoder=encoder_model # ä¼ å…¥Encoderç”¨äºŽç›¸ä¼¼åº¦è®¡ç®—
        )
        
        if found_rank > 0:
            mrr_score = 1.0 / found_rank
            mrr_scores.append(mrr_score)
            
            if found_rank == 1:
                hit_at_1 += 1
            if found_rank <= 3:
                hit_at_3 += 1
            if found_rank <= 5:
                hit_at_5 += 1
            if found_rank <= 10:
                hit_at_10 += 1
        else:
            mrr_scores.append(0.0) # å¦‚æžœæ²¡æ‰¾åˆ°ï¼ŒMRRè´¡çŒ®ä¸º0

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    total_samples = len(eval_data)
    mrr = np.mean(mrr_scores)
    hit_at_1_rate = hit_at_1 / total_samples
    hit_at_3_rate = hit_at_3 / total_samples
    hit_at_5_rate = hit_at_5 / total_samples
    hit_at_10_rate = hit_at_10 / total_samples
    
    print(f"\n=== è¯„ä¼°ç»“æžœ ===")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"MRR: {mrr:.4f}")
    print(f"Hit@1: {hit_at_1_rate:.4f} ({hit_at_1}/{total_samples})")
    print(f"Hit@3: {hit_at_3_rate:.4f} ({hit_at_3}/{total_samples})")
    print(f"Hit@5: {hit_at_5_rate:.4f} ({hit_at_5}/{total_samples})")
    print(f"Hit@10: {hit_at_10_rate:.4f} ({hit_at_10}/{total_samples})")
    
    return mrr

def main():
    parser = argparse.ArgumentParser(description="GPUç‰ˆæœ¬çš„TatQAæ•°æ®é›†MRRè¯„ä¼°")
    parser.add_argument("--encoder_model_name", type=str, 
                       default="models/finetuned_finbert_tatqa",
                       help="Encoderæ¨¡åž‹è·¯å¾„")
    parser.add_argument("--reranker_model_name", type=str, 
                       default="Qwen/Qwen3-Reranker-0.6B",
                       help="Rerankeræ¨¡åž‹è·¯å¾„æˆ–Hugging Face ID")
    parser.add_argument("--eval_jsonl", type=str, 
                       default="evaluate_mrr/tatqa_eval_enhanced.jsonl",
                       help="è¯„ä¼°æ•°æ®JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--base_raw_data_path", type=str, 
                       default="data/tatqa_dataset_raw/",
                       help="TatQAåŽŸå§‹æ•°æ®è·¯å¾„")
    parser.add_argument("--top_k_retrieval", type=int, default=100, 
                       help="Encoderæ£€ç´¢çš„top-kæ•°é‡")
    parser.add_argument("--top_k_rerank", type=int, default=10, 
                       help="Rerankeré‡æŽ’åºåŽçš„top-kæ•°é‡")
    parser.add_argument("--batch_size", type=int, default=32, 
                       help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--force_cpu", action="store_true", 
                       help="å¼ºåˆ¶ä½¿ç”¨CPUï¼ˆç”¨äºŽè°ƒè¯•ï¼‰")
    parser.add_argument("--max_eval_samples", type=int, default=None, 
                       help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆç”¨äºŽå¿«é€Ÿæµ‹è¯•ï¼‰")
    
    args = parser.parse_args()

    # è®¾å¤‡é€‰æ‹©
    if args.force_cpu:
        device = "cpu"
        print("å¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
    else:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")
        if device == "cuda":
            print(f"GPU: {torch.cuda.get_device_name()}")
            print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

    # ==================== åŠ è½½ Encoder æ¨¡åž‹ ====================
    print(f"\n1. åŠ è½½ Encoder æ¨¡åž‹: {args.encoder_model_name}")
    try:
        encoder_model = SentenceTransformer(args.encoder_model_name)
        encoder_model.to(device)
        print("âœ… Encoder æ¨¡åž‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åŠ è½½ Encoder æ¨¡åž‹å¤±è´¥: {e}")
        return

    # ==================== åŠ è½½ Reranker æ¨¡åž‹ ====================
    print(f"\n2. åŠ è½½ Reranker æ¨¡åž‹: {args.reranker_model_name}")
    try:
        reranker_model = QwenReranker(
            model_name=args.reranker_model_name,
            device=device,
            use_quantization=True,
            quantization_type="4bit"
        )
        print("âœ… Reranker æ¨¡åž‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åŠ è½½ Reranker æ¨¡åž‹å¤±è´¥: {e}")
        return

    # ==================== åŠ è½½è¯„ä¼°æ•°æ® ====================
    print(f"\n3. åŠ è½½è¯„ä¼°æ•°æ®: {args.eval_jsonl}")
    if not Path(args.eval_jsonl).exists():
        print(f"âŒ è¯„ä¼°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.eval_jsonl}")
        return
        
    raw_eval_data = []
    with open(args.eval_jsonl, "r", encoding="utf-8") as f:
        for line in f:
            raw_eval_data.append(json.loads(line))
    
    eval_data = []
    for item in raw_eval_data:
        is_valid_item = isinstance(item, dict) and "context" in item and \
                        (get_question_or_query(item) is not None)
        
        if is_valid_item:
            eval_data.append(item)
        else:
            print(f"è­¦å‘Šï¼šè·³è¿‡æ— æ•ˆæ ·æœ¬: {item}")
    
    if args.max_eval_samples:
        eval_data = eval_data[:args.max_eval_samples]
        print(f"é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°ä¸º: {args.max_eval_samples}")
    
    print(f"âœ… åŠ è½½äº† {len(eval_data)} ä¸ªæœ‰æ•ˆè¯„ä¼°æ ·æœ¬")

    if not eval_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„è¯„ä¼°æ ·æœ¬")
        return

    # ==================== æž„å»ºæ£€ç´¢åº“ ====================
    print(f"\n4. æž„å»ºæ£€ç´¢åº“: {args.base_raw_data_path}")
    corpus_input_paths = [
        Path(args.base_raw_data_path) / "tatqa_dataset_train.json",
        Path(args.base_raw_data_path) / "tatqa_dataset_dev.json",
        Path(args.base_raw_data_path) / "tatqa_dataset_test_gold.json"
    ]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    existing_paths = [p for p in corpus_input_paths if p.exists()]
    if not existing_paths:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•TatQAåŽŸå§‹æ•°æ®æ–‡ä»¶")
        print(f"æœŸæœ›çš„æ–‡ä»¶:")
        for p in corpus_input_paths:
            print(f"  - {p}")
        return
    
    corpus_chunks = process_tatqa_to_qca_for_corpus(existing_paths)
    print(f"âœ… æž„å»ºäº† {len(corpus_chunks)} ä¸ªChunkä½œä¸ºæ£€ç´¢åº“")

    # ==================== è®¡ç®— MRR ====================
    print(f"\n5. å¼€å§‹è¯„ä¼°...")
    mrr = compute_mrr_with_reranker_gpu(
        encoder_model=encoder_model,
        reranker_model=reranker_model,
        eval_data=eval_data,
        corpus_chunks=corpus_chunks,
        top_k_retrieval=args.top_k_retrieval,
        top_k_rerank=args.top_k_rerank,
        batch_size=args.batch_size
    )
    
    print(f"\nðŸŽ¯ æœ€ç»ˆç»“æžœ: MRR = {mrr:.4f}")

if __name__ == "__main__":
    main() 