#!/usr/bin/env python3
"""
é›†æˆè¯„ä¼°ç³»ç»Ÿ - æ”¯æŒå¤šç§æ£€ç´¢å’Œé‡æ’åºæ–¹æ³•çš„å¯¹æ¯”å®éªŒ
æ•´åˆEncoderã€FAISSã€Rerankerã€å…ƒæ•°æ®è¿‡æ»¤ç­‰æ‰€æœ‰æ¨¡å—
æ”¯æŒå¤šGPUå¹¶è¡Œå¤„ç†
ä½¿ç”¨ä¸çœŸå®RAGç³»ç»Ÿç›¸åŒçš„ç»„ä»¶å’Œé€»è¾‘
"""

import json
import argparse
import torch
import torch.nn.functional as F
import numpy as np
from pathlib import Path
from tqdm import tqdm
import re
import sys
import os
import time
import multiprocessing as mp
from multiprocessing import Process, Queue
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from enum import Enum

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥çœŸå®RAGç³»ç»Ÿçš„ç»„ä»¶
from xlm.components.encoder.finbert import FinbertEncoder
from xlm.components.retriever.bilingual_retriever import BilingualRetriever
from xlm.components.retriever.reranker import QwenReranker
from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata

class RetrievalMethod(Enum):
    """æ£€ç´¢æ–¹æ³•æšä¸¾"""
    ENCODER_ONLY = "encoder_only"
    ENCODER_FAISS = "encoder_faiss"
    ENCODER_RERANKER = "encoder_reranker"
    ENCODER_FAISS_RERANKER = "encoder_faiss_reranker"

@dataclass
class EvaluationConfig:
    """è¯„ä¼°é…ç½®"""
    eval_data_path: str
    corpus_data_path: str
    encoder_model: str
    reranker_model: str
    retrieval_method: RetrievalMethod
    top_k_retrieval: int = 100
    top_k_rerank: int = 10
    max_samples: int = 100
    use_metadata_filter: bool = False
    use_faiss: bool = False
    device: str = "cuda"
    batch_size: int = 4
    max_length: int = 512
    num_gpus: int = 2  # æ–°å¢ï¼šGPUæ•°é‡

@dataclass
class EvaluationResult:
    """è¯„ä¼°ç»“æœ"""
    method: str
    mrr_retrieval: float
    mrr_rerank: float
    avg_retrieval_time: float
    avg_rerank_time: float
    total_queries: int
    valid_queries: int
    skipped_queries: int

class MetadataExtractor:
    """å…ƒæ•°æ®æå–å™¨"""
    
    @staticmethod
    def extract_alphafin_metadata(context_text: str) -> dict:
        """ä»AlphaFinä¸Šä¸‹æ–‡æ–‡æœ¬ä¸­æå–å…ƒæ•°æ®"""
        metadata = {
            "company_name": "",
            "stock_code": "",
            "report_date": "",
            "report_title": ""
        }
        
        # æå–å…¬å¸åç§°å’Œè‚¡ç¥¨ä»£ç 
        company_pattern = r'([^ï¼ˆ]+)ï¼ˆ([0-9]{6}ï¼‰)'
        match = re.search(company_pattern, context_text)
        if match:
            metadata["company_name"] = match.group(1).strip()
            metadata["stock_code"] = match.group(2).strip()
        
        # æå–æŠ¥å‘Šæ—¥æœŸ
        date_pattern = r'(\d{4}-\d{2}-\d{2})'
        dates = re.findall(date_pattern, context_text)
        if dates:
            metadata["report_date"] = dates[0]
        
        # æå–æŠ¥å‘Šæ ‡é¢˜
        title_pattern = r'ç ”ç©¶æŠ¥å‘Šï¼Œå…¶æ ‡é¢˜æ˜¯ï¼š"([^"]+)"'
        title_match = re.search(title_pattern, context_text)
        if title_match:
            metadata["report_title"] = title_match.group(1).strip()
        
        return metadata

class MetadataFilter:
    """å…ƒæ•°æ®è¿‡æ»¤å™¨"""
    
    def __init__(self, dataset_type: str = "alphafin"):
        self.dataset_type = dataset_type
        self.extractor = MetadataExtractor()
    
    def filter_corpus(self, corpus_documents: List[DocumentWithMetadata], target_metadata: dict) -> List[DocumentWithMetadata]:
        """æ ¹æ®å…ƒæ•°æ®è¿‡æ»¤æ£€ç´¢åº“"""
        if not target_metadata or not any(target_metadata.values()):
            return corpus_documents
        
        filtered_corpus = []
        filter_criteria = []
        
        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        for field, value in target_metadata.items():
            if value:
                filter_criteria.append((field, value))
        
        if not filter_criteria:
            return corpus_documents
        
        print(f"ğŸ” åº”ç”¨å…ƒæ•°æ®è¿‡æ»¤: {filter_criteria}")
        
        for doc in corpus_documents:
            content_metadata = self.extractor.extract_alphafin_metadata(doc.content)
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ‰€æœ‰è¿‡æ»¤æ¡ä»¶
            matches_all = True
            for field, value in filter_criteria:
                if content_metadata.get(field) != value:
                    matches_all = False
                    break
            
            if matches_all:
                filtered_corpus.append(doc)
        
        print(f"ğŸ“Š è¿‡æ»¤ç»“æœ: {len(filtered_corpus)}/{len(corpus_documents)} ä¸ªæ–‡æ¡£")
        return filtered_corpus

class RAGSystemManager:
    """RAGç³»ç»Ÿç®¡ç†å™¨ - ä½¿ç”¨çœŸå®RAGç³»ç»Ÿç»„ä»¶"""
    
    def __init__(self, config: EvaluationConfig, gpu_id: int = 0):
        self.config = config
        self.gpu_id = gpu_id
        self.device = torch.device(f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu")
        
        # RAGç³»ç»Ÿç»„ä»¶
        self.encoder_en = None
        self.encoder_ch = None
        self.retriever = None
        self.reranker = None
        
    def load_encoders(self):
        """åŠ è½½ç¼–ç å™¨"""
        try:
            print(f"ğŸ“– GPU {self.gpu_id}: åŠ è½½ç¼–ç å™¨æ¨¡å‹: {self.config.encoder_model}")
            
            # åŠ è½½è‹±æ–‡ç¼–ç å™¨
            self.encoder_en = FinbertEncoder(
                model_name=self.config.encoder_model,
                cache_dir="cache"
            )
            
            # åŠ è½½ä¸­æ–‡ç¼–ç å™¨ï¼ˆä½¿ç”¨ç›¸åŒæ¨¡å‹æˆ–æŒ‡å®šä¸­æ–‡æ¨¡å‹ï¼‰
            self.encoder_ch = FinbertEncoder(
                model_name=self.config.encoder_model,  # å¯ä»¥æ ¹æ®éœ€è¦æŒ‡å®šä¸åŒçš„ä¸­æ–‡æ¨¡å‹
                cache_dir="cache"
            )
            
            print(f"âœ… GPU {self.gpu_id}: ç¼–ç å™¨åŠ è½½å®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ GPU {self.gpu_id}: åŠ è½½ç¼–ç å™¨å¤±è´¥: {e}")
            return False
    
    def load_reranker(self):
        """åŠ è½½é‡æ’åºå™¨"""
        try:
            print(f"ğŸ“– GPU {self.gpu_id}: åŠ è½½é‡æ’åºæ¨¡å‹: {self.config.reranker_model}")
            
            self.reranker = QwenReranker(
                model_name=self.config.reranker_model,
                device=f"cuda:{self.gpu_id}" if torch.cuda.is_available() else "cpu",
                cache_dir="cache",
                use_quantization=True,
                quantization_type="4bit"
            )
            
            print(f"âœ… GPU {self.gpu_id}: é‡æ’åºæ¨¡å‹åŠ è½½å®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ GPU {self.gpu_id}: åŠ è½½é‡æ’åºæ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def create_retriever(self, corpus_documents_en: List[DocumentWithMetadata], 
                        corpus_documents_ch: Optional[List[DocumentWithMetadata]] = None):
        """åˆ›å»ºæ£€ç´¢å™¨"""
        try:
            print(f"ğŸ”§ GPU {self.gpu_id}: åˆ›å»ºBilingualRetriever...")
            
            if corpus_documents_ch is None:
                corpus_documents_ch = []
            
            if self.encoder_en is None or self.encoder_ch is None:
                print(f"âŒ GPU {self.gpu_id}: ç¼–ç å™¨æœªåŠ è½½")
                return False
            
            self.retriever = BilingualRetriever(
                encoder_en=self.encoder_en,
                encoder_ch=self.encoder_ch,
                corpus_documents_en=corpus_documents_en,
                corpus_documents_ch=corpus_documents_ch,
                use_faiss=self.config.use_faiss,
                use_gpu=True,
                batch_size=self.config.batch_size,
                cache_dir="cache",
                use_existing_embedding_index=False
            )
            
            print(f"âœ… GPU {self.gpu_id}: BilingualRetrieveråˆ›å»ºå®Œæˆ")
            return True
        except Exception as e:
            print(f"âŒ GPU {self.gpu_id}: åˆ›å»ºæ£€ç´¢å™¨å¤±è´¥: {e}")
            return False
    
    def retrieve_documents(self, query: str, top_k: int, language: str = "english") -> Tuple[List[DocumentWithMetadata], List[float]]:
        """æ£€ç´¢æ–‡æ¡£ - ä½¿ç”¨çœŸå®RAGç³»ç»Ÿé€»è¾‘"""
        if not self.retriever:
            return [], []
        
        try:
            # ä½¿ç”¨BilingualRetrieverçš„retrieveæ–¹æ³•
            result = self.retriever.retrieve(
                text=query,
                top_k=top_k,
                return_scores=True,
                language=language
            )
            
            if isinstance(result, tuple):
                documents, scores = result
            else:
                documents = result
                scores = []
            
            return documents, scores
        except Exception as e:
            print(f"âŒ GPU {self.gpu_id}: æ£€ç´¢å¤±è´¥: {e}")
            return [], []
    
    def rerank_documents(self, query: str, documents: List[DocumentWithMetadata], top_k: int) -> List[DocumentWithMetadata]:
        """é‡æ’åºæ–‡æ¡£ - ä½¿ç”¨QwenReranker"""
        if not self.reranker or not documents:
            return documents[:top_k]
        
        try:
            # æå–æ–‡æ¡£å†…å®¹
            doc_texts = [doc.content for doc in documents]
            
            # ä½¿ç”¨QwenRerankeré‡æ’åº
            reranked_items = self.reranker.rerank(
                query=query,
                documents=doc_texts,
                batch_size=self.config.batch_size
            )
            
            # æ ¹æ®é‡æ’åºç»“æœé‡æ–°æ’åˆ—æ–‡æ¡£
            content_to_doc_map = {doc.content: doc for doc in documents}
            reranked_docs = []
            
            for doc_text, score in reranked_items[:top_k]:
                if doc_text in content_to_doc_map:
                    reranked_docs.append(content_to_doc_map[doc_text])
            
            return reranked_docs
        except Exception as e:
            print(f"âŒ GPU {self.gpu_id}: é‡æ’åºå¤±è´¥: {e}")
            return documents[:top_k]

class IntegratedEvaluator:
    """é›†æˆè¯„ä¼°å™¨ - ä½¿ç”¨çœŸå®RAGç³»ç»Ÿç»„ä»¶"""
    
    def __init__(self, config: EvaluationConfig):
        self.config = config
    
    def load_data(self) -> Tuple[List[Dict], List[DocumentWithMetadata], List[DocumentWithMetadata]]:
        """åŠ è½½æ•°æ®"""
        print("ğŸ“– åŠ è½½è¯„ä¼°æ•°æ®...")
        
        # åŠ è½½è¯„ä¼°æ•°æ®
        eval_data = []
        with open(self.config.eval_data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    eval_data.append(json.loads(line))
        
        if self.config.max_samples:
            eval_data = eval_data[:self.config.max_samples]
        
        print(f"âœ… åŠ è½½äº† {len(eval_data)} ä¸ªè¯„ä¼°æ ·æœ¬")
        
        # åŠ è½½æ£€ç´¢åº“æ•°æ®
        print("ğŸ“– åŠ è½½æ£€ç´¢åº“æ•°æ®...")
        corpus_documents_en = []
        corpus_documents_ch = []
        
        with open(self.config.corpus_data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    doc_id = item.get('doc_id', str(len(corpus_documents_en)))
                    content = item.get('context', item.get('content', ''))
                    
                    if content:
                        # åˆ›å»ºDocumentWithMetadataå¯¹è±¡
                        metadata = DocumentMetadata(
                            source=item.get('source', 'unknown'),
                            created_at=item.get('created_at', ''),
                            author=item.get('author', ''),
                            language=item.get('language', 'english')
                        )
                        
                        doc = DocumentWithMetadata(
                            content=content,
                            metadata=metadata
                        )
                        
                        # æ ¹æ®è¯­è¨€åˆ†é…åˆ°ä¸åŒè¯­æ–™åº“
                        if item.get('language', 'english') == 'chinese':
                            corpus_documents_ch.append(doc)
                        else:
                            corpus_documents_en.append(doc)
        
        print(f"âœ… åŠ è½½äº† {len(corpus_documents_en)} ä¸ªè‹±æ–‡æ–‡æ¡£, {len(corpus_documents_ch)} ä¸ªä¸­æ–‡æ–‡æ¡£")
        
        return eval_data, corpus_documents_en, corpus_documents_ch
    
    def evaluate_single_query(self, query_item: Dict, rag_manager: RAGSystemManager) -> Tuple[int, int, float, float]:
        """è¯„ä¼°å•ä¸ªæŸ¥è¯¢"""
        query = query_item.get('query', query_item.get('question', ''))
        correct_context = query_item.get('context', '')
        
        if not query or not correct_context:
            return 0, 0, 0.0, 0.0
        
        # æ£€æµ‹è¯­è¨€
        try:
            from langdetect import detect
            language = detect(query)
            language = 'zh' if language.startswith('zh') else 'en'
        except:
            language = 'en'  # é»˜è®¤è‹±æ–‡
        
        start_time = time.time()
        
        # 1. åˆå§‹æ£€ç´¢
        retrieved_docs, retrieval_scores = rag_manager.retrieve_documents(
            query=query, 
            top_k=self.config.top_k_retrieval,
            language=language
        )
        
        retrieval_time = time.time() - start_time
        
        # æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å
        retrieval_rank = 0
        for rank, doc in enumerate(retrieved_docs, 1):
            if doc.content == correct_context:
                retrieval_rank = rank
                break
        
        # 2. é‡æ’åºï¼ˆå¦‚æœéœ€è¦ï¼‰
        rerank_rank = 0
        rerank_time = 0.0
        
        if self.config.retrieval_method in [RetrievalMethod.ENCODER_RERANKER, RetrievalMethod.ENCODER_FAISS_RERANKER]:
            if retrieved_docs:
                start_time = time.time()
                
                # é‡æ’åº
                reranked_docs = rag_manager.rerank_documents(
                    query=query,
                    documents=retrieved_docs,
                    top_k=self.config.top_k_rerank
                )
                
                # æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å
                for rank, doc in enumerate(reranked_docs, 1):
                    if doc.content == correct_context:
                        rerank_rank = rank
                        break
                
                rerank_time = time.time() - start_time
        
        return retrieval_rank, rerank_rank, retrieval_time, rerank_time
    
    def evaluate(self) -> Optional[EvaluationResult]:
        """æ‰§è¡Œè¯„ä¼°"""
        # åŠ è½½æ•°æ®
        eval_data, corpus_documents_en, corpus_documents_ch = self.load_data()
        
        # åˆ›å»ºRAGç³»ç»Ÿç®¡ç†å™¨
        rag_manager = RAGSystemManager(self.config)
        
        # åŠ è½½ç¼–ç å™¨
        if not rag_manager.load_encoders():
            return None
        
        # åŠ è½½é‡æ’åºå™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.config.retrieval_method in [RetrievalMethod.ENCODER_RERANKER, RetrievalMethod.ENCODER_FAISS_RERANKER]:
            if not rag_manager.load_reranker():
                return None
        
        # åˆ›å»ºæ£€ç´¢å™¨
        if not rag_manager.create_retriever(corpus_documents_en, corpus_documents_ch):
            return None
        
        # å…ƒæ•°æ®è¿‡æ»¤
        if self.config.use_metadata_filter:
            metadata_filter = MetadataFilter()
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­éœ€è¦æ ¹æ®æŸ¥è¯¢æå–ç›®æ ‡å…ƒæ•°æ®
            target_metadata = {}  # å¯ä»¥æ ¹æ®éœ€è¦è®¾ç½®
            corpus_documents_en = metadata_filter.filter_corpus(corpus_documents_en, target_metadata)
            corpus_documents_ch = metadata_filter.filter_corpus(corpus_documents_ch, target_metadata)
        
        # è¯„ä¼°
        all_retrieval_ranks = []
        all_rerank_ranks = []
        all_retrieval_times = []
        all_rerank_times = []
        skipped_queries = 0
        
        for query_item in tqdm(eval_data, desc="è¯„ä¼°æŸ¥è¯¢"):
            try:
                retrieval_rank, rerank_rank, retrieval_time, rerank_time = self.evaluate_single_query(
                    query_item, rag_manager
                )
                
                all_retrieval_ranks.append(retrieval_rank)
                all_rerank_ranks.append(rerank_rank)
                all_retrieval_times.append(retrieval_time)
                all_rerank_times.append(rerank_time)
                
            except Exception as e:
                print(f"âŒ æŸ¥è¯¢è¯„ä¼°å¤±è´¥: {e}")
                skipped_queries += 1
                all_retrieval_ranks.append(0)
                all_rerank_ranks.append(0)
                all_retrieval_times.append(0.0)
                all_rerank_times.append(0.0)
        
        # è®¡ç®—æŒ‡æ ‡
        mrr_retrieval = self.calculate_mrr(all_retrieval_ranks)
        mrr_rerank = self.calculate_mrr(all_rerank_ranks)
        avg_retrieval_time = float(np.mean(all_retrieval_times)) if all_retrieval_times else 0.0
        avg_rerank_time = float(np.mean(all_rerank_times)) if all_rerank_times else 0.0
        
        return EvaluationResult(
            method=self.config.retrieval_method.value,
            mrr_retrieval=mrr_retrieval,
            mrr_rerank=mrr_rerank,
            avg_retrieval_time=avg_retrieval_time,
            avg_rerank_time=avg_rerank_time,
            total_queries=len(eval_data),
            valid_queries=len(eval_data) - skipped_queries,
            skipped_queries=skipped_queries
        )
    
    def calculate_mrr(self, rankings: List[int]) -> float:
        """è®¡ç®—MRRåˆ†æ•°"""
        if not rankings:
            return 0.0
        
        reciprocal_ranks = []
        for rank in rankings:
            if rank > 0:
                reciprocal_ranks.append(1.0 / rank)
            else:
                reciprocal_ranks.append(0.0)
        
        return float(np.mean(reciprocal_ranks))

def gpu_worker_evaluation(gpu_id: int, data_queue: Queue, result_queue: Queue, config: EvaluationConfig):
    """GPUå·¥ä½œè¿›ç¨‹å‡½æ•°"""
    try:
        # è®¾ç½®CUDAè®¾å¤‡
        torch.cuda.set_device(gpu_id)
        print(f"ğŸš€ GPU {gpu_id}: å¼€å§‹å·¥ä½œï¼Œè®¾å¤‡: cuda:{gpu_id}")
        
        # åˆ›å»ºGPUç‰¹å®šçš„é…ç½®
        gpu_config = EvaluationConfig(
            eval_data_path=config.eval_data_path,
            corpus_data_path=config.corpus_data_path,
            encoder_model=config.encoder_model,
            reranker_model=config.reranker_model,
            retrieval_method=config.retrieval_method,
            top_k_retrieval=config.top_k_retrieval,
            top_k_rerank=config.top_k_rerank,
            max_samples=config.max_samples,
            use_metadata_filter=config.use_metadata_filter,
            use_faiss=config.use_faiss,
            device=f"cuda:{gpu_id}",
            batch_size=config.batch_size,
            max_length=config.max_length,
            num_gpus=1  # å•ä¸ªGPU
        )
        
        # åˆ›å»ºè¯„ä¼°å™¨
        evaluator = IntegratedEvaluator(gpu_config)
        
        # å¤„ç†æ•°æ®
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–æ•°æ®
                batch_data = data_queue.get(timeout=1)
                if batch_data is None:  # ç»“æŸä¿¡å·
                    break
                
                print(f"ğŸ“Š GPU {gpu_id}: å¤„ç†æ‰¹æ¬¡ï¼Œæ ·æœ¬æ•°: {len(batch_data)}")
                
                # æ‰§è¡Œè¯„ä¼°
                result = evaluator.evaluate()
                
                # å‘é€ç»“æœ
                result_queue.put((gpu_id, result))
                print(f"âœ… GPU {gpu_id}: è¯„ä¼°å®Œæˆ")
                
            except Exception as e:
                print(f"âŒ GPU {gpu_id}: å¤„ç†å¤±è´¥: {e}")
                result_queue.put((gpu_id, None))
                break
        
    except Exception as e:
        print(f"âŒ GPU {gpu_id}: å·¥ä½œè¿›ç¨‹å¤±è´¥: {e}")
        result_queue.put((gpu_id, None))

def run_multi_gpu_evaluation(config: EvaluationConfig) -> Optional[EvaluationResult]:
    """è¿è¡Œå¤šGPUè¯„ä¼°"""
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œå›é€€åˆ°å•GPUæ¨¡å¼")
        config.num_gpus = 1
        config.device = "cpu"
    
    available_gpus = torch.cuda.device_count()
    if config.num_gpus > available_gpus:
        print(f"âš ï¸ è¯·æ±‚çš„GPUæ•°é‡ ({config.num_gpus}) è¶…è¿‡å¯ç”¨GPUæ•°é‡ ({available_gpus})")
        config.num_gpus = available_gpus
    
    print(f"ğŸš€ ä½¿ç”¨ {config.num_gpus} ä¸ªGPUè¿›è¡Œå¹¶è¡Œè¯„ä¼°")
    
    # åŠ è½½æ•°æ®
    print("ğŸ“– åŠ è½½æ•°æ®...")
    evaluator = IntegratedEvaluator(config)
    eval_data, corpus_documents_en, corpus_documents_ch = evaluator.load_data()
    
    # æ•°æ®åˆ†å‰²
    total_samples = len(eval_data)
    samples_per_gpu = total_samples // config.num_gpus
    remainder = total_samples % config.num_gpus
    
    print(f"ğŸ“Š æ•°æ®åˆ†å‰²:")
    start_idx = 0
    gpu_data = []
    for gpu_id in range(config.num_gpus):
        current_batch_size = samples_per_gpu + (1 if gpu_id < remainder else 0)
        end_idx = start_idx + current_batch_size
        gpu_samples = eval_data[start_idx:end_idx]
        gpu_data.append(gpu_samples)
        print(f"  GPU {gpu_id}: {len(gpu_samples)} ä¸ªæ ·æœ¬")
        start_idx = end_idx
    
    # åˆ›å»ºè¿›ç¨‹é—´é€šä¿¡é˜Ÿåˆ—
    data_queues = [Queue() for _ in range(config.num_gpus)]
    result_queue = Queue()
    
    # å¯åŠ¨GPUå·¥ä½œè¿›ç¨‹
    processes = []
    for gpu_id in range(config.num_gpus):
        p = Process(target=gpu_worker_evaluation, args=(
            gpu_id, data_queues[gpu_id], result_queue, config
        ))
        p.start()
        processes.append(p)
        print(f"ğŸš€ GPU {gpu_id} è¿›ç¨‹å·²å¯åŠ¨")
    
    # å‘é€æ•°æ®åˆ°å„ä¸ªGPU
    for gpu_id in range(config.num_gpus):
        data_queues[gpu_id].put(gpu_data[gpu_id])
        data_queues[gpu_id].put(None)  # ç»“æŸä¿¡å·
        print(f"ğŸ“¤ æ•°æ®å·²å‘é€åˆ° GPU {gpu_id}")
    
    # æ”¶é›†ç»“æœ
    all_results = []
    for _ in tqdm(range(config.num_gpus), desc="æ”¶é›†GPUç»“æœ"):
        gpu_id, result = result_queue.get()
        if result:
            all_results.append(result)
            print(f"ğŸ“¥ æ”¶åˆ° GPU {gpu_id} çš„ç»“æœ")
        else:
            print(f"âŒ GPU {gpu_id} è¿”å›ç©ºç»“æœ")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    for p in processes:
        p.join()
    
    # åˆå¹¶ç»“æœ
    if not all_results:
        print("âŒ æ²¡æœ‰æ”¶åˆ°ä»»ä½•æœ‰æ•ˆç»“æœ")
        return None
    
    # ç®€å•åˆå¹¶ç­–ç•¥ï¼šå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆç»“æœ
    # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯èƒ½éœ€è¦æ›´å¤æ‚çš„åˆå¹¶ç­–ç•¥
    final_result = all_results[0]
    print(f"âœ… å¤šGPUè¯„ä¼°å®Œæˆï¼Œä½¿ç”¨ GPU 0 çš„ç»“æœ")
    
    return final_result

class ComparisonExperiment:
    """å¯¹æ¯”å®éªŒç®¡ç†å™¨"""
    
    def __init__(self, base_config: EvaluationConfig):
        self.base_config = base_config
        self.results = []
    
    def run_comparison(self, methods: List[RetrievalMethod]) -> List[EvaluationResult]:
        """è¿è¡Œå¯¹æ¯”å®éªŒ"""
        print("ğŸ”¬ å¼€å§‹å¯¹æ¯”å®éªŒ")
        print("="*60)
        
        for method in methods:
            print(f"\nğŸ“Š æµ‹è¯•æ–¹æ³•: {method.value}")
            print("-" * 40)
            
            # åˆ›å»ºé…ç½®
            config = EvaluationConfig(
                eval_data_path=self.base_config.eval_data_path,
                corpus_data_path=self.base_config.corpus_data_path,
                encoder_model=self.base_config.encoder_model,
                reranker_model=self.base_config.reranker_model,
                retrieval_method=method,
                top_k_retrieval=self.base_config.top_k_retrieval,
                top_k_rerank=self.base_config.top_k_rerank,
                max_samples=self.base_config.max_samples,
                use_metadata_filter=self.base_config.use_metadata_filter,
                use_faiss=(method in [RetrievalMethod.ENCODER_FAISS, RetrievalMethod.ENCODER_FAISS_RERANKER]),
                device=self.base_config.device,
                batch_size=self.base_config.batch_size,
                max_length=self.base_config.max_length,
                num_gpus=self.base_config.num_gpus
            )
            
            # æ‰§è¡Œè¯„ä¼°
            if config.num_gpus > 1:
                result = run_multi_gpu_evaluation(config)
            else:
                evaluator = IntegratedEvaluator(config)
                result = evaluator.evaluate()
            
            if result:
                self.results.append(result)
                self.print_result(result)
            else:
                print(f"âŒ æ–¹æ³• {method.value} è¯„ä¼°å¤±è´¥")
        
        return self.results
    
    def print_result(self, result: EvaluationResult):
        """æ‰“å°å•ä¸ªç»“æœ"""
        print(f"ğŸ“ˆ ç»“æœ:")
        print(f"  - æ£€ç´¢MRR @{self.base_config.top_k_retrieval}: {result.mrr_retrieval:.4f}")
        print(f"  - é‡æ’åºMRR @{self.base_config.top_k_rerank}: {result.mrr_rerank:.4f}")
        print(f"  - å¹³å‡æ£€ç´¢æ—¶é—´: {result.avg_retrieval_time:.3f}s")
        print(f"  - å¹³å‡é‡æ’åºæ—¶é—´: {result.avg_rerank_time:.3f}s")
        print(f"  - æœ‰æ•ˆæŸ¥è¯¢æ•°: {result.valid_queries}/{result.total_queries}")
    
    def print_comparison_summary(self):
        """æ‰“å°å¯¹æ¯”æ€»ç»“"""
        if not self.results:
            print("âŒ æ²¡æœ‰å¯æ¯”è¾ƒçš„ç»“æœ")
            return
        
        print("\n" + "="*60)
        print("ğŸ“Š å¯¹æ¯”å®éªŒæ€»ç»“")
        print("="*60)
        
        # æŒ‰æ£€ç´¢MRRæ’åº
        sorted_results = sorted(self.results, key=lambda x: x.mrr_retrieval, reverse=True)
        
        print(f"{'æ–¹æ³•':<25} {'æ£€ç´¢MRR':<10} {'é‡æ’åºMRR':<12} {'æ£€ç´¢æ—¶é—´':<10} {'é‡æ’åºæ—¶é—´':<12}")
        print("-" * 80)
        
        for result in sorted_results:
            print(f"{result.method:<25} {result.mrr_retrieval:<10.4f} {result.mrr_rerank:<12.4f} "
                  f"{result.avg_retrieval_time:<10.3f} {result.avg_rerank_time:<12.3f}")
        
        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
        best_retrieval = max(self.results, key=lambda x: x.mrr_retrieval)
        best_rerank = max(self.results, key=lambda x: x.mrr_rerank)
        
        print(f"\nğŸ† æœ€ä½³æ£€ç´¢æ–¹æ³•: {best_retrieval.method} (MRR: {best_retrieval.mrr_retrieval:.4f})")
        print(f"ğŸ† æœ€ä½³é‡æ’åºæ–¹æ³•: {best_rerank.method} (MRR: {best_rerank.mrr_rerank:.4f})")

def main():
    parser = argparse.ArgumentParser(description="é›†æˆè¯„ä¼°ç³»ç»Ÿ - ä½¿ç”¨çœŸå®RAGç³»ç»Ÿç»„ä»¶")
    parser.add_argument("--eval_data", type=str, 
                       default="evaluate_mrr/alphafin_eval.jsonl",
                       help="è¯„ä¼°æ•°æ®æ–‡ä»¶")
    parser.add_argument("--corpus_data", type=str,
                       default="data/alphafin/alphafin_merged_generated_qa_full_dedup.json",
                       help="æ£€ç´¢åº“æ•°æ®æ–‡ä»¶")
    parser.add_argument("--encoder_model", type=str,
                       default="models/finetuned_finbert_tatqa",
                       help="ç¼–ç å™¨æ¨¡å‹åç§°")
    parser.add_argument("--reranker_model", type=str,
                       default="Qwen/Qwen3-Reranker-0.6B",
                       help="é‡æ’åºæ¨¡å‹åç§°")
    parser.add_argument("--method", type=str,
                       choices=["encoder_only", "encoder_faiss", "encoder_reranker", "encoder_faiss_reranker", "all"],
                       default="all",
                       help="è¯„ä¼°æ–¹æ³•")
    parser.add_argument("--top_k_retrieval", type=int, default=100,
                       help="æ£€ç´¢top-k")
    parser.add_argument("--top_k_rerank", type=int, default=10,
                       help="é‡æ’åºtop-k")
    parser.add_argument("--max_samples", type=int, default=100,
                       help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°")
    parser.add_argument("--use_metadata_filter", action="store_true",
                       help="æ˜¯å¦ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤")
    parser.add_argument("--device", type=str, default="cuda",
                       help="è®¾å¤‡é€‰æ‹©")
    parser.add_argument("--num_gpus", type=int, default=2,
                       help="ä½¿ç”¨çš„GPUæ•°é‡")
    
    args = parser.parse_args()
    
    print("ğŸš€ é›†æˆè¯„ä¼°ç³»ç»Ÿ - ä½¿ç”¨çœŸå®RAGç³»ç»Ÿç»„ä»¶")
    print(f"ğŸ“Š é…ç½®:")
    print(f"  - è¯„ä¼°æ•°æ®: {args.eval_data}")
    print(f"  - æ£€ç´¢åº“æ•°æ®: {args.corpus_data}")
    print(f"  - ç¼–ç å™¨æ¨¡å‹: {args.encoder_model}")
    print(f"  - é‡æ’åºæ¨¡å‹: {args.reranker_model}")
    print(f"  - è¯„ä¼°æ–¹æ³•: {args.method}")
    print(f"  - æœ€å¤§æ ·æœ¬æ•°: {args.max_samples}")
    print(f"  - å…ƒæ•°æ®è¿‡æ»¤: {'å¯ç”¨' if args.use_metadata_filter else 'ç¦ç”¨'}")
    print(f"  - GPUæ•°é‡: {args.num_gpus}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.eval_data).exists():
        print(f"âŒ è¯„ä¼°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.eval_data}")
        return
    
    if not Path(args.corpus_data).exists():
        print(f"âŒ æ£€ç´¢åº“æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.corpus_data}")
        return
    
    # åˆ›å»ºåŸºç¡€é…ç½®
    base_config = EvaluationConfig(
        eval_data_path=args.eval_data,
        corpus_data_path=args.corpus_data,
        encoder_model=args.encoder_model,
        reranker_model=args.reranker_model,
        retrieval_method=RetrievalMethod.ENCODER_ONLY,  # å ä½ç¬¦
        top_k_retrieval=args.top_k_retrieval,
        top_k_rerank=args.top_k_rerank,
        max_samples=args.max_samples,
        use_metadata_filter=args.use_metadata_filter,
        device=args.device,
        num_gpus=args.num_gpus
    )
    
    # ç¡®å®šè¦æµ‹è¯•çš„æ–¹æ³•
    if args.method == "all":
        methods = [
            RetrievalMethod.ENCODER_ONLY,
            RetrievalMethod.ENCODER_FAISS,
            RetrievalMethod.ENCODER_RERANKER,
            RetrievalMethod.ENCODER_FAISS_RERANKER
        ]
    else:
        methods = [RetrievalMethod(args.method)]
    
    # è¿è¡Œå¯¹æ¯”å®éªŒ
    experiment = ComparisonExperiment(base_config)
    results = experiment.run_comparison(methods)
    
    # æ‰“å°æ€»ç»“
    experiment.print_comparison_summary()
    
    print(f"\nğŸ‰ è¯„ä¼°å®Œæˆï¼")

if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass  # å¦‚æœå·²ç»è®¾ç½®è¿‡äº†ï¼Œå°±å¿½ç•¥
    main() 