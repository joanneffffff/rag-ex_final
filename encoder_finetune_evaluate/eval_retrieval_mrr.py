#!/usr/bin/env python3
"""
æµ‹è¯•æ£€ç´¢è´¨é‡ - MRRè¯„ä¼° (CPUç‰ˆæœ¬)
ä½¿ç”¨evaluate_mrr/alphafin_eval.jsonlå’Œtatqa_eval_enhanced.jsonlä½œä¸ºæµ‹è¯•é›†
æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. è¯„ä¼°æ•°æ®contextåŠ å…¥çŸ¥è¯†åº“ - æµ‹è¯•çœŸå®æ£€ç´¢èƒ½åŠ›
2. è¯„ä¼°æ•°æ®contextä¸åŠ å…¥çŸ¥è¯†åº“ - é¿å…æ•°æ®æ³„éœ²

æ”¹è¿›çš„åŒ¹é…ç­–ç•¥ï¼š
1. relevant_doc_idsåŒ¹é…ï¼ˆæœ€ä¸¥æ ¼ï¼Œé€‚ç”¨äºè‹±æ–‡æ•°æ®ï¼‰
2. IDåŒ¹é…ï¼ˆé€‚ç”¨äºä¸­æ–‡æ•°æ®ï¼‰
3. å†…å®¹å“ˆå¸ŒåŒ¹é…
4. ç›¸ä¼¼åº¦åŒ¹é…
5. æ¨¡ç³Šæ–‡æœ¬åŒ¹é…

è¯„ä¼°é€»è¾‘å·²æ›´æ–°ä¸º Encoder + FAISS + Reranker ç»„åˆ
"""

import sys
import os
import json
import numpy as np
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from tqdm import tqdm

# ç¡®ä¿å¯¼å…¥è·¯å¾„æ­£ç¡®ï¼Œæ ¹æ®ä½ çš„é¡¹ç›®ç»“æ„è°ƒæ•´
sys.path.append(str(Path(__file__).parent.parent)) # å‡è®¾ xlm æ¨¡å—åœ¨ä¸Šä¸€çº§ç›®å½•

# å¯¼å…¥å¿…è¦çš„ç±»å‹å’Œç»„ä»¶
from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata
from xlm.components.retriever.bilingual_retriever import BilingualRetriever
from xlm.components.encoder.finbert import FinbertEncoder # ä½ çš„Encoderç±»
from xlm.components.retriever.reranker import Reranker # ä½ çš„Rerankerç±»
from xlm.utils.optimized_data_loader import OptimizedDataLoader # ä½ çš„æ•°æ®åŠ è½½å™¨
from config.parameters import Config # ä½ çš„é…ç½®ç±»

# å¯¼å…¥å¢å¼ºç‰ˆè¯„ä¼°å‡½æ•° (å‡è®¾å®ƒå­˜åœ¨å¹¶èƒ½å¤Ÿå¤„ç†å¤šç§åŒ¹é…ç­–ç•¥)
# å‡è®¾ evaluate_mrr/enhanced_evaluation_functions.py åœ¨åŒä¸€ç›®å½•æˆ–å¯è®¿é—®è·¯å¾„
from enhanced_evaluation_functions import find_correct_document_rank_enhanced 


def load_eval_data(eval_file: str) -> List[Dict[str, Any]]:
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    data = []
    with open(eval_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def calculate_mrr(ranks: List[int]) -> float:
    """è®¡ç®—MRR (Mean Reciprocal Rank)"""
    if not ranks:
        return 0.0
    reciprocal_ranks = [1.0 / rank if rank > 0 else 0.0 for rank in ranks]
    return float(np.mean(reciprocal_ranks))

def calculate_hit_rate(ranks: List[int], k: int = 1) -> float:
    """è®¡ç®—Hit@k"""
    if not ranks:
        return 0.0
    hits = [1 if rank <= k and rank > 0 else 0 for rank in ranks]
    return float(np.mean(hits))

def evaluate_dataset(eval_data: List[Dict[str, Any]], 
                     retriever: BilingualRetriever, 
                     encoder: FinbertEncoder, 
                     language: str, 
                     dataset_name: str,
                     reranker: Optional[Reranker] = None) -> Dict[str, float]:
    """
    è¯„ä¼°å•ä¸ªæ•°æ®é›†çš„æ£€ç´¢è´¨é‡ (Encoder + FAISS + Reranker)
    
    Args:
        eval_data: è¯„ä¼°æ•°æ®åˆ—è¡¨
        retriever: BilingualRetriever å®ä¾‹ (è´Ÿè´£åˆæ­¥çš„Encoder + FAISSæ£€ç´¢)
        encoder: å¯¹åº”è¯­è¨€çš„ç¼–ç å™¨ (ç”¨äº find_correct_document_rank_enhanced ä¸­çš„ç›¸ä¼¼åº¦è®¡ç®—)
        language: è¯­è¨€ ('zh' æˆ– 'en')
        dataset_name: æ•°æ®é›†åç§°
        reranker: Reranker å®ä¾‹ (å¯é€‰ï¼Œç”¨äºé‡æ’åº)
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"å¼€å§‹è¯„ä¼° {dataset_name} æ•°æ®é›† ({len(eval_data)} ä¸ªæ ·æœ¬) - ä½¿ç”¨ Encoder + FAISS{' + Reranker' if reranker else ''}...")
    
    mrr_scores = []
    hit_at_1 = 0
    hit_at_3 = 0
    hit_at_5 = 0
    hit_at_10 = 0
    found_samples = 0
    
    for i, sample in enumerate(tqdm(eval_data, desc=f"è¯„ä¼° {dataset_name}")):
        query = sample.get('query', sample.get('question', '')) # å…¼å®¹TatQAå’ŒAlphaFin
        gold_context_content = sample.get('context', '') # è¿™æ˜¯Gold Contextï¼Œç”¨äºåŒ¹é…

        if not query or not gold_context_content:
            continue
        
        try:
            # 1. æ‰§è¡Œåˆæ­¥æ£€ç´¢ (Encoder + FAISS)
            retrieved_result = retriever.retrieve(
                text=query, 
                top_k=50, # åˆå§‹æ£€ç´¢Kå€¼å¯ä»¥è®¾é«˜ä¸€äº›ï¼Œç»™Rerankeræ›´å¤šé€‰æ‹©
                return_scores=True, 
                language=language
            )
            
            initial_retrieved_docs: List[DocumentWithMetadata] # æ˜ç¡®ç±»å‹
            if isinstance(retrieved_result, tuple):
                initial_retrieved_docs, initial_scores = retrieved_result
            else:
                initial_retrieved_docs = retrieved_result
                initial_scores = []
            
            # -----------------------------------------------------------
            # 2. æ‰§è¡Œé‡æ’åº (Reranker)
            final_retrieved_docs_for_ranking: List[DocumentWithMetadata]
            if reranker and initial_retrieved_docs:
                # è·å–æ–‡æ¡£å†…å®¹åˆ—è¡¨ç”¨äºrerank
                docs_content_for_rerank = [doc.content for doc in initial_retrieved_docs]
                
                # è°ƒç”¨rerankerï¼Œè¿”å›æ’åºåçš„ (doc_text, score) å…ƒç»„åˆ—è¡¨
                reranked_items = reranker.rerank(
                    query=query, 
                    documents=docs_content_for_rerank, 
                    batch_size=4
                )

                # æ ¹æ®reranked_itemsçš„é¡ºåºï¼Œé‡å»º DocumentWithMetadata åˆ—è¡¨
                # éœ€è¦ä¸€ä¸ªä»contentåˆ°åŸå§‹DocumentWithMetadataçš„æ˜ å°„ï¼Œä»¥ä¿ç•™åŸå§‹IDç­‰å…ƒæ•°æ®
                content_to_original_doc_map = {doc.content: doc for doc in initial_retrieved_docs}
                
                temp_reranked_docs = []
                for doc_text, score in reranked_items[:20]:  # å–å‰20ä¸ªç»“æœ
                    original_doc = content_to_original_doc_map.get(doc_text)
                    if original_doc:
                        # ç¡®ä¿DocWithMetadataå¯¹è±¡è¢«æ·»åŠ åˆ°åˆ—è¡¨ä¸­
                        temp_reranked_docs.append(original_doc)
                
                final_retrieved_docs_for_ranking = temp_reranked_docs
            else:
                final_retrieved_docs_for_ranking = initial_retrieved_docs # å¦‚æœæ²¡æœ‰rerankerï¼Œä½¿ç”¨åˆæ­¥æ£€ç´¢ç»“æœ
            # -----------------------------------------------------------

            # 3. æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆçš„æ’å
            found_rank = find_correct_document_rank_enhanced(
                context=gold_context_content, # Gold Contextï¼Œæ¥è‡ªè¯„ä¼°æ ·æœ¬
                retrieved_docs=final_retrieved_docs_for_ranking, # ç»è¿‡é‡æ’åºçš„æ–‡æ¡£åˆ—è¡¨
                sample=sample, # å®Œæ•´çš„è¯„ä¼°æ ·æœ¬ï¼ŒåŒ…å« relevant_doc_ids ç­‰
                encoder=encoder # å¯¹åº”è¯­è¨€çš„ç¼–ç å™¨ (ç”¨äºç›¸ä¼¼åº¦ç­‰åŒ¹é…)
            )
            
            if found_rank > 0:
                found_samples += 1
                mrr_score = 1.0 / found_rank
                mrr_scores.append(mrr_score)
                
                if found_rank == 1:
                    hit_at_1 += 1
                if found_rank <= 3:
                    hit_at_3 += 1
                if found_rank <= 5:
                    hit_at_5 += 1
                if found_rank <= 10:
                    hit_at_10 += 1
            else:
                mrr_scores.append(0.0) # å¦‚æœæ²¡æ‰¾åˆ°ï¼ŒMRRè´¡çŒ®ä¸º0
                
        except Exception as e:
            print(f" Â æ ·æœ¬ {i} ({query[:30]}...) å¤„ç†å¤±è´¥: {e}")
            # traceback.print_exc() # æ‰“å°è¯¦ç»†é”™è¯¯ä¿¡æ¯ï¼Œè°ƒè¯•æ—¶ä½¿ç”¨
            mrr_scores.append(0.0) # å¤±è´¥çš„æ ·æœ¬MRRè´¡çŒ®ä¸º0
    
    # è®¡ç®—æŒ‡æ ‡
    total_samples = len(eval_data)
    mrr = sum(mrr_scores) / total_samples if total_samples > 0 else 0.0
    hit_at_1_rate = hit_at_1 / total_samples if total_samples > 0 else 0.0
    hit_at_3_rate = hit_at_3 / total_samples if total_samples > 0 else 0.0
    hit_at_5_rate = hit_at_5 / total_samples if total_samples > 0 else 0.0
    hit_at_10_rate = hit_at_10 / total_samples if total_samples > 0 else 0.0
    
    print(f" Â {dataset_name} è¯„ä¼°å®Œæˆ:")
    print(f" Â  Â MRR: {mrr:.4f}")
    print(f" Â  Â Hit@1: {hit_at_1_rate:.4f} ({hit_at_1}/{total_samples})")
    print(f" Â  Â Hit@3: {hit_at_3_rate:.4f} ({hit_at_3}/{total_samples})")
    print(f" Â  Â Hit@5: {hit_at_5_rate:.4f} ({hit_at_5}/{total_samples})")
    print(f" Â  Â Hit@10: {hit_at_10_rate:.4f} ({hit_at_10}/{total_samples})")
    print(f" Â  Â æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬æ•°: {found_samples}/{total_samples}") # æ‰¾åˆ°çš„æ ·æœ¬æ•°

    return {
        'mrr': mrr,
        'hit_at_1': hit_at_1_rate,
        'hit_at_3': hit_at_3_rate,
        'hit_at_5': hit_at_5_rate,
        'hit_at_10': hit_at_10_rate,
        'total_samples': total_samples,
        'found_samples': found_samples
    }

def test_retrieval_with_eval_context(include_eval_data: bool = True):
    """æµ‹è¯•æ£€ç´¢è´¨é‡ - å¯é€‰æ‹©æ˜¯å¦åŒ…å«è¯„ä¼°æ•°æ®åˆ°çŸ¥è¯†åº“"""
    mode = "åŒ…å«è¯„ä¼°æ•°æ®" if include_eval_data else "ä¸åŒ…å«è¯„ä¼°æ•°æ®"
    print("=" * 60)
    print(f"æµ‹è¯•æ£€ç´¢è´¨é‡ - MRRè¯„ä¼° ({mode}) - CPUç‰ˆæœ¬")
    print("=" * 60)
    
    try:
        config = Config()
        
        print("1. åŠ è½½ç¼–ç å™¨ï¼ˆCPUæ¨¡å¼ï¼‰...")
        encoder_ch = FinbertEncoder(
            model_name="./models/finetuned_alphafin_zh_optimized",
            cache_dir=config.encoder.cache_dir,
            device="cpu"  # å¼ºåˆ¶ä½¿ç”¨CPU
        )
        encoder_en = FinbertEncoder(
            model_name="models/finetuned_finbert_tatqa",
            cache_dir=config.encoder.cache_dir,
            device="cpu"  # å¼ºåˆ¶ä½¿ç”¨CPU
        )
        print(" Â  âœ… ç¼–ç å™¨åŠ è½½æˆåŠŸï¼ˆCPUæ¨¡å¼ï¼‰")
        
        print("\n2. åŠ è½½Rerankeræ¨¡å‹ï¼ˆCPUæ¨¡å¼ï¼‰...") # æ–°å¢ï¼šåŠ è½½Rerankeråˆ°æ­¤å‡½æ•°
        reranker_model = Reranker(
            model_name=config.reranker.model_name, 
            cache_dir=config.reranker.cache_dir, 
            device="cpu"
        )
        print(" Â  âœ… RerankeråŠ è½½æˆåŠŸï¼ˆCPUæ¨¡å¼ï¼‰")
        
        print("\n3. åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆçŸ¥è¯†åº“ï¼‰...")
        data_loader = OptimizedDataLoader(
            data_dir="data",
            max_samples=-1,  # åŠ è½½æ‰€æœ‰æ•°æ®
            chinese_document_level=True,
            english_chunk_level=True,
            include_eval_data=include_eval_data  # ç›´æ¥æ§åˆ¶æ˜¯å¦åŒ…å«è¯„ä¼°æ•°æ®
        )
        
        chinese_chunks = data_loader.chinese_docs
        english_chunks = data_loader.english_docs
        
        print(f" Â  âœ… è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ:")
        print(f" Â  Â  Â ä¸­æ–‡chunks: {len(chinese_chunks)}")
        print(f" Â  Â  Â è‹±æ–‡chunks: {len(english_chunks)}")
        
        print("\n4. åŠ è½½è¯„ä¼°æ•°æ®...")
        alphafin_eval = load_eval_data("evaluate_mrr/alphafin_eval.jsonl")
        tatqa_eval = load_eval_data("evaluate_mrr/tatqa_eval_enhanced.jsonl")
        
        print(f" Â  âœ… è¯„ä¼°æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f" Â  Â  Â AlphaFinè¯„ä¼°æ ·æœ¬: {len(alphafin_eval)}")
        print(f" Â  Â  Â TatQAå¢å¼ºç‰ˆè¯„ä¼°æ ·æœ¬: {len(tatqa_eval)}")
        
        print("\n5. åˆ›å»ºæ£€ç´¢å™¨ï¼ˆCPUæ¨¡å¼ï¼‰...")
        retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,
            corpus_documents_en=english_chunks,
            corpus_documents_ch=chinese_chunks,
            use_faiss=True,
            use_gpu=False,  # å¼ºåˆ¶ä¸ä½¿ç”¨GPU
            batch_size=4,   # å‡å°batch_sizeä»¥é€‚åº”CPU
            cache_dir=config.encoder.cache_dir
        )
        print(" Â  âœ… æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸï¼ˆCPUæ¨¡å¼ï¼‰")
        
        # è¯„ä¼°ä¸­æ–‡æ•°æ®
        print(f"\n--- è¯„ä¼°ä¸­æ–‡æ•°æ® (AlphaFin) ---")
        chinese_results = evaluate_dataset(
            eval_data=alphafin_eval,
            retriever=retriever,
            encoder=encoder_ch,
            language='zh',
            dataset_name="AlphaFin",
            reranker=reranker_model # ä¼ é€’Rerankerå®ä¾‹
        )
        
        # è¯„ä¼°è‹±æ–‡æ•°æ®
        print(f"\n--- è¯„ä¼°è‹±æ–‡æ•°æ® (TatQAå¢å¼ºç‰ˆ) ---")
        english_results = evaluate_dataset(
            eval_data=tatqa_eval,
            retriever=retriever,
            encoder=encoder_en,
            language='en',
            dataset_name="TatQA",
            reranker=reranker_model # ä¼ é€’Rerankerå®ä¾‹
        )
        
        # æ±‡æ€»ç»“æœ
        print("\n" + "=" * 60)
        print("è¯„ä¼°ç»“æœæ±‡æ€»")
        print("=" * 60)
        
        print(f"ä¸­æ–‡æ•°æ® (AlphaFin):")
        print(f" Â MRR: {chinese_results['mrr']:.4f}")
        print(f" Â Hit@1: {chinese_results['hit_at_1']:.4f}")
        print(f" Â Hit@3: {chinese_results['hit_at_3']:.4f}")
        print(f" Â Hit@5: {chinese_results['hit_at_5']:.4f}")
        print(f" Â Hit@10: {chinese_results['hit_at_10']:.4f}")
        print(f" Â æ€»æ ·æœ¬æ•°: {chinese_results['total_samples']}")
        print(f" Â æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬æ•°: {chinese_results['found_samples']}")
        
        print(f"\nè‹±æ–‡æ•°æ® (TatQAå¢å¼ºç‰ˆ):")
        print(f" Â MRR: {english_results['mrr']:.4f}")
        print(f" Â Hit@1: {english_results['hit_at_1']:.4f}")
        print(f" Â Hit@3: {english_results['hit_at_3']:.4f}")
        print(f" Â Hit@5: {english_results['hit_at_5']:.4f}")
        print(f" Â Hit@10: {english_results['hit_at_10']:.4f}")
        print(f" Â æ€»æ ·æœ¬æ•°: {english_results['total_samples']}")
        print(f" Â æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬æ•°: {english_results['found_samples']}")
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_samples = chinese_results['total_samples'] + english_results['total_samples']
        total_found = chinese_results['found_samples'] + english_results['found_samples']
        all_mrr_scores = chinese_results['mrr_scores_raw'] + english_results['mrr_scores_raw'] # å‡è®¾evaluate_datasetè¿”å›mrr_scores_raw
        overall_mrr = sum(all_mrr_scores) / total_samples if total_samples > 0 else 0.0

        # éœ€è¦é‡æ–°è®¡ç®—æ€»ä½“çš„Hit@Kï¼Œå› ä¸ºä¸èƒ½ç®€å•æ±‚å¹³å‡
        overall_hit1 = (chinese_results['hit_at_1'] * chinese_results['total_samples'] + english_results['hit_at_1'] * english_results['total_samples']) / total_samples if total_samples > 0 else 0.0
        overall_hit_at_3 = (chinese_results['hit_at_3'] * chinese_results['total_samples'] + english_results['hit_at_3'] * english_results['total_samples']) / total_samples if total_samples > 0 else 0.0
        overall_hit_at_5 = (chinese_results['hit_at_5'] * chinese_results['total_samples'] + english_results['hit_at_5'] * english_results['total_samples']) / total_samples if total_samples > 0 else 0.0
        overall_hit_at_10 = (chinese_results['hit_at_10'] * chinese_results['total_samples'] + english_results['hit_at_10'] * english_results['total_samples']) / total_samples if total_samples > 0 else 0.0

        print(f"\næ€»ä½“æ£€ç´¢:")
        print(f" Â æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f" Â æ€»ä½“MRR: {overall_mrr:.4f}")
        print(f" Â æ€»ä½“Hit@1: {overall_hit1:.4f}")
        print(f" Â æ€»ä½“Hit@3: {overall_hit_at_3:.4f}")
        print(f" Â æ€»ä½“Hit@5: {overall_hit_at_5:.4f}")
        print(f" Â æ€»ä½“Hit@10: {overall_hit_at_10:.4f}")
        print(f" Â æ€»æ‰¾åˆ°æ•°: {total_found}")
        print(f" Â æ€»ä½“å¬å›ç‡: {total_found/total_samples:.4f}")
        
        return {
            'chinese': chinese_results,
            'english': english_results,
            'overall': {
                'mrr': overall_mrr,
                'hit_at_1': overall_hit1,
                'hit_at_3': overall_hit_at_3,
                'hit_at_5': overall_hit_at_5,
                'hit_at_10': overall_hit_at_10,
                'total_samples': total_samples,
                'found_samples': total_found
            }
        }
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def compare_retrieval_modes():
    """æ¯”è¾ƒä¸åŒæ£€ç´¢æ¨¡å¼çš„æ•ˆæœ"""
    print("=" * 60)
    print("æ¯”è¾ƒä¸åŒæ£€ç´¢æ¨¡å¼çš„æ•ˆæœ")
    print("=" * 60)
    
    # æµ‹è¯•åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    print("\n1. æµ‹è¯•åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼...")
    results_with_eval = test_retrieval_with_eval_context(include_eval_data=True)
    
    # æµ‹è¯•ä¸åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    print("\n2. æµ‹è¯•ä¸åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼...")
    results_without_eval = test_retrieval_with_eval_context(include_eval_data=False)
    
    # æ¯”è¾ƒç»“æœ
    print("\n" + "=" * 60)
    print("æ¨¡å¼æ¯”è¾ƒç»“æœ")
    print("=" * 60)
    
    if results_with_eval and results_without_eval:
        print("åŒ…å«è¯„ä¼°æ•°æ® vs ä¸åŒ…å«è¯„ä¼°æ•°æ®:")
        print(f"ä¸­æ–‡MRR: {results_with_eval['chinese']['mrr']:.4f} vs {results_without_eval['chinese']['mrr']:.4f}")
        print(f"è‹±æ–‡MRR: {results_with_eval['english']['mrr']:.4f} vs {results_without_eval['english']['mrr']:.4f}")
        print(f"æ€»ä½“MRR: {results_with_eval['overall']['mrr']:.4f} vs {results_without_eval['overall']['mrr']:.4f}")
    else:
        print("âŒ æ¯”è¾ƒå¤±è´¥")

def test_retrieval_quality():
    """æµ‹è¯•æ£€ç´¢è´¨é‡"""
    print("=" * 60)
    print("æµ‹è¯•æ£€ç´¢è´¨é‡")
    print("=" * 60)
    
    # é»˜è®¤æµ‹è¯•ä¸åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    results = test_retrieval_with_eval_context(include_eval_data=False)
    
    if results:
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print(f"æ€»ä½“MRR: {results['overall']['mrr']:.4f}")
        print(f"æ€»ä½“Hit@1: {results['overall']['hit_at_1']:.4f}")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")

def evaluate_retrieval_quality(include_eval_data=True, max_eval_samples=None):
    """
    å®Œæ•´è¯„ä¼°æ£€ç´¢è´¨é‡ (CPUç‰ˆæœ¬)
    
    Args:
        include_eval_data: æ˜¯å¦åŒ…å«è¯„ä¼°æ•°æ®åˆ°çŸ¥è¯†åº“
        max_eval_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°
    """
    print("=" * 60)
    print(f"å®Œæ•´è¯„ä¼°æ£€ç´¢è´¨é‡ (CPUç‰ˆæœ¬)")
    print(f"åŒ…å«è¯„ä¼°æ•°æ®: {include_eval_data}")
    print(f"æœ€å¤§æ ·æœ¬æ•°: {max_eval_samples if max_eval_samples else 'æ‰€æœ‰'}")
    print("=" * 60)
    
    try:
        config = Config()
        
        print("1. åŠ è½½ç¼–ç å™¨ï¼ˆCPUæ¨¡å¼ï¼‰...")
        encoder_ch = FinbertEncoder(
            model_name="./models/finetuned_alphafin_zh_optimized",
            cache_dir=config.encoder.cache_dir,
            device="cpu"
        )
        encoder_en = FinbertEncoder(
            model_name="models/finetuned_finbert_tatqa",
            cache_dir=config.encoder.cache_dir,
            device="cpu"
        )
        print(" Â  âœ… ç¼–ç å™¨åŠ è½½æˆåŠŸ")
        
        print("\n2. åŠ è½½Rerankeræ¨¡å‹ï¼ˆCPUæ¨¡å¼ï¼‰...") 
        reranker_model = Reranker(
            model_name=config.reranker.model_name, 
            cache_dir=config.reranker.cache_dir, 
            device="cpu"
        )
        print(" Â  âœ… RerankeråŠ è½½æˆåŠŸ")
        
        print("\n3. åŠ è½½æ•°æ®...")
        data_loader = OptimizedDataLoader(
            data_dir="data",
            max_samples=-1,
            chinese_document_level=True,
            english_chunk_level=True,
            include_eval_data=include_eval_data
        )
        
        chinese_chunks = data_loader.chinese_docs
        english_chunks = data_loader.english_docs
        
        print(f" Â  âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f" Â  Â  Â ä¸­æ–‡chunks: {len(chinese_chunks)}")
        print(f" Â  Â  Â è‹±æ–‡chunks: {len(english_chunks)}")
        
        print("\n4. åŠ è½½è¯„ä¼°æ•°æ®...")
        alphafin_eval = load_eval_data("evaluate_mrr/alphafin_eval.jsonl")
        tatqa_eval = load_eval_data("evaluate_mrr/tatqa_eval_enhanced.jsonl") 
        
        if max_eval_samples:
            alphafin_eval = alphafin_eval[:max_eval_samples]
            tatqa_eval = tatqa_eval[:max_eval_samples]
        
        print(f" Â  âœ… è¯„ä¼°æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f" Â  Â  Â AlphaFinè¯„ä¼°æ ·æœ¬: {len(alphafin_eval)}")
        print(f" Â  Â  Â TatQAå¢å¼ºç‰ˆè¯„ä¼°æ ·æœ¬: {len(tatqa_eval)}")
        
        print("\n5. åˆ›å»ºæ£€ç´¢å™¨ï¼ˆCPUæ¨¡å¼ï¼‰...")
        retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,
            corpus_documents_en=english_chunks,
            corpus_documents_ch=chinese_chunks,
            use_faiss=True,
            use_gpu=False,
            batch_size=4,
            cache_dir=config.encoder.cache_dir
        )
        print(" Â  âœ… æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        
        # è¯„ä¼°ä¸­æ–‡æ•°æ®
        print(f"\n--- è¯„ä¼°ä¸­æ–‡æ•°æ® (AlphaFin) ---")
        chinese_results = evaluate_dataset(
            eval_data=alphafin_eval,
            retriever=retriever,
            encoder=encoder_ch,
            language='zh',
            dataset_name="AlphaFin",
            reranker=reranker_model # ä¼ é€’Rerankerå®ä¾‹
        )
        
        # è¯„ä¼°è‹±æ–‡æ•°æ®
        print(f"\n--- è¯„ä¼°è‹±æ–‡æ•°æ® (TatQAå¢å¼ºç‰ˆ) ---")
        english_results = evaluate_dataset(
            eval_data=tatqa_eval,
            retriever=retriever,
            encoder=encoder_en,
            language='en',
            dataset_name="TatQA",
            reranker=reranker_model # ä¼ é€’Rerankerå®ä¾‹
        )
        
        # æ±‡æ€»ç»“æœ
        print(f"\n=== è¯„ä¼°ç»“æœæ±‡æ€» ===")
        print(f"ä¸­æ–‡æ•°æ® (AlphaFin):")
        print(f" Â MRR: {chinese_results['mrr']:.4f}")
        print(f" Â Hit@1: {chinese_results['hit_at_1']:.4f}")
        print(f" Â Hit@3: {chinese_results['hit_at_3']:.4f}")
        print(f" Â Hit@5: {chinese_results['hit_at_5']:.4f}")
        print(f" Â Hit@10: {chinese_results['hit_at_10']:.4f}")
        print(f" Â æ€»æ ·æœ¬æ•°: {chinese_results['total_samples']}")
        print(f" Â æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬æ•°: {chinese_results['found_samples']}")
        
        print(f"\nè‹±æ–‡æ•°æ® (TatQAå¢å¼ºç‰ˆ):")
        print(f" Â MRR: {english_results['mrr']:.4f}")
        print(f" Â Hit@1: {english_results['hit_at_1']:.4f}")
        print(f" Â Hit@3: {english_results['hit_at_3']:.4f}")
        print(f" Â Hit@5: {english_results['hit_at_5']:.4f}")
        print(f" Â Hit@10: {english_results['hit_at_10']:.4f}")
        print(f" Â æ€»æ ·æœ¬æ•°: {english_results['total_samples']}")
        print(f" Â æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬æ•°: {english_results['found_samples']}")
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        # è¿™é‡Œçš„æ€»ä½“æŒ‡æ ‡è®¡ç®—æ–¹å¼å·²è°ƒæ•´ä¸ºå¯¹å„ä¸ªæ ·æœ¬çš„MRRå’ŒHit@Kè¿›è¡Œæ±‡æ€»ï¼Œè€Œéç®€å•å¹³å‡
        all_mrr_scores = chinese_results['mrr_scores_raw'] + english_results['mrr_scores_raw'] 
        all_hit_at_1_raw = chinese_results['hit_at_1_raw'] + english_results['hit_at_1_raw']
        all_hit_at_3_raw = chinese_results['hit_at_3_raw'] + english_results['hit_at_3_raw']
        all_hit_at_5_raw = chinese_results['hit_at_5_raw'] + english_results['hit_at_5_raw']
        all_hit_at_10_raw = chinese_results['hit_at_10_raw'] + english_results['hit_at_10_raw']

        total_samples_overall = len(all_mrr_scores) # è¿™é‡Œçš„total_samples_overallå°±æ˜¯æ‰€æœ‰æ ·æœ¬çš„æ€»æ•°
        
        overall_mrr = sum(all_mrr_scores) / total_samples_overall if total_samples_overall > 0 else 0.0
        overall_hit_at_1 = sum(all_hit_at_1_raw) / total_samples_overall if total_samples_overall > 0 else 0.0
        overall_hit_at_3 = sum(all_hit_at_3_raw) / total_samples_overall if total_samples_overall > 0 else 0.0
        overall_hit_at_5 = sum(all_hit_at_5_raw) / total_samples_overall if total_samples_overall > 0 else 0.0
        overall_hit_at_10 = sum(all_hit_at_10_raw) / total_samples_overall if total_samples_overall > 0 else 0.0
        
        total_found_overall = chinese_results['found_samples'] + english_results['found_samples']

        print(f"\næ€»ä½“æ£€ç´¢:")
        print(f" Â æ€»æ ·æœ¬æ•°: {total_samples_overall}")
        print(f" Â æ€»ä½“MRR: {overall_mrr:.4f}")
        print(f" Â æ€»ä½“Hit@1: {overall_hit_at_1:.4f}")
        print(f" Â æ€»ä½“Hit@3: {overall_hit_at_3:.4f}")
        print(f" Â æ€»ä½“Hit@5: {overall_hit_at_5:.4f}")
        print(f" Â æ€»ä½“Hit@10: {overall_hit_at_10:.4f}")
        print(f" Â æ€»æ‰¾åˆ°æ•°: {total_found_overall}")
        print(f" Â æ€»ä½“å¬å›ç‡: {total_found_overall/total_samples_overall:.4f}")
        
        return {
            'chinese': chinese_results,
            'english': english_results,
            'overall': {
                'mrr': overall_mrr,
                'hit_at_1': overall_hit_at_1,
                'hit_at_3': overall_hit_at_3,
                'hit_at_5': overall_hit_at_5,
                'hit_at_10': overall_hit_at_10,
                'total_samples': total_samples_overall,
                'found_samples': total_found_overall
            }
        }
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def compare_retrieval_modes():
    """æ¯”è¾ƒä¸åŒæ£€ç´¢æ¨¡å¼çš„æ•ˆæœ"""
    print("=" * 60)
    print("æ¯”è¾ƒä¸åŒæ£€ç´¢æ¨¡å¼çš„æ•ˆæœ")
    print("=" * 60)
    
    # æµ‹è¯•åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    print("\n1. æµ‹è¯•åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼...")
    results_with_eval = evaluate_retrieval_quality(include_eval_data=True) # è°ƒç”¨ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°
    
    # æµ‹è¯•ä¸åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    print("\n2. æµ‹è¯•ä¸åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼...")
    results_without_eval = evaluate_retrieval_quality(include_eval_data=False) # è°ƒç”¨ç»Ÿä¸€çš„è¯„ä¼°å‡½æ•°
    
    # æ¯”è¾ƒç»“æœ
    print("\n" + "=" * 60)
    print("æ¨¡å¼æ¯”è¾ƒç»“æœ")
    print("=" * 60)
    
    if results_with_eval and results_without_eval:
        print("åŒ…å«è¯„ä¼°æ•°æ® vs ä¸åŒ…å«è¯„ä¼°æ•°æ®:")
        print(f"ä¸­æ–‡MRR: {results_with_eval['chinese']['mrr']:.4f} vs {results_without_eval['chinese']['mrr']:.4f}")
        print(f"è‹±æ–‡MRR: {results_with_eval['english']['mrr']:.4f} vs {results_without_eval['english']['mrr']:.4f}")
        print(f"æ€»ä½“MRR: {results_with_eval['overall']['mrr']:.4f} vs {results_without_eval['overall']['mrr']:.4f}")
    else:
        print("âŒ æ¯”è¾ƒå¤±è´¥")

def test_retrieval_quality():
    """æµ‹è¯•æ£€ç´¢è´¨é‡ (æ—§ç‰ˆå‡½æ•°ï¼Œæ¨èä½¿ç”¨ evaluate_retrieval_quality)"""
    print("=" * 60)
    print("æµ‹è¯•æ£€ç´¢è´¨é‡")
    print("=" * 60)
    
    # é»˜è®¤æµ‹è¯•ä¸åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    results = evaluate_retrieval_quality(include_eval_data=False)
    
    if results:
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print(f"æ€»ä½“MRR: {results['overall']['mrr']:.4f}")
        print(f"æ€»ä½“Hit@1: {results['overall']['hit_at_1']:.4f}")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")

# --- ä¸»æ‰§è¡Œé€»è¾‘ ---
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯„ä¼°æ£€ç´¢è´¨é‡ï¼ˆCPUç‰ˆæœ¬ï¼‰")
    parser.add_argument("--include_eval_data", action="store_true", 
                        help="æ˜¯å¦å°†è¯„ä¼°æ•°æ®åŒ…å«åœ¨çŸ¥è¯†åº“ä¸­")
    parser.add_argument("--max_samples", type=int, default=None,
                        help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºè¯„ä¼°æ‰€æœ‰æ ·æœ¬")
    parser.add_argument("--test_mode", action="store_true",
                        help="æµ‹è¯•æ¨¡å¼ï¼Œåªè¯„ä¼°å°‘é‡æ ·æœ¬ï¼ˆå·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ --max_samplesï¼‰")
    parser.add_argument("--compare_modes", action="store_true",
                        help="æ¯”è¾ƒä¸åŒæ£€ç´¢æ¨¡å¼ï¼ˆçŸ¥è¯†åº“æ˜¯å¦åŒ…å«è¯„ä¼°æ•°æ®ï¼‰")
    parser.add_argument("--full_eval", action="store_true",
                        help="æ‰§è¡Œå®Œæ•´è¯„ä¼°æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ï¼Œè®¡ç®—æ‰€æœ‰æŒ‡æ ‡")
    
    args = parser.parse_args()
    
    if args.compare_modes:
        compare_retrieval_modes()
    elif args.full_eval: # æ–°å¢çš„å®Œæ•´è¯„ä¼°æ¨¡å¼
        evaluate_retrieval_quality(
            include_eval_data=args.include_eval_data,
            max_eval_samples=args.max_samples
        )
    else: # å…¼å®¹æ—§çš„ test_mode æˆ–å…¶ä»–é»˜è®¤è¡Œä¸ºï¼Œä½†æ¨èä½¿ç”¨ --full_eval
        # å¦‚æœæ—¢æ²¡æœ‰ --compare_modes ä¹Ÿæ²¡æœ‰ --full_evalï¼Œåˆ™æ‰§è¡Œé»˜è®¤çš„ test_retrieval_quality
        # ï¼ˆå¯èƒ½åªè¯„ä¼°å°‘é‡æ ·æœ¬æˆ–æ—§é€»è¾‘ï¼‰
        print("Warning: Running default or old test mode. Consider using --full_eval for comprehensive assessment.")
        test_retrieval_quality() # é»˜è®¤æˆ–æ—§çš„ç®€åŒ–æµ‹è¯•