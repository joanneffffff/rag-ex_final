#!/usr/bin/env python3
"""
æµ‹è¯•æ£€ç´¢è´¨é‡ - MRRè¯„ä¼° (CPUç‰ˆæœ¬)
ä½¿ç”¨evaluate_mrr/alphafin_eval.jsonlå’Œtatqa_eval_enhanced.jsonlä½œä¸ºæµ‹è¯•é›†
æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. è¯„ä¼°æ•°æ®contextåŠ å…¥çŸ¥è¯†åº“ - æµ‹è¯•çœŸå®æ£€ç´¢èƒ½åŠ›
2. è¯„ä¼°æ•°æ®contextä¸åŠ å…¥çŸ¥è¯†åº“ - é¿å…æ•°æ®æ³„éœ²

æ”¹è¿›çš„åŒ¹é…ç­–ç•¥ï¼š
1. relevant_doc_idsåŒ¹é…ï¼ˆæœ€ä¸¥æ ¼ï¼Œé€‚ç”¨äºè‹±æ–‡æ•°æ®ï¼‰
2. IDåŒ¹é…ï¼ˆé€‚ç”¨äºä¸­æ–‡æ•°æ®ï¼‰
3. å†…å®¹å“ˆå¸ŒåŒ¹é…
4. ç›¸ä¼¼åº¦åŒ¹é…
5. æ¨¡ç³Šæ–‡æœ¬åŒ¹é…
"""

import sys
import os
import json
import numpy as np
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from tqdm import tqdm

sys.path.append(str(Path(__file__).parent))

# å¯¼å…¥å¿…è¦çš„ç±»å‹
from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata

# å¯¼å…¥å¢å¼ºç‰ˆè¯„ä¼°å‡½æ•°
sys.path.append(str(Path(__file__).parent.parent))
from enhanced_evaluation_functions import find_correct_document_rank_enhanced

def load_eval_data(eval_file: str) -> List[Dict[str, Any]]:
    """åŠ è½½è¯„ä¼°æ•°æ®"""
    data = []
    with open(eval_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

def calculate_mrr(ranks: List[int]) -> float:
    """è®¡ç®—MRR (Mean Reciprocal Rank)"""
    if not ranks:
        return 0.0
    reciprocal_ranks = [1.0 / rank if rank > 0 else 0.0 for rank in ranks]
    return float(np.mean(reciprocal_ranks))

def calculate_hit_rate(ranks: List[int], k: int = 1) -> float:
    """è®¡ç®—Hit@k"""
    if not ranks:
        return 0.0
    hits = [1 if rank <= k and rank > 0 else 0 for rank in ranks]
    return float(np.mean(hits))

def calculate_content_hash(text: str) -> str:
    """è®¡ç®—æ–‡æœ¬å†…å®¹çš„å“ˆå¸Œå€¼"""
    return hashlib.md5(text.encode('utf-8')).hexdigest()

def find_correct_document_rank(
    context: str, 
    retrieved_docs: List[DocumentWithMetadata], 
    sample: Dict[str, Any],
    encoder=None
) -> int:
    """
    ä½¿ç”¨å¤šç§ç­–ç•¥æŸ¥æ‰¾æ­£ç¡®ç­”æ¡ˆçš„æ’åï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
    
    Args:
        context: æ­£ç¡®ç­”æ¡ˆçš„context
        retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
        sample: è¯„ä¼°æ ·æœ¬
        encoder: ç¼–ç å™¨ï¼ˆç”¨äºç›¸ä¼¼åº¦è®¡ç®—ï¼‰
    
    Returns:
        æ‰¾åˆ°çš„æ’åï¼Œ0è¡¨ç¤ºæœªæ‰¾åˆ°
    """
    # ä½¿ç”¨å¢å¼ºç‰ˆå‡½æ•°
    return find_correct_document_rank_enhanced(context, retrieved_docs, sample, encoder)

def test_retrieval_with_eval_context(include_eval_data: bool = True):
    """æµ‹è¯•æ£€ç´¢è´¨é‡ - å¯é€‰æ‹©æ˜¯å¦åŒ…å«è¯„ä¼°æ•°æ®åˆ°çŸ¥è¯†åº“"""
    mode = "åŒ…å«è¯„ä¼°æ•°æ®" if include_eval_data else "ä¸åŒ…å«è¯„ä¼°æ•°æ®"
    print("=" * 60)
    print(f"æµ‹è¯•æ£€ç´¢è´¨é‡ - MRRè¯„ä¼° ({mode}) - CPUç‰ˆæœ¬")
    print("=" * 60)
    
    try:
        from config.parameters import Config
        from xlm.components.retriever.bilingual_retriever import BilingualRetriever
        from xlm.components.encoder.finbert import FinbertEncoder
        from xlm.utils.optimized_data_loader import OptimizedDataLoader
        
        config = Config()
        
        print("1. åŠ è½½ç¼–ç å™¨ï¼ˆCPUæ¨¡å¼ï¼‰...")
        encoder_ch = FinbertEncoder(
            model_name="./models/finetuned_alphafin_zh_optimized",
            cache_dir=config.encoder.cache_dir,
            device="cpu"  # å¼ºåˆ¶ä½¿ç”¨CPU
        )
        encoder_en = FinbertEncoder(
            model_name="models/finetuned_finbert_tatqa",
            cache_dir=config.encoder.cache_dir,
            device="cpu"  # å¼ºåˆ¶ä½¿ç”¨CPU
        )
        print("   âœ… ç¼–ç å™¨åŠ è½½æˆåŠŸï¼ˆCPUæ¨¡å¼ï¼‰")
        
        print("\n2. åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆçŸ¥è¯†åº“ï¼‰...")
        data_loader = OptimizedDataLoader(
            data_dir="data",
            max_samples=-1,  # åŠ è½½æ‰€æœ‰æ•°æ®
            chinese_document_level=True,
            english_chunk_level=True,
            include_eval_data=include_eval_data  # ç›´æ¥æ§åˆ¶æ˜¯å¦åŒ…å«è¯„ä¼°æ•°æ®
        )
        
        chinese_chunks = data_loader.chinese_docs
        english_chunks = data_loader.english_docs
        
        print(f"   âœ… è®­ç»ƒæ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"      ä¸­æ–‡chunks: {len(chinese_chunks)}")
        print(f"      è‹±æ–‡chunks: {len(english_chunks)}")
        
        print("\n3. åŠ è½½è¯„ä¼°æ•°æ®...")
        alphafin_eval = load_eval_data("evaluate_mrr/alphafin_eval.jsonl")
        # ä½¿ç”¨å¢å¼ºç‰ˆTatQAæ•°æ®
        tatqa_eval = load_eval_data("evaluate_mrr/tatqa_eval_enhanced.jsonl")
        
        print(f"   âœ… è¯„ä¼°æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"      AlphaFinè¯„ä¼°æ ·æœ¬: {len(alphafin_eval)}")
        print(f"      TatQAå¢å¼ºç‰ˆè¯„ä¼°æ ·æœ¬: {len(tatqa_eval)}")
        
        print("\n4. åˆ›å»ºæ£€ç´¢å™¨ï¼ˆCPUæ¨¡å¼ï¼‰...")
        retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,
            corpus_documents_en=english_chunks,
            corpus_documents_ch=chinese_chunks,
            use_faiss=True,
            use_gpu=False,  # å¼ºåˆ¶ä¸ä½¿ç”¨GPU
            batch_size=4,   # å‡å°batch_sizeä»¥é€‚åº”CPU
            cache_dir=config.encoder.cache_dir
        )
        print("   âœ… æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸï¼ˆCPUæ¨¡å¼ï¼‰")
        
        print("\n5. æµ‹è¯•ä¸­æ–‡æ£€ç´¢è´¨é‡ (AlphaFin)...")
        chinese_ranks = []
        chinese_queries = []
        
        for i, sample in enumerate(tqdm(alphafin_eval[:50], desc="æµ‹è¯•ä¸­æ–‡æ£€ç´¢")):  # å‡å°‘æµ‹è¯•æ ·æœ¬ä»¥é€‚åº”CPU
            query = sample.get('question', '')
            context = sample.get('context', '')
            
            if not query or not context:
                continue
                
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_result = retriever.retrieve(
                text=query, 
                top_k=20, 
                return_scores=True, 
                language='zh'
            )
            
            # å¤„ç†è¿”å›å€¼ï¼šå¯èƒ½æ˜¯å…ƒç»„(documents, scores)æˆ–åªæ˜¯documents
            if isinstance(retrieved_result, tuple):
                retrieved_docs, scores = retrieved_result
            else:
                retrieved_docs = retrieved_result
                scores = []
            
            # æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨æ£€ç´¢ç»“æœä¸­
            found_rank = find_correct_document_rank(
                context, retrieved_docs, sample, encoder_ch
            )
            
            chinese_ranks.append(found_rank)
            chinese_queries.append(query)
            
            if i < 3:  # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                print(f"   æŸ¥è¯¢ {i+1}: {query[:50]}...")
                print(f"   æ‰¾åˆ°ä½ç½®: {found_rank}")
                if found_rank > 0:
                    print(f"   ç›¸å…³æ–‡æ¡£: {retrieved_docs[found_rank-1].content[:100]}...")
                print()
        
        print("\n6. æµ‹è¯•è‹±æ–‡æ£€ç´¢è´¨é‡ (TatQAå¢å¼ºç‰ˆ)...")
        english_ranks = []
        english_queries = []
        
        for i, sample in enumerate(tqdm(tatqa_eval[:50], desc="æµ‹è¯•è‹±æ–‡æ£€ç´¢")):  # å‡å°‘æµ‹è¯•æ ·æœ¬ä»¥é€‚åº”CPU
            query = sample.get('query', '')
            context = sample.get('context', '')
            
            if not query or not context:
                continue
                
            # æ£€ç´¢ç›¸å…³æ–‡æ¡£
            retrieved_result = retriever.retrieve(
                text=query, 
                top_k=20, 
                return_scores=True, 
                language='en'
            )
            
            # å¤„ç†è¿”å›å€¼ï¼šå¯èƒ½æ˜¯å…ƒç»„(documents, scores)æˆ–åªæ˜¯documents
            if isinstance(retrieved_result, tuple):
                retrieved_docs, scores = retrieved_result
            else:
                retrieved_docs = retrieved_result
                scores = []
            
            # æ£€æŸ¥æ­£ç¡®ç­”æ¡ˆæ˜¯å¦åœ¨æ£€ç´¢ç»“æœä¸­ï¼ˆä½¿ç”¨å¢å¼ºç‰ˆå‡½æ•°ï¼‰
            found_rank = find_correct_document_rank_enhanced(
                context, retrieved_docs, sample, encoder_en
            )
            
            english_ranks.append(found_rank)
            english_queries.append(query)
            
            if i < 3:  # æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
                print(f"   Query {i+1}: {query[:50]}...")
                print(f"   Found at rank: {found_rank}")
                print(f"   Relevant doc IDs: {sample.get('relevant_doc_ids', [])}")
                if found_rank > 0:
                    print(f"   Relevant doc: {retrieved_docs[found_rank-1].content[:100]}...")
                print()
        
        print("\n" + "=" * 60)
        print(f"æ£€ç´¢è´¨é‡è¯„ä¼°ç»“æœ ({mode}) - CPUç‰ˆæœ¬")
        print("=" * 60)
        
        # è®¡ç®—ä¸­æ–‡æ£€ç´¢æŒ‡æ ‡
        chinese_mrr = calculate_mrr(chinese_ranks)
        chinese_hit1 = calculate_hit_rate(chinese_ranks, k=1)
        chinese_hit5 = calculate_hit_rate(chinese_ranks, k=5)
        chinese_hit10 = calculate_hit_rate(chinese_ranks, k=10)
        
        print(f"ä¸­æ–‡æ£€ç´¢ (AlphaFin):")
        print(f"  æ ·æœ¬æ•°: {len(chinese_ranks)}")
        print(f"  MRR: {chinese_mrr:.4f}")
        print(f"  Hit@1: {chinese_hit1:.4f}")
        print(f"  Hit@5: {chinese_hit5:.4f}")
        print(f"  Hit@10: {chinese_hit10:.4f}")
        
        # è®¡ç®—è‹±æ–‡æ£€ç´¢æŒ‡æ ‡
        english_mrr = calculate_mrr(english_ranks)
        english_hit1 = calculate_hit_rate(english_ranks, k=1)
        english_hit5 = calculate_hit_rate(english_ranks, k=5)
        english_hit10 = calculate_hit_rate(english_ranks, k=10)
        
        print(f"\nè‹±æ–‡æ£€ç´¢ (TatQAå¢å¼ºç‰ˆ):")
        print(f"  æ ·æœ¬æ•°: {len(english_ranks)}")
        print(f"  MRR: {english_mrr:.4f}")
        print(f"  Hit@1: {english_hit1:.4f}")
        print(f"  Hit@5: {english_hit5:.4f}")
        print(f"  Hit@10: {english_hit10:.4f}")
        
        # æ€»ä½“æŒ‡æ ‡
        all_ranks = chinese_ranks + english_ranks
        overall_mrr = calculate_mrr(all_ranks)
        overall_hit1 = calculate_hit_rate(all_ranks, k=1)
        overall_hit5 = calculate_hit_rate(all_ranks, k=5)
        overall_hit10 = calculate_hit_rate(all_ranks, k=10)
        
        print(f"\næ€»ä½“æ£€ç´¢:")
        print(f"  æ ·æœ¬æ•°: {len(all_ranks)}")
        print(f"  MRR: {overall_mrr:.4f}")
        print(f"  Hit@1: {overall_hit1:.4f}")
        print(f"  Hit@5: {overall_hit5:.4f}")
        print(f"  Hit@10: {overall_hit10:.4f}")
        
        return {
            'chinese': {
                'mrr': chinese_mrr,
                'hit_at_1': chinese_hit1,
                'hit_at_5': chinese_hit5,
                'hit_at_10': chinese_hit10,
                'samples': len(chinese_ranks)
            },
            'english': {
                'mrr': english_mrr,
                'hit_at_1': english_hit1,
                'hit_at_5': english_hit5,
                'hit_at_10': english_hit10,
                'samples': len(english_ranks)
            },
            'overall': {
                'mrr': overall_mrr,
                'hit_at_1': overall_hit1,
                'hit_at_5': overall_hit5,
                'hit_at_10': overall_hit10,
                'samples': len(all_ranks)
            }
        }
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def compare_retrieval_modes():
    """æ¯”è¾ƒä¸åŒæ£€ç´¢æ¨¡å¼çš„æ•ˆæœ"""
    print("=" * 60)
    print("æ¯”è¾ƒä¸åŒæ£€ç´¢æ¨¡å¼çš„æ•ˆæœ")
    print("=" * 60)
    
    # æµ‹è¯•åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    print("\n1. æµ‹è¯•åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼...")
    results_with_eval = test_retrieval_with_eval_context(include_eval_data=True)
    
    # æµ‹è¯•ä¸åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    print("\n2. æµ‹è¯•ä¸åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼...")
    results_without_eval = test_retrieval_with_eval_context(include_eval_data=False)
    
    # æ¯”è¾ƒç»“æœ
    print("\n" + "=" * 60)
    print("æ¨¡å¼æ¯”è¾ƒç»“æœ")
    print("=" * 60)
    
    if results_with_eval and results_without_eval:
        print("åŒ…å«è¯„ä¼°æ•°æ® vs ä¸åŒ…å«è¯„ä¼°æ•°æ®:")
        print(f"ä¸­æ–‡MRR: {results_with_eval['chinese']['mrr']:.4f} vs {results_without_eval['chinese']['mrr']:.4f}")
        print(f"è‹±æ–‡MRR: {results_with_eval['english']['mrr']:.4f} vs {results_without_eval['english']['mrr']:.4f}")
        print(f"æ€»ä½“MRR: {results_with_eval['overall']['mrr']:.4f} vs {results_without_eval['overall']['mrr']:.4f}")
    else:
        print("âŒ æ¯”è¾ƒå¤±è´¥")

def test_retrieval_quality():
    """æµ‹è¯•æ£€ç´¢è´¨é‡"""
    print("=" * 60)
    print("æµ‹è¯•æ£€ç´¢è´¨é‡")
    print("=" * 60)
    
    # é»˜è®¤æµ‹è¯•ä¸åŒ…å«è¯„ä¼°æ•°æ®çš„æ¨¡å¼
    results = test_retrieval_with_eval_context(include_eval_data=False)
    
    if results:
        print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print(f"æ€»ä½“MRR: {results['overall']['mrr']:.4f}")
        print(f"æ€»ä½“Hit@1: {results['overall']['hit_at_1']:.4f}")
    else:
        print("âŒ æµ‹è¯•å¤±è´¥")

def evaluate_retrieval_quality(include_eval_data=True, max_eval_samples=None):
    """
    å®Œæ•´è¯„ä¼°æ£€ç´¢è´¨é‡
    
    Args:
        include_eval_data: æ˜¯å¦åŒ…å«è¯„ä¼°æ•°æ®åˆ°çŸ¥è¯†åº“
        max_eval_samples: æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°
    """
    print("=" * 60)
    print(f"å®Œæ•´è¯„ä¼°æ£€ç´¢è´¨é‡ (CPUç‰ˆæœ¬)")
    print(f"åŒ…å«è¯„ä¼°æ•°æ®: {include_eval_data}")
    print(f"æœ€å¤§æ ·æœ¬æ•°: {max_eval_samples}")
    print("=" * 60)
    
    try:
        from config.parameters import Config
        from xlm.components.retriever.bilingual_retriever import BilingualRetriever
        from xlm.components.encoder.finbert import FinbertEncoder
        from xlm.utils.optimized_data_loader import OptimizedDataLoader
        
        config = Config()
        
        print("1. åŠ è½½ç¼–ç å™¨ï¼ˆCPUæ¨¡å¼ï¼‰...")
        encoder_ch = FinbertEncoder(
            model_name="./models/finetuned_alphafin_zh_optimized",
            cache_dir=config.encoder.cache_dir,
            device="cpu"
        )
        encoder_en = FinbertEncoder(
            model_name="models/finetuned_finbert_tatqa",
            cache_dir=config.encoder.cache_dir,
            device="cpu"
        )
        print("   âœ… ç¼–ç å™¨åŠ è½½æˆåŠŸ")
        
        print("\n2. åŠ è½½æ•°æ®...")
        data_loader = OptimizedDataLoader(
            data_dir="data",
            max_samples=-1,
            chinese_document_level=True,
            english_chunk_level=True,
            include_eval_data=include_eval_data
        )
        
        chinese_chunks = data_loader.chinese_docs
        english_chunks = data_loader.english_docs
        
        print(f"   âœ… æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"      ä¸­æ–‡chunks: {len(chinese_chunks)}")
        print(f"      è‹±æ–‡chunks: {len(english_chunks)}")
        
        print("\n3. åŠ è½½è¯„ä¼°æ•°æ®...")
        alphafin_eval = load_eval_data("evaluate_mrr/alphafin_eval.jsonl")
        tatqa_eval = load_eval_data("evaluate_mrr/tatqa_eval_enhanced.jsonl")  # ä½¿ç”¨å¢å¼ºç‰ˆ
        
        if max_eval_samples:
            alphafin_eval = alphafin_eval[:max_eval_samples]
            tatqa_eval = tatqa_eval[:max_eval_samples]
        
        print(f"   âœ… è¯„ä¼°æ•°æ®åŠ è½½æˆåŠŸ:")
        print(f"      AlphaFinè¯„ä¼°æ ·æœ¬: {len(alphafin_eval)}")
        print(f"      TatQAå¢å¼ºç‰ˆè¯„ä¼°æ ·æœ¬: {len(tatqa_eval)}")
        
        print("\n4. åˆ›å»ºæ£€ç´¢å™¨ï¼ˆCPUæ¨¡å¼ï¼‰...")
        retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,
            corpus_documents_en=english_chunks,
            corpus_documents_ch=chinese_chunks,
            use_faiss=True,
            use_gpu=False,
            batch_size=4,
            cache_dir=config.encoder.cache_dir
        )
        print("   âœ… æ£€ç´¢å™¨åˆ›å»ºæˆåŠŸ")
        
        # è¯„ä¼°ä¸­æ–‡æ•°æ®
        print(f"\n--- è¯„ä¼°ä¸­æ–‡æ•°æ® (AlphaFin) ---")
        chinese_results = evaluate_dataset(
            eval_data=alphafin_eval,
            retriever=retriever,
            encoder=encoder_ch,
            language='zh',
            dataset_name="AlphaFin"
        )
        
        # è¯„ä¼°è‹±æ–‡æ•°æ®
        print(f"\n--- è¯„ä¼°è‹±æ–‡æ•°æ® (TatQAå¢å¼ºç‰ˆ) ---")
        english_results = evaluate_dataset(
            eval_data=tatqa_eval,
            retriever=retriever,
            encoder=encoder_en,
            language='en',
            dataset_name="TatQA"
        )
        
        # æ±‡æ€»ç»“æœ
        print(f"\n=== è¯„ä¼°ç»“æœæ±‡æ€» ===")
        print(f"ä¸­æ–‡æ•°æ® (AlphaFin):")
        print(f"  MRR: {chinese_results['mrr']:.4f}")
        print(f"  Hit@1: {chinese_results['hit_at_1']:.4f}")
        print(f"  Hit@3: {chinese_results['hit_at_3']:.4f}")
        print(f"  Hit@5: {chinese_results['hit_at_5']:.4f}")
        print(f"  Hit@10: {chinese_results['hit_at_10']:.4f}")
        print(f"  æ€»æ ·æœ¬æ•°: {chinese_results['total_samples']}")
        print(f"  æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬æ•°: {chinese_results['found_samples']}")
        
        print(f"\nè‹±æ–‡æ•°æ® (TatQAå¢å¼ºç‰ˆ):")
        print(f"  MRR: {english_results['mrr']:.4f}")
        print(f"  Hit@1: {english_results['hit_at_1']:.4f}")
        print(f"  Hit@3: {english_results['hit_at_3']:.4f}")
        print(f"  Hit@5: {english_results['hit_at_5']:.4f}")
        print(f"  Hit@10: {english_results['hit_at_10']:.4f}")
        print(f"  æ€»æ ·æœ¬æ•°: {english_results['total_samples']}")
        print(f"  æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ ·æœ¬æ•°: {english_results['found_samples']}")
        
        # è®¡ç®—æ€»ä½“æŒ‡æ ‡
        total_samples = chinese_results['total_samples'] + english_results['total_samples']
        total_found = chinese_results['found_samples'] + english_results['found_samples']
        overall_mrr = (chinese_results['mrr'] + english_results['mrr']) / 2
        overall_hit_at_1 = (chinese_results['hit_at_1'] + english_results['hit_at_1']) / 2
        
        print(f"\næ€»ä½“æŒ‡æ ‡:")
        print(f"  æ€»ä½“MRR: {overall_mrr:.4f}")
        print(f"  æ€»ä½“Hit@1: {overall_hit_at_1:.4f}")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  æ€»æ‰¾åˆ°æ•°: {total_found}")
        print(f"  æ€»ä½“å¬å›ç‡: {total_found/total_samples:.4f}")
        
        return {
            'chinese': chinese_results,
            'english': english_results,
            'overall': {
                'mrr': overall_mrr,
                'hit_at_1': overall_hit_at_1,
                'total_samples': total_samples,
                'found_samples': total_found
            }
        }
        
    except Exception as e:
        print(f"âŒ è¯„ä¼°å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def evaluate_dataset(eval_data, retriever, encoder, language, dataset_name):
    """
    è¯„ä¼°å•ä¸ªæ•°æ®é›†çš„æ£€ç´¢è´¨é‡
    
    Args:
        eval_data: è¯„ä¼°æ•°æ®åˆ—è¡¨
        retriever: æ£€ç´¢å™¨
        encoder: ç¼–ç å™¨
        language: è¯­è¨€ ('zh' æˆ– 'en')
        dataset_name: æ•°æ®é›†åç§°
    
    Returns:
        è¯„ä¼°ç»“æœå­—å…¸
    """
    print(f"å¼€å§‹è¯„ä¼° {dataset_name} æ•°æ®é›† ({len(eval_data)} ä¸ªæ ·æœ¬)...")
    
    mrr_scores = []
    hit_at_1 = 0
    hit_at_3 = 0
    hit_at_5 = 0
    hit_at_10 = 0
    found_samples = 0
    
    for i, sample in enumerate(eval_data):
        if i % 50 == 0:  # å‡å°‘è¿›åº¦æ˜¾ç¤ºé¢‘ç‡ä»¥é€‚åº”CPU
            print(f"  å¤„ç†è¿›åº¦: {i}/{len(eval_data)}")
        
        query = sample.get('query', sample.get('question', ''))
        context = sample.get('context', '')
        
        if not query or not context:
            continue
        
        try:
            # æ£€ç´¢
            retrieved_result = retriever.retrieve(
                text=query, 
                top_k=20, 
                return_scores=True, 
                language=language
            )
            
            if isinstance(retrieved_result, tuple):
                retrieved_docs, scores = retrieved_result
            else:
                retrieved_docs = retrieved_result
                scores = []
            
            # æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’åï¼ˆä½¿ç”¨å¢å¼ºç‰ˆå‡½æ•°ï¼‰
            found_rank = find_correct_document_rank_enhanced(
                context=context,
                retrieved_docs=retrieved_docs,
                sample=sample,
                encoder=encoder
            )
            
            if found_rank > 0:
                found_samples += 1
                mrr_score = 1.0 / found_rank
                mrr_scores.append(mrr_score)
                
                if found_rank == 1:
                    hit_at_1 += 1
                if found_rank <= 3:
                    hit_at_3 += 1
                if found_rank <= 5:
                    hit_at_5 += 1
                if found_rank <= 10:
                    hit_at_10 += 1
            else:
                mrr_scores.append(0.0)
                
        except Exception as e:
            print(f"   æ ·æœ¬ {i} å¤„ç†å¤±è´¥: {e}")
            mrr_scores.append(0.0)
    
    # è®¡ç®—æŒ‡æ ‡
    total_samples = len(eval_data)
    mrr = sum(mrr_scores) / total_samples if total_samples > 0 else 0.0
    hit_at_1_rate = hit_at_1 / total_samples if total_samples > 0 else 0.0
    hit_at_3_rate = hit_at_3 / total_samples if total_samples > 0 else 0.0
    hit_at_5_rate = hit_at_5 / total_samples if total_samples > 0 else 0.0
    hit_at_10_rate = hit_at_10 / total_samples if total_samples > 0 else 0.0
    
    print(f"  {dataset_name} è¯„ä¼°å®Œæˆ:")
    print(f"    MRR: {mrr:.4f}")
    print(f"    Hit@1: {hit_at_1_rate:.4f} ({hit_at_1}/{total_samples})")
    print(f"    Hit@3: {hit_at_3_rate:.4f} ({hit_at_3}/{total_samples})")
    print(f"    Hit@5: {hit_at_5_rate:.4f} ({hit_at_5}/{total_samples})")
    print(f"    Hit@10: {hit_at_10_rate:.4f} ({hit_at_10}/{total_samples})")
    print(f"    æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆ: {found_samples}/{total_samples}")
    
    return {
        'mrr': mrr,
        'hit_at_1': hit_at_1_rate,
        'hit_at_3': hit_at_3_rate,
        'hit_at_5': hit_at_5_rate,
        'hit_at_10': hit_at_10_rate,
        'total_samples': total_samples,
        'found_samples': found_samples
    }

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯„ä¼°æ£€ç´¢è´¨é‡ï¼ˆCPUç‰ˆæœ¬ï¼‰")
    parser.add_argument("--include_eval_data", action="store_true", 
                       help="æ˜¯å¦å°†è¯„ä¼°æ•°æ®åŒ…å«åœ¨çŸ¥è¯†åº“ä¸­")
    parser.add_argument("--max_samples", type=int, default=None,
                       help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºè¯„ä¼°æ‰€æœ‰æ ·æœ¬")
    parser.add_argument("--test_mode", action="store_true",
                       help="æµ‹è¯•æ¨¡å¼ï¼Œåªè¯„ä¼°å°‘é‡æ ·æœ¬")
    parser.add_argument("--compare_modes", action="store_true",
                       help="æ¯”è¾ƒä¸åŒæ£€ç´¢æ¨¡å¼")
    
    args = parser.parse_args()
    
    if args.compare_modes:
        print("=== æ¯”è¾ƒæ£€ç´¢æ¨¡å¼ ===")
        compare_retrieval_modes()
    elif args.test_mode:
        print("=== æµ‹è¯•æ¨¡å¼ï¼ˆCPUç‰ˆæœ¬ï¼‰===")
        test_retrieval_with_eval_context(include_eval_data=args.include_eval_data)
    else:
        print("=== å®Œæ•´è¯„ä¼°æ¨¡å¼ï¼ˆCPUç‰ˆæœ¬ï¼‰===")
        # é»˜è®¤è¯„ä¼°æ‰€æœ‰æ•°æ®
        max_samples = args.max_samples if args.max_samples else None
        results = evaluate_retrieval_quality(
            include_eval_data=args.include_eval_data,
            max_eval_samples=max_samples
        )
        
        if results:
            print("\nğŸ‰ è¯„ä¼°å®Œæˆï¼")
            print(f"æ€»ä½“MRR: {results['overall']['mrr']:.4f}")
            print(f"æ€»ä½“Hit@1: {results['overall']['hit_at_1']:.4f}")
        else:
            print("âŒ è¯„ä¼°å¤±è´¥") 