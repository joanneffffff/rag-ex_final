#!/usr/bin/env python3
"""
å¤šGPUå¹¶è¡Œç‰ˆæœ¬çš„RAGç³»ç»ŸçœŸå®æ£€ç´¢é€»è¾‘TatQAæ•°æ®é›†MRRè¯„ä¼°
åŒ…æ‹¬FAISSç´¢å¼•ã€BilingualRetrieverå’ŒQwen3-Rerankerï¼Œæ”¯æŒåŒGPUå¹¶è¡Œ
"""

import json
import argparse
import numpy as np
import torch
from pathlib import Path
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import sys
import multiprocessing as mp
from multiprocessing import Process, Queue
import time
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata
from xlm.components.encoder.finbert import FinbertEncoder
from xlm.components.retriever.bilingual_retriever import BilingualRetriever
from xlm.components.retriever.reranker import QwenReranker
from xlm.utils.dual_language_loader import DualLanguageLoader
from enhanced_evaluation_functions import find_correct_document_rank_enhanced

def get_question_or_query(item_dict):
    """ä»æ ·æœ¬ä¸­æå–é—®é¢˜æˆ–æŸ¥è¯¢"""
    if "query" in item_dict:
        return item_dict["query"]
    elif "question" in item_dict:
        return item_dict["question"]
    else:
        return None

def extract_unit_from_paragraph(paragraphs):
    """ä»æ®µè½ä¸­æå–å•ä½ä¿¡æ¯"""
    unit_info = ""
    for para in paragraphs:
        if isinstance(para, dict):
            text = para.get("text", "")
        else:
            text = para
        if "million" in text.lower() or "thousand" in text.lower():
            if "million" in text.lower():
                unit_info = "million USD"
            elif "thousand" in text.lower():
                unit_info = "thousand USD"
            break
    return unit_info

def table_to_natural_text(table_dict, caption="", unit_info=""):
    """å°†è¡¨æ ¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æ–‡æœ¬"""
    import re
    rows = table_dict.get("table", [])
    lines = []

    if caption:
        lines.append(f"Table Topic: {caption}.")

    if not rows:
        return ""

    headers = rows[0]
    data_rows = rows[1:]

    for i, row in enumerate(data_rows):
        if not row or all(str(v).strip() == "" for v in row):
            continue

        if len(row) > 1 and str(row[0]).strip() != "" and all(str(v).strip() == "" for v in row[1:]):
            lines.append(f"Table Category: {str(row[0]).strip()}.")
            continue

        row_name = str(row[0]).strip().replace('.', '')

        data_descriptions = []
        for h_idx, v in enumerate(row):
            if h_idx == 0:
                continue
            header = headers[h_idx] if h_idx < len(headers) else f"Column {h_idx+1}"
            value = str(v).strip()
            if value:
                if re.match(r'^-?\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?$|^\(\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?\)$', value):
                    formatted_value = value.replace('$', '')
                    if unit_info:
                        if formatted_value.startswith('(') and formatted_value.endswith(')'):
                             formatted_value = f"(${formatted_value[1:-1]} {unit_info})"
                        else:
                             formatted_value = f"${formatted_value} {unit_info}"
                    else:
                        formatted_value = f"${formatted_value}"
                else:
                    formatted_value = value
                data_descriptions.append(f"{header} is {formatted_value}")
        if row_name and data_descriptions:
            lines.append(f"Details for item {row_name}: {'; '.join(data_descriptions)}.")
        elif data_descriptions:
            lines.append(f"Other data item: {'; '.join(data_descriptions)}.")
        elif row_name:
            lines.append(f"Data item: {row_name}.")
    return "\n".join(lines)

def process_tatqa_to_qca_for_corpus(input_paths):
    """å¤„ç†TatQAåŸå§‹æ•°æ®ï¼Œæ„å»ºæ£€ç´¢åº“"""
    all_chunks = []
    global_doc_counter = 0

    for input_path in input_paths:
        if not Path(input_path).exists():
            print(f"è­¦å‘Šï¼šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {input_path}")
            continue
            
        with open(input_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            print(f"è­¦å‘Šï¼šæ–‡ä»¶ {Path(input_path).name} çš„é¡¶å±‚ç»“æ„ä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•ä½œä¸ºå•ä¸ªæ–‡æ¡£å¤„ç†ã€‚")
            data = [data]

        for i, item in enumerate(data):
            if not isinstance(item, dict):
                print(f"è­¦å‘Šï¼šæ–‡ä»¶ {Path(input_path).name} ä¸­å‘ç°éå­—å…¸é¡¹ï¼Œè·³è¿‡ã€‚é¡¹å†…å®¹ï¼š{item}")
                continue
            
            doc_id = item.get("doc_id")
            if doc_id is None:
                doc_id = f"generated_doc_{global_doc_counter}_{Path(input_path).stem}_{i}"
                global_doc_counter += 1

            paragraphs = item.get("paragraphs", [])
            tables = item.get("tables", [])

            unit_info = extract_unit_from_paragraph(paragraphs)

            for p_idx, para in enumerate(paragraphs):
                para_text = para.get("text", "") if isinstance(para, dict) else para
                if para_text.strip():
                    all_chunks.append({
                        "doc_id": doc_id,
                        "chunk_id": f"para_{p_idx}",
                        "text": para_text.strip(),
                        "source_type": "paragraph"
                    })
            
            for t_idx, table in enumerate(tables):
                table_text = table_to_natural_text(table, table.get("caption", ""), unit_info)
                if table_text.strip():
                    all_chunks.append({
                        "doc_id": doc_id,
                        "chunk_id": f"table_{t_idx}",
                        "text": table_text.strip(),
                        "source_type": "table"
                    })
    return all_chunks

def gpu_worker_rag_system(gpu_id, data_queue, result_queue, encoder_model_name, reranker_model_name, 
                         corpus_chunks, top_k_retrieval, top_k_rerank, batch_size):
    """GPUå·¥ä½œè¿›ç¨‹å‡½æ•° - RAGç³»ç»Ÿç‰ˆæœ¬"""
    try:
        # è®¾ç½®CUDAè®¾å¤‡
        torch.cuda.set_device(gpu_id)
        device = torch.device(f'cuda:{gpu_id}')
        print(f"GPU {gpu_id}: å¼€å§‹å·¥ä½œï¼Œè®¾å¤‡: {device}")
        
        # 1. åˆå§‹åŒ–ç¼–ç å™¨
        print(f"GPU {gpu_id}: åŠ è½½Encoderæ¨¡å‹...")
        encoder_en = FinbertEncoder(
            model_name=encoder_model_name,
            cache_dir="cache"
        )
        print(f"GPU {gpu_id}: Encoderæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # 2. å°†corpus_chunksè½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼
        print(f"GPU {gpu_id}: è½¬æ¢æ£€ç´¢åº“æ ¼å¼...")
        corpus_documents = []
        for chunk in corpus_chunks:
            doc = DocumentWithMetadata(
                content=chunk["text"],
                metadata=DocumentMetadata(
                    doc_id=chunk.get('doc_id'),
                    source=chunk.get('source_type', 'unknown'),
                    created_at="",
                    author="",
                    language="english"
                )
            )
            corpus_documents.append(doc)
        print(f"GPU {gpu_id}: è½¬æ¢äº† {len(corpus_documents)} ä¸ªæ–‡æ¡£")
        
        # 3. åˆå§‹åŒ–BilingualRetrieverï¼ˆåªä½¿ç”¨è‹±æ–‡éƒ¨åˆ†ï¼‰
        print(f"GPU {gpu_id}: åˆå§‹åŒ–BilingualRetriever...")
        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„ä¸­æ–‡ç¼–ç å™¨ï¼ˆå› ä¸ºBilingualRetrieveréœ€è¦ä¸¤ä¸ªç¼–ç å™¨ï¼‰
        encoder_ch = FinbertEncoder(
            model_name=encoder_model_name,  # ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹
            cache_dir="cache"
        )
        
        retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,  # ä½¿ç”¨è™šæ‹Ÿä¸­æ–‡ç¼–ç å™¨
            corpus_documents_en=corpus_documents,
            corpus_documents_ch=[],  # ç©ºçš„ä¸­æ–‡æ–‡æ¡£åˆ—è¡¨
            use_faiss=True,  # ä½¿ç”¨FAISSç´¢å¼•
            use_gpu=True,
            batch_size=batch_size,
            cache_dir="cache",
            use_existing_embedding_index=False  # å¼ºåˆ¶é‡æ–°è®¡ç®—åµŒå…¥
        )
        print(f"GPU {gpu_id}: BilingualRetrieveråˆå§‹åŒ–å®Œæˆ")
        
        # 4. åˆå§‹åŒ–Reranker
        print(f"GPU {gpu_id}: åŠ è½½Rerankeræ¨¡å‹...")
        reranker = QwenReranker(
            model_name=reranker_model_name,
            device=f"cuda:{gpu_id}",
            cache_dir="cache",
            use_quantization=True,
            quantization_type="4bit"
        )
        print(f"GPU {gpu_id}: Rerankeræ¨¡å‹åŠ è½½å®Œæˆ")
        
        # 5. å¤„ç†æ•°æ®
        batch_results = []
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–æ•°æ®
                batch_data = data_queue.get(timeout=1)
                if batch_data is None:  # ç»“æŸä¿¡å·
                    break
                
                print(f"GPU {gpu_id}: å¤„ç†æ‰¹æ¬¡ï¼Œæ ·æœ¬æ•°: {len(batch_data)}")
                
                for i, sample in enumerate(batch_data):
                    try:
                        query_text = get_question_or_query(sample)
                        correct_chunk_content = sample.get("context", "")
                        
                        if not query_text or not correct_chunk_content:
                            batch_results.append(0.0)
                            continue
                        
                        # 1. ä½¿ç”¨RAGç³»ç»Ÿçš„çœŸå®æ£€ç´¢é€»è¾‘
                        retrieved_result = retriever.retrieve(
                            text=query_text,
                            top_k=top_k_retrieval,
                            return_scores=True,
                            language="english"
                        )
                        
                        if isinstance(retrieved_result, tuple):
                            retrieved_docs, retrieval_scores = retrieved_result
                        else:
                            retrieved_docs = retrieved_result
                            retrieval_scores = []
                        
                        if not retrieved_docs:
                            batch_results.append(0.0)
                            continue
                        
                        # 2. ä½¿ç”¨Rerankeré‡æ’åº
                        try:
                            # å‡†å¤‡é‡æ’åºçš„æ–‡æ¡£
                            docs_for_rerank = []
                            for doc in retrieved_docs:
                                docs_for_rerank.append({
                                    'content': doc.content,
                                    'metadata': doc.metadata.__dict__
                                })
                            
                            # æ‰§è¡Œé‡æ’åº
                            reranked_results = reranker.rerank_with_metadata(
                                query=query_text,
                                documents_with_metadata=docs_for_rerank,
                                batch_size=2  # å°æ‰¹æ¬¡é¿å…å†…å­˜ä¸è¶³
                            )
                            
                            # æå–é‡æ’åºåçš„æ–‡æ¡£å’Œåˆ†æ•°
                            final_retrieved_docs_for_ranking = []
                            for result in reranked_results[:top_k_rerank]:
                                doc = DocumentWithMetadata(
                                    content=result['content'],
                                    metadata=DocumentMetadata(**result['metadata'])
                                )
                                final_retrieved_docs_for_ranking.append(doc)
                            
                        except Exception as e:
                            print(f"GPU {gpu_id} é‡æ’åºå¤±è´¥: {e}")
                            # å¦‚æœé‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ
                            final_retrieved_docs_for_ranking = retrieved_docs[:top_k_rerank]
                        
                        # 3. æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å
                        found_rank = find_correct_document_rank_enhanced(
                            context=correct_chunk_content,
                            retrieved_docs=final_retrieved_docs_for_ranking,
                            sample=sample,
                            encoder=encoder_en
                        )
                        
                        if found_rank > 0:
                            mrr_score = 1.0 / found_rank
                            batch_results.append(mrr_score)
                        else:
                            batch_results.append(0.0)
                            
                    except Exception as e:
                        print(f"GPU {gpu_id} æ ·æœ¬ {i} å¤„ç†å¤±è´¥: {e}")
                        batch_results.append(0.0)
                
                print(f"GPU {gpu_id}: æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œç»“æœæ•°: {len(batch_results)}")
                
            except Exception as e:
                print(f"GPU {gpu_id} é˜Ÿåˆ—å¤„ç†å¤±è´¥: {e}")
                break
        
        # å‘é€ç»“æœ
        result_queue.put((gpu_id, batch_results))
        print(f"GPU {gpu_id}: å·¥ä½œå®Œæˆï¼Œç»“æœå·²å‘é€")
        
    except Exception as e:
        print(f"GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¤±è´¥: {e}")
        result_queue.put((gpu_id, []))

def compute_mrr_with_rag_system_multi_gpu(encoder_model_name, reranker_model_name, eval_data, corpus_chunks, 
                                         top_k_retrieval=100, top_k_rerank=10, batch_size=32, num_gpus=2):
    """å¤šGPUå¹¶è¡Œç‰ˆæœ¬çš„RAGç³»ç»ŸMRRè®¡ç®—"""
    
    # æ•°æ®åˆ†å‰²
    total_samples = len(eval_data)
    samples_per_gpu = total_samples // num_gpus
    remainder = total_samples % num_gpus
    
    print(f"ä½¿ç”¨ {num_gpus} ä¸ªGPUè¿›è¡Œå¹¶è¡Œå¤„ç†")
    print(f"æ•°æ®åˆ†å‰²å®Œæˆï¼š")
    
    start_idx = 0
    gpu_data = []
    for gpu_id in range(num_gpus):
        # åˆ†é…æ ·æœ¬ï¼Œå¤„ç†ä½™æ•°
        current_batch_size = samples_per_gpu + (1 if gpu_id < remainder else 0)
        end_idx = start_idx + current_batch_size
        gpu_samples = eval_data[start_idx:end_idx]
        gpu_data.append(gpu_samples)
        
        print(f"  GPU {gpu_id}: {len(gpu_samples)} ä¸ªæ ·æœ¬")
        start_idx = end_idx
    
    # åˆ›å»ºè¿›ç¨‹é—´é€šä¿¡é˜Ÿåˆ—
    data_queues = [mp.Queue() for _ in range(num_gpus)]
    result_queue = mp.Queue()
    
    # å¯åŠ¨GPUå·¥ä½œè¿›ç¨‹
    processes = []
    for gpu_id in range(num_gpus):
        p = Process(target=gpu_worker_rag_system, args=(
            gpu_id, data_queues[gpu_id], result_queue, encoder_model_name, reranker_model_name,
            corpus_chunks, top_k_retrieval, top_k_rerank, batch_size
        ))
        p.start()
        processes.append(p)
        print(f"GPU {gpu_id} è¿›ç¨‹å·²å¯åŠ¨")
    
    # å‘é€æ•°æ®åˆ°å„ä¸ªGPU
    for gpu_id in range(num_gpus):
        data_queues[gpu_id].put(gpu_data[gpu_id])
        data_queues[gpu_id].put(None)  # ç»“æŸä¿¡å·
        print(f"æ•°æ®å·²å‘é€åˆ° GPU {gpu_id}")
    
    # æ”¶é›†ç»“æœ
    all_results = []
    for _ in range(num_gpus):
        gpu_id, results = result_queue.get()
        all_results.extend(results)
        print(f"æ”¶åˆ° GPU {gpu_id} çš„ç»“æœï¼Œæ ·æœ¬æ•°: {len(results)}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    for p in processes:
        p.join()
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    mrr = np.mean(all_results)
    hit_at_1 = sum(1 for score in all_results if score == 1.0)
    hit_at_3 = sum(1 for score in all_results if score >= 1.0/3)
    hit_at_5 = sum(1 for score in all_results if score >= 1.0/5)
    hit_at_10 = sum(1 for score in all_results if score >= 1.0/10)
    
    total_samples = len(all_results)
    hit_at_1_rate = hit_at_1 / total_samples
    hit_at_3_rate = hit_at_3 / total_samples
    hit_at_5_rate = hit_at_5 / total_samples
    hit_at_10_rate = hit_at_10 / total_samples
    
    print(f"\n=== å¤šGPUå¹¶è¡ŒRAGç³»ç»Ÿè¯„ä¼°ç»“æœ ===")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"MRR: {mrr:.4f}")
    print(f"Hit@1: {hit_at_1_rate:.4f} ({hit_at_1}/{total_samples})")
    print(f"Hit@3: {hit_at_3_rate:.4f} ({hit_at_3}/{total_samples})")
    print(f"Hit@5: {hit_at_5_rate:.4f} ({hit_at_5}/{total_samples})")
    print(f"Hit@10: {hit_at_10_rate:.4f} ({hit_at_10}/{total_samples})")
    
    return mrr

def main():
    parser = argparse.ArgumentParser(description="å¤šGPUå¹¶è¡Œç‰ˆæœ¬çš„RAGç³»ç»ŸçœŸå®æ£€ç´¢é€»è¾‘TatQAæ•°æ®é›†MRRè¯„ä¼°")
    parser.add_argument("--encoder_model_name", type=str, 
                       default="models/finetuned_finbert_tatqa",
                       help="Encoderæ¨¡å‹è·¯å¾„")
    parser.add_argument("--reranker_model_name", type=str, 
                       default="Qwen/Qwen3-Reranker-0.6B",
                       help="Rerankeræ¨¡å‹è·¯å¾„æˆ–Hugging Face ID")
    parser.add_argument("--eval_jsonl", type=str, 
                       default="evaluate_mrr/tatqa_eval_enhanced.jsonl",
                       help="è¯„ä¼°æ•°æ®JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--base_raw_data_path", type=str, 
                       default="data/tatqa_dataset_raw/",
                       help="TatQAåŸå§‹æ•°æ®è·¯å¾„")
    parser.add_argument("--top_k_retrieval", type=int, default=100, 
                       help="Encoderæ£€ç´¢çš„top-kæ•°é‡")
    parser.add_argument("--top_k_rerank", type=int, default=10, 
                       help="Rerankeré‡æ’åºåçš„top-kæ•°é‡")
    parser.add_argument("--batch_size", type=int, default=32, 
                       help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--num_gpus", type=int, default=2, 
                       help="ä½¿ç”¨çš„GPUæ•°é‡")
    parser.add_argument("--max_eval_samples", type=int, default=None, 
                       help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰")
    
    args = parser.parse_args()

    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨GPU")
        return
    
    available_gpus = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {available_gpus} ä¸ªGPU")
    for i in range(available_gpus):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")

    # åŠ è½½è¯„ä¼°æ•°æ®
    print(f"\n1. åŠ è½½è¯„ä¼°æ•°æ®: {args.eval_jsonl}")
    if not Path(args.eval_jsonl).exists():
        print(f"âŒ è¯„ä¼°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.eval_jsonl}")
        return
        
    raw_eval_data = []
    with open(args.eval_jsonl, "r", encoding="utf-8") as f:
        for line in f:
            raw_eval_data.append(json.loads(line))
    
    eval_data = []
    for item in raw_eval_data:
        is_valid_item = isinstance(item, dict) and "context" in item and \
                        (get_question_or_query(item) is not None)
        
        if is_valid_item:
            eval_data.append(item)
        else:
            print(f"è­¦å‘Šï¼šè·³è¿‡æ— æ•ˆæ ·æœ¬: {item}")
    
    if args.max_eval_samples:
        eval_data = eval_data[:args.max_eval_samples]
        print(f"é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°ä¸º: {args.max_eval_samples}")
    
    print(f"âœ… åŠ è½½äº† {len(eval_data)} ä¸ªæœ‰æ•ˆè¯„ä¼°æ ·æœ¬")

    if not eval_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„è¯„ä¼°æ ·æœ¬")
        return

    # æ„å»ºæ£€ç´¢åº“
    print(f"\n2. æ„å»ºæ£€ç´¢åº“: {args.base_raw_data_path}")
    corpus_input_paths = [
        Path(args.base_raw_data_path) / "tatqa_dataset_train.json",
        Path(args.base_raw_data_path) / "tatqa_dataset_dev.json",
        Path(args.base_raw_data_path) / "tatqa_dataset_test_gold.json"
    ]
    
    existing_paths = [p for p in corpus_input_paths if p.exists()]
    if not existing_paths:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•TatQAåŸå§‹æ•°æ®æ–‡ä»¶")
        return
    
    corpus_chunks = process_tatqa_to_qca_for_corpus(existing_paths)
    print(f"âœ… æ„å»ºäº† {len(corpus_chunks)} ä¸ªChunkä½œä¸ºæ£€ç´¢åº“")

    # å¤šGPUå¹¶è¡ŒRAGç³»ç»Ÿè®¡ç®— MRR
    print(f"\n3. å¼€å§‹å¤šGPUå¹¶è¡ŒRAGç³»ç»Ÿè¯„ä¼°...")
    mrr = compute_mrr_with_rag_system_multi_gpu(
        encoder_model_name=args.encoder_model_name,
        reranker_model_name=args.reranker_model_name,
        eval_data=eval_data,
        corpus_chunks=corpus_chunks,
        top_k_retrieval=args.top_k_retrieval,
        top_k_rerank=args.top_k_rerank,
        batch_size=args.batch_size,
        num_gpus=args.num_gpus
    )
    
    print(f"\nğŸ¯ æœ€ç»ˆç»“æœ: MRR = {mrr:.4f}")

if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    mp.set_start_method('spawn', force=True)
    main() 