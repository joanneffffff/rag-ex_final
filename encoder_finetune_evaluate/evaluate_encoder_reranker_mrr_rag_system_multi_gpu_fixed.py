#!/usr/bin/env python3
"""
ä¿®å¤ç‰ˆå¤šGPUå¹¶è¡ŒRAGç³»ç»ŸçœŸå®æ£€ç´¢é€»è¾‘MRRè¯„ä¼°
ä½¿ç”¨å¢å¼ºç‰ˆè®­ç»ƒæ•°æ®ä½œä¸ºæ£€ç´¢åº“ï¼Œç¡®ä¿ä¸è¯„ä¼°æ•°æ®åŒ¹é…
"""

import json
import argparse
import torch
import numpy as np
from pathlib import Path
import multiprocessing as mp
from multiprocessing import Process
from tqdm import tqdm
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
try:
    from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata
    from xlm.components.encoder.finbert import FinbertEncoder
    from xlm.components.retriever.bilingual_retriever import BilingualRetriever
    from xlm.components.retriever.reranker import QwenReranker
    from encoder_finetune_evaluate.enhanced_evaluation_functions import find_correct_document_rank_enhanced
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    print("è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    sys.exit(1)

def get_question_or_query(item_dict):
    """æå–é—®é¢˜æˆ–æŸ¥è¯¢"""
    if "question" in item_dict:
        return item_dict["question"]
    elif "query" in item_dict:
        return item_dict["query"]
    return None

def load_enhanced_corpus(corpus_file: str) -> list:
    """åŠ è½½å¢å¼ºç‰ˆè®­ç»ƒæ•°æ®ä½œä¸ºæ£€ç´¢åº“"""
    print(f"ğŸ”„ åŠ è½½å¢å¼ºç‰ˆæ£€ç´¢åº“: {corpus_file}")
    
    if not Path(corpus_file).exists():
        print(f"âŒ æ£€ç´¢åº“æ–‡ä»¶ä¸å­˜åœ¨: {corpus_file}")
        return []
    
    corpus_chunks = []
    with open(corpus_file, "r", encoding="utf-8") as f:
        for line in f:
            chunk = json.loads(line)
            # ä½¿ç”¨relevant_doc_idsä¸­çš„å“ˆå¸Œå€¼ä½œä¸ºdoc_id
            relevant_doc_ids = chunk.get("relevant_doc_ids", [])
            if relevant_doc_ids:
                doc_id = relevant_doc_ids[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªå“ˆå¸Œå€¼
            else:
                doc_id = chunk.get("doc_id", "")
            
            # çŸ¥è¯†åº“æ–‡ä»¶ä½¿ç”¨"text"å­—æ®µï¼Œè¯„ä¼°æ•°æ®ä½¿ç”¨"context"å­—æ®µ
            content = chunk.get("text", chunk.get("context", ""))
            corpus_chunks.append({
                "text": content,
                "doc_id": doc_id,
                "source_type": chunk.get("source_type", "enhanced")
            })
    
    print(f"âœ… åŠ è½½äº† {len(corpus_chunks)} ä¸ªå¢å¼ºç‰ˆchunk")
    return corpus_chunks

def gpu_worker_rag_system_fixed(gpu_id, data_queue, result_queue, encoder_model_name, reranker_model_name, 
                               corpus_chunks, top_k_retrieval, top_k_rerank, batch_size):
    """ä¿®å¤ç‰ˆGPUå·¥ä½œè¿›ç¨‹å‡½æ•°"""
    try:
        # è®¾ç½®CUDAè®¾å¤‡
        torch.cuda.set_device(gpu_id)
        device = torch.device(f'cuda:{gpu_id}')
        print(f"GPU {gpu_id}: å¼€å§‹å·¥ä½œï¼Œè®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡å‹
        print(f"GPU {gpu_id}: åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½Encoderæ¨¡å‹
        encoder_en = FinbertEncoder(
            model_name=encoder_model_name,
            cache_dir="cache"
        )
        print(f"GPU {gpu_id}: Encoderæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # å°†corpus_chunksè½¬æ¢ä¸ºDocumentWithMetadataæ ¼å¼
        print(f"GPU {gpu_id}: è½¬æ¢æ£€ç´¢åº“æ ¼å¼...")
        corpus_documents = []
        for chunk in corpus_chunks:
            doc = DocumentWithMetadata(
                content=chunk["text"],
                metadata=DocumentMetadata(
                    doc_id=chunk.get('doc_id'),
                    source=chunk.get('source_type', 'enhanced'),
                    created_at="",
                    author="",
                    language="english"
                )
            )
            corpus_documents.append(doc)
        print(f"GPU {gpu_id}: è½¬æ¢äº† {len(corpus_documents)} ä¸ªæ–‡æ¡£")
        
        # åˆå§‹åŒ–BilingualRetrieverï¼ˆåªä½¿ç”¨è‹±æ–‡éƒ¨åˆ†ï¼‰
        print(f"GPU {gpu_id}: åˆå§‹åŒ–BilingualRetriever...")
        # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„ä¸­æ–‡ç¼–ç å™¨ï¼ˆå› ä¸ºBilingualRetrieveréœ€è¦ä¸¤ä¸ªç¼–ç å™¨ï¼‰
        encoder_ch = FinbertEncoder(
            model_name=encoder_model_name,  # ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹
            cache_dir="cache"
        )
        
        retriever = BilingualRetriever(
            encoder_en=encoder_en,
            encoder_ch=encoder_ch,  # ä½¿ç”¨è™šæ‹Ÿä¸­æ–‡ç¼–ç å™¨
            corpus_documents_en=corpus_documents,
            corpus_documents_ch=[],  # ç©ºçš„ä¸­æ–‡æ–‡æ¡£åˆ—è¡¨
            use_faiss=False,  # ä¸ä½¿ç”¨FAISSï¼Œé¿å…ç´¢å¼•é—®é¢˜
            use_gpu=True,
            batch_size=8,  # å‡å°ç¼–ç æ‰¹æ¬¡å¤§å°
            cache_dir="cache",
            use_existing_embedding_index=False  # å¼ºåˆ¶é‡æ–°è®¡ç®—åµŒå…¥
        )
        print(f"GPU {gpu_id}: BilingualRetrieveråˆå§‹åŒ–å®Œæˆ")
        
        # åˆå§‹åŒ–Reranker
        print(f"GPU {gpu_id}: åŠ è½½Rerankeræ¨¡å‹...")
        reranker = QwenReranker(
            model_name=reranker_model_name,
            device=f"cuda:{gpu_id}",
            cache_dir="cache",
            use_quantization=True,
            quantization_type="4bit"
        )
        print(f"GPU {gpu_id}: Rerankeræ¨¡å‹åŠ è½½å®Œæˆ")
        
        # å¤„ç†æ•°æ®
        batch_results = []
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–æ•°æ®
                batch_data = data_queue.get(timeout=1)
                if batch_data is None:  # ç»“æŸä¿¡å·
                    break
                
                print(f"GPU {gpu_id}: å¤„ç†æ‰¹æ¬¡ï¼Œæ ·æœ¬æ•°: {len(batch_data)}")
                
                # æ·»åŠ tqdmè¿›åº¦æ¡
                from tqdm import tqdm
                for i, sample in enumerate(tqdm(batch_data, desc=f"GPU {gpu_id} å¤„ç†æ ·æœ¬", leave=False)):
                    try:
                        query_text = get_question_or_query(sample)
                        correct_chunk_content = sample.get("context", "")
                        
                        if not query_text or not correct_chunk_content:
                            batch_results.append(0.0)
                            continue
                        
                        # 1. ä½¿ç”¨RAGç³»ç»Ÿçš„çœŸå®æ£€ç´¢é€»è¾‘
                        retrieved_result = retriever.retrieve(
                            text=query_text,
                            top_k=top_k_retrieval,
                            return_scores=True,
                            language="english"
                        )
                        
                        if isinstance(retrieved_result, tuple):
                            retrieved_docs, retrieval_scores = retrieved_result
                        else:
                            retrieved_docs = retrieved_result
                            retrieval_scores = []
                        
                        if not retrieved_docs:
                            batch_results.append(0.0)
                            continue
                        
                        # 2. ä½¿ç”¨Rerankeré‡æ’åº
                        try:
                            # å‡†å¤‡é‡æ’åºçš„æ–‡æ¡£
                            docs_for_rerank = []
                            for doc in retrieved_docs:
                                docs_for_rerank.append({
                                    'content': doc.content,
                                    'metadata': doc.metadata.__dict__
                                })
                            
                            # æ‰§è¡Œé‡æ’åº
                            reranked_results = reranker.rerank_with_metadata(
                                query=query_text,
                                documents_with_metadata=docs_for_rerank,
                                batch_size=2  # å°æ‰¹æ¬¡é¿å…å†…å­˜ä¸è¶³
                            )
                            
                            # æå–é‡æ’åºåçš„æ–‡æ¡£å’Œåˆ†æ•°
                            final_retrieved_docs_for_ranking = []
                            for result in reranked_results[:top_k_rerank]:
                                doc = DocumentWithMetadata(
                                    content=result['content'],
                                    metadata=DocumentMetadata(**result['metadata'])
                                )
                                final_retrieved_docs_for_ranking.append(doc)
                            
                        except Exception as e:
                            print(f"GPU {gpu_id} é‡æ’åºå¤±è´¥: {e}")
                            # å¦‚æœé‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ
                            final_retrieved_docs_for_ranking = retrieved_docs[:top_k_rerank]
                        
                        # 3. æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å
                        found_rank = find_correct_document_rank_enhanced(
                            context=correct_chunk_content,
                            retrieved_docs=final_retrieved_docs_for_ranking,
                            sample=sample,
                            encoder=encoder_en
                        )
                        
                        if found_rank > 0:
                            mrr_score = 1.0 / found_rank
                            batch_results.append(mrr_score)
                        else:
                            batch_results.append(0.0)
                            
                    except Exception as e:
                        print(f"GPU {gpu_id} æ ·æœ¬ {i} å¤„ç†å¤±è´¥: {e}")
                        batch_results.append(0.0)
                
                print(f"GPU {gpu_id}: æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œç»“æœæ•°: {len(batch_results)}")
                
            except Exception as e:
                print(f"GPU {gpu_id} é˜Ÿåˆ—å¤„ç†å¤±è´¥: {e}")
                break
        
        # å‘é€ç»“æœ
        result_queue.put((gpu_id, batch_results))
        print(f"GPU {gpu_id}: å·¥ä½œå®Œæˆï¼Œç»“æœå·²å‘é€")
        
    except Exception as e:
        print(f"GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¤±è´¥: {e}")
        result_queue.put((gpu_id, []))

def compute_mrr_with_rag_system_multi_gpu_fixed(encoder_model_name, reranker_model_name, eval_data, corpus_chunks, 
                                               top_k_retrieval=100, top_k_rerank=10, batch_size=32, num_gpus=2):
    """ä¿®å¤ç‰ˆå¤šGPUå¹¶è¡ŒRAGç³»ç»ŸMRRè®¡ç®—"""
    
    # æ•°æ®åˆ†å‰²
    total_samples = len(eval_data)
    samples_per_gpu = total_samples // num_gpus
    remainder = total_samples % num_gpus
    
    print(f"ä½¿ç”¨ {num_gpus} ä¸ªGPUè¿›è¡Œå¹¶è¡Œå¤„ç†")
    print(f"æ•°æ®åˆ†å‰²å®Œæˆï¼š")
    
    start_idx = 0
    gpu_data = []
    for gpu_id in range(num_gpus):
        # åˆ†é…æ ·æœ¬ï¼Œå¤„ç†ä½™æ•°
        current_batch_size = samples_per_gpu + (1 if gpu_id < remainder else 0)
        end_idx = start_idx + current_batch_size
        gpu_samples = eval_data[start_idx:end_idx]
        gpu_data.append(gpu_samples)
        
        print(f"  GPU {gpu_id}: {len(gpu_samples)} ä¸ªæ ·æœ¬")
        start_idx = end_idx
    
    # åˆ›å»ºè¿›ç¨‹é—´é€šä¿¡é˜Ÿåˆ—
    data_queues = [mp.Queue() for _ in range(num_gpus)]
    result_queue = mp.Queue()
    
    # å¯åŠ¨GPUå·¥ä½œè¿›ç¨‹
    processes = []
    for gpu_id in range(num_gpus):
        p = Process(target=gpu_worker_rag_system_fixed, args=(
            gpu_id, data_queues[gpu_id], result_queue, encoder_model_name, reranker_model_name,
            corpus_chunks, top_k_retrieval, top_k_rerank, batch_size
        ))
        p.start()
        processes.append(p)
        print(f"GPU {gpu_id} è¿›ç¨‹å·²å¯åŠ¨")
    
    # å‘é€æ•°æ®åˆ°å„ä¸ªGPU
    for gpu_id in range(num_gpus):
        data_queues[gpu_id].put(gpu_data[gpu_id])
        data_queues[gpu_id].put(None)  # ç»“æŸä¿¡å·
        print(f"æ•°æ®å·²å‘é€åˆ° GPU {gpu_id}")
    
    # æ”¶é›†ç»“æœ
    all_results = []
    from tqdm import tqdm
    for _ in tqdm(range(num_gpus), desc="æ”¶é›†GPUç»“æœ"):
        gpu_id, results = result_queue.get()
        all_results.extend(results)
        print(f"æ”¶åˆ° GPU {gpu_id} çš„ç»“æœï¼Œæ ·æœ¬æ•°: {len(results)}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    for p in processes:
        p.join()
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    mrr = np.mean(all_results)
    hit_at_1 = sum(1 for score in all_results if score == 1.0)
    hit_at_3 = sum(1 for score in all_results if score >= 1.0/3)
    hit_at_5 = sum(1 for score in all_results if score >= 1.0/5)
    hit_at_10 = sum(1 for score in all_results if score >= 1.0/10)
    
    total_samples = len(all_results)
    hit_at_1_rate = hit_at_1 / total_samples
    hit_at_3_rate = hit_at_3 / total_samples
    hit_at_5_rate = hit_at_5 / total_samples
    hit_at_10_rate = hit_at_10 / total_samples
    
    print(f"\n=== ä¿®å¤ç‰ˆå¤šGPUå¹¶è¡ŒRAGç³»ç»Ÿè¯„ä¼°ç»“æœ ===")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"MRR: {mrr:.4f}")
    print(f"Hit@1: {hit_at_1_rate:.4f} ({hit_at_1}/{total_samples})")
    print(f"Hit@3: {hit_at_3_rate:.4f} ({hit_at_3}/{total_samples})")
    print(f"Hit@5: {hit_at_5_rate:.4f} ({hit_at_5}/{total_samples})")
    print(f"Hit@10: {hit_at_10_rate:.4f} ({hit_at_10}/{total_samples})")
    
    return mrr

def main():
    parser = argparse.ArgumentParser(description="ä¿®å¤ç‰ˆå¤šGPUå¹¶è¡ŒRAGç³»ç»ŸçœŸå®æ£€ç´¢é€»è¾‘MRRè¯„ä¼°")
    parser.add_argument("--encoder_model_name", type=str, 
                       default="models/finetuned_finbert_tatqa",
                       help="Encoderæ¨¡å‹è·¯å¾„")
    parser.add_argument("--reranker_model_name", type=str, 
                       default="Qwen/Qwen3-Reranker-0.6B",
                       help="Rerankeræ¨¡å‹è·¯å¾„æˆ–Hugging Face ID")
    parser.add_argument("--eval_jsonl", type=str, 
                       default="evaluate_mrr/tatqa_eval_enhanced.jsonl",
                       help="è¯„ä¼°æ•°æ®JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--corpus_jsonl", type=str, 
                       default="evaluate_mrr/tatqa_knowledge_base.jsonl",
                       help="çŸ¥è¯†åº“JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--top_k_retrieval", type=int, default=100, 
                       help="Encoderæ£€ç´¢çš„top-kæ•°é‡")
    parser.add_argument("--top_k_rerank", type=int, default=10, 
                       help="Rerankeré‡æ’åºåçš„top-kæ•°é‡")
    parser.add_argument("--batch_size", type=int, default=32, 
                       help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--num_gpus", type=int, default=2, 
                       help="ä½¿ç”¨çš„GPUæ•°é‡")
    parser.add_argument("--max_eval_samples", type=int, default=None, 
                       help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰")
    
    args = parser.parse_args()

    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨GPU")
        return
    
    available_gpus = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {available_gpus} ä¸ªGPU")
    for i in range(available_gpus):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")

    # åŠ è½½è¯„ä¼°æ•°æ®
    print(f"\n1. åŠ è½½è¯„ä¼°æ•°æ®: {args.eval_jsonl}")
    if not Path(args.eval_jsonl).exists():
        print(f"âŒ è¯„ä¼°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.eval_jsonl}")
        return
        
    raw_eval_data = []
    with open(args.eval_jsonl, "r", encoding="utf-8") as f:
        for line in f:
            raw_eval_data.append(json.loads(line))
    
    eval_data = []
    for item in raw_eval_data:
        is_valid_item = isinstance(item, dict) and "context" in item and \
                        (get_question_or_query(item) is not None)
        
        if is_valid_item:
            eval_data.append(item)
        else:
            print(f"è­¦å‘Šï¼šè·³è¿‡æ— æ•ˆæ ·æœ¬: {item}")
    
    if args.max_eval_samples:
        eval_data = eval_data[:args.max_eval_samples]
        print(f"é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°ä¸º: {args.max_eval_samples}")
    
    print(f"âœ… åŠ è½½äº† {len(eval_data)} ä¸ªæœ‰æ•ˆè¯„ä¼°æ ·æœ¬")

    if not eval_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„è¯„ä¼°æ ·æœ¬")
        return

    # åŠ è½½å¢å¼ºç‰ˆæ£€ç´¢åº“
    print(f"\n2. åŠ è½½å¢å¼ºç‰ˆæ£€ç´¢åº“: {args.corpus_jsonl}")
    corpus_chunks = load_enhanced_corpus(args.corpus_jsonl)
    
    if not corpus_chunks:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ£€ç´¢åº“æ•°æ®")
        return

    # å¤šGPUå¹¶è¡ŒRAGç³»ç»Ÿè®¡ç®— MRR
    print(f"\n3. å¼€å§‹ä¿®å¤ç‰ˆå¤šGPUå¹¶è¡ŒRAGç³»ç»Ÿè¯„ä¼°...")
    mrr = compute_mrr_with_rag_system_multi_gpu_fixed(
        encoder_model_name=args.encoder_model_name,
        reranker_model_name=args.reranker_model_name,
        eval_data=eval_data,
        corpus_chunks=corpus_chunks,
        top_k_retrieval=args.top_k_retrieval,
        top_k_rerank=args.top_k_rerank,
        batch_size=args.batch_size,
        num_gpus=args.num_gpus
    )
    
    print(f"\nğŸ¯ æœ€ç»ˆç»“æœ: MRR = {mrr:.4f}")

if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass  # å¦‚æœå·²ç»è®¾ç½®è¿‡äº†ï¼Œå°±å¿½ç•¥
    main() 