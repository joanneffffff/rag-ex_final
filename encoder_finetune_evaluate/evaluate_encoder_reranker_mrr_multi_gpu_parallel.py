#!/usr/bin/env python3
"""
çœŸæ­£å¹¶è¡Œçš„å¤šGPUç‰ˆæœ¬TatQAæ•°æ®é›†MRRè¯„ä¼°
ä½¿ç”¨å¤šè¿›ç¨‹å®ç°çœŸæ­£çš„å¹¶è¡Œå¤„ç†
"""

import json
import argparse
import numpy as np
import torch
import torch.nn.functional as F
from pathlib import Path
from typing import List, Dict, Any, Optional
from tqdm import tqdm
import multiprocessing as mp
from multiprocessing import Process, Queue
import time
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from xlm.dto.dto import DocumentWithMetadata, DocumentMetadata
from enhanced_evaluation_functions import find_correct_document_rank_enhanced

def get_question_or_query(item_dict):
    """ä»æ ·æœ¬ä¸­æå–é—®é¢˜æˆ–æŸ¥è¯¢"""
    if "query" in item_dict:
        return item_dict["query"]
    elif "question" in item_dict:
        return item_dict["question"]
    else:
        return None

def extract_unit_from_paragraph(paragraphs):
    """ä»æ®µè½ä¸­æå–å•ä½ä¿¡æ¯"""
    unit_info = ""
    for para in paragraphs:
        if isinstance(para, dict):
            text = para.get("text", "")
        else:
            text = para
        if "million" in text.lower() or "thousand" in text.lower():
            if "million" in text.lower():
                unit_info = "million USD"
            elif "thousand" in text.lower():
                unit_info = "thousand USD"
            break
    return unit_info

def table_to_natural_text(table_dict, caption="", unit_info=""):
    """å°†è¡¨æ ¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æ–‡æœ¬"""
    import re
    rows = table_dict.get("table", [])
    lines = []

    if caption:
        lines.append(f"Table Topic: {caption}.")

    if not rows:
        return ""

    headers = rows[0]
    data_rows = rows[1:]

    for i, row in enumerate(data_rows):
        if not row or all(str(v).strip() == "" for v in row):
            continue

        if len(row) > 1 and str(row[0]).strip() != "" and all(str(v).strip() == "" for v in row[1:]):
            lines.append(f"Table Category: {str(row[0]).strip()}.")
            continue

        row_name = str(row[0]).strip().replace('.', '')

        data_descriptions = []
        for h_idx, v in enumerate(row):
            if h_idx == 0:
                continue
            header = headers[h_idx] if h_idx < len(headers) else f"Column {h_idx+1}"
            value = str(v).strip()
            if value:
                if re.match(r'^-?\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?$|^\(\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?\)$', value):
                    formatted_value = value.replace('$', '')
                    if unit_info:
                        if formatted_value.startswith('(') and formatted_value.endswith(')'):
                             formatted_value = f"(${formatted_value[1:-1]} {unit_info})"
                        else:
                             formatted_value = f"${formatted_value} {unit_info}"
                    else:
                        formatted_value = f"${formatted_value}"
                else:
                    formatted_value = value
                data_descriptions.append(f"{header} is {formatted_value}")
        if row_name and data_descriptions:
            lines.append(f"Details for item {row_name}: {'; '.join(data_descriptions)}.")
        elif data_descriptions:
            lines.append(f"Other data item: {'; '.join(data_descriptions)}.")
        elif row_name:
            lines.append(f"Data item: {row_name}.")
    return "\n".join(lines)

def process_tatqa_to_qca_for_corpus(input_paths):
    """å¤„ç†TatQAåŸå§‹æ•°æ®ï¼Œæ„å»ºæ£€ç´¢åº“"""
    all_chunks = []
    global_doc_counter = 0

    for input_path in input_paths:
        if not Path(input_path).exists():
            print(f"è­¦å‘Šï¼šæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {input_path}")
            continue
            
        with open(input_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            print(f"è­¦å‘Šï¼šæ–‡ä»¶ {Path(input_path).name} çš„é¡¶å±‚ç»“æ„ä¸æ˜¯åˆ—è¡¨ï¼Œå°è¯•ä½œä¸ºå•ä¸ªæ–‡æ¡£å¤„ç†ã€‚")
            data = [data]

        for i, item in enumerate(data):
            if not isinstance(item, dict):
                print(f"è­¦å‘Šï¼šæ–‡ä»¶ {Path(input_path).name} ä¸­å‘ç°éå­—å…¸é¡¹ï¼Œè·³è¿‡ã€‚é¡¹å†…å®¹ï¼š{item}")
                continue
            
            doc_id = item.get("doc_id")
            if doc_id is None:
                doc_id = f"generated_doc_{global_doc_counter}_{Path(input_path).stem}_{i}"
                global_doc_counter += 1

            paragraphs = item.get("paragraphs", [])
            tables = item.get("tables", [])

            unit_info = extract_unit_from_paragraph(paragraphs)

            for p_idx, para in enumerate(paragraphs):
                para_text = para.get("text", "") if isinstance(para, dict) else para
                if para_text.strip():
                    all_chunks.append({
                        "doc_id": doc_id,
                        "chunk_id": f"para_{p_idx}",
                        "text": para_text.strip(),
                        "source_type": "paragraph"
                    })
            
            for t_idx, table in enumerate(tables):
                table_text = table_to_natural_text(table, table.get("caption", ""), unit_info)
                if table_text.strip():
                    all_chunks.append({
                        "doc_id": doc_id,
                        "chunk_id": f"table_{t_idx}",
                        "text": table_text.strip(),
                        "source_type": "table"
                    })
    return all_chunks

def gpu_worker(gpu_id, data_queue, result_queue, encoder_model_name, reranker_model_name, 
               corpus_chunks, top_k_retrieval, top_k_rerank, batch_size):
    """GPUå·¥ä½œè¿›ç¨‹å‡½æ•°"""
    try:
        # è®¾ç½®CUDAè®¾å¤‡
        torch.cuda.set_device(gpu_id)
        device = torch.device(f'cuda:{gpu_id}')
        print(f"GPU {gpu_id}: å¼€å§‹å·¥ä½œï¼Œè®¾å¤‡: {device}")
        
        # åŠ è½½æ¨¡å‹
        print(f"GPU {gpu_id}: åŠ è½½æ¨¡å‹...")
        
        # åŠ è½½Encoderæ¨¡å‹
        from sentence_transformers import SentenceTransformer
        encoder_model = SentenceTransformer(encoder_model_name, device=str(device))
        print(f"GPU {gpu_id}: Encoderæ¨¡å‹åŠ è½½å®Œæˆ")
        
        # åŠ è½½Rerankeræ¨¡å‹ (Qwen3-Reranker-0.6Bæ˜¯CausalLMï¼Œä¸æ˜¯CrossEncoder)
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        tokenizer = AutoTokenizer.from_pretrained(reranker_model_name, padding_side='left')
        model = AutoModelForCausalLM.from_pretrained(reranker_model_name, torch_dtype=torch.float16).to(device).eval()
        
        # è®¾ç½®token IDs
        token_false_id = tokenizer.convert_tokens_to_ids("no")
        token_true_id = tokenizer.convert_tokens_to_ids("yes")
        max_length = 4096  # å‡å°‘åºåˆ—é•¿åº¦ä»¥èŠ‚çœå†…å­˜
        
        # è®¾ç½®prefixå’Œsuffix
        prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
        suffix = "<|im_end|>\n<|im_start|>assistant\n<think>\n\n</think>\n\n"
        prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)
        suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)
        
        def format_instruction(instruction, query, doc):
            if instruction is None:
                instruction = 'Given a web search query, retrieve relevant passages that answer the query'
            output = "<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}".format(instruction=instruction, query=query, doc=doc)
            return output
        
        def process_inputs(pairs):
            # ç›´æ¥ç”¨tokenizeræ‰¹é‡åˆ†è¯ï¼Œpaddingåˆ°max_lengthï¼Œè‡ªåŠ¨æˆªæ–­
            inputs = tokenizer(
                pairs,
                padding='max_length',
                truncation=True,
                max_length=max_length,
                return_tensors="pt"
            )
            for key in inputs:
                inputs[key] = inputs[key].to(model.device)
            return inputs
        
        @torch.no_grad()
        def compute_logits(inputs, **kwargs):
            batch_scores = model(**inputs).logits[:, -1, :]
            true_vector = batch_scores[:, token_true_id]
            false_vector = batch_scores[:, token_false_id]
            batch_scores = torch.stack([false_vector, true_vector], dim=1)
            batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)
            scores = batch_scores[:, 1].exp().tolist()
            return scores
        
        reranker_model = {
            'model': model,
            'tokenizer': tokenizer,
            'format_instruction': format_instruction,
            'process_inputs': process_inputs,
            'compute_logits': compute_logits
        }
        print(f"GPU {gpu_id}: Rerankeræ¨¡å‹åŠ è½½å®Œæˆ")
        
        # ç¼–ç æ£€ç´¢åº“
        print(f"GPU {gpu_id}: ç¼–ç æ£€ç´¢åº“...")
        corpus_texts = [chunk["text"] for chunk in corpus_chunks]
        corpus_embeddings = encoder_model.encode(corpus_texts, batch_size=batch_size, show_progress_bar=False)
        corpus_embeddings = torch.tensor(corpus_embeddings, device=device)
        print(f"GPU {gpu_id}: æ£€ç´¢åº“ç¼–ç å®Œæˆ")
        
        # å¤„ç†æ•°æ®
        batch_results = []
        while True:
            try:
                # ä»é˜Ÿåˆ—è·å–æ•°æ®
                batch_data = data_queue.get(timeout=1)
                if batch_data is None:  # ç»“æŸä¿¡å·
                    break
                
                print(f"GPU {gpu_id}: å¤„ç†æ‰¹æ¬¡ï¼Œæ ·æœ¬æ•°: {len(batch_data)}")
                
                for i, sample in enumerate(batch_data):
                    try:
                        query_text = get_question_or_query(sample)
                        correct_chunk_content = sample.get("context", "")
                        
                        if not query_text or not correct_chunk_content:
                            batch_results.append(0.0)
                            continue
                        
                        # 1. ç¼–ç æŸ¥è¯¢
                        query_embedding = encoder_model.encode([query_text], convert_to_tensor=True, device=str(device))
                        
                        # 2. æ£€ç´¢ç›¸å…³æ–‡æ¡£
                        with torch.no_grad():
                            scores = F.cosine_similarity(query_embedding, corpus_embeddings, dim=1)
                            top_indices = torch.topk(scores, k=top_k_retrieval).indices
                        
                        retrieved_chunks = [corpus_chunks[idx.item()] for idx in top_indices]
                        docs_for_rerank = [chunk["text"] for chunk in retrieved_chunks]
                        
                        if not docs_for_rerank:
                            batch_results.append(0.0)
                            continue
                        
                        # 3. é‡æ’åº
                        try:
                            # ä½¿ç”¨Qwen3-Rerankerçš„æ­£ç¡®è°ƒç”¨æ–¹å¼ï¼Œåˆ†æ‰¹å¤„ç†ä»¥é¿å…å†…å­˜ä¸è¶³
                            task = 'Given a web search query, retrieve relevant passages that answer the query'
                            reranker_scores = []
                            
                            # åˆ†æ‰¹å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š2ä¸ªæ–‡æ¡£
                            batch_size = 2
                            for i in range(0, len(docs_for_rerank), batch_size):
                                batch_docs = docs_for_rerank[i:i+batch_size]
                                pairs = [reranker_model['format_instruction'](task, query_text, doc) for doc in batch_docs]
                                
                                # å¤„ç†è¾“å…¥
                                inputs = reranker_model['process_inputs'](pairs)
                                
                                # è®¡ç®—åˆ†æ•°
                                batch_scores = reranker_model['compute_logits'](inputs)
                                reranker_scores.extend(batch_scores)
                                
                                # æ¸…ç†å†…å­˜
                                del inputs
                                torch.cuda.empty_cache()
                                
                        except Exception as e:
                            print(f"GPU {gpu_id} é‡æ’åºå¤±è´¥: {e}")
                            reranker_scores = [1.0] * len(docs_for_rerank)
                        
                        # 4. åˆ›å»ºDocumentWithMetadataåˆ—è¡¨
                        scored_retrieved_chunks_with_scores = []
                        for j, score in enumerate(reranker_scores):
                            chunk = retrieved_chunks[j]
                            scored_retrieved_chunks_with_scores.append({
                                "chunk": chunk, 
                                "score": score
                            })
                        
                        # æŒ‰åˆ†æ•°é™åºæ’åº
                        scored_retrieved_chunks_with_scores.sort(key=lambda x: x["score"], reverse=True)
                        
                        # åˆ›å»ºæœ€ç»ˆæ–‡æ¡£åˆ—è¡¨
                        final_retrieved_docs_for_ranking = []
                        for item in scored_retrieved_chunks_with_scores[:top_k_rerank]:
                            chunk = item["chunk"]
                            final_retrieved_docs_for_ranking.append(DocumentWithMetadata(
                                content=chunk['text'],
                                metadata=DocumentMetadata(
                                    doc_id=chunk.get('doc_id'),
                                    source=chunk.get('source_type', 'unknown'),
                                    created_at="",
                                    author="",
                                    language="english"
                                )
                            ))
                        
                        # 5. æ‰¾åˆ°æ­£ç¡®ç­”æ¡ˆçš„æ’å
                        found_rank = find_correct_document_rank_enhanced(
                            context=correct_chunk_content,
                            retrieved_docs=final_retrieved_docs_for_ranking,
                            sample=sample,
                            encoder=encoder_model
                        )
                        
                        if found_rank > 0:
                            mrr_score = 1.0 / found_rank
                            batch_results.append(mrr_score)
                        else:
                            batch_results.append(0.0)
                            
                    except Exception as e:
                        print(f"GPU {gpu_id} æ ·æœ¬ {i} å¤„ç†å¤±è´¥: {e}")
                        batch_results.append(0.0)
                
                print(f"GPU {gpu_id}: æ‰¹æ¬¡å¤„ç†å®Œæˆï¼Œç»“æœæ•°: {len(batch_results)}")
                
            except Exception as e:
                print(f"GPU {gpu_id} é˜Ÿåˆ—å¤„ç†å¤±è´¥: {e}")
                break
        
        # å‘é€ç»“æœ
        result_queue.put((gpu_id, batch_results))
        print(f"GPU {gpu_id}: å·¥ä½œå®Œæˆï¼Œç»“æœå·²å‘é€")
        
    except Exception as e:
        print(f"GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¤±è´¥: {e}")
        result_queue.put((gpu_id, []))

def compute_mrr_with_reranker_multi_gpu_parallel(encoder_model_name, reranker_model_name, eval_data, corpus_chunks, 
                                                top_k_retrieval=100, top_k_rerank=10, batch_size=32, num_gpus=2):
    """çœŸæ­£å¹¶è¡Œçš„å¤šGPU MRRè®¡ç®—"""
    
    # æ•°æ®åˆ†å‰²
    total_samples = len(eval_data)
    samples_per_gpu = total_samples // num_gpus
    remainder = total_samples % num_gpus
    
    print(f"ä½¿ç”¨ {num_gpus} ä¸ªGPUè¿›è¡Œå¹¶è¡Œå¤„ç†")
    print(f"æ•°æ®åˆ†å‰²å®Œæˆï¼š")
    
    start_idx = 0
    gpu_data = []
    for gpu_id in range(num_gpus):
        # åˆ†é…æ ·æœ¬ï¼Œå¤„ç†ä½™æ•°
        current_batch_size = samples_per_gpu + (1 if gpu_id < remainder else 0)
        end_idx = start_idx + current_batch_size
        gpu_samples = eval_data[start_idx:end_idx]
        gpu_data.append(gpu_samples)
        
        print(f"  GPU {gpu_id}: {len(gpu_samples)} ä¸ªæ ·æœ¬")
        start_idx = end_idx
    
    # åˆ›å»ºè¿›ç¨‹é—´é€šä¿¡é˜Ÿåˆ—
    data_queues = [mp.Queue() for _ in range(num_gpus)]
    result_queue = mp.Queue()
    
    # å¯åŠ¨GPUå·¥ä½œè¿›ç¨‹
    processes = []
    for gpu_id in range(num_gpus):
        p = Process(target=gpu_worker, args=(
            gpu_id, data_queues[gpu_id], result_queue, encoder_model_name, reranker_model_name,
            corpus_chunks, top_k_retrieval, top_k_rerank, batch_size
        ))
        p.start()
        processes.append(p)
        print(f"GPU {gpu_id} è¿›ç¨‹å·²å¯åŠ¨")
    
    # å‘é€æ•°æ®åˆ°å„ä¸ªGPU
    for gpu_id in range(num_gpus):
        data_queues[gpu_id].put(gpu_data[gpu_id])
        data_queues[gpu_id].put(None)  # ç»“æŸä¿¡å·
        print(f"æ•°æ®å·²å‘é€åˆ° GPU {gpu_id}")
    
    # æ”¶é›†ç»“æœ
    all_results = []
    for _ in range(num_gpus):
        gpu_id, results = result_queue.get()
        all_results.extend(results)
        print(f"æ”¶åˆ° GPU {gpu_id} çš„ç»“æœï¼Œæ ·æœ¬æ•°: {len(results)}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    for p in processes:
        p.join()
    
    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    mrr = np.mean(all_results)
    hit_at_1 = sum(1 for score in all_results if score == 1.0)
    hit_at_3 = sum(1 for score in all_results if score >= 1.0/3)
    hit_at_5 = sum(1 for score in all_results if score >= 1.0/5)
    hit_at_10 = sum(1 for score in all_results if score >= 1.0/10)
    
    total_samples = len(all_results)
    hit_at_1_rate = hit_at_1 / total_samples
    hit_at_3_rate = hit_at_3 / total_samples
    hit_at_5_rate = hit_at_5 / total_samples
    hit_at_10_rate = hit_at_10 / total_samples
    
    print(f"\n=== å¹¶è¡Œå¤šGPUè¯„ä¼°ç»“æœ ===")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"MRR: {mrr:.4f}")
    print(f"Hit@1: {hit_at_1_rate:.4f} ({hit_at_1}/{total_samples})")
    print(f"Hit@3: {hit_at_3_rate:.4f} ({hit_at_3}/{total_samples})")
    print(f"Hit@5: {hit_at_5_rate:.4f} ({hit_at_5}/{total_samples})")
    print(f"Hit@10: {hit_at_10_rate:.4f} ({hit_at_10}/{total_samples})")
    
    return mrr

def main():
    parser = argparse.ArgumentParser(description="çœŸæ­£å¹¶è¡Œçš„å¤šGPUç‰ˆæœ¬TatQAæ•°æ®é›†MRRè¯„ä¼°")
    parser.add_argument("--encoder_model_name", type=str, 
                       default="models/finetuned_finbert_tatqa",
                       help="Encoderæ¨¡å‹è·¯å¾„")
    parser.add_argument("--reranker_model_name", type=str, 
                       default="Qwen/Qwen3-Reranker-0.6B",
                       help="Rerankeræ¨¡å‹è·¯å¾„æˆ–Hugging Face ID")
    parser.add_argument("--eval_jsonl", type=str, 
                       default="evaluate_mrr/tatqa_eval_enhanced.jsonl",
                       help="è¯„ä¼°æ•°æ®JSONLæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--base_raw_data_path", type=str, 
                       default="data/tatqa_dataset_raw/",
                       help="TatQAåŸå§‹æ•°æ®è·¯å¾„")
    parser.add_argument("--top_k_retrieval", type=int, default=100, 
                       help="Encoderæ£€ç´¢çš„top-kæ•°é‡")
    parser.add_argument("--top_k_rerank", type=int, default=10, 
                       help="Rerankeré‡æ’åºåçš„top-kæ•°é‡")
    parser.add_argument("--batch_size", type=int, default=32, 
                       help="æ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--num_gpus", type=int, default=2, 
                       help="ä½¿ç”¨çš„GPUæ•°é‡")
    parser.add_argument("--max_eval_samples", type=int, default=None, 
                       help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰")
    
    args = parser.parse_args()

    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•ä½¿ç”¨GPU")
        return
    
    available_gpus = torch.cuda.device_count()
    print(f"âœ… æ£€æµ‹åˆ° {available_gpus} ä¸ªGPU")
    for i in range(available_gpus):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")

    # åŠ è½½è¯„ä¼°æ•°æ®
    print(f"\n1. åŠ è½½è¯„ä¼°æ•°æ®: {args.eval_jsonl}")
    if not Path(args.eval_jsonl).exists():
        print(f"âŒ è¯„ä¼°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.eval_jsonl}")
        return
        
    raw_eval_data = []
    with open(args.eval_jsonl, "r", encoding="utf-8") as f:
        for line in f:
            raw_eval_data.append(json.loads(line))
    
    eval_data = []
    for item in raw_eval_data:
        is_valid_item = isinstance(item, dict) and "context" in item and \
                        (get_question_or_query(item) is not None)
        
        if is_valid_item:
            eval_data.append(item)
        else:
            print(f"è­¦å‘Šï¼šè·³è¿‡æ— æ•ˆæ ·æœ¬: {item}")
    
    if args.max_eval_samples:
        eval_data = eval_data[:args.max_eval_samples]
        print(f"é™åˆ¶è¯„ä¼°æ ·æœ¬æ•°ä¸º: {args.max_eval_samples}")
    
    print(f"âœ… åŠ è½½äº† {len(eval_data)} ä¸ªæœ‰æ•ˆè¯„ä¼°æ ·æœ¬")

    if not eval_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„è¯„ä¼°æ ·æœ¬")
        return

    # æ„å»ºæ£€ç´¢åº“
    print(f"\n2. æ„å»ºæ£€ç´¢åº“: {args.base_raw_data_path}")
    corpus_input_paths = [
        Path(args.base_raw_data_path) / "tatqa_dataset_train.json",
        Path(args.base_raw_data_path) / "tatqa_dataset_dev.json",
        Path(args.base_raw_data_path) / "tatqa_dataset_test_gold.json"
    ]
    
    existing_paths = [p for p in corpus_input_paths if p.exists()]
    if not existing_paths:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•TatQAåŸå§‹æ•°æ®æ–‡ä»¶")
        return
    
    corpus_chunks = process_tatqa_to_qca_for_corpus(existing_paths)
    print(f"âœ… æ„å»ºäº† {len(corpus_chunks)} ä¸ªChunkä½œä¸ºæ£€ç´¢åº“")

    # å¹¶è¡Œå¤šGPUè®¡ç®— MRR
    print(f"\n3. å¼€å§‹å¹¶è¡Œå¤šGPUè¯„ä¼°...")
    mrr = compute_mrr_with_reranker_multi_gpu_parallel(
        encoder_model_name=args.encoder_model_name,
        reranker_model_name=args.reranker_model_name,
        eval_data=eval_data,
        corpus_chunks=corpus_chunks,
        top_k_retrieval=args.top_k_retrieval,
        top_k_rerank=args.top_k_rerank,
        batch_size=args.batch_size,
        num_gpus=args.num_gpus
    )
    
    print(f"\nğŸ¯ æœ€ç»ˆç»“æœ: MRR = {mrr:.4f}")

if __name__ == "__main__":
    # è®¾ç½®å¤šè¿›ç¨‹å¯åŠ¨æ–¹æ³•
    mp.set_start_method('spawn', force=True)
    main() 