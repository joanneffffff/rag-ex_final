#!/usr/bin/env python3
"""
å¯¹æ¯”æ—§é€»è¾‘å’Œæ–°é€»è¾‘çš„å·®å¼‚
"""

import re
from collections import Counter
from typing import List

NOT_FOUND_REPLY_ENGLISH = "I cannot find the answer in the provided context."

# æ—§é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
def normalize_answer_english_old(s: str) -> str:
    """æ—§ç‰ˆæœ¬çš„è‹±æ–‡ç­”æ¡ˆæ ‡å‡†åŒ–"""
    if not s:
        return ""
    s = ' '.join(s.split())
    s = re.sub(r'[^\w\s]', '', s)
    return s.strip().lower()

def get_tokens_english_old(s: str) -> List[str]:
    """æ—§ç‰ˆæœ¬çš„è‹±æ–‡tokenè·å–"""
    return s.split()

def calculate_f1_score_old(prediction: str, ground_truth: str) -> float:
    """æ—§ç‰ˆæœ¬çš„F1è®¡ç®—"""
    pred_tokens = set(get_tokens_english_old(normalize_answer_english_old(prediction)))
    gt_tokens = set(get_tokens_english_old(normalize_answer_english_old(ground_truth)))
    
    if not gt_tokens:
        return 1.0 if not pred_tokens else 0.0
    
    intersection = pred_tokens & gt_tokens
    precision = len(intersection) / len(pred_tokens) if pred_tokens else 0.0
    recall = len(intersection) / len(gt_tokens)
    
    if precision + recall == 0:
        return 0.0
    
    return 2 * precision * recall / (precision + recall)

def calculate_exact_match_old(prediction: str, ground_truth: str) -> float:
    """æ—§ç‰ˆæœ¬çš„EMè®¡ç®—"""
    pred_normalized = normalize_answer_english_old(prediction)
    gt_normalized = normalize_answer_english_old(ground_truth)
    return 1.0 if pred_normalized == gt_normalized else 0.0

# æ–°é€»è¾‘
def _shared_text_standardizer_english(text: str) -> str:
    """
    Helper function to standardize English text for both answer extraction and F1 score calculation.
    Strictly follows the rules from the English Prompt Template.
    """
    text = text.strip()
    
    # Lowercase all text
    text = text.lower()

    # é€’å½’æ›¿æ¢æ‰€æœ‰ \text{...} ä¸º ...ï¼ˆä¿ç•™å†…å®¹ï¼‰
    while True:
        new_text = re.sub(r'\\text\{([^}]*)\}', r'\1', text, flags=re.DOTALL)
        if new_text == text:
            break
        text = new_text
    # å…¶ä½™ LaTeX æ ¼å¼ç›´æ¥å»æ‰
    text = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', '', text)
    text = re.sub(r'\\[a-zA-Z]+', '', text)
    
    # Remove currency symbols and common unit words based on prompt rule
    text = re.sub(r'\b(million|billion|thousand|trillion|usd|eur|gbp|m|b)\b', '', text, flags=re.IGNORECASE).strip()
    text = re.sub(r'[\$Â£â‚¬]', '', text).strip()

    # Remove commas from numbers
    text = text.replace(',', '')

    # Handle negative numbers in parentheses (e.g., "(33)" -> "-33")
    if text.startswith('(') and text.endswith(')'):
        text = '-' + text[1:-1]
    
    # Normalize percentages
    text = text.replace(' percent', '%').replace('pct', '%')
    text = re.sub(r'(\d+\.?\d*)\s*%', r'\1%', text)
    
    # Remove common introductory phrases
    text = re.sub(r'^(the\s*answer\s*is|it\s*was|the\s*value\s*is|resulting\s*in|this\s*represents|the\s*effect\s*is|therefore|so|thus|in\s*conclusion|final\s*answer\s*is|final\s*number\s*is)\s*', '', text, flags=re.IGNORECASE).strip()
    
    # Remove trailing punctuation
    if text.endswith('%'):
        text = re.sub(r'[\.,;]$', '', text).strip()
    else:
        text = re.sub(r'[\.,;%]$', '', text).strip() 
    
    # Final cleanup of whitespace
    text = ' '.join(text.split()).strip()

    return text

def calculate_f1_score_new(prediction: str, ground_truth: str) -> float:
    """æ–°ç‰ˆæœ¬çš„F1è®¡ç®—"""
    
    normalized_prediction = _shared_text_standardizer_english(prediction).lower()
    normalized_ground_truth = _shared_text_standardizer_english(ground_truth).lower()

    # Handle cases where the model explicitly states "I cannot find the answer..."
    if normalized_prediction == NOT_FOUND_REPLY_ENGLISH.lower():
        return 1.0 if normalized_ground_truth == NOT_FOUND_REPLY_ENGLISH.lower() else 0.0
    
    # Handle cases where the ground truth is "I cannot find the answer...", but the model gave a factual answer (which is an error)
    if normalized_ground_truth == NOT_FOUND_REPLY_ENGLISH.lower():
        return 0.0

    prediction_tokens = normalized_prediction.split()
    ground_truth_tokens = normalized_ground_truth.split()

    if not ground_truth_tokens: 
        return 1.0 if not prediction_tokens else 0.0
    if not prediction_tokens: 
        return 0.0

    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)
    num_same = sum(common.values())
    if num_same == 0: 
        return 0.0

    precision = 1.0 * num_same / len(prediction_tokens)
    recall = 1.0 * num_same / len(ground_truth_tokens)
    f1 = (2 * precision * recall) / (precision + recall)
    return f1

def calculate_exact_match_new(prediction: str, ground_truth: str) -> float:
    """æ–°ç‰ˆæœ¬çš„EMè®¡ç®—"""
    return 1.0 if _shared_text_standardizer_english(prediction).lower() == _shared_text_standardizer_english(ground_truth).lower() else 0.0

def process_answer(answer: str) -> str:
    """ç§»é™¤ç­”æ¡ˆä¸­çš„[Reranker: Enabled]æ–‡æœ¬"""
    if not answer:
        return answer
    answer = re.sub(r'^\[Reranker: Enabled\]\s*', '', answer)
    return answer.strip()

# æµ‹è¯•ç”¨ä¾‹
test_cases = [
    {
        "prediction": "[Reranker: Enabled] 1%",
        "ground_truth": "$0.3 million",
        "description": "è´§å¸å’Œå•ä½è¯å¤„ç†"
    },
    {
        "prediction": "[Reranker: Enabled] The answer is 12.5%",
        "ground_truth": "12.5%",
        "description": "ç§»é™¤å‰ç¼€çŸ­è¯­"
    },
    {
        "prediction": "[Reranker: Enabled] I cannot find the answer in the provided context.",
        "ground_truth": "geographic distribution of pretax income from continuing operations",
        "description": "æ— æ³•æ‰¾åˆ°ç­”æ¡ˆçš„æƒ…å†µ"
    },
    {
        "prediction": "[Reranker: Enabled] I cannot find the answer in the provided context.",
        "ground_truth": "I cannot find the answer in the provided context.",
        "description": "åŒæ–¹éƒ½æ— æ³•æ‰¾åˆ°ç­”æ¡ˆ"
    },
    {
        "prediction": "[Reranker: Enabled] $1,234.56",
        "ground_truth": "1234.56",
        "description": "æ•°å­—æ ¼å¼æ ‡å‡†åŒ–"
    }
]

print("ğŸ” å¯¹æ¯”æ—§é€»è¾‘å’Œæ–°é€»è¾‘")
print("=" * 80)

for i, test_case in enumerate(test_cases, 1):
    print(f"\nğŸ“‹ æµ‹è¯•ç”¨ä¾‹ {i}: {test_case['description']}")
    print(f"åŸå§‹é¢„æµ‹: '{test_case['prediction']}'")
    print(f"åŸå§‹çœŸå®: '{test_case['ground_truth']}'")
    
    # ç§»é™¤å‰ç¼€
    cleaned_prediction = process_answer(test_case['prediction'])
    print(f"æ¸…ç†åé¢„æµ‹: '{cleaned_prediction}'")
    
    # æ—§é€»è¾‘
    f1_old = calculate_f1_score_old(cleaned_prediction, test_case['ground_truth'])
    em_old = calculate_exact_match_old(cleaned_prediction, test_case['ground_truth'])
    
    # æ–°é€»è¾‘
    f1_new = calculate_f1_score_new(cleaned_prediction, test_case['ground_truth'])
    em_new = calculate_exact_match_new(cleaned_prediction, test_case['ground_truth'])
    
    print(f"\nğŸ“Š æ—§é€»è¾‘ç»“æœ:")
    print(f"  æ ‡å‡†åŒ–é¢„æµ‹: '{normalize_answer_english_old(cleaned_prediction)}'")
    print(f"  æ ‡å‡†åŒ–çœŸå®: '{normalize_answer_english_old(test_case['ground_truth'])}'")
    print(f"  F1: {f1_old:.4f}")
    print(f"  EM: {em_old:.4f}")
    
    print(f"\nğŸ“Š æ–°é€»è¾‘ç»“æœ:")
    print(f"  æ ‡å‡†åŒ–é¢„æµ‹: '{_shared_text_standardizer_english(cleaned_prediction)}'")
    print(f"  æ ‡å‡†åŒ–çœŸå®: '{_shared_text_standardizer_english(test_case['ground_truth'])}'")
    print(f"  F1: {f1_new:.4f}")
    print(f"  EM: {em_new:.4f}")
    
    print(f"\nğŸ“ˆ å˜åŒ–:")
    print(f"  F1å˜åŒ–: {f1_new - f1_old:+.4f}")
    print(f"  EMå˜åŒ–: {em_new - em_old:+.4f}")

print("\nâœ… å¯¹æ¯”å®Œæˆ") 