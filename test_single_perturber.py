#!/usr/bin/env python3
"""
æµ‹è¯•å•ä¸ªæ‰°åŠ¨å™¨çš„æ•ˆæœ
ç”¨äºå¿«é€ŸéªŒè¯æ‰°åŠ¨ç³»ç»Ÿæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from xlm.components.rag_system.rag_system import RagSystem
from xlm.components.retriever.enhanced_retriever import EnhancedRetriever
from xlm.components.generator.local_llm_generator import LocalLLMGenerator
from xlm.explainer.generic_retriever_explainer import GenericRetrieverExplainer
from xlm.modules.perturber.leave_one_out_perturber import LeaveOneOutPerturber
from xlm.modules.comparator.embedding_comparator import EmbeddingComparator
from xlm.modules.tokenizer.custom_tokenizer import CustomTokenizer
from xlm.dto.dto import ExplanationGranularity
from config.parameters import Config

def test_single_perturber():
    """æµ‹è¯•å•ä¸ªæ‰°åŠ¨å™¨ï¼ˆLeave-One-Outï¼‰"""
    print("ğŸ§ª æµ‹è¯•å•ä¸ªæ‰°åŠ¨å™¨æ•ˆæœ")
    print("=" * 50)
    
    try:
        # 1. åˆå§‹åŒ–ç»„ä»¶
        print("ğŸ”§ åˆå§‹åŒ–ç»„ä»¶...")
        config = Config()
        
        # RAGç»„ä»¶
        generator = LocalLLMGenerator()
        retriever = EnhancedRetriever(config=config)
        
        # æ‰°åŠ¨ç³»ç»Ÿç»„ä»¶
        tokenizer = CustomTokenizer()
        from xlm.components.encoder.finbert import FinbertEncoder
        encoder = FinbertEncoder(model_name="ProsusAI/finbert")
        comparator = EmbeddingComparator(encoder=encoder)
        
        # æ‰°åŠ¨å™¨
        perturber = LeaveOneOutPerturber()
        
        print("âœ… ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
        
        # 2. æµ‹è¯•é—®é¢˜
        test_question = "é¦–é’¢è‚¡ä»½åœ¨2023å¹´ä¸ŠåŠå¹´çš„ä¸šç»©è¡¨ç°å¦‚ä½•ï¼Ÿ"
        print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_question}")
        
        # 3. è¿è¡Œæ ‡å‡†RAG
        print(f"\nğŸš€ è¿è¡Œæ ‡å‡†RAG...")
        rag_system = RagSystem(
            retriever=retriever,
            generator=generator,
            retriever_top_k=5
        )
        
        result = rag_system.run(test_question)
        print(f"âœ… æ ‡å‡†RAGè¿è¡ŒæˆåŠŸ")
        print(f"æ£€ç´¢æ–‡æ¡£æ•°: {len(result.retrieved_documents)}")
        print(f"ç”Ÿæˆå›ç­”: {result.generated_responses[0][:100]}...")
        
        # 4. è¿è¡Œæ‰°åŠ¨å®éªŒ
        print(f"\nğŸ”¬ è¿è¡ŒLeave-One-Outæ‰°åŠ¨å®éªŒ...")
        
        # åˆ›å»ºè§£é‡Šå™¨
        explainer = GenericRetrieverExplainer(
            perturber=perturber,
            comparator=comparator,
            retriever=retriever,
            tokenizer=tokenizer
        )
        
        # è·å–å‚è€ƒç»“æœ
        reference_doc, reference_score = explainer.get_reference(test_question)
        print(f"å‚è€ƒæ–‡æ¡£: {reference_doc[:100]}...")
        print(f"å‚è€ƒåˆ†æ•°: {reference_score:.4f}")
        
        # è¿›è¡Œæ‰°åŠ¨åˆ†æ
        explanation = explainer.explain(
            user_input=test_question,
            granularity=ExplanationGranularity.WORD_LEVEL,
            reference_text=reference_doc,
            reference_score=str(reference_score)
        )
        
        # 5. æ˜¾ç¤ºç»“æœ
        print(f"\nğŸ“Š æ‰°åŠ¨åˆ†æç»“æœ:")
        print(f"åˆ†æçš„ç‰¹å¾æ•°é‡: {len(explanation.explanations)}")
        
        if explanation.explanations:
            print(f"\nğŸ† Top 5 é‡è¦ç‰¹å¾:")
            for i, feature in enumerate(explanation.explanations[:5], 1):
                print(f"{i}. '{feature.feature}' - é‡è¦æ€§: {feature.score:.4f}")
        else:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°é‡è¦ç‰¹å¾")
        
        print(f"\nğŸ‰ æ‰°åŠ¨å®éªŒæˆåŠŸå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æ‰°åŠ¨å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def test_perturber_types():
    """æµ‹è¯•ä¸åŒç±»å‹çš„æ‰°åŠ¨å™¨"""
    print("\nğŸ”¬ æµ‹è¯•ä¸åŒç±»å‹çš„æ‰°åŠ¨å™¨")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–åŸºç¡€ç»„ä»¶
        config = Config()
        tokenizer = CustomTokenizer()
        from xlm.components.encoder.finbert import FinbertEncoder
        encoder = FinbertEncoder(model_name="ProsusAI/finbert")
        comparator = EmbeddingComparator(encoder=encoder)
        
        # æµ‹è¯•æ–‡æœ¬
        test_text = "é¦–é’¢è‚¡ä»½åœ¨2023å¹´ä¸ŠåŠå¹´çš„ä¸šç»©è¡¨ç°ä¸ä½³"
        test_features = ["é¦–é’¢è‚¡ä»½", "2023å¹´", "ä¸ŠåŠå¹´", "ä¸šç»©", "è¡¨ç°"]
        
        # æµ‹è¯•ä¸åŒæ‰°åŠ¨å™¨
        perturbers = {
            'leave_one_out': LeaveOneOutPerturber(),
            'random_word': None,  # éœ€è¦ç‰¹æ®Šå¤„ç†
            'reorder': None,      # éœ€è¦ç‰¹æ®Šå¤„ç†
        }
        
        print(f"ğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}")
        print(f"ğŸ“ æµ‹è¯•ç‰¹å¾: {test_features}")
        
        # æµ‹è¯•Leave-One-Outæ‰°åŠ¨å™¨
        print(f"\nğŸ” æµ‹è¯•Leave-One-Outæ‰°åŠ¨å™¨...")
        loo_perturber = perturbers['leave_one_out']
        perturbations = loo_perturber.perturb(text=test_text, features=test_features)
        
        print(f"âœ… ç”Ÿæˆæ‰°åŠ¨æ–‡æœ¬æ•°é‡: {len(perturbations)}")
        for i, perturbed in enumerate(perturbations, 1):
            print(f"{i}. {perturbed}")
        
        print(f"\nğŸ‰ æ‰°åŠ¨å™¨æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æ‰°åŠ¨å™¨æµ‹è¯•å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª æ‰°åŠ¨å™¨æµ‹è¯•")
    print("=" * 60)
    
    # æµ‹è¯•å•ä¸ªæ‰°åŠ¨å™¨
    success1 = test_single_perturber()
    
    # æµ‹è¯•æ‰°åŠ¨å™¨ç±»å‹
    success2 = test_perturber_types()
    
    print(f"\nğŸ“‹ æµ‹è¯•æ€»ç»“:")
    print(f"âœ… å•ä¸ªæ‰°åŠ¨å™¨æµ‹è¯•: {'æˆåŠŸ' if success1 else 'å¤±è´¥'}")
    print(f"âœ… æ‰°åŠ¨å™¨ç±»å‹æµ‹è¯•: {'æˆåŠŸ' if success2 else 'å¤±è´¥'}")
    
    if success1 and success2:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ‰°åŠ¨ç³»ç»Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨")
        print(f"ğŸ’¡ å¯ä»¥å¼€å§‹è¿›è¡Œå®Œæ•´çš„æ‰°åŠ¨ç­–ç•¥å®éªŒ")
    else:
        print(f"\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")

if __name__ == "__main__":
    main() 