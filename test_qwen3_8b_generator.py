#!/usr/bin/env python3
"""
æµ‹è¯•Qwen3-8Bä½œä¸ºç”Ÿæˆå™¨çš„æ•ˆæœ
æ¯”è¾ƒä¸Fin-R1æ¨¡å‹çš„å·®å¼‚
"""

import os
import sys
import torch
from typing import List, Dict, Any
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config.parameters import Config
from xlm.components.generator.local_llm_generator import LocalLLMGenerator


def test_qwen3_8b_generator():
    """æµ‹è¯•Qwen3-8Bç”Ÿæˆå™¨"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•Qwen3-8Bç”Ÿæˆå™¨...")
    
    # åŠ è½½é…ç½®
    config = Config()
    print(f"ğŸ“‹ å½“å‰ç”Ÿæˆå™¨é…ç½®:")
    print(f"   æ¨¡å‹: {config.generator.model_name}")
    print(f"   é‡åŒ–: {config.generator.use_quantization} ({config.generator.quantization_type})")
    print(f"   æœ€å¤§token: {config.generator.max_new_tokens}")
    print(f"   æ¸©åº¦: {config.generator.temperature}")
    print(f"   Top-p: {config.generator.top_p}")
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    print("\nğŸ”§ åˆå§‹åŒ–Qwen3-8Bç”Ÿæˆå™¨...")
    try:
        generator = LocalLLMGenerator(
            model_name=config.generator.model_name,
            cache_dir=config.generator.cache_dir,
            device="cuda:1",  # ä½¿ç”¨GPU 1
            use_quantization=config.generator.use_quantization,
            quantization_type=config.generator.quantization_type
        )
        print("âœ… Qwen3-8Bç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ Qwen3-8Bç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return False
    
    # æµ‹è¯•é—®é¢˜åˆ—è¡¨
    test_questions = [
        "ä»€ä¹ˆæ˜¯è‚¡ç¥¨æŠ•èµ„ï¼Ÿ",
        "è¯·è§£é‡Šå€ºåˆ¸çš„åŸºæœ¬æ¦‚å¿µ",
        "åŸºé‡‘æŠ•èµ„ä¸è‚¡ç¥¨æŠ•èµ„æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "ä»€ä¹ˆæ˜¯å¸‚ç›ˆç‡ï¼Ÿ",
        "è¯·è§£é‡Šä»€ä¹ˆæ˜¯ETFåŸºé‡‘"
    ]
    
    # æµ‹è¯•promptæ¨¡æ¿
    prompt_templates = {
        "simple": "è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}",
        "clean": "é—®é¢˜ï¼š{question}\nå›ç­”ï¼š",
        "detailed": "åŸºäºé‡‘èçŸ¥è¯†ï¼Œè¯·è¯¦ç»†å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}\nè¯·æä¾›å‡†ç¡®ã€æ¸…æ™°çš„è§£é‡Šã€‚"
    }
    
    print(f"\nğŸ§ª å¼€å§‹ç”Ÿæˆæµ‹è¯•...")
    print(f"   æµ‹è¯•é—®é¢˜æ•°é‡: {len(test_questions)}")
    print(f"   Promptæ¨¡æ¿æ•°é‡: {len(prompt_templates)}")
    
    results = {}
    
    for template_name, template in prompt_templates.items():
        print(f"\nğŸ“ æµ‹è¯•æ¨¡æ¿: {template_name}")
        print(f"   æ¨¡æ¿: {template}")
        
        template_results = []
        
        for i, question in enumerate(test_questions):
            print(f"\n   ğŸ” é—®é¢˜ {i+1}: {question}")
            
            # æ„å»ºprompt
            prompt = template.format(question=question)
            
            try:
                # è®°å½•å¼€å§‹æ—¶é—´
                start_time = time.time()
                
                # ç”Ÿæˆå›ç­”
                response = generator.generate(
                    texts=[prompt]
                )[0]  # generateæ–¹æ³•è¿”å›åˆ—è¡¨ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ 
                
                # è®°å½•ç»“æŸæ—¶é—´
                end_time = time.time()
                generation_time = end_time - start_time
                
                # ç»Ÿè®¡tokenæ•°é‡
                response_tokens = len(response.split())
                
                print(f"   âœ… ç”ŸæˆæˆåŠŸ")
                print(f"      å›ç­”: {response[:100]}...")
                print(f"      é•¿åº¦: {response_tokens} tokens")
                print(f"      æ—¶é—´: {generation_time:.2f}s")
                
                template_results.append({
                    "question": question,
                    "response": response,
                    "tokens": response_tokens,
                    "time": generation_time
                })
                
            except Exception as e:
                print(f"   âŒ ç”Ÿæˆå¤±è´¥: {e}")
                template_results.append({
                    "question": question,
                    "response": f"ç”Ÿæˆå¤±è´¥: {e}",
                    "tokens": 0,
                    "time": 0
                })
        
        results[template_name] = template_results
    
    # è¾“å‡ºæ€»ç»“
    print(f"\nğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"=" * 50)
    
    for template_name, template_results in results.items():
        print(f"\nğŸ“ æ¨¡æ¿: {template_name}")
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_generations = [r for r in template_results if "ç”Ÿæˆå¤±è´¥" not in r["response"]]
        failed_generations = [r for r in template_results if "ç”Ÿæˆå¤±è´¥" in r["response"]]
        
        if successful_generations:
            avg_tokens = sum(r["tokens"] for r in successful_generations) / len(successful_generations)
            avg_time = sum(r["time"] for r in successful_generations) / len(successful_generations)
            
            print(f"   æˆåŠŸç”Ÿæˆ: {len(successful_generations)}/{len(template_results)}")
            print(f"   å¹³å‡tokenæ•°: {avg_tokens:.1f}")
            print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {avg_time:.2f}s")
        else:
            print(f"   æˆåŠŸç”Ÿæˆ: 0/{len(template_results)}")
        
        if failed_generations:
            print(f"   å¤±è´¥ç”Ÿæˆ: {len(failed_generations)}")
    
    # å†…å­˜ä½¿ç”¨æƒ…å†µ
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.memory_allocated(device=1) / 1024**3
        print(f"\nğŸ’¾ GPUå†…å­˜ä½¿ç”¨: {gpu_memory:.2f}GB")
    
    print(f"\nâœ… Qwen3-8Bç”Ÿæˆå™¨æµ‹è¯•å®Œæˆ")
    return True


def compare_with_fin_r1():
    """æ¯”è¾ƒQwen3-8Bä¸Fin-R1çš„æ•ˆæœ"""
    print("\nğŸ”„ æ¯”è¾ƒQwen3-8Bä¸Fin-R1çš„æ•ˆæœ...")
    
    # ä¿å­˜å½“å‰é…ç½®
    config = Config()
    original_model = config.generator.model_name
    
    # æµ‹è¯•é—®é¢˜
    test_question = "ä»€ä¹ˆæ˜¯è‚¡ç¥¨æŠ•èµ„ï¼Ÿ"
    test_prompt = f"è¯·å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{test_question}"
    
    results = {}
    
    # æµ‹è¯•Qwen3-8B
    print(f"\nğŸ“ æµ‹è¯•Qwen3-8B...")
    try:
        config.generator.model_name = "Qwen/Qwen3-8B"
        generator_qwen = LocalLLMGenerator(
            model_name=config.generator.model_name,
            device="cuda:1"
        )
        
        start_time = time.time()
        response_qwen = generator_qwen.generate(texts=[test_prompt])[0]
        time_qwen = time.time() - start_time
        
        results["Qwen3-8B"] = {
            "response": response_qwen,
            "time": time_qwen,
            "tokens": len(response_qwen.split())
        }
        
        print(f"   âœ… Qwen3-8Bç”ŸæˆæˆåŠŸ")
        print(f"      å›ç­”: {response_qwen[:100]}...")
        print(f"      æ—¶é—´: {time_qwen:.2f}s")
        print(f"      Tokenæ•°: {results['Qwen3-8B']['tokens']}")
        
    except Exception as e:
        print(f"   âŒ Qwen3-8Bæµ‹è¯•å¤±è´¥: {e}")
        results["Qwen3-8B"] = {"error": str(e)}
    
    # æµ‹è¯•Fin-R1
    print(f"\nğŸ“ æµ‹è¯•Fin-R1...")
    try:
        config.generator.model_name = "SUFE-AIFLM-Lab/Fin-R1"
        generator_fin = LocalLLMGenerator(
            model_name=config.generator.model_name,
            device="cuda:1"
        )
        
        start_time = time.time()
        response_fin = generator_fin.generate(texts=[test_prompt])[0]
        time_fin = time.time() - start_time
        
        results["Fin-R1"] = {
            "response": response_fin,
            "time": time_fin,
            "tokens": len(response_fin.split())
        }
        
        print(f"   âœ… Fin-R1ç”ŸæˆæˆåŠŸ")
        print(f"      å›ç­”: {response_fin[:100]}...")
        print(f"      æ—¶é—´: {time_fin:.2f}s")
        print(f"      Tokenæ•°: {results['Fin-R1']['tokens']}")
        
    except Exception as e:
        print(f"   âŒ Fin-R1æµ‹è¯•å¤±è´¥: {e}")
        results["Fin-R1"] = {"error": str(e)}
    
    # è¾“å‡ºæ¯”è¾ƒç»“æœ
    print(f"\nğŸ“Š æ¨¡å‹æ¯”è¾ƒç»“æœ:")
    print(f"=" * 50)
    
    if "Qwen3-8B" in results and "Fin-R1" in results:
        if "error" not in results["Qwen3-8B"] and "error" not in results["Fin-R1"]:
            print(f"\nğŸ” å›ç­”é•¿åº¦æ¯”è¾ƒ:")
            print(f"   Qwen3-8B: {results['Qwen3-8B']['tokens']} tokens")
            print(f"   Fin-R1: {results['Fin-R1']['tokens']} tokens")
            
            print(f"\nâ±ï¸ ç”Ÿæˆé€Ÿåº¦æ¯”è¾ƒ:")
            print(f"   Qwen3-8B: {results['Qwen3-8B']['time']:.2f}s")
            print(f"   Fin-R1: {results['Fin-R1']['time']:.2f}s")
            
            print(f"\nğŸ“ å›ç­”é£æ ¼æ¯”è¾ƒ:")
            print(f"   Qwen3-8B: {results['Qwen3-8B']['response'][:200]}...")
            print(f"   Fin-R1: {results['Fin-R1']['response'][:200]}...")
        else:
            print("âŒ æ— æ³•æ¯”è¾ƒï¼šè‡³å°‘æœ‰ä¸€ä¸ªæ¨¡å‹ç”Ÿæˆå¤±è´¥")
    else:
        print("âŒ æ— æ³•æ¯”è¾ƒï¼šæ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
    
    # æ¢å¤åŸå§‹é…ç½®
    config.generator.model_name = original_model
    print(f"\nâœ… æ¨¡å‹æ¯”è¾ƒå®Œæˆ")


if __name__ == "__main__":
    print("ğŸ§ª Qwen3-8Bç”Ÿæˆå™¨æµ‹è¯•è„šæœ¬")
    print("=" * 50)
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    if torch.cuda.is_available():
        print(f"âœ… CUDAå¯ç”¨ï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"   GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("âŒ CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
    
    # è¿è¡Œæµ‹è¯•
    success = test_qwen3_8b_generator()
    
    if success:
        # è¯¢é—®æ˜¯å¦è¿›è¡Œæ¯”è¾ƒæµ‹è¯•
        try:
            choice = input("\nğŸ¤” æ˜¯å¦è¿›è¡Œä¸Fin-R1çš„æ¯”è¾ƒæµ‹è¯•ï¼Ÿ(y/n): ").lower().strip()
            if choice in ['y', 'yes', 'æ˜¯']:
                compare_with_fin_r1()
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    
    print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼") 