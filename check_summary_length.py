#!/usr/bin/env python3
"""
æ£€æŸ¥summaryå­—æ®µçš„é•¿åº¦åˆ†å¸ƒ
"""

import json
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np

def analyze_summary_lengths():
    """åˆ†æsummaryå­—æ®µçš„é•¿åº¦åˆ†å¸ƒ"""
    print("ğŸ“Š åˆ†æAlphaFinæ•°æ®ä¸­summaryå­—æ®µçš„é•¿åº¦åˆ†å¸ƒ")
    print("=" * 60)
    
    data_path = Path("data/alphafin/alphafin_merged_generated_qa.json")
    
    if not data_path.exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")
        return
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“ˆ æ€»è®°å½•æ•°: {len(data)}")
    
    # ç»Ÿè®¡summaryé•¿åº¦
    summary_lengths = []
    long_summaries = []
    
    for i, record in enumerate(data):
        summary = record.get('summary', '')
        if summary:
            length = len(summary)
            summary_lengths.append(length)
            
            # è®°å½•è¶…é•¿çš„summary
            if length > 8192:
                long_summaries.append({
                    'index': i,
                    'length': length,
                    'summary': summary[:200] + '...' if len(summary) > 200 else summary,
                    'company': record.get('company_name', 'N/A')
                })
    
    if not summary_lengths:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„summaryå­—æ®µ")
        return
    
    # åŸºæœ¬ç»Ÿè®¡
    avg_length = sum(summary_lengths) / len(summary_lengths)
    max_length = max(summary_lengths)
    min_length = min(summary_lengths)
    median_length = sorted(summary_lengths)[len(summary_lengths) // 2]
    
    print(f"\nğŸ“ Summaryé•¿åº¦ç»Ÿè®¡:")
    print(f"   å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
    print(f"   ä¸­ä½æ•°é•¿åº¦: {median_length} å­—ç¬¦")
    print(f"   æœ€å°é•¿åº¦: {min_length} å­—ç¬¦")
    print(f"   æœ€å¤§é•¿åº¦: {max_length} å­—ç¬¦")
    
    # é•¿åº¦åˆ†å¸ƒ
    print(f"\nğŸ“Š é•¿åº¦åˆ†å¸ƒ:")
    length_ranges = [
        (0, 100, "0-100å­—ç¬¦"),
        (100, 500, "100-500å­—ç¬¦"),
        (500, 1000, "500-1000å­—ç¬¦"),
        (1000, 2000, "1000-2000å­—ç¬¦"),
        (2000, 5000, "2000-5000å­—ç¬¦"),
        (5000, 8192, "5000-8192å­—ç¬¦"),
        (8192, float('inf'), "è¶…è¿‡8192å­—ç¬¦")
    ]
    
    for start, end, label in length_ranges:
        count = len([l for l in summary_lengths if start <= l < end])
        percentage = (count / len(summary_lengths)) * 100
        print(f"   {label}: {count} æ¡ ({percentage:.1f}%)")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¶…é•¿çš„summary
    if long_summaries:
        print(f"\nâš ï¸  å‘ç° {len(long_summaries)} ä¸ªè¶…è¿‡8192å­—ç¬¦çš„summary:")
        for item in long_summaries[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"   ç´¢å¼• {item['index']}: {item['length']} å­—ç¬¦")
            print(f"   å…¬å¸: {item['company']}")
            print(f"   å†…å®¹é¢„è§ˆ: {item['summary']}")
            print()
        
        if len(long_summaries) > 5:
            print(f"   ... è¿˜æœ‰ {len(long_summaries) - 5} ä¸ªè¶…é•¿summary")
    else:
        print(f"\nâœ… æ‰€æœ‰summaryéƒ½åœ¨8192å­—ç¬¦ä»¥å†…")
    
    # åˆ†æé•¿åº¦åˆ†å¸ƒ
    print(f"\nğŸ” é•¿åº¦åˆ†æ:")
    if avg_length < 1000:
        print(f"   âœ… Summaryå¹³å‡é•¿åº¦è¾ƒçŸ­ ({avg_length:.1f}å­—ç¬¦)ï¼Œé€‚åˆFAISSç´¢å¼•")
    elif avg_length < 3000:
        print(f"   âš ï¸  Summaryå¹³å‡é•¿åº¦ä¸­ç­‰ ({avg_length:.1f}å­—ç¬¦)ï¼Œéœ€è¦å…³æ³¨")
    else:
        print(f"   âŒ Summaryå¹³å‡é•¿åº¦è¾ƒé•¿ ({avg_length:.1f}å­—ç¬¦)ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
    
    if max_length > 8192:
        print(f"   âŒ å­˜åœ¨è¶…è¿‡8192å­—ç¬¦çš„summaryï¼Œéœ€è¦å¤„ç†")
    else:
        print(f"   âœ… æ‰€æœ‰summaryéƒ½åœ¨8192å­—ç¬¦ä»¥å†…ï¼Œæ— éœ€é¢å¤–å¤„ç†")
    
    # å»ºè®®
    print(f"\nğŸ’¡ å»ºè®®:")
    if max_length <= 8192:
        print(f"   âœ… å½“å‰summaryé•¿åº¦é€‚åˆFAISSç´¢å¼•ï¼Œæ— éœ€é¢å¤–chunking")
    else:
        print(f"   âš ï¸  å»ºè®®å¯¹è¶…é•¿summaryè¿›è¡Œæˆªæ–­æˆ–åˆ†å‰²")
        print(f"   ğŸ“ å¯ä»¥è€ƒè™‘åœ¨_build_faiss_indexä¸­æ·»åŠ é•¿åº¦æ£€æŸ¥")

def check_faiss_index_impact():
    """æ£€æŸ¥å¯¹FAISSç´¢å¼•çš„å½±å“"""
    print(f"\nğŸ” å¯¹FAISSç´¢å¼•çš„å½±å“åˆ†æ:")
    print("=" * 60)
    
    data_path = Path("data/alphafin/alphafin_merged_generated_qa.json")
    
    if not data_path.exists():
        return
    
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ç»Ÿè®¡ç”¨äºFAISSç´¢å¼•çš„æ–‡æœ¬é•¿åº¦
    faiss_text_lengths = []
    
    for record in data:
        summary = record.get('summary', '')
        if summary:
            faiss_text_lengths.append(len(summary))
    
    if not faiss_text_lengths:
        return
    
    avg_length = sum(faiss_text_lengths) / len(faiss_text_lengths)
    max_length = max(faiss_text_lengths)
    
    print(f"FAISSç´¢å¼•æ–‡æœ¬ç»Ÿè®¡:")
    print(f"   å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
    print(f"   æœ€å¤§é•¿åº¦: {max_length} å­—ç¬¦")
    print(f"   æ€»æ–‡æœ¬æ•°: {len(faiss_text_lengths)}")
    
    # è¯„ä¼°æ€§èƒ½å½±å“
    if avg_length < 500:
        print(f"   âœ… å¹³å‡é•¿åº¦è¾ƒçŸ­ï¼ŒFAISSç´¢å¼•æ€§èƒ½è‰¯å¥½")
    elif avg_length < 1000:
        print(f"   âš ï¸  å¹³å‡é•¿åº¦ä¸­ç­‰ï¼ŒFAISSç´¢å¼•æ€§èƒ½å¯æ¥å—")
    else:
        print(f"   âŒ å¹³å‡é•¿åº¦è¾ƒé•¿ï¼Œå¯èƒ½å½±å“FAISSç´¢å¼•æ€§èƒ½")
    
    if max_length > 8192:
        print(f"   âŒ å­˜åœ¨è¶…é•¿æ–‡æœ¬ï¼Œå¯èƒ½å¯¼è‡´å†…å­˜é—®é¢˜")
    else:
        print(f"   âœ… æ‰€æœ‰æ–‡æœ¬éƒ½åœ¨åˆç†é•¿åº¦èŒƒå›´å†…")

if __name__ == "__main__":
    analyze_summary_lengths()
    check_faiss_index_impact() 