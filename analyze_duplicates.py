#!/usr/bin/env python3
"""
åˆ†æçŸ¥è¯†åº“ä¸­çš„é‡å¤å†…å®¹
"""

import json
from collections import defaultdict
import hashlib

def analyze_duplicates():
    """åˆ†æçŸ¥è¯†åº“ä¸­çš„é‡å¤å†…å®¹"""
    
    knowledge_base_file = "data/unified/tatqa_knowledge_base_combined.jsonl"
    
    print("ğŸ” åˆ†æçŸ¥è¯†åº“é‡å¤å†…å®¹...")
    
    # æ”¶é›†æ‰€æœ‰æ–‡æ¡£
    documents = []
    with open(knowledge_base_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                doc = json.loads(line)
                documents.append(doc)
    
    print(f"ğŸ“Š æ€»æ–‡æ¡£æ•°: {len(documents)}")
    
    # æŒ‰contextåˆ†ç»„
    context_groups = defaultdict(list)
    for doc in documents:
        context = doc.get('context', '')
        if context:
            # æ ‡å‡†åŒ–contextç”¨äºæ¯”è¾ƒ
            normalized_context = ' '.join(context.split())
            context_groups[normalized_context].append(doc)
    
    # æ‰¾å‡ºé‡å¤çš„context
    duplicates = {context: docs for context, docs in context_groups.items() if len(docs) > 1}
    
    print(f"\nğŸ“‹ é‡å¤å†…å®¹ç»Ÿè®¡:")
    print(f"   - å”¯ä¸€contextæ•°é‡: {len(context_groups)}")
    print(f"   - æœ‰é‡å¤çš„contextæ•°é‡: {len(duplicates)}")
    print(f"   - é‡å¤æ–‡æ¡£æ€»æ•°: {sum(len(docs) for docs in duplicates.values())}")
    
    if duplicates:
        print(f"\nğŸ” é‡å¤å†…å®¹è¯¦æƒ…:")
        for i, (context, docs) in enumerate(list(duplicates.items())[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"\né‡å¤ç»„ {i+1} (å…±{len(docs)}ä¸ªæ–‡æ¡£):")
            print(f"Contexté¢„è§ˆ: {context[:100]}...")
            
            for j, doc in enumerate(docs):
                doc_id = doc.get('doc_id', '')
                source = doc.get('source', '')
                print(f"  {j+1}. {doc_id} ({source})")
                
                # æ˜¾ç¤ºIDç›¸å…³å­—æ®µ
                if 'all_doc_ids' in doc:
                    print(f"     æ‰€æœ‰doc_ids: {doc['all_doc_ids']}")
                if 'all_table_ids' in doc:
                    print(f"     æ‰€æœ‰table_ids: {doc['all_table_ids']}")
                if 'all_paragraph_ids' in doc:
                    print(f"     æ‰€æœ‰paragraph_ids: {doc['all_paragraph_ids']}")
        
        if len(duplicates) > 10:
            print(f"\n... è¿˜æœ‰ {len(duplicates) - 10} ä¸ªé‡å¤ç»„æœªæ˜¾ç¤º")
    
    # åˆ†æé‡å¤çš„åŸå› 
    print(f"\nğŸ” é‡å¤åŸå› åˆ†æ:")
    
    # æŒ‰sourceç»Ÿè®¡
    source_stats = defaultdict(int)
    for docs in duplicates.values():
        for doc in docs:
            source = doc.get('source', '')
            source_stats[source] += 1
    
    print(f"é‡å¤æ–‡æ¡£çš„æ¥æºåˆ†å¸ƒ:")
    for source, count in source_stats.items():
        print(f"  {source}: {count} ä¸ªæ–‡æ¡£")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è·¨sourceçš„é‡å¤
    cross_source_duplicates = 0
    for context, docs in duplicates.items():
        sources = set(doc.get('source', '') for doc in docs)
        if len(sources) > 1:
            cross_source_duplicates += 1
    
    print(f"\nè·¨æ¥æºé‡å¤çš„contextæ•°é‡: {cross_source_duplicates}")
    
    return duplicates

if __name__ == "__main__":
    analyze_duplicates() 