#!/usr/bin/env python3
"""
RAGç³»ç»Ÿå¤šè¯­è¨€ç«¯åˆ°ç«¯æµ‹è¯•è„šæœ¬
æ”¯æŒåˆ†åˆ«æµ‹è¯•ä¸­æ–‡å’Œè‹±æ–‡æ•°æ®é›†ï¼Œæ”¯æŒæ•°æ®é‡‡æ ·
æ¯10ä¸ªæ•°æ®ä¿å­˜ä¸€æ¬¡åŸå§‹æ•°æ®
"""

import sys
import os
import time
import json
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import argparse
from tqdm import tqdm
import numpy as np
import jieba
import random

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥RAGç³»ç»Ÿç»„ä»¶
from xlm.ui.optimized_rag_ui import OptimizedRagUI
from config.parameters import config

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('e2e_test_multilingual.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class MultilingualRagSystemAdapter:
    """
    å¤šè¯­è¨€RAGç³»ç»Ÿé€‚é…å™¨ï¼Œæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡æ•°æ®é›†çš„è¯„ä¼°
    """
    
    # ç±»çº§åˆ«çš„ç¼“å­˜ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
    _instance = None
    _initialized = False
    
    def __new__(cls, *args, **kwargs):
        """å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ªRAGç³»ç»Ÿå®ä¾‹"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, enable_reranker: bool = True, enable_stock_prediction: bool = False):
        """
        åˆå§‹åŒ–å¤šè¯­è¨€RAGç³»ç»Ÿé€‚é…å™¨
        
        Args:
            enable_reranker: æ˜¯å¦å¯ç”¨é‡æ’åºå™¨
            enable_stock_prediction: æ˜¯å¦å¯ç”¨è‚¡ç¥¨é¢„æµ‹æ¨¡å¼
        """
        # å¦‚æœå·²ç»åˆå§‹åŒ–è¿‡ï¼Œç›´æ¥è¿”å›
        if self._initialized:
            return
            
        self.enable_reranker = enable_reranker
        self.enable_stock_prediction = enable_stock_prediction
        self.rag_ui = None
        self.initialized = False
        
    def initialize(self):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ"""
        # å¦‚æœå·²ç»åˆå§‹åŒ–è¿‡ï¼Œç›´æ¥è¿”å›
        if self._initialized:
            logger.info("âœ… RAGç³»ç»Ÿå·²ç»åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return
            
        try:
            logger.info("ğŸ”„ æ­£åœ¨åˆå§‹åŒ–å¤šè¯­è¨€RAGç³»ç»Ÿ...")
            
            # åˆå§‹åŒ–RAG UIç³»ç»Ÿï¼ˆä¼šè‡ªåŠ¨è°ƒç”¨_init_componentsï¼‰
            self.rag_ui = OptimizedRagUI(
                enable_reranker=self.enable_reranker,
                use_existing_embedding_index=True  # ä½¿ç”¨ç°æœ‰ç´¢å¼•ä»¥åŠ å¿«æµ‹è¯•
            )
            
            # ä¸éœ€è¦æ‰‹åŠ¨è°ƒç”¨_init_components()ï¼Œå› ä¸ºOptimizedRagUIåœ¨åˆå§‹åŒ–æ—¶ä¼šè‡ªåŠ¨è°ƒç”¨
            
            self.initialized = True
            self._initialized = True  # è®¾ç½®ç±»çº§åˆ«çš„åˆå§‹åŒ–æ ‡å¿—
            logger.info("âœ… å¤šè¯­è¨€RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ å¤šè¯­è¨€RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def process_query(self, query: str, datasource: str = "auto", enable_stock_prediction_override: Optional[bool] = None) -> Dict[str, Any]:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢ï¼Œè¿”å›å®Œæ•´çš„RAGç³»ç»Ÿå“åº”
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            datasource: æ•°æ®æºï¼ˆautoè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹ï¼‰
            enable_stock_prediction_override: ä¸´æ—¶è¦†ç›–è‚¡ç¥¨é¢„æµ‹æ¨¡å¼è®¾ç½®
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œæ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
        """
        if not self.initialized:
            raise RuntimeError("RAGç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨initialize()")
        
        start_time = time.time()
        
        try:
            # æ£€æŸ¥RAG UIæ˜¯å¦å·²åˆå§‹åŒ–
            if self.rag_ui is None:
                raise RuntimeError("RAG UIæœªåˆå§‹åŒ–")
            
            # è®°å½•ç”Ÿæˆå¼€å§‹æ—¶é—´
            generation_start_time = time.time()
            
            # ä½¿ç”¨ä¸RAGç³»ç»Ÿå®Œå…¨ç›¸åŒçš„å¤„ç†é€»è¾‘
            # å¦‚æœæä¾›äº†è¦†ç›–å‚æ•°ï¼Œä½¿ç”¨è¦†ç›–å€¼ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤è®¾ç½®
            stock_prediction_checkbox = self.enable_stock_prediction
            if enable_stock_prediction_override is not None:
                stock_prediction_checkbox = enable_stock_prediction_override
            
            answer, html_content = self.rag_ui._process_question(
                question=query,
                datasource=datasource,
                reranker_checkbox=self.enable_reranker,
                stock_prediction_checkbox=stock_prediction_checkbox
            )
            
            # è®°å½•ç”Ÿæˆç»“æŸæ—¶é—´å’Œè®¡ç®—Tokenæ•°
            generation_end_time = time.time()
            generation_time = generation_end_time - generation_start_time
            
            # è®¡ç®—Tokenæ•°ï¼ˆéœ€è¦æ£€æµ‹è¯­è¨€ï¼‰
            try:
                from langdetect import detect
                language = detect(query)
                is_chinese = language.startswith('zh')
            except:
                # å¦‚æœè¯­è¨€æ£€æµ‹å¤±è´¥ï¼Œæ ¹æ®å­—ç¬¦åˆ¤æ–­
                chinese_chars = sum(1 for char in query if '\u4e00' <= char <= '\u9fff')
                is_chinese = chinese_chars > 0
            
            token_count = count_tokens(answer, "chinese" if is_chinese else "english")
            
            # æå–æ‘˜è¦å’Œæ™ºèƒ½é€‰æ‹©çš„ä¸Šä¸‹æ–‡ï¼ˆä»HTMLå†…å®¹ä¸­è§£æï¼‰
            summary_context = self._extract_summary_and_context(html_content)
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            return {
                "query": query,
                "answer": answer,
                "html_content": html_content,
                "summary_context": summary_context,  # æ–°å¢ï¼šæ‘˜è¦å’Œæ™ºèƒ½é€‰æ‹©çš„ä¸Šä¸‹æ–‡
                "processing_time": processing_time,
                "generation_time": generation_time,
                "token_count": token_count,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"âŒ æŸ¥è¯¢å¤„ç†å¤±è´¥: {e}")
            return {
                "query": query,
                "answer": f"å¤„ç†å¤±è´¥: {str(e)}",
                "html_content": "",
                "summary_context": "",
                "processing_time": time.time() - start_time,
                "generation_time": 0.0,
                "token_count": 0,
                "success": False,
                "error": str(e)
            }
    
    def _extract_summary_and_context(self, html_content: str) -> str:
        """
        ä»HTMLå†…å®¹ä¸­æå–æ‘˜è¦å’Œæ™ºèƒ½é€‰æ‹©çš„ä¸Šä¸‹æ–‡
        
        Args:
            html_content: HTMLæ ¼å¼çš„ä¸Šä¸‹æ–‡å†…å®¹
            
        Returns:
            æå–çš„æ‘˜è¦å’Œä¸Šä¸‹æ–‡
        """
        if not html_content:
            return ""
        
        try:
            # ç®€å•çš„æ–‡æœ¬æå–ï¼Œç§»é™¤HTMLæ ‡ç­¾
            import re
            # ç§»é™¤HTMLæ ‡ç­¾
            text_content = re.sub(r'<[^>]+>', '', html_content)
            # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
            text_content = re.sub(r'\s+', ' ', text_content).strip()
            
            # é™åˆ¶é•¿åº¦ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
            if len(text_content) > 2000:
                text_content = text_content[:2000] + "..."
            
            return text_content
        except Exception as e:
            logger.warning(f"æå–ä¸Šä¸‹æ–‡å¤±è´¥: {e}")
            return html_content[:1000] if html_content else ""


def normalize_answer_chinese(s: str) -> str:
    """æ ‡å‡†åŒ–ä¸­æ–‡ç­”æ¡ˆ"""
    if not s:
        return ""
    
    # ç§»é™¤"è§£æ"åŠå…¶åé¢çš„å†…å®¹
    import re
    # æŸ¥æ‰¾"è§£æ"çš„ä½ç½®ï¼Œç§»é™¤å®ƒåŠå…¶åé¢çš„æ‰€æœ‰å†…å®¹
    parse_index = s.find("è§£æ")
    if parse_index != -1:
        s = s[:parse_index]
    
    s = ' '.join(s.split())
    s = re.sub(r'[^\u4e00-\u9fff\w\s]', '', s)
    return s.strip()


def normalize_answer_english(s: str) -> str:
    """æ ‡å‡†åŒ–è‹±æ–‡ç­”æ¡ˆ"""
    if not s:
        return ""
    s = ' '.join(s.split())
    import re
    s = re.sub(r'[^\w\s]', '', s)
    return s.strip().lower()


def get_tokens_chinese(s: str) -> List[str]:
    """ä½¿ç”¨jiebaåˆ†è¯è·å–ä¸­æ–‡tokenåˆ—è¡¨"""
    return list(jieba.cut(s))


def get_tokens_english(s: str) -> List[str]:
    """è·å–è‹±æ–‡tokenåˆ—è¡¨"""
    return s.split()


def calculate_f1_score(prediction: str, ground_truth: str, language: str = "chinese") -> float:
    """è®¡ç®—F1-scoreï¼Œæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡"""
    if language == "chinese":
        pred_tokens = set(get_tokens_chinese(normalize_answer_chinese(prediction)))
        gt_tokens = set(get_tokens_chinese(normalize_answer_chinese(ground_truth)))
    else:
        pred_tokens = set(get_tokens_english(normalize_answer_english(prediction)))
        gt_tokens = set(get_tokens_english(normalize_answer_english(ground_truth)))
    
    if not gt_tokens:
        return 1.0 if not pred_tokens else 0.0
    
    intersection = pred_tokens & gt_tokens
    precision = len(intersection) / len(pred_tokens) if pred_tokens else 0.0
    recall = len(intersection) / len(gt_tokens)
    
    if precision + recall == 0:
        return 0.0
    
    return 2 * precision * recall / (precision + recall)


def calculate_exact_match(prediction: str, ground_truth: str, language: str = "chinese") -> float:
    """è®¡ç®—Exact Matchï¼Œæ”¯æŒä¸­æ–‡å’Œè‹±æ–‡"""
    if language == "chinese":
        pred_normalized = normalize_answer_chinese(prediction)
        gt_normalized = normalize_answer_chinese(ground_truth)
    else:
        pred_normalized = normalize_answer_english(prediction)
        gt_normalized = normalize_answer_english(ground_truth)
    
    return 1.0 if pred_normalized == gt_normalized else 0.0


def count_tokens(text: str, language: str = "chinese") -> int:
    """
    è®¡ç®—æ–‡æœ¬çš„Tokenæ•°é‡
    
    Args:
        text: è¦è®¡ç®—çš„æ–‡æœ¬
        language: è¯­è¨€ç±»å‹
        
    Returns:
        Tokenæ•°é‡
    """
    if not text:
        return 0
    
    if language == "chinese":
        # ä¸­æ–‡ä½¿ç”¨jiebaåˆ†è¯
        tokens = get_tokens_chinese(text)
    else:
        # è‹±æ–‡ä½¿ç”¨ç©ºæ ¼åˆ†è¯
        tokens = get_tokens_english(text)
    
    return len(tokens)


def is_stock_prediction_query(test_item: Dict[str, Any]) -> bool:
    """
    æ£€æµ‹æ•°æ®é¡¹æ˜¯å¦ä¸ºè‚¡ç¥¨é¢„æµ‹æŒ‡ä»¤
    
    Args:
        test_item: æµ‹è¯•æ•°æ®é¡¹
        
    Returns:
        æ˜¯å¦ä¸ºè‚¡ç¥¨é¢„æµ‹æŸ¥è¯¢
    """
    # æ£€æŸ¥instructionå­—æ®µ
    instruction = test_item.get("instruction", "")
    
    # åªæœ‰å½“instructionç­‰äºç‰¹å®šå­—ç¬¦ä¸²æ—¶ï¼Œæ‰è®¤ä¸ºæ˜¯è‚¡ç¥¨é¢„æµ‹æŸ¥è¯¢
    stock_prediction_instruction = "è¯·æ ¹æ®ä¸‹æ–¹æä¾›çš„è¯¥è‚¡ç¥¨ç›¸å…³ç ”æŠ¥ä¸æ•°æ®ï¼Œå¯¹è¯¥è‚¡ç¥¨çš„ä¸‹ä¸ªæœˆçš„æ¶¨è·Œï¼Œè¿›è¡Œé¢„æµ‹ï¼Œè¯·ç»™å‡ºæ˜ç¡®çš„ç­”æ¡ˆï¼Œ\"æ¶¨\" æˆ–è€… \"è·Œ\"ã€‚åŒæ—¶ç»™å‡ºè¿™ä¸ªè‚¡ç¥¨ä¸‹æœˆçš„æ¶¨è·Œæ¦‚ç‡ï¼Œåˆ†åˆ«æ˜¯:æå¤§ï¼Œè¾ƒå¤§ï¼Œä¸­ä¸Šï¼Œä¸€èˆ¬ã€‚"
    
    if instruction.strip() == stock_prediction_instruction:
        return True
    
    return False


def load_test_dataset(data_path: str, sample_size: Optional[int] = None) -> Tuple[List[Dict[str, Any]], str]:
    """
    åŠ è½½æµ‹è¯•æ•°æ®é›†
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        sample_size: é‡‡æ ·æ•°é‡ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®
        
    Returns:
        (æµ‹è¯•æ•°æ®åˆ—è¡¨, è¯­è¨€)
    """
    logger.info(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®é›†: {data_path}")
    
    # æ£€æµ‹è¯­è¨€
    if "alphafin" in data_path.lower():
        language = "chinese"
    elif "tatqa" in data_path.lower():
        language = "english"
    else:
        language = "unknown"
    
    logger.info(f"ğŸŒ æ£€æµ‹åˆ°è¯­è¨€: {language}")
    
    dataset = []
    
    if data_path.endswith('.jsonl'):
        with open(data_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    dataset.append(json.loads(line))
    elif data_path.endswith('.json'):
        with open(data_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
    
    if sample_size and sample_size < len(dataset):
        random.seed(42)  # å›ºå®šéšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
        dataset = random.sample(dataset, sample_size)
        logger.info(f"ğŸ“Š éšæœºé‡‡æ · {sample_size} ä¸ªæ ·æœ¬")
    
    logger.info(f"âœ… åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} ä¸ªæµ‹è¯•æ ·æœ¬")
    return dataset, language


def save_raw_data_batch(raw_data_batch: List[Dict[str, Any]], data_path: str, batch_num: int):
    """
    ä¿å­˜åŸå§‹æ•°æ®æ‰¹æ¬¡
    
    Args:
        raw_data_batch: åŸå§‹æ•°æ®æ‰¹æ¬¡
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        batch_num: æ‰¹æ¬¡ç¼–å·
    """
    # ä»æ•°æ®è·¯å¾„ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    data_name = Path(data_path).stem
    output_dir = f"raw_data_{data_name}"
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    # æ·»åŠ æ—¶é—´æˆ³åˆ°æ–‡ä»¶å
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    output_file = Path(output_dir) / f"batch_{batch_num:03d}_{timestamp}.json"
    
    # æ„å»ºåŒ…å«æ—¶é—´æˆ³çš„æ•°æ®ç»“æ„
    output_data = {
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "batch_num": batch_num,
        "data_path": data_path,
        "total_samples": len(raw_data_batch),
        "successful_samples": sum(1 for item in raw_data_batch if item.get("success", False)),
        "failed_samples": sum(1 for item in raw_data_batch if not item.get("success", True)),
        "data": raw_data_batch
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    logger.info(f"ğŸ“ ä¿å­˜åŸå§‹æ•°æ®æ‰¹æ¬¡ {batch_num} åˆ°: {output_file}")
    logger.info(f"â° æ—¶é—´æˆ³: {output_data['timestamp']}")
    logger.info(f"ğŸ“Š æ‰¹æ¬¡ç»Ÿè®¡: æˆåŠŸ {output_data['successful_samples']} ä¸ª, å¤±è´¥ {output_data['failed_samples']} ä¸ª")


def test_single_dataset(
    data_path: str,
    sample_size: Optional[int] = None,
    enable_reranker: bool = True,
    enable_stock_prediction: bool = False
) -> Dict[str, Any]:
    """
    æµ‹è¯•å•ä¸ªæ•°æ®é›†
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        sample_size: é‡‡æ ·æ•°é‡
        enable_reranker: æ˜¯å¦å¯ç”¨é‡æ’åºå™¨
        enable_stock_prediction: æ˜¯å¦å¯ç”¨è‚¡ç¥¨é¢„æµ‹æ¨¡å¼
        
    Returns:
        æµ‹è¯•ç»“æœæ‘˜è¦
    """
    logger.info(f"ğŸš€ å¼€å§‹æµ‹è¯•æ•°æ®é›†: {data_path}")
    logger.info(f"é‡æ’åºå™¨: {'å¯ç”¨' if enable_reranker else 'ç¦ç”¨'}")
    logger.info(f"è‚¡ç¥¨é¢„æµ‹: {'å¯ç”¨' if enable_stock_prediction else 'ç¦ç”¨'}")
    
    # 2. åŠ è½½æµ‹è¯•æ•°æ®
    dataset, language = load_test_dataset(data_path, sample_size)
    
    # 3. é¢„æ£€æµ‹è‚¡ç¥¨é¢„æµ‹æŸ¥è¯¢ï¼Œå†³å®šæ˜¯å¦å¯ç”¨è‚¡ç¥¨é¢„æµ‹æ¨¡å¼
    stock_prediction_queries = set()
    should_enable_stock_prediction = enable_stock_prediction
    
    if "alphafin" in data_path.lower() and language == "chinese":
        for i, test_item in enumerate(dataset):
            if is_stock_prediction_query(test_item):
                stock_prediction_queries.add(i)
        
        if stock_prediction_queries and not enable_stock_prediction:
            logger.info(f"ğŸ”® æ£€æµ‹åˆ° {len(stock_prediction_queries)} ä¸ªè‚¡ç¥¨é¢„æµ‹æŸ¥è¯¢ï¼Œå°†è‡ªåŠ¨å¯ç”¨è‚¡ç¥¨é¢„æµ‹æ¨¡å¼")
            should_enable_stock_prediction = True
    
    # 1. åˆå§‹åŒ–RAGç³»ç»Ÿé€‚é…å™¨ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼Œä½¿ç”¨æœ€ç»ˆç¡®å®šçš„é…ç½®ï¼‰
    rag_adapter = MultilingualRagSystemAdapter(
        enable_reranker=enable_reranker,
        enable_stock_prediction=should_enable_stock_prediction
    )
    rag_adapter.initialize()
    
    # 4. è¿è¡Œæµ‹è¯•
    results = []
    total_processing_time = 0.0
    raw_data_batch = []
    batch_num = 1
    
    logger.info(f"ğŸ”„ å¼€å§‹å¤„ç†æµ‹è¯•æ ·æœ¬...")
    
    for i, test_item in enumerate(tqdm(dataset, desc=f"å¤„ç†æ ·æœ¬")):
        # è·å–æŸ¥è¯¢å’Œæ ‡å‡†ç­”æ¡ˆ
        # ä¼˜å…ˆä½¿ç”¨generated_questionï¼Œä¸RAGç³»ç»Ÿä¿æŒä¸€è‡´
        query = test_item.get("generated_question", "") or test_item.get("query", "") or test_item.get("question", "")
        ground_truth = test_item.get("answer", "") or test_item.get("expected_answer", "")
        
        if not query:
            logger.warning(f"âš ï¸ æ ·æœ¬ {i} ç¼ºå°‘æŸ¥è¯¢ï¼Œè·³è¿‡")
            continue
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè‚¡ç¥¨é¢„æµ‹æŸ¥è¯¢ï¼ˆä½¿ç”¨é¢„æ£€æµ‹ç»“æœï¼‰
        auto_stock_prediction = i in stock_prediction_queries
        if auto_stock_prediction:
            logger.info(f"ğŸ”® å¤„ç†è‚¡ç¥¨é¢„æµ‹æŸ¥è¯¢: {query[:50]}...")
        
        # å¤„ç†æŸ¥è¯¢ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„é€‚é…å™¨ï¼‰
        # å¦‚æœå½“å‰æ ·æœ¬æ˜¯è‚¡ç¥¨é¢„æµ‹æŸ¥è¯¢ï¼ŒåŠ¨æ€å¯ç”¨è‚¡ç¥¨é¢„æµ‹æ¨¡å¼
        if auto_stock_prediction:
            result = rag_adapter.process_query(query, enable_stock_prediction_override=True)
        else:
            result = rag_adapter.process_query(query, enable_stock_prediction_override=False)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        if result["success"] and ground_truth:
            f1_score = calculate_f1_score(result["answer"], ground_truth, language)
            exact_match = calculate_exact_match(result["answer"], ground_truth, language)
        else:
            f1_score = 0.0
            exact_match = 0.0
        
        # è®°å½•ç»“æœ
        test_result = {
            "sample_id": i,
            "query": query,
            "ground_truth": ground_truth,
            "predicted_answer": result["answer"],
            "f1_score": f1_score,
            "exact_match": exact_match,
            "processing_time": result["processing_time"],
            "generation_time": result.get("generation_time", 0.0),
            "token_count": result.get("token_count", 0),
            "success": result["success"],
            "language": language,
            "auto_stock_prediction": auto_stock_prediction
        }
        
        if not result["success"]:
            test_result["error"] = result.get("error", "æœªçŸ¥é”™è¯¯")
        
        results.append(test_result)
        total_processing_time += result["processing_time"]
        
        # æ„å»ºåŸå§‹æ•°æ®è®°å½•
        raw_data_record = {
            "sample_id": i,
            "query": query,
            "summary_context": result.get("summary_context", ""),  # ä½¿ç”¨æ‘˜è¦å’Œæ™ºèƒ½é€‰æ‹©çš„ä¸Šä¸‹æ–‡
            "answer": result["answer"],
            "expected_answer": ground_truth,
            "em": exact_match,
            "f1": f1_score,
            "processing_time": result["processing_time"],
            "generation_time": result.get("generation_time", 0.0),
            "token_count": result.get("token_count", 0),
            "success": result["success"],
            "language": language,
            "auto_stock_prediction": auto_stock_prediction
        }
        
        if not result["success"]:
            raw_data_record["error"] = result.get("error", "æœªçŸ¥é”™è¯¯")
        
        raw_data_batch.append(raw_data_record)
        
        # æ¯å¤„ç†10ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡åŸå§‹æ•°æ®
        if len(raw_data_batch) >= 10:
            save_raw_data_batch(raw_data_batch, data_path, batch_num)
            raw_data_batch = []
            batch_num += 1
        
        # æ¯å¤„ç†10ä¸ªæ ·æœ¬è¾“å‡ºä¸€æ¬¡è¿›åº¦
        if (i + 1) % 10 == 0:
            avg_time = total_processing_time / (i + 1)
            avg_f1 = np.mean([r["f1_score"] for r in results])
            avg_em = np.mean([r["exact_match"] for r in results])
            logger.info(f"ğŸ“Š è¿›åº¦: {i+1}/{len(dataset)}, å¹³å‡F1: {avg_f1:.4f}, å¹³å‡EM: {avg_em:.4f}, å¹³å‡æ—¶é—´: {avg_time:.2f}s")
    
    # ä¿å­˜å‰©ä½™çš„åŸå§‹æ•°æ®
    if raw_data_batch:
        save_raw_data_batch(raw_data_batch, data_path, batch_num)
    
    # è®¡ç®—ç»Ÿè®¡
    successful_results = [r for r in results if r["success"]]
    
    if successful_results:
        avg_f1 = np.mean([r["f1_score"] for r in successful_results])
        avg_em = np.mean([r["exact_match"] for r in successful_results])
        avg_time = np.mean([r["processing_time"] for r in successful_results])
        total_time = sum([r["processing_time"] for r in successful_results])
        avg_generation_time = np.mean([r["generation_time"] for r in successful_results])
        avg_token_count = np.mean([r["token_count"] for r in successful_results])
        total_tokens = sum([r["token_count"] for r in successful_results])
    else:
        avg_f1 = avg_em = avg_time = total_time = avg_generation_time = avg_token_count = total_tokens = 0.0
    
    # ç»Ÿè®¡è‚¡ç¥¨é¢„æµ‹æ£€æµ‹æƒ…å†µ
    stock_prediction_detected = sum(1 for r in results if r.get("auto_stock_prediction", False))
    
    # ç”Ÿæˆç»“æœæ‘˜è¦
    summary = {
        "data_path": data_path,
        "language": language,
        "total_samples": len(dataset),
        "successful_samples": len(successful_results),
        "success_rate": len(successful_results) / len(dataset) if dataset else 0.0,
        "average_f1_score": avg_f1,
        "average_exact_match": avg_em,
        "average_processing_time": avg_time,
        "total_processing_time": total_time,
        "average_generation_time": avg_generation_time,
        "average_token_count": avg_token_count,
        "total_tokens": total_tokens,
        "enable_reranker": enable_reranker,
        "enable_stock_prediction": should_enable_stock_prediction,
        "stock_prediction_detected": stock_prediction_detected,
        "detailed_results": results
    }
    
    # è¾“å‡ºæ‘˜è¦
    logger.info("ğŸ‰ æ•°æ®é›†æµ‹è¯•å®Œæˆï¼")
    print_dataset_summary(summary)
    
    return summary


def print_dataset_summary(summary: Dict[str, Any]):
    """æ‰“å°æ•°æ®é›†æµ‹è¯•æ‘˜è¦"""
    print("\n" + "="*80)
    print(f"ğŸ¯ æ•°æ®é›†æµ‹è¯•ç»“æœ: {summary['data_path']}")
    print("="*80)
    
    print(f"ğŸ“Š æµ‹è¯•æŒ‡æ ‡:")
    print(f"   æ•°æ®è·¯å¾„: {summary['data_path']}")
    print(f"   è¯­è¨€: {summary['language']}")
    print(f"   æ€»æ ·æœ¬æ•°: {summary['total_samples']}")
    print(f"   æˆåŠŸæ ·æœ¬æ•°: {summary['successful_samples']}")
    print(f"   æˆåŠŸç‡: {summary['success_rate']:.2%}")
    print(f"   å¹³å‡F1-score: {summary['average_f1_score']:.4f}")
    print(f"   å¹³å‡Exact Match: {summary['average_exact_match']:.4f}")
    print(f"   å¹³å‡å¤„ç†æ—¶é—´: {summary['average_processing_time']:.2f}ç§’")
    print(f"   æ€»å¤„ç†æ—¶é—´: {summary['total_processing_time']:.2f}ç§’")
    print(f"   å¹³å‡ç”Ÿæˆæ—¶é—´: {summary['average_generation_time']:.2f}ç§’")
    print(f"   å¹³å‡Tokenæ•°: {summary['average_token_count']:.1f}")
    print(f"   æ€»Tokenæ•°: {summary['total_tokens']}")
    print(f"   é‡æ’åºå™¨: {'å¯ç”¨' if summary['enable_reranker'] else 'ç¦ç”¨'}")
    print(f"   è‚¡ç¥¨é¢„æµ‹: {'å¯ç”¨' if summary['enable_stock_prediction'] else 'ç¦ç”¨'}")
    print(f"   è‡ªåŠ¨æ£€æµ‹è‚¡ç¥¨é¢„æµ‹: {summary.get('stock_prediction_detected', 0)} ä¸ª")
    
    print("="*80)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿå•æ•°æ®é›†ç«¯åˆ°ç«¯æµ‹è¯•")
    parser.add_argument("--data_path", type=str, required=True,
                       help="æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--sample_size", type=int, default=None,
                       help="é‡‡æ ·æ•°é‡ (é»˜è®¤ä½¿ç”¨å…¨éƒ¨æ•°æ®)")
    parser.add_argument("--disable_reranker", action="store_true",
                       help="ç¦ç”¨é‡æ’åºå™¨")
    parser.add_argument("--enable_stock_prediction", action="store_true",
                       help="å¯ç”¨è‚¡ç¥¨é¢„æµ‹æ¨¡å¼")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.data_path).exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {args.data_path}")
        return
    
    # è¿è¡Œå•æ•°æ®é›†æµ‹è¯•
    summary = test_single_dataset(
        data_path=args.data_path,
        sample_size=args.sample_size,
        enable_reranker=not args.disable_reranker,
        enable_stock_prediction=args.enable_stock_prediction
    )


if __name__ == "__main__":
    main() 