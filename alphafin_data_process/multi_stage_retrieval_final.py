import json
import pickle
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from collections import defaultdict
import re
from datetime import datetime
import sys
import numpy as np

# éœ€è¦å®‰è£…çš„ä¾èµ–ï¼špip install faiss-cpu sentence-transformers torch
try:
    import faiss
    from sentence_transformers import SentenceTransformer
    import torch
except ImportError as e:
    print(f"è¯·å®‰è£…å¿…è¦çš„ä¾èµ–: pip install faiss-cpu sentence-transformers torch")
    print(f"é”™è¯¯: {e}")
    exit(1)

# å¯¼å…¥ç°æœ‰çš„QwenReranker
try:
    sys.path.append(str(Path(__file__).parent.parent))
    from xlm.components.retriever.reranker import QwenReranker
    from config.parameters import Config, DEFAULT_CACHE_DIR
except ImportError as e:
    print(f"æ— æ³•å¯¼å…¥QwenRerankeræˆ–Config: {e}")
    print("è¯·ç¡®ä¿xlmç›®å½•ç»“æ„æ­£ç¡®")
    exit(1)

from xlm.components.prompt_templates.template_loader import template_loader

def load_json_or_jsonl(file_path: Path) -> List[Dict]:
    """
    å…¼å®¹åŠ è½½JSONæˆ–JSONLæ ¼å¼æ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ•°æ®åˆ—è¡¨
    """
    print(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
    
    try:
        # é¦–å…ˆå°è¯•ä½œä¸ºJSONåŠ è½½
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                print(f"æˆåŠŸåŠ è½½JSONæ ¼å¼æ–‡ä»¶ï¼Œå…± {len(data)} æ¡è®°å½•")
                return data
            except json.JSONDecodeError as e:
                print(f"JSONæ ¼å¼è§£æå¤±è´¥: {e}")
                print("å°è¯•ä½œä¸ºJSONLæ ¼å¼åŠ è½½...")
                
                # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
                f.seek(0)
                
                # å°è¯•ä½œä¸ºJSONLåŠ è½½
                data = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:  # è·³è¿‡ç©ºè¡Œ
                        try:
                            item = json.loads(line)
                            data.append(item)
                        except json.JSONDecodeError as line_error:
                            print(f"è­¦å‘Š: ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {line_error}")
                            print(f"é—®é¢˜è¡Œå†…å®¹: {line[:100]}...")
                            continue
                
                print(f"æˆåŠŸåŠ è½½JSONLæ ¼å¼æ–‡ä»¶ï¼Œå…± {len(data)} æ¡è®°å½•")
                return data
                
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []
    except Exception as e:
        print(f"é”™è¯¯: è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return []

class MultiStageRetrievalSystem:
    """
    å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿï¼š
    1. Pre-filtering: åŸºäºå…ƒæ•°æ®ï¼ˆä»…ä¸­æ–‡æ•°æ®æ”¯æŒï¼‰
    2. FAISSæ£€ç´¢: åŸºäºgenerated_questionå’Œsummaryç”Ÿæˆç»Ÿä¸€åµŒå…¥ç´¢å¼•
    3. Reranker: åŸºäºoriginal_contextä½¿ç”¨Qwen3-0.6Bè¿›è¡Œé‡æ’åº
    
    æ”¯æŒè‹±æ–‡å’Œä¸­æ–‡æ•°æ®é›†ï¼Œä½¿ç”¨ç°æœ‰é…ç½®çš„æ¨¡å‹
    - ä¸­æ–‡æ•°æ®ï¼ˆAlphaFinï¼‰ï¼šæ”¯æŒå…ƒæ•°æ®é¢„è¿‡æ»¤ + FAISS + Qwené‡æ’åº
    - è‹±æ–‡æ•°æ®ï¼ˆTatQAï¼‰ï¼šä»…æ”¯æŒFAISS + Qwené‡æ’åºï¼ˆæ— å…ƒæ•°æ®ï¼‰
    """
    
    def __init__(self, data_path: Path, dataset_type: str = "chinese", use_existing_config: bool = True):
        """
        åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            dataset_type: æ•°æ®é›†ç±»å‹ ("chinese" æˆ– "english")
            use_existing_config: æ˜¯å¦ä½¿ç”¨ç°æœ‰é…ç½®
        """
        self.data_path = data_path
        self.dataset_type = dataset_type
        self.use_existing_config = use_existing_config
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.data = []
        self.embedding_model = None
        self.faiss_index = None
        self.valid_indices = []
        self.qwen_reranker = None
        self.llm_generator = None
        
        # å…ƒæ•°æ®ç´¢å¼•
        self.metadata_index = {
            'company_name': {},
            'stock_code': {},
            'report_date': {},
            'company_stock': {}
        }
        
        # è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„
        self.stock_company_mapping = {}
        self.company_stock_mapping = {}
        
        # é…ç½®
        self.config = None
        self.model_name = None
        
        # æ–‡æ¡£åˆ°chunksçš„æ˜ å°„ï¼ˆç”¨äºé‡æ’åºï¼‰
        self.doc_to_chunks_mapping = {}
        
        # åŠ è½½é…ç½®
        if use_existing_config:
            self._load_config()
        
        # åŠ è½½æ•°æ®
        self._load_data()
        
        # åŠ è½½è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„
        self._load_stock_company_mapping()
        
        # æ„å»ºå…ƒæ•°æ®ç´¢å¼•
        self._build_metadata_index()
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self._init_embedding_model()
        
        # æ„å»ºFAISSç´¢å¼•
        self._build_faiss_index()
        
        # åˆå§‹åŒ–é‡æ’åºå™¨
        self._init_qwen_reranker()
        
        # åˆå§‹åŒ–LLMç”Ÿæˆå™¨
        self._init_llm_generator()
    
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            from config.parameters import Config
            self.config = Config()
            
            # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©ç¼–ç å™¨
            if self.dataset_type == "chinese":
                # ä½¿ç”¨ä¸­æ–‡ç¼–ç å™¨
                self.model_name = self.config.encoder.chinese_model_path
                print(f"ä½¿ç”¨ä¸­æ–‡ç¼–ç å™¨: {self.model_name}")
            else:
                # ä½¿ç”¨è‹±æ–‡ç¼–ç å™¨
                self.model_name = self.config.encoder.english_model_path
                print(f"ä½¿ç”¨è‹±æ–‡ç¼–ç å™¨: {self.model_name}")
            
            print("ä½¿ç”¨ç°æœ‰é…ç½®åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ")
        except Exception as e:
            print(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
            # å›é€€åˆ°é»˜è®¤æ¨¡å‹
            if self.dataset_type == "chinese":
                self.model_name = "distiluse-base-multilingual-cased-v2"
                print(f"ä½¿ç”¨é»˜è®¤ä¸­æ–‡ç¼–ç å™¨: {self.model_name}")
            else:
                self.model_name = "all-MiniLM-L6-v2"
                print(f"ä½¿ç”¨é»˜è®¤è‹±æ–‡ç¼–ç å™¨: {self.model_name}")
    
    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")
        
        # åŠ è½½åŸå§‹AlphaFinæ•°æ®ç”¨äºFAISSç´¢å¼•
        print("åŠ è½½åŸå§‹AlphaFinæ•°æ®ç”¨äºFAISSç´¢å¼•...")
        self.data = load_json_or_jsonl(self.data_path)
        print(f"åŠ è½½äº† {len(self.data)} æ¡åŸå§‹è®°å½•")
        
        # å»ºç«‹doc_idåˆ°chunksçš„æ˜ å°„
        print("å»ºç«‹doc_idåˆ°chunksçš„æ˜ å°„å…³ç³»...")
        self.doc_to_chunks_mapping = {}
        
        for doc_idx, record in enumerate(self.data):
            if self.dataset_type == "chinese":
                # å¯¹äºä¸­æ–‡æ•°æ®ï¼Œç”Ÿæˆchunks
                original_context = record.get('original_context', '')
                company_name = record.get('company_name', 'å…¬å¸')
                
                if original_context:
                    # ä½¿ç”¨convert_json_context_to_natural_language_chunkså‡½æ•°
                    from xlm.utils.optimized_data_loader import convert_json_context_to_natural_language_chunks
                    chunks = convert_json_context_to_natural_language_chunks(original_context, company_name)
                    
                    if chunks:
                        self.doc_to_chunks_mapping[doc_idx] = chunks
                    else:
                        # å¦‚æœæ²¡æœ‰chunksï¼Œä½¿ç”¨summaryä½œä¸ºfallback
                        self.doc_to_chunks_mapping[doc_idx] = [record.get('summary', '')]
                else:
                    # å¦‚æœæ²¡æœ‰original_contextï¼Œä½¿ç”¨summary
                    self.doc_to_chunks_mapping[doc_idx] = [record.get('summary', '')]
            else:
                # è‹±æ–‡æ•°æ®ï¼Œä½¿ç”¨contextæˆ–content
                context = record.get('context', '') or record.get('content', '')
                self.doc_to_chunks_mapping[doc_idx] = [context]
        
        print(f"å»ºç«‹äº† {len(self.doc_to_chunks_mapping)} ä¸ªdoc_idåˆ°chunksçš„æ˜ å°„")
        
        # ç»Ÿè®¡chunksæ€»æ•°
        total_chunks = sum(len(chunks) for chunks in self.doc_to_chunks_mapping.values())
        print(f"æ€»å…±ç”Ÿæˆäº† {total_chunks} ä¸ªchunksç”¨äºé‡æ’åº")
        
        # ä½¿ç”¨åŸå§‹æ•°æ®ä½œä¸ºä¸»è¦æ•°æ®
        # self.data = self.original_data # original_data is removed
        
        print(f"æ•°æ®é›†ç±»å‹: {self.dataset_type}")
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        if self.data and isinstance(self.data[0], dict):
            sample_record = self.data[0]
            print(f"æ•°æ®å­—æ®µ: {list(sample_record.keys())}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å…ƒæ•°æ®å­—æ®µ
            has_metadata = any(field in sample_record for field in ['company_name', 'stock_code', 'report_date'])
            print(f"åŒ…å«å…ƒæ•°æ®å­—æ®µ: {has_metadata}")
    
    def _load_stock_company_mapping(self):
        """åŠ è½½è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„æ–‡ä»¶"""
        if self.dataset_type == "chinese":
            # å°è¯•ä»å¤šä¸ªè·¯å¾„åŠ è½½æ˜ å°„æ–‡ä»¶
            possible_paths = [
                Path("data/astock_code_company_name.csv"),
                Path(__file__).parent.parent / "data" / "astock_code_company_name.csv",
                Path(__file__).parent / "data" / "astock_code_company_name.csv"
            ]
            
            mapping_path = None
            for path in possible_paths:
                if path.exists():
                    mapping_path = path
                    break
            
            if mapping_path:
                try:
                    import pandas as pd
                    df = pd.read_csv(mapping_path, encoding='utf-8')
                    
                    # æ„å»ºåŒå‘æ˜ å°„
                    for _, row in df.iterrows():
                        stock_code = str(row['stock_code']).strip()
                        company_name = str(row['company_name']).strip()
                        
                        if stock_code and company_name:
                            # è‚¡ç¥¨ä»£ç  -> å…¬å¸åç§°
                            self.stock_company_mapping[stock_code] = company_name
                            # å…¬å¸åç§° -> è‚¡ç¥¨ä»£ç 
                            self.company_stock_mapping[company_name] = stock_code
                    
                    print(f"æˆåŠŸåŠ è½½è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„æ–‡ä»¶: {mapping_path}")
                    print(f"è‚¡ç¥¨ä»£ç æ˜ å°„æ•°é‡: {len(self.stock_company_mapping)}")
                    print(f"å…¬å¸åç§°æ˜ å°„æ•°é‡: {len(self.company_stock_mapping)}")
                    
                except Exception as e:
                    print(f"åŠ è½½è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„æ–‡ä»¶å¤±è´¥: {e}")
                    print(f"æ–‡ä»¶è·¯å¾„: {mapping_path}")
                    self.stock_company_mapping = {}
                    self.company_stock_mapping = {}
            else:
                print("è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨")
                print(f"å°è¯•çš„è·¯å¾„: {[str(p) for p in possible_paths]}")
                self.stock_company_mapping = {}
                self.company_stock_mapping = {}
        else:
            print("è‹±æ–‡æ•°æ®é›†ï¼Œä¸åŠ è½½è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„æ–‡ä»¶")
            self.stock_company_mapping = {}
            self.company_stock_mapping = {}

    def _build_metadata_index(self):
        """æ„å»ºå…ƒæ•°æ®ç´¢å¼•ç”¨äºpre-filteringï¼ˆä»…ä¸­æ–‡æ•°æ®ï¼‰"""
        if self.dataset_type != "chinese":
            print("éä¸­æ–‡æ•°æ®é›†ï¼Œè·³è¿‡å…ƒæ•°æ®ç´¢å¼•æ„å»º")
            return
            
        print("æ­£åœ¨æ„å»ºå…ƒæ•°æ®ç´¢å¼•...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…ƒæ•°æ®å­—æ®µ
        if not self.data:
            print("æ•°æ®æ ¼å¼ä¸æ”¯æŒå…ƒæ•°æ®ç´¢å¼•")
            return
            
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        if hasattr(self.data[0], 'content'):
            # DocumentWithMetadataæ ¼å¼
            print("ä½¿ç”¨DocumentWithMetadataæ ¼å¼ï¼Œè·³è¿‡å…ƒæ•°æ®ç´¢å¼•æ„å»º")
            print("æ³¨æ„ï¼šchunkçº§åˆ«çš„æ•°æ®ä¸æ”¯æŒå…ƒæ•°æ®é¢„è¿‡æ»¤")
            return
        elif isinstance(self.data[0], dict):
            # å­—å…¸æ ¼å¼
            sample_record = self.data[0]
            has_metadata = any(field in sample_record for field in ['company_name', 'stock_code', 'report_date'])
            
            if not has_metadata:
                print("æ•°æ®ä¸åŒ…å«å…ƒæ•°æ®å­—æ®µï¼Œè·³è¿‡å…ƒæ•°æ®ç´¢å¼•æ„å»º")
                return
            
            # æŒ‰å…¬å¸åç§°ç´¢å¼•
            self.metadata_index['company_name'] = defaultdict(list)
            # æŒ‰è‚¡ç¥¨ä»£ç ç´¢å¼•
            self.metadata_index['stock_code'] = defaultdict(list)
            # æŒ‰æŠ¥å‘Šæ—¥æœŸç´¢å¼•
            self.metadata_index['report_date'] = defaultdict(list)
            # æŒ‰å…¬å¸åç§°+è‚¡ç¥¨ä»£ç ç»„åˆç´¢å¼•
            self.metadata_index['company_stock'] = defaultdict(list)
            
            for idx, record in enumerate(self.data):
                # å…¬å¸åç§°ç´¢å¼•
                if record.get('company_name'):
                    company_name = record['company_name'].strip().lower()
                    self.metadata_index['company_name'][company_name].append(idx)
                
                # è‚¡ç¥¨ä»£ç ç´¢å¼•
                if record.get('stock_code'):
                    stock_code = str(record['stock_code']).strip().lower()
                    self.metadata_index['stock_code'][stock_code].append(idx)
                
                # æŠ¥å‘Šæ—¥æœŸç´¢å¼•
                if record.get('report_date'):
                    report_date = record['report_date'].strip()
                    self.metadata_index['report_date'][report_date].append(idx)
                
                # å…¬å¸åç§°+è‚¡ç¥¨ä»£ç ç»„åˆç´¢å¼•
                if record.get('company_name') and record.get('stock_code'):
                    company_name = record['company_name'].strip().lower()
                    stock_code = str(record['stock_code']).strip().lower()
                    key = f"{company_name}_{stock_code}"
                    self.metadata_index['company_stock'][key].append(idx)
            
            print(f"å…ƒæ•°æ®ç´¢å¼•æ„å»ºå®Œæˆ:")
            print(f"  - å…¬å¸åç§°: {len(self.metadata_index['company_name'])} ä¸ª")
            print(f"  - è‚¡ç¥¨ä»£ç : {len(self.metadata_index['stock_code'])} ä¸ª")
            print(f"  - æŠ¥å‘Šæ—¥æœŸ: {len(self.metadata_index['report_date'])} ä¸ª")
            print(f"  - å…¬å¸+è‚¡ç¥¨ç»„åˆ: {len(self.metadata_index['company_stock'])} ä¸ª")
    
    def _init_embedding_model(self):
        """åˆå§‹åŒ–å¥å­åµŒå…¥æ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
        print(f"æ¨¡å‹ç±»å‹: {'å¤šè¯­è¨€ç¼–ç å™¨' if self.dataset_type == 'chinese' else 'è‹±æ–‡ç¼–ç å™¨'}")
        
        # ä½¿ç”¨ç°æœ‰é…ç½®çš„ç¼“å­˜ç›®å½•
        cache_dir = None
        if self.config:
            cache_dir = self.config.encoder.cache_dir
            print(f"ä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•: {cache_dir}")
        
        try:
            # ä»é…ç½®ä¸­è·å–è®¾å¤‡è®¾ç½®
            device = "cuda:0"  # é»˜è®¤å€¼
            if self.config and hasattr(self.config, 'encoder'):
                device = self.config.encoder.device or "cuda:0"
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¾®è°ƒæ¨¡å‹è·¯å¾„
            if "finetuned" in self.model_name or "models/" in self.model_name:
                print(f"æ£€æµ‹åˆ°å¾®è°ƒæ¨¡å‹ï¼Œä½¿ç”¨FinbertEncoder...")
                from xlm.components.encoder.finbert import FinbertEncoder
                self.embedding_model = FinbertEncoder(
                    model_name=self.model_name,
                    cache_dir=cache_dir,
                    device=device
                )
                print(f"å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ ({device})")
            else:
                # ä½¿ç”¨SentenceTransformeråŠ è½½HuggingFaceæ¨¡å‹
                from sentence_transformers import SentenceTransformer
                self.embedding_model = SentenceTransformer(self.model_name, cache_folder=cache_dir)
                # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                if hasattr(self.embedding_model, 'to'):
                    self.embedding_model.to(device)
                print(f"HuggingFaceæ¨¡å‹åŠ è½½å®Œæˆ ({device})")
        except Exception as e:
            print(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("å°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹...")
            try:
                # ä»é…ç½®ä¸­è·å–è®¾å¤‡è®¾ç½®
                device = "cuda:0"  # é»˜è®¤å€¼
                if self.config and hasattr(self.config, 'encoder'):
                    device = self.config.encoder.device or "cuda:0"
                
                # å›é€€åˆ°é»˜è®¤æ¨¡å‹
                if self.dataset_type == "chinese":
                    fallback_model = "distiluse-base-multilingual-cased-v2"
                else:
                    fallback_model = "all-MiniLM-L6-v2"
                print(f"ä½¿ç”¨å›é€€æ¨¡å‹: {fallback_model}")
                from sentence_transformers import SentenceTransformer
                self.embedding_model = SentenceTransformer(fallback_model)
                # å°†æ¨¡å‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
                if hasattr(self.embedding_model, 'to'):
                    self.embedding_model.to(device)
                print(f"å›é€€æ¨¡å‹åŠ è½½æˆåŠŸ ({device})")
            except Exception as e2:
                print(f"å›é€€æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                self.embedding_model = None
    
    def _build_faiss_index(self):
        """æ„å»ºFAISSç´¢å¼•"""
        if self.embedding_model is None:
            print("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡FAISSç´¢å¼•æ„å»º")
            return
            
        print("æ­£åœ¨æ„å»ºFAISSç´¢å¼•...")
        print("ä¸­æ–‡æ•°æ®ï¼šä½¿ç”¨summaryå­—æ®µè¿›è¡Œå‘é‡ç¼–ç ")
        print("è‹±æ–‡æ•°æ®ï¼šä½¿ç”¨context/contentå­—æ®µè¿›è¡Œå‘é‡ç¼–ç ")
        
        # å‡†å¤‡ç”¨äºåµŒå…¥çš„æ–‡æœ¬
        texts_for_embedding = []
        valid_indices = []
        
        for idx, record in enumerate(self.data):
            # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©ä¸åŒçš„æ–‡æœ¬ç»„åˆç­–ç•¥
            if self.dataset_type == "chinese":
                # ä¸­æ–‡æ•°æ®ï¼šåªä½¿ç”¨summary
                summary = record.get('summary', '')
                
                if summary:
                    texts_for_embedding.append(summary)
                    valid_indices.append(idx)
                else:
                    continue
            else:
                # è‹±æ–‡æ•°æ®ï¼šä½¿ç”¨contextæˆ–contentå­—æ®µ
                context = record.get('context', '') or record.get('content', '')
                
                if context:
                    texts_for_embedding.append(context)
                    valid_indices.append(idx)
                else:
                    continue
        
        if not texts_for_embedding:
            print("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬ç”¨äºåµŒå…¥")
            return
        
        # ç”ŸæˆåµŒå…¥
        print(f"æ­£åœ¨ç¼–ç  {len(texts_for_embedding)} ä¸ªæ–‡æœ¬...")
        embeddings = self.embedding_model.encode(texts_for_embedding, show_progress_bar=True)
        
        # æ„å»ºFAISSç´¢å¼•
        dimension = embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        self.faiss_index.add(embeddings.astype('float32'))
        
        # ä¿å­˜æœ‰æ•ˆç´¢å¼•çš„æ˜ å°„
        self.valid_indices = valid_indices
        
        print(f"FAISSç´¢å¼•æ„å»ºå®Œæˆï¼Œç»´åº¦: {dimension}")
        print(f"æœ‰æ•ˆç´¢å¼•æ•°é‡: {len(self.valid_indices)}")
        print(f"åŸºäºsummaryæ„å»ºç´¢å¼•ï¼Œç”¨äºç²—ç²’åº¦æ£€ç´¢")
    
    def _init_qwen_reranker(self):
        """åˆå§‹åŒ–Qwen reranker"""
        print("æ­£åœ¨åˆå§‹åŒ–Qwen reranker...")
        try:
            # ä½¿ç”¨ç°æœ‰é…ç½®
            model_name = "Qwen/Qwen3-Reranker-0.6B"
            cache_dir = DEFAULT_CACHE_DIR  # ä½¿ç”¨DEFAULT_CACHE_DIR
            use_quantization = True
            quantization_type = "4bit"
            
            if self.config:
                model_name = self.config.reranker.model_name
                cache_dir = self.config.reranker.cache_dir or DEFAULT_CACHE_DIR  # ç¡®ä¿ä¸ä¸ºNone
                use_quantization = self.config.reranker.use_quantization
                quantization_type = self.config.reranker.quantization_type
            
            print(f"ä½¿ç”¨é…ç½®çš„é‡æ’åºå™¨: {model_name}")
            print(f"ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"é‡åŒ–: {use_quantization} ({quantization_type})")
            
            # ä»é…ç½®ä¸­è·å–è®¾å¤‡è®¾ç½®
            device = "cpu"  # é»˜è®¤ä½¿ç”¨CPU
            if self.config and hasattr(self.config, 'reranker'):
                device = self.config.reranker.device or "cpu"
            
            # ä½¿ç”¨ç°æœ‰çš„QwenReranker
            self.qwen_reranker = QwenReranker(
                model_name=model_name,
                device=device,
                cache_dir=cache_dir,
                use_quantization=use_quantization,
                quantization_type=quantization_type
            )
            print(f"Qwen rerankeråˆå§‹åŒ–å®Œæˆ ({device})")
        except Exception as e:
            print(f"Qwen rerankeråˆå§‹åŒ–å¤±è´¥: {e}")
            self.qwen_reranker = None
    
    def _init_llm_generator(self):
        """åˆå§‹åŒ–LLMç”Ÿæˆå™¨"""
        print("æ­£åœ¨åˆå§‹åŒ–LLMç”Ÿæˆå™¨...")
        try:
            # é‡ç”¨ç°æœ‰çš„LocalLLMGenerator
            from xlm.components.generator.local_llm_generator import LocalLLMGenerator
            
            # ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°
            model_name = None  # è®©LocalLLMGeneratorä»configè¯»å–
            cache_dir = None   # è®©LocalLLMGeneratorä»configè¯»å–
            device = None      # è®©LocalLLMGeneratorä»configè¯»å–
            use_quantization = None  # è®©LocalLLMGeneratorä»configè¯»å–
            quantization_type = None  # è®©LocalLLMGeneratorä»configè¯»å–
            
            if self.config:
                # å¦‚æœconfigä¸­æœ‰generatoré…ç½®ï¼Œä½¿ç”¨å®ƒ
                if hasattr(self.config, 'generator'):
                    model_name = self.config.generator.model_name
                    cache_dir = self.config.generator.cache_dir
                    device = self.config.generator.device
                    use_quantization = self.config.generator.use_quantization
                    quantization_type = self.config.generator.quantization_type
            
            # é¦–å…ˆå°è¯•GPUæ¨¡å¼
            try:
                print(f"å°è¯•GPUæ¨¡å¼åŠ è½½LLMç”Ÿæˆå™¨: {device}")
                self.llm_generator = LocalLLMGenerator(
                    model_name=model_name,
                    cache_dir=cache_dir,
                    device=device,
                    use_quantization=use_quantization,
                    quantization_type=quantization_type
                )
                print("âœ… LLMç”Ÿæˆå™¨GPUæ¨¡å¼åˆå§‹åŒ–å®Œæˆ")
            except Exception as gpu_error:
                print(f"âŒ GPUæ¨¡å¼åŠ è½½å¤±è´¥: {gpu_error}")
                print("å›é€€åˆ°CPUæ¨¡å¼...")
                
                # å›é€€åˆ°CPUæ¨¡å¼
                try:
                    self.llm_generator = LocalLLMGenerator(
                        model_name=model_name,
                        cache_dir=cache_dir,
                        device="cpu",  # å¼ºåˆ¶ä½¿ç”¨CPU
                        use_quantization=False,  # CPUæ¨¡å¼ä¸ä½¿ç”¨é‡åŒ–
                        quantization_type=None
                    )
                    print("âœ… LLMç”Ÿæˆå™¨CPUæ¨¡å¼åˆå§‹åŒ–å®Œæˆ")
                except Exception as cpu_error:
                    print(f"âŒ CPUæ¨¡å¼ä¹Ÿå¤±è´¥: {cpu_error}")
                    self.llm_generator = None
                    
        except Exception as e:
            print(f"LLMç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.llm_generator = None
    
    def pre_filter(self, 
                   company_name: Optional[str] = None,
                   stock_code: Optional[str] = None,
                   report_date: Optional[str] = None,
                   max_candidates: int = 1000) -> List[int]:
        """
        åŸºäºå…ƒæ•°æ®è¿›è¡Œé¢„è¿‡æ»¤ï¼ˆä»…ä¸­æ–‡æ•°æ®æ”¯æŒï¼‰
        å½“ä½¿ç”¨é¢„è¿‡æ»¤æ—¶ï¼Œè‡ªåŠ¨å¯ç”¨è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„ä»¥æé«˜åŒ¹é…å‡†ç¡®æ€§
        
        Args:
            company_name: å…¬å¸åç§°
            stock_code: è‚¡ç¥¨ä»£ç 
            report_date: æŠ¥å‘Šæ—¥æœŸ
            max_candidates: æœ€å¤§å€™é€‰æ•°é‡
            
        Returns:
            å€™é€‰è®°å½•ç´¢å¼•åˆ—è¡¨
        """
        if self.dataset_type != "chinese":
            print("éä¸­æ–‡æ•°æ®é›†ï¼Œè·³è¿‡é¢„è¿‡æ»¤")
            return list(range(len(self.data)))
        
        print("å¼€å§‹å…ƒæ•°æ®é¢„è¿‡æ»¤...")
        print("è‡ªåŠ¨å¯ç”¨è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„ä»¥æé«˜åŒ¹é…å‡†ç¡®æ€§")
        
        # å¦‚æœæ²¡æœ‰æä¾›ä»»ä½•è¿‡æ»¤æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰è®°å½•
        if not any([company_name, stock_code, report_date]):
            print("æ— è¿‡æ»¤æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰è®°å½•")
            return list(range(len(self.data)))
        
        # ä½¿ç”¨è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„è¿›è¡Œå¢å¼ºåŒ¹é…
        enhanced_company_name = company_name
        enhanced_stock_code = stock_code
        
        # è‡ªåŠ¨å¯ç”¨æ˜ å°„åŠŸèƒ½æ¥æé«˜åŒ¹é…å‡†ç¡®æ€§
        # å¦‚æœæä¾›äº†è‚¡ç¥¨ä»£ç ï¼Œå°è¯•è·å–å¯¹åº”çš„å…¬å¸åç§°
        if stock_code and not company_name:
            mapped_company = self.stock_company_mapping.get(stock_code)
            if mapped_company:
                enhanced_company_name = mapped_company
                print(f"é€šè¿‡è‚¡ç¥¨ä»£ç æ˜ å°„æ‰¾åˆ°å…¬å¸åç§°: {stock_code} -> {mapped_company}")
        
        # å¦‚æœæä¾›äº†å…¬å¸åç§°ï¼Œå°è¯•è·å–å¯¹åº”çš„è‚¡ç¥¨ä»£ç 
        if company_name and not stock_code:
            mapped_stock = self.company_stock_mapping.get(company_name)
            if mapped_stock:
                enhanced_stock_code = mapped_stock
                print(f"é€šè¿‡å…¬å¸åç§°æ˜ å°„æ‰¾åˆ°è‚¡ç¥¨ä»£ç : {company_name} -> {mapped_stock}")
        
        # ä¼˜å…ˆä½¿ç”¨ç»„åˆç´¢å¼•ï¼ˆå…¬å¸åç§°+è‚¡ç¥¨ä»£ç ï¼‰
        if enhanced_company_name and enhanced_stock_code:
            company_name_lower = enhanced_company_name.strip().lower()
            stock_code_lower = str(enhanced_stock_code).strip().lower()
            key = f"{company_name_lower}_{stock_code_lower}"
            
            if key in self.metadata_index['company_stock']:
                indices = self.metadata_index['company_stock'][key]
                print(f"ç»„åˆè¿‡æ»¤: å…¬å¸'{enhanced_company_name}' + è‚¡ç¥¨'{enhanced_stock_code}' åŒ¹é… {len(indices)} æ¡è®°å½•")
                return indices[:max_candidates]
            else:
                print(f"ç»„åˆè¿‡æ»¤: å…¬å¸'{enhanced_company_name}' + è‚¡ç¥¨'{enhanced_stock_code}' æ— åŒ¹é…è®°å½•")
                # å¦‚æœç»„åˆåŒ¹é…å¤±è´¥ï¼Œå°è¯•å•ç‹¬åŒ¹é…
                return self._fallback_filter(enhanced_company_name, enhanced_stock_code, report_date, max_candidates)
        
        # å¦‚æœåªæä¾›äº†å…¬å¸åç§°
        elif enhanced_company_name:
            company_name_lower = enhanced_company_name.strip().lower()
            if company_name_lower in self.metadata_index['company_name']:
                indices = self.metadata_index['company_name'][company_name_lower]
                print(f"å…¬å¸åç§°è¿‡æ»¤: '{enhanced_company_name}' åŒ¹é… {len(indices)} æ¡è®°å½•")
                return indices[:max_candidates]
            else:
                print(f"å…¬å¸åç§°è¿‡æ»¤: '{enhanced_company_name}' æ— åŒ¹é…è®°å½•")
                # å°è¯•æ¨¡ç³ŠåŒ¹é…
                return self._fuzzy_company_match(enhanced_company_name, max_candidates)
        
        # å¦‚æœåªæä¾›äº†è‚¡ç¥¨ä»£ç 
        elif enhanced_stock_code:
            stock_code_lower = str(enhanced_stock_code).strip().lower()
            if stock_code_lower in self.metadata_index['stock_code']:
                indices = self.metadata_index['stock_code'][stock_code_lower]
                print(f"è‚¡ç¥¨ä»£ç è¿‡æ»¤: '{enhanced_stock_code}' åŒ¹é… {len(indices)} æ¡è®°å½•")
                return indices[:max_candidates]
            else:
                print(f"è‚¡ç¥¨ä»£ç è¿‡æ»¤: '{enhanced_stock_code}' æ— åŒ¹é…è®°å½•")
                return []
        
        # å¦‚æœåªæä¾›äº†æŠ¥å‘Šæ—¥æœŸ
        elif report_date:
            report_date_str = report_date.strip()
            if report_date_str in self.metadata_index['report_date']:
                indices = self.metadata_index['report_date'][report_date_str]
                print(f"æŠ¥å‘Šæ—¥æœŸè¿‡æ»¤: '{report_date}' åŒ¹é… {len(indices)} æ¡è®°å½•")
                return indices[:max_candidates]
            else:
                print(f"æŠ¥å‘Šæ—¥æœŸè¿‡æ»¤: '{report_date}' æ— åŒ¹é…è®°å½•")
                return []
        
        print("é¢„è¿‡æ»¤å®Œæˆï¼Œå€™é€‰æ–‡æ¡£æ•°: 0")
        return []
    
    def _fallback_filter(self, company_name: Optional[str], stock_code: Optional[str], 
                        report_date: Optional[str], max_candidates: int) -> List[int]:
        """ç»„åˆåŒ¹é…å¤±è´¥æ—¶çš„å›é€€ç­–ç•¥"""
        print("ç»„åˆåŒ¹é…å¤±è´¥ï¼Œå°è¯•å•ç‹¬åŒ¹é…...")
        
        all_indices = set()
        
        # å°è¯•å…¬å¸åç§°åŒ¹é…
        if company_name:
            company_name_lower = company_name.strip().lower()
            if company_name_lower in self.metadata_index['company_name']:
                indices = self.metadata_index['company_name'][company_name_lower]
                all_indices.update(indices)
                print(f"å›é€€å…¬å¸åç§°åŒ¹é…: {len(indices)} æ¡è®°å½•")
        
        # å°è¯•è‚¡ç¥¨ä»£ç åŒ¹é…
        if stock_code:
            stock_code_lower = str(stock_code).strip().lower()
            if stock_code_lower in self.metadata_index['stock_code']:
                indices = self.metadata_index['stock_code'][stock_code_lower]
                all_indices.update(indices)
                print(f"å›é€€è‚¡ç¥¨ä»£ç åŒ¹é…: {len(indices)} æ¡è®°å½•")
        
        # å°è¯•æŠ¥å‘Šæ—¥æœŸåŒ¹é…
        if report_date:
            report_date_str = report_date.strip()
            if report_date_str in self.metadata_index['report_date']:
                indices = self.metadata_index['report_date'][report_date_str]
                all_indices.update(indices)
                print(f"å›é€€æŠ¥å‘Šæ—¥æœŸåŒ¹é…: {len(indices)} æ¡è®°å½•")
        
        result = list(all_indices)[:max_candidates]
        print(f"å›é€€ç­–ç•¥æ€»åŒ¹é…: {len(result)} æ¡è®°å½•")
        return result
    
    def _fuzzy_company_match(self, company_name: str, max_candidates: int) -> List[int]:
        """æ¨¡ç³Šå…¬å¸åç§°åŒ¹é…"""
        print(f"å°è¯•æ¨¡ç³ŠåŒ¹é…å…¬å¸åç§°: {company_name}")
        
        company_name_lower = company_name.strip().lower()
        all_indices = set()
        
        # åœ¨å…ƒæ•°æ®ç´¢å¼•ä¸­æŸ¥æ‰¾åŒ…å«è¯¥å…¬å¸åç§°çš„è®°å½•
        for indexed_name, indices in self.metadata_index['company_name'].items():
            if (company_name_lower in indexed_name or 
                indexed_name in company_name_lower or
                any(word in indexed_name for word in company_name_lower.split())):
                all_indices.update(indices)
                print(f"æ¨¡ç³ŠåŒ¹é…: '{indexed_name}' -> {len(indices)} æ¡è®°å½•")
        
        result = list(all_indices)[:max_candidates]
        print(f"æ¨¡ç³ŠåŒ¹é…æ€»ç»“æœ: {len(result)} æ¡è®°å½•")
        return result
    
    def faiss_search(self, query: str, candidate_indices: List[int], top_k: int = 100) -> List[Tuple[int, float]]:
        """
        ä½¿ç”¨FAISSè¿›è¡Œå‘é‡æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidate_indices: å€™é€‰è®°å½•ç´¢å¼•
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            (ç´¢å¼•, ç›¸ä¼¼åº¦åˆ†æ•°) çš„åˆ—è¡¨
        """
        if self.faiss_index is None:
            print("FAISSç´¢å¼•æœªåˆå§‹åŒ–")
            return []
        
        print(f"å¼€å§‹FAISSæ£€ç´¢ï¼Œå€™é€‰æ–‡æ¡£æ•°: {len(candidate_indices)}")
        
        # å¦‚æœæ²¡æœ‰å€™é€‰æ–‡æ¡£ï¼Œç›´æ¥è¿”å›ç©ºç»“æœ
        if not candidate_indices:
            print("æ²¡æœ‰å€™é€‰æ–‡æ¡£ï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        try:
            query_embedding = self.embedding_model.encode([query])
            print(f"æŸ¥è¯¢åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œç»´åº¦: {query_embedding.shape}")
        except Exception as e:
            print(f"æŸ¥è¯¢åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            return []
        
        # ä½¿ç”¨ç°æœ‰FAISSç´¢å¼•è¿›è¡Œé«˜æ•ˆæœç´¢
        print("ä½¿ç”¨ç°æœ‰FAISSç´¢å¼•è¿›è¡Œé«˜æ•ˆæœç´¢")
        
        try:
            # åœ¨å®Œæ•´FAISSç´¢å¼•ä¸Šæœç´¢ï¼Œç„¶åè¿‡æ»¤å€™é€‰æ–‡æ¡£
            search_k = min(top_k * 5, len(self.valid_indices))  # æœç´¢æ›´å¤šä»¥ç¡®ä¿è¦†ç›–å€™é€‰æ–‡æ¡£
            scores, indices = self.faiss_index.search(query_embedding.astype('float32'), search_k)
            
            # å°†FAISSç´¢å¼•æ˜ å°„å›åŸå§‹æ•°æ®ç´¢å¼•ï¼Œå¹¶é™åˆ¶åœ¨å€™é€‰æ–‡æ¡£èŒƒå›´å†…
            results = []
            candidate_set = set(candidate_indices)
            
            for faiss_idx, score in zip(indices[0], scores[0]):
                if faiss_idx < len(self.valid_indices):
                    original_idx = self.valid_indices[faiss_idx]
                    # æ£€æŸ¥æ˜¯å¦åœ¨å€™é€‰åˆ—è¡¨ä¸­
                    if original_idx in candidate_set:
                        results.append((original_idx, float(score)))
                        # å¦‚æœå·²ç»æ‰¾åˆ°è¶³å¤Ÿçš„å€™é€‰ï¼Œæå‰ç»“æŸ
                        if len(results) >= top_k:
                            break
            
            print(f"FAISSæ£€ç´¢å®Œæˆï¼Œæœ‰æ•ˆç»“æœ: {len(results)} æ¡è®°å½•")
            return results
            
        except Exception as e:
            print(f"FAISSæœç´¢å¤±è´¥: {e}")
            return []
    
    def rerank(self, 
               query: str, 
               candidate_results: List[Tuple[int, float]], 
               top_k: int = 10) -> List[Tuple[int, float, float]]:  # æ”¹ä¸º10ï¼Œä¸é…ç½®æ–‡ä»¶ä¸€è‡´
        """
        ä½¿ç”¨Qwené‡æ’åºå™¨å¯¹å€™é€‰ç»“æœè¿›è¡Œé‡æ’åº
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidate_results: å€™é€‰ç»“æœåˆ—è¡¨ [(doc_idx, faiss_score), ...]
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            é‡æ’åºåçš„ç»“æœåˆ—è¡¨ [(doc_idx, faiss_score, reranker_score), ...]
        """
        if not self.qwen_reranker or not candidate_results:
            print("é‡æ’åºå™¨ä¸å¯ç”¨æˆ–æ²¡æœ‰å€™é€‰ç»“æœ")
            return [(idx, score, 0.0) for idx, score in candidate_results[:top_k]]
        
        print(f"å¼€å§‹é‡æ’åº {len(candidate_results)} æ¡å€™é€‰ç»“æœ...")
        
        # å‡†å¤‡é‡æ’åºçš„æ–‡æ¡£ - ä½¿ç”¨doc_idåˆ°chunksçš„æ˜ å°„
        docs_for_rerank = []
        doc_to_rerank_mapping = []
        
        for doc_idx, faiss_score in candidate_results:
            if doc_idx in self.doc_to_chunks_mapping:
                chunks = self.doc_to_chunks_mapping[doc_idx]
                for chunk in chunks:
                    if chunk.strip():  # è·³è¿‡ç©ºchunk
                        docs_for_rerank.append(chunk)
                        doc_to_rerank_mapping.append((doc_idx, faiss_score))
            else:
                # å¦‚æœæ‰¾ä¸åˆ°æ˜ å°„ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
                if doc_idx < len(self.data):
                    record = self.data[doc_idx]
                    if self.dataset_type == "chinese":
                        content = record.get('summary', '')
                    else:
                        content = record.get('context', '') or record.get('content', '')
                    if content.strip():
                        docs_for_rerank.append(content)
                        doc_to_rerank_mapping.append((doc_idx, faiss_score))
        
        print(f"å‡†å¤‡é‡æ’åº {len(docs_for_rerank)} ä¸ªchunks...")
        
        if not docs_for_rerank:
            print("æ²¡æœ‰å¯é‡æ’åºçš„æ–‡æ¡£")
            return [(idx, score, 0.0) for idx, score in candidate_results[:top_k]]
        
        # ä½¿ç”¨Qwené‡æ’åºå™¨è¿›è¡Œé‡æ’åº
        try:
            reranked_results = self.qwen_reranker.rerank(query, docs_for_rerank, batch_size=1)  # å‡å°åˆ°1ä»¥é¿å…GPUå†…å­˜ä¸è¶³
            print(f"é‡æ’åºå™¨å¤„ç†å®Œæˆï¼Œè¿”å› {len(reranked_results)} ä¸ªç»“æœ")
        except Exception as e:
            print(f"é‡æ’åºå¤±è´¥: {e}")
            # å›é€€åˆ°åŸå§‹ç»“æœ
            return [(idx, score, 0.0) for idx, score in candidate_results[:top_k]]
        
        # å°†é‡æ’åºç»“æœæ˜ å°„å›åŸå§‹æ–‡æ¡£ç´¢å¼•
        final_results = []
        for doc_text, reranker_score in reranked_results:
            # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ–‡æ¡£ç´¢å¼•
            for i, (doc_idx, faiss_score) in enumerate(doc_to_rerank_mapping):
                if i < len(docs_for_rerank) and docs_for_rerank[i] == doc_text:
                    # ç»„åˆåˆ†æ•°ï¼šFAISSåˆ†æ•° + é‡æ’åºåˆ†æ•°
                    combined_score = faiss_score + reranker_score
                    final_results.append((doc_idx, faiss_score, combined_score))
                    break
        
        # æŒ‰ç»„åˆåˆ†æ•°æ’åº
        final_results.sort(key=lambda x: x[2], reverse=True)
        
        print(f"é‡æ’åºå®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
        return final_results[:top_k]
    
    def generate_answer(self, query: str, candidate_results: List[Tuple[int, float, float]], top_k_for_context: int = 5) -> str:
        """
        ç”ŸæˆLLMç­”æ¡ˆ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–ï¼Œå¤§å¹…ç¼©çŸ­ä¼ é€’ç»™LLMçš„ä¸Šä¸‹æ–‡
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidate_results: å€™é€‰ç»“æœåˆ—è¡¨ [(doc_idx, faiss_score, reranker_score), ...]
            top_k_for_context: ç”¨äºç”Ÿæˆä¸Šä¸‹æ–‡çš„å€™é€‰æ•°é‡
            
        Returns:
            ç”Ÿæˆçš„LLMç­”æ¡ˆ
        """
        if not candidate_results:
            print("æ²¡æœ‰å€™é€‰ç»“æœï¼Œæ— æ³•ç”Ÿæˆç­”æ¡ˆ")
            return ""
        
        print(f"å¼€å§‹ç”ŸæˆLLMç­”æ¡ˆ...")
        print(f"åŸå§‹æŸ¥è¯¢: '{query}'")
        print(f"æŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")
        
        # ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–ï¼Œé™åˆ¶åœ¨2000å­—ç¬¦ä»¥å†…
        context = self.extract_relevant_context(query, candidate_results, max_chars=2000)
        
        print(f"æ™ºèƒ½æå–çš„ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} ä¸ªå­—ç¬¦")
        
        # ä½¿ç”¨LLMç”Ÿæˆå™¨ç”Ÿæˆç­”æ¡ˆ
        if self.llm_generator:
            try:
                # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©promptæ¨¡æ¿
                if self.dataset_type == "chinese":
                    # ä¸­æ–‡promptæ¨¡æ¿
                    # è·å–Top1æ–‡æ¡£çš„summary
                    if candidate_results and candidate_results[0][0] < len(self.data):
                        top1_record = self.data[candidate_results[0][0]]
                        summary = top1_record.get('summary', '')
                        if not summary:
                            # å¦‚æœæ²¡æœ‰summaryå­—æ®µï¼Œä½¿ç”¨contextå‰200å­—ç¬¦
                            summary = context[:200] + "..." if len(context) > 200 else context
                    else:
                        summary = context[:200] + "..." if len(context) > 200 else context
                    
                    prompt = template_loader.format_template(
                        "multi_stage_chinese_template",
                        context=context, 
                        query=query,
                        summary=summary
                    )
                else:
                    # è‹±æ–‡promptæ¨¡æ¿
                    prompt = template_loader.format_template(
                        "multi_stage_english_template",
                        context=context, 
                        query=query
                    )
                
                if prompt is None:
                    # å›é€€åˆ°ç®€å•prompt
                    if self.dataset_type == "chinese":
                        prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context}\n\né—®é¢˜ï¼š{query}\n\nå›ç­”ï¼š"
                    else:
                        prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"
                
                # ===== è¯¦ç»†çš„Promptè°ƒè¯•ä¿¡æ¯ =====
                print("\n" + "="*80)
                print("ğŸ” PROMPTè°ƒè¯•ä¿¡æ¯")
                print("="*80)
                print(f"ğŸ“ æ¨¡æ¿åç§°: {'multi_stage_chinese_template' if self.dataset_type == 'chinese' else 'multi_stage_english_template'}")
                print(f"ğŸ“ å®Œæ•´Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
                print(f"ğŸ“‹ åŸå§‹æŸ¥è¯¢: '{query}'")
                print(f"ğŸ“‹ æŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")
                print(f"ğŸ“„ ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
                print(f"ğŸ“„ ä¸Šä¸‹æ–‡å‰200å­—ç¬¦: '{context[:200]}...'")
                print(f"ğŸ“„ ä¸Šä¸‹æ–‡å200å­—ç¬¦: '...{context[-200:]}'")
                
                # æ£€æŸ¥Promptæ˜¯å¦è¢«æˆªæ–­
                if len(prompt) > 10000:
                    print("âš ï¸  WARNING: Prompté•¿åº¦è¶…è¿‡10000å­—ç¬¦ï¼Œå¯èƒ½è¢«æˆªæ–­")
                else:
                    print("âœ… Prompté•¿åº¦æ­£å¸¸")
                
                # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åœ¨Promptä¸­
                if query in prompt:
                    print("âœ… æŸ¥è¯¢æ­£ç¡®åŒ…å«åœ¨Promptä¸­")
                else:
                    print("âŒ æŸ¥è¯¢æœªåœ¨Promptä¸­æ‰¾åˆ°ï¼")
                    print(f"   æœŸæœ›çš„æŸ¥è¯¢: '{query}'")
                    print(f"   Promptä¸­çš„æŸ¥è¯¢éƒ¨åˆ†: '{prompt.split('é—®é¢˜ï¼š')[-1].split('å›ç­”ï¼š')[0] if 'é—®é¢˜ï¼š' in prompt else 'NOT_FOUND'}'")
                
                # æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦åœ¨Promptä¸­
                if context[:100] in prompt:
                    print("âœ… ä¸Šä¸‹æ–‡æ­£ç¡®åŒ…å«åœ¨Promptä¸­")
                else:
                    print("âŒ ä¸Šä¸‹æ–‡æœªåœ¨Promptä¸­æ‰¾åˆ°ï¼")
                
                print("\n" + "="*80)
                print("ğŸ“¤ å‘é€ç»™LLMçš„å®Œæ•´Prompt:")
                print("="*80)
                print(prompt)
                print("="*80)
                print("ğŸ“¤ Promptç»“æŸ")
                print("="*80 + "\n")
                
                # ç”Ÿæˆç­”æ¡ˆ
                answer = self.llm_generator.generate(texts=[prompt])[0]
                
                # ===== ç­”æ¡ˆè°ƒè¯•ä¿¡æ¯ =====
                print("\n" + "="*80)
                print("ğŸ“¥ LLMç”Ÿæˆçš„ç­”æ¡ˆ:")
                print("="*80)
                print(answer)
                print("="*80)
                print("ğŸ“¥ ç­”æ¡ˆç»“æŸ")
                print("="*80 + "\n")
                
                return answer
            except Exception as e:
                print(f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}")
                return "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ã€‚"
        else:
            return "æœªé…ç½®LLMç”Ÿæˆå™¨ã€‚"
    
    def search(self, 
               query: str,
               company_name: Optional[str] = None,
               stock_code: Optional[str] = None,
               report_date: Optional[str] = None,
               top_k: int = 10,
               use_prefilter: bool = True) -> Dict:  # ç§»é™¤æ˜ å°„å¼€å…³å‚æ•°
        """
        å®Œæ•´çš„å¤šé˜¶æ®µæ£€ç´¢æµç¨‹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            company_name: å…¬å¸åç§°ï¼ˆå¯é€‰ï¼Œä»…ä¸­æ–‡æ•°æ®ï¼‰
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¯é€‰ï¼Œä»…ä¸­æ–‡æ•°æ®ï¼‰
            report_date: æŠ¥å‘Šæ—¥æœŸï¼ˆå¯é€‰ï¼Œä»…ä¸­æ–‡æ•°æ®ï¼‰
            top_k: è¿”å›å‰kä¸ªç»“æœ
            use_prefilter: æ˜¯å¦ä½¿ç”¨é¢„è¿‡æ»¤ï¼ˆé»˜è®¤Trueï¼Œä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨å¯ç”¨æ˜ å°„åŠŸèƒ½ï¼‰
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        print(f"\nå¼€å§‹å¤šé˜¶æ®µæ£€ç´¢...")
        print(f"æŸ¥è¯¢: {query}")
        print(f"æ•°æ®é›†ç±»å‹: {self.dataset_type}")
        print(f"é¢„è¿‡æ»¤å¼€å…³: {'å¼€å¯' if use_prefilter else 'å…³é—­'}")
        if use_prefilter:
            print("é¢„è¿‡æ»¤æ¨¡å¼ä¸‹è‡ªåŠ¨å¯ç”¨è‚¡ç¥¨ä»£ç å’Œå…¬å¸åç§°æ˜ å°„")
        
        if self.dataset_type == "chinese":
            if company_name:
                print(f"å…¬å¸åç§°: {company_name}")
            if stock_code:
                print(f"è‚¡ç¥¨ä»£ç : {stock_code}")
            if report_date:
                print(f"æŠ¥å‘Šæ—¥æœŸ: {report_date}")
        else:
            print("è‹±æ–‡æ•°æ®é›†ï¼Œä¸æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤")
        
        # ä½¿ç”¨é…ç½®çš„æ£€ç´¢å‚æ•°
        retrieval_top_k = 100
        rerank_top_k = top_k
        
        if self.config:
            retrieval_top_k = self.config.retriever.retrieval_top_k
            rerank_top_k = self.config.retriever.rerank_top_k
        
        # 1. Pre-filteringï¼ˆæ ¹æ®å¼€å…³å†³å®šæ˜¯å¦ä½¿ç”¨ï¼‰
        if use_prefilter and self.dataset_type == "chinese":
            print("ç¬¬ä¸€æ­¥ï¼šå¯ç”¨å…ƒæ•°æ®é¢„è¿‡æ»¤...")
            candidate_indices = self.pre_filter(company_name, stock_code, report_date) # é¢„è¿‡æ»¤æ—¶è‡ªåŠ¨å¯ç”¨æ˜ å°„
            print(f"é¢„è¿‡æ»¤ç»“æœ: {len(candidate_indices)} ä¸ªå€™é€‰æ–‡æ¡£")
            
            # å¦‚æœé¢„è¿‡æ»¤æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£ï¼Œå›é€€åˆ°å…¨é‡FAISSæ£€ç´¢
            if len(candidate_indices) == 0:
                print("é¢„è¿‡æ»¤æ— ç»“æœï¼Œå›é€€åˆ°å…¨é‡FAISSæ£€ç´¢...")
                candidate_indices = list(range(len(self.data)))
                print(f"å›é€€åˆ°å…¨é‡æ£€ç´¢ï¼Œå€™é€‰æ–‡æ¡£æ•°: {len(candidate_indices)}")
        else:
            print("ç¬¬ä¸€æ­¥ï¼šè·³è¿‡å…ƒæ•°æ®é¢„è¿‡æ»¤ï¼Œä½¿ç”¨å…¨é‡æ£€ç´¢...")
            candidate_indices = list(range(len(self.data)))
            print(f"å…¨é‡æ£€ç´¢ï¼Œå€™é€‰æ–‡æ¡£æ•°: {len(candidate_indices)}")
        
        # 2. FAISSæ£€ç´¢ - åŸºäºé¢„è¿‡æ»¤ç»“æœï¼Œä½†ç¡®ä¿æ£€ç´¢åˆ°é…ç½®çš„å€™é€‰æ•°é‡
        print("ç¬¬äºŒæ­¥ï¼šåŸºäºå€™é€‰æ–‡æ¡£è¿›è¡ŒFAISSæ£€ç´¢...")
        # å¦‚æœå€™é€‰ç»“æœå°‘äºé…ç½®çš„æ£€ç´¢æ•°é‡ï¼Œä½¿ç”¨å€™é€‰ç»“æœï¼›å¦åˆ™ä½¿ç”¨é…ç½®çš„æ£€ç´¢æ•°é‡
        actual_top_k = min(retrieval_top_k, len(candidate_indices))
        faiss_results = self.faiss_search(query, candidate_indices, top_k=actual_top_k)
        print(f"FAISSæ£€ç´¢ç»“æœ: {len(faiss_results)} ä¸ªæ–‡æ¡£")
        final_faiss_results = faiss_results
        
        # 3. Qwen Reranker
        print("ç¬¬ä¸‰æ­¥ï¼šå¼€å§‹é‡æ’åº...")
        final_results = self.rerank(query, final_faiss_results, top_k=rerank_top_k)
        print(f"é‡æ’åºå®Œæˆ: {len(final_results)} ä¸ªchunks")
        print("é‡æ’åºå™¨å¤„ç†å®Œæˆ")
        
        # 4. LLMç­”æ¡ˆç”Ÿæˆ - å°†é‡æ’åºåçš„Top-K1ä¸ªchunksæ‹¼æ¥ä½œä¸ºä¸Šä¸‹æ–‡
        llm_answer = self.generate_answer(query, final_results, top_k_for_context=5)
        
        # 5. æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for idx, faiss_score, combined_score in final_results:
            record = self.data[idx]
            
            # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©ä¸åŒçš„å­—æ®µ
            if hasattr(record, 'content'):
                # DocumentWithMetadataæ ¼å¼
                result = {
                    'index': idx,
                    'faiss_score': faiss_score,
                    'combined_score': combined_score,
                    'content': record.content[:200] + '...' if len(record.content) > 200 else record.content,
                    'source': record.metadata.source if hasattr(record.metadata, 'source') else 'unknown',
                    'language': record.metadata.language if hasattr(record.metadata, 'language') else 'unknown'
                }
            else:
                # å­—å…¸æ ¼å¼
                if self.dataset_type == "chinese":
                    # ä¸­æ–‡æ•°æ®ï¼šä½¿ç”¨original_context
                    context = record.get('original_context', '')
                    result = {
                        'index': idx,
                        'faiss_score': faiss_score,
                        'combined_score': combined_score,
                        'context': context[:200] + '...' if len(context) > 200 else context,
                        'company_name': record.get('company_name', ''),
                        'stock_code': record.get('stock_code', ''),
                        'report_date': record.get('report_date', ''),
                        'summary': record.get('summary', '')[:200] + '...' if len(record.get('summary', '')) > 200 else record.get('summary', ''),
                        'generated_question': record.get('generated_question', ''),
                        'original_question': record.get('original_question', ''),
                        'original_answer': record.get('original_answer', '')
                    }
                else:
                    # è‹±æ–‡æ•°æ®ï¼šä½¿ç”¨context
                    context = record.get('context', '') or record.get('content', '')
                    result = {
                        'index': idx,
                        'faiss_score': faiss_score,
                        'combined_score': combined_score,
                        'context': context[:200] + '...' if len(context) > 200 else context,
                        'question': record.get('question', ''),
                        'answer': record.get('answer', '')
                    }
            
            formatted_results.append(result)
        
        # æ·»åŠ LLMç”Ÿæˆçš„ç­”æ¡ˆåˆ°ç»“æœä¸­
        final_output = {
            'retrieved_documents': formatted_results,
            'llm_answer': llm_answer,
            'query': query,
            'total_documents': len(formatted_results),
            'use_prefilter': use_prefilter  # æ·»åŠ é¢„è¿‡æ»¤çŠ¶æ€åˆ°è¾“å‡º
        }
        
        print(f"æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(formatted_results)} æ¡ç»“æœ")
        print(f"LLMç­”æ¡ˆç”Ÿæˆå®Œæˆ")
        return final_output
    
    def save_index(self, output_dir: Path):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        if self.faiss_index:
            faiss.write_index(self.faiss_index, str(output_dir / "faiss_index.bin"))
        
        # ä¿å­˜å…ƒæ•°æ®ç´¢å¼•ï¼ˆä»…ä¸­æ–‡æ•°æ®ï¼‰
        if self.dataset_type == "chinese":
            with open(output_dir / "metadata_index.pkl", 'wb') as f:
                pickle.dump(self.metadata_index, f)
        
        # ä¿å­˜æœ‰æ•ˆç´¢å¼•æ˜ å°„
        with open(output_dir / "valid_indices.pkl", 'wb') as f:
            pickle.dump(self.valid_indices, f)
        
        # ä¿å­˜æ•°æ®é›†ç±»å‹ä¿¡æ¯
        with open(output_dir / "dataset_info.json", 'w') as f:
            json.dump({
                'dataset_type': self.dataset_type,
                'model_name': self.model_name
            }, f, indent=2)
        
        print(f"ç´¢å¼•å·²ä¿å­˜åˆ°: {output_dir}")
    
    def load_index(self, index_dir: Path):
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        # åŠ è½½æ•°æ®é›†ä¿¡æ¯
        info_path = index_dir / "dataset_info.json"
        if info_path.exists():
            with open(info_path, 'r') as f:
                info = json.load(f)
                self.dataset_type = info.get('dataset_type', 'chinese')
                self.model_name = info.get('model_name', 'all-MiniLM-L6-v2')
        
        # åŠ è½½FAISSç´¢å¼•
        faiss_path = index_dir / "faiss_index.bin"
        if faiss_path.exists():
            self.faiss_index = faiss.read_index(str(faiss_path))
        
        # åŠ è½½å…ƒæ•°æ®ç´¢å¼•ï¼ˆä»…ä¸­æ–‡æ•°æ®ï¼‰
        if self.dataset_type == "chinese":
            metadata_path = index_dir / "metadata_index.pkl"
            if metadata_path.exists():
                with open(metadata_path, 'rb') as f:
                    self.metadata_index = pickle.load(f)
        
        # åŠ è½½æœ‰æ•ˆç´¢å¼•æ˜ å°„
        valid_indices_path = index_dir / "valid_indices.pkl"
        if valid_indices_path.exists():
            with open(valid_indices_path, 'rb') as f:
                self.valid_indices = pickle.load(f)
        
        print(f"ç´¢å¼•å·²ä» {index_dir} åŠ è½½")
        print(f"æ•°æ®é›†ç±»å‹: {self.dataset_type}")

    def extract_relevant_context(self, query: str, candidate_results: List[Tuple[int, float, float]], max_chars: int = 2000) -> str:
        """
        å¯¹Top1æ–‡æ¡£æ™ºèƒ½æå–ç›¸å…³ä¸Šä¸‹æ–‡
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidate_results: å€™é€‰ç»“æœåˆ—è¡¨
            max_chars: æœ€å¤§å­—ç¬¦æ•°é™åˆ¶
            
        Returns:
            Top1æ–‡æ¡£æ™ºèƒ½æå–çš„ç›¸å…³ä¸Šä¸‹æ–‡
        """
        print(f"ğŸ” å¼€å§‹å¯¹Top1æ–‡æ¡£æ™ºèƒ½æå–ç›¸å…³ä¸Šä¸‹æ–‡...")
        print(f"ğŸ“‹ æŸ¥è¯¢: {query}")
        print(f"ğŸ“Š å€™é€‰æ–‡æ¡£æ•°: {len(candidate_results)}")
        
        if not candidate_results:
            print("âŒ æ²¡æœ‰å€™é€‰ç»“æœ")
            return ""
        
        # è·å–Top1æ–‡æ¡£
        top1_idx, top1_faiss_score, top1_reranker_score = candidate_results[0]
        
        if top1_idx >= len(self.data):
            print(f"âŒ Top1æ–‡æ¡£ç´¢å¼•è¶…å‡ºèŒƒå›´: {top1_idx}")
            return ""
        
        record = self.data[top1_idx]
        print(f"âœ… ä½¿ç”¨Top1æ–‡æ¡£ (ç´¢å¼•: {top1_idx}, FAISSåˆ†æ•°: {top1_faiss_score:.4f}, é‡æ’åºåˆ†æ•°: {top1_reranker_score:.4f})")
        
        # è·å–Top1æ–‡æ¡£çš„å®Œæ•´context
        if self.dataset_type == "chinese":
            full_context = record.get('original_context', '')
            if not full_context:
                full_context = record.get('summary', '')
        else:
            full_context = record.get('context', '') or record.get('content', '')
        
        if not full_context:
            print("âŒ Top1æ–‡æ¡£æ²¡æœ‰contextå†…å®¹")
            return ""
        
        print(f"ğŸ“„ Top1æ–‡æ¡£å®Œæ•´contexté•¿åº¦: {len(full_context)} å­—ç¬¦")
        
        # å¯¹Top1æ–‡æ¡£è¿›è¡Œæ™ºèƒ½æå–
        # æå–æŸ¥è¯¢å…³é”®è¯
        query_keywords = self._extract_keywords(query)
        print(f"ğŸ”‘ æŸ¥è¯¢å…³é”®è¯: {query_keywords}")
        
        # æ™ºèƒ½æå–ç›¸å…³å¥å­
        relevant_sentences = self._extract_relevant_sentences(full_context, query_keywords, max_chars_per_doc=max_chars)
        
        # æ‹¼æ¥ä¸Šä¸‹æ–‡
        context = "\n\n".join(relevant_sentences)
        
        print(f"âœ… Top1æ–‡æ¡£æ™ºèƒ½æå–å®Œæˆ:")
        print(f"   ğŸ“ åŸå§‹é•¿åº¦: {len(full_context)} å­—ç¬¦")
        print(f"   ğŸ“ æå–åé•¿åº¦: {len(context)} å­—ç¬¦")
        print(f"   ğŸ“„ å¥å­æ•°: {len(relevant_sentences)}")
        print(f"   ğŸ“ å‰100å­—ç¬¦: {context[:100]}...")
        
        return context
    
    def _extract_keywords(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        keywords = []
        
        # æå–è‚¡ç¥¨ä»£ç 
        import re
        stock_pattern = r'[A-Z]{2}\d{4}|[A-Z]{2}\d{6}|\d{6}'
        stock_matches = re.findall(stock_pattern, query)
        keywords.extend(stock_matches)
        
        # æå–å…¬å¸åç§°
        company_pattern = r'([A-Za-z\u4e00-\u9fff]+)(?:å…¬å¸|é›†å›¢|è‚¡ä»½|æœ‰é™)'
        company_matches = re.findall(company_pattern, query)
        keywords.extend(company_matches)
        
        # æå–å¹´ä»½
        year_pattern = r'20\d{2}å¹´'
        year_matches = re.findall(year_pattern, query)
        keywords.extend(year_matches)
        
        # æå–å…³é”®æ¦‚å¿µ
        key_concepts = ['åˆ©æ¶¦', 'è¥æ”¶', 'å¢é•¿', 'ä¸šç»©', 'é¢„æµ‹', 'åŸå› ', 'ä¸»è¦', 'æŒç»­']
        for concept in key_concepts:
            if concept in query:
                keywords.append(concept)
        
        return list(set(keywords))
    
    def _extract_relevant_sentences(self, content: str, keywords: List[str], max_chars_per_doc: int = 800) -> List[str]:
        """ä»æ–‡æ¡£ä¸­æå–ä¸å…³é”®è¯æœ€ç›¸å…³çš„å¥å­"""
        if not content or not keywords:
            return []
        
        # æŒ‰å¥å­åˆ†å‰²
        import re
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # è®¡ç®—æ¯ä¸ªå¥å­çš„ç›¸å…³æ€§åˆ†æ•°
        sentence_scores = []
        for sentence in sentences:
            score = 0
            for keyword in keywords:
                if keyword in sentence:
                    score += 1
            # è€ƒè™‘å¥å­é•¿åº¦ï¼Œé¿å…è¿‡é•¿çš„å¥å­
            if len(sentence) > 200:
                score *= 0.5
            sentence_scores.append((sentence, score))
        
        # æŒ‰åˆ†æ•°æ’åº
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„å¥å­
        selected_sentences = []
        total_chars = 0
        
        for sentence, score in sentence_scores:
            if score > 0 and total_chars + len(sentence) <= max_chars_per_doc:
                selected_sentences.append(sentence)
                total_chars += len(sentence)
        
        return selected_sentences

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ"""
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_path = Path("data/alphafin/alphafin_merged_generated_qa.json")
    index_dir = Path("data/alphafin/retrieval_index")
    
    # åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿï¼ˆä¸­æ–‡æ•°æ®ï¼‰
    print("æ­£åœ¨åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿï¼ˆä¸­æ–‡æ•°æ®ï¼‰...")
    retrieval_system = MultiStageRetrievalSystem(data_path, dataset_type="chinese")
    
    # ä¿å­˜ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
    retrieval_system.save_index(index_dir)
    
    # æ¼”ç¤ºæ£€ç´¢
    print("\n" + "="*50)
    print("æ£€ç´¢æ¼”ç¤º")
    print("="*50)
    
    # ç¤ºä¾‹æŸ¥è¯¢1ï¼šåŸºäºå…¬å¸åç§°çš„æ£€ç´¢ï¼ˆä»…ä¸­æ–‡æ•°æ®æ”¯æŒï¼‰
    print("\nç¤ºä¾‹1: åŸºäºå…¬å¸åç§°çš„æ£€ç´¢")
    results1 = retrieval_system.search(
        query="å…¬å¸ä¸šç»©è¡¨ç°å¦‚ä½•ï¼Ÿ",
        company_name="ä¸­å›½å®æ­¦",
        top_k=5
    )
    
    for i, result in enumerate(results1['retrieved_documents']):
        print(f"\nç»“æœ {i+1}:")
        print(f"  å…¬å¸: {result['company_name']}")
        print(f"  è‚¡ç¥¨ä»£ç : {result['stock_code']}")
        print(f"  æ‘˜è¦: {result['summary']}")
        print(f"  ç›¸ä¼¼åº¦åˆ†æ•°: {result['combined_score']:.4f}")
    
    # ç¤ºä¾‹æŸ¥è¯¢2ï¼šé€šç”¨æ£€ç´¢
    print("\nç¤ºä¾‹2: é€šç”¨æ£€ç´¢ï¼ˆæ— å…ƒæ•°æ®è¿‡æ»¤ï¼‰")
    results2 = retrieval_system.search(
        query="é’¢é“è¡Œä¸šå‘å±•è¶‹åŠ¿",
        top_k=5
    )
    
    for i, result in enumerate(results2['retrieved_documents']):
        print(f"\nç»“æœ {i+1}:")
        print(f"  å…¬å¸: {result['company_name']}")
        print(f"  è‚¡ç¥¨ä»£ç : {result['stock_code']}")
        print(f"  æ‘˜è¦: {result['summary']}")
        print(f"  ç›¸ä¼¼åº¦åˆ†æ•°: {result['combined_score']:.4f}")

if __name__ == '__main__':
    main() 