import json
import pickle
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from collections import defaultdict
import re
from datetime import datetime
import sys
import numpy as np

# éœ€è¦å®‰è£…çš„ä¾èµ–ï¼špip install faiss-cpu sentence-transformers torch
try:
    import faiss
    from sentence_transformers import SentenceTransformer
    import torch
except ImportError as e:
    print(f"è¯·å®‰è£…å¿…è¦çš„ä¾èµ–: pip install faiss-cpu sentence-transformers torch")
    print(f"é”™è¯¯: {e}")
    exit(1)

# å¯¼å…¥ç°æœ‰çš„QwenReranker
try:
    sys.path.append(str(Path(__file__).parent.parent))
    from xlm.components.retriever.reranker import QwenReranker
    from config.parameters import Config, DEFAULT_CACHE_DIR
except ImportError as e:
    print(f"æ— æ³•å¯¼å…¥QwenRerankeræˆ–Config: {e}")
    print("è¯·ç¡®ä¿xlmç›®å½•ç»“æ„æ­£ç¡®")
    exit(1)

from xlm.components.prompt_templates.template_loader import template_loader

def load_json_or_jsonl(file_path: Path) -> List[Dict]:
    """
    å…¼å®¹åŠ è½½JSONæˆ–JSONLæ ¼å¼æ–‡ä»¶
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        
    Returns:
        æ•°æ®åˆ—è¡¨
    """
    print(f"æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶: {file_path}")
    
    try:
        # é¦–å…ˆå°è¯•ä½œä¸ºJSONåŠ è½½
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
                print(f"æˆåŠŸåŠ è½½JSONæ ¼å¼æ–‡ä»¶ï¼Œå…± {len(data)} æ¡è®°å½•")
                return data
            except json.JSONDecodeError as e:
                print(f"JSONæ ¼å¼è§£æå¤±è´¥: {e}")
                print("å°è¯•ä½œä¸ºJSONLæ ¼å¼åŠ è½½...")
                
                # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
                f.seek(0)
                
                # å°è¯•ä½œä¸ºJSONLåŠ è½½
                data = []
                for line_num, line in enumerate(f, 1):
                    line = line.strip()
                    if line:  # è·³è¿‡ç©ºè¡Œ
                        try:
                            item = json.loads(line)
                            data.append(item)
                        except json.JSONDecodeError as line_error:
                            print(f"è­¦å‘Š: ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {line_error}")
                            print(f"é—®é¢˜è¡Œå†…å®¹: {line[:100]}...")
                            continue
                
                print(f"æˆåŠŸåŠ è½½JSONLæ ¼å¼æ–‡ä»¶ï¼Œå…± {len(data)} æ¡è®°å½•")
                return data
                
    except FileNotFoundError:
        print(f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return []
    except Exception as e:
        print(f"é”™è¯¯: è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return []

class MultiStageRetrievalSystem:
    """
    å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿï¼š
    1. Pre-filtering: åŸºäºå…ƒæ•°æ®ï¼ˆä»…ä¸­æ–‡æ•°æ®æ”¯æŒï¼‰
    2. FAISSæ£€ç´¢: åŸºäºgenerated_questionå’Œsummaryç”Ÿæˆç»Ÿä¸€åµŒå…¥ç´¢å¼•
    3. Reranker: åŸºäºoriginal_contextä½¿ç”¨Qwen3-0.6Bè¿›è¡Œé‡æ’åº
    
    æ”¯æŒè‹±æ–‡å’Œä¸­æ–‡æ•°æ®é›†ï¼Œä½¿ç”¨ç°æœ‰é…ç½®çš„æ¨¡å‹
    - ä¸­æ–‡æ•°æ®ï¼ˆAlphaFinï¼‰ï¼šæ”¯æŒå…ƒæ•°æ®é¢„è¿‡æ»¤ + FAISS + Qwené‡æ’åº
    - è‹±æ–‡æ•°æ®ï¼ˆTatQAï¼‰ï¼šä»…æ”¯æŒFAISS + Qwené‡æ’åºï¼ˆæ— å…ƒæ•°æ®ï¼‰
    """
    
    def __init__(self, data_path: Path, dataset_type: str = "chinese", use_existing_config: bool = True):
        """
        åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            dataset_type: æ•°æ®é›†ç±»å‹ ("chinese" æˆ– "english")
            use_existing_config: æ˜¯å¦ä½¿ç”¨ç°æœ‰é…ç½®
        """
        self.data_path = data_path
        self.dataset_type = dataset_type
        self.data = []
        self.original_data = []
        self.doc_to_chunks_mapping = {}
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.embedding_model = None
        self.faiss_index = None
        self.qwen_reranker = None
        self.llm_generator = None  # æ·»åŠ LLMç”Ÿæˆå™¨
        self.valid_indices = []
        self.metadata_index = defaultdict(dict)
        
        # é…ç½®
        self.config = None
        self.model_name = "all-MiniLM-L6-v2"
        
        if use_existing_config:
            try:
                from config.parameters import Config
                self.config = Config()
                
                # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©ç¼–ç å™¨
                if self.dataset_type == "chinese":
                    # ä½¿ç”¨ä¸­æ–‡ç¼–ç å™¨
                    self.model_name = self.config.encoder.chinese_model_path
                    print(f"ä½¿ç”¨ä¸­æ–‡ç¼–ç å™¨: {self.model_name}")
                else:
                    # ä½¿ç”¨è‹±æ–‡ç¼–ç å™¨
                    self.model_name = self.config.encoder.english_model_path
                    print(f"ä½¿ç”¨è‹±æ–‡ç¼–ç å™¨: {self.model_name}")
                
                print("ä½¿ç”¨ç°æœ‰é…ç½®åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ")
            except Exception as e:
                print(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
                # å›é€€åˆ°é»˜è®¤æ¨¡å‹
                if self.dataset_type == "chinese":
                    self.model_name = "distiluse-base-multilingual-cased-v2"
                    print(f"ä½¿ç”¨é»˜è®¤ä¸­æ–‡ç¼–ç å™¨: {self.model_name}")
                else:
                    self.model_name = "all-MiniLM-L6-v2"
                    print(f"ä½¿ç”¨é»˜è®¤è‹±æ–‡ç¼–ç å™¨: {self.model_name}")
        
        # åŠ è½½æ•°æ®
        self._load_data()
        
        # æ„å»ºç´¢å¼•
        self._build_metadata_index()
        self._init_embedding_model()
        self._build_faiss_index()
        self._init_qwen_reranker()
        self._init_llm_generator()  # åˆå§‹åŒ–LLMç”Ÿæˆå™¨
    
    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")
        
        # åŠ è½½åŸå§‹AlphaFinæ•°æ®ç”¨äºFAISSç´¢å¼•
        print("åŠ è½½åŸå§‹AlphaFinæ•°æ®ç”¨äºFAISSç´¢å¼•...")
        self.original_data = load_json_or_jsonl(self.data_path)
        print(f"åŠ è½½äº† {len(self.original_data)} æ¡åŸå§‹è®°å½•")
        
        # å»ºç«‹doc_idåˆ°chunksçš„æ˜ å°„
        print("å»ºç«‹doc_idåˆ°chunksçš„æ˜ å°„å…³ç³»...")
        self.doc_to_chunks_mapping = {}
        
        for doc_idx, record in enumerate(self.original_data):
            if self.dataset_type == "chinese":
                # å¯¹äºä¸­æ–‡æ•°æ®ï¼Œç”Ÿæˆchunks
                original_context = record.get('original_context', '')
                company_name = record.get('company_name', 'å…¬å¸')
                
                if original_context:
                    # ä½¿ç”¨convert_json_context_to_natural_language_chunkså‡½æ•°
                    from xlm.utils.optimized_data_loader import convert_json_context_to_natural_language_chunks
                    chunks = convert_json_context_to_natural_language_chunks(original_context, company_name)
                    
                    if chunks:
                        self.doc_to_chunks_mapping[doc_idx] = chunks
                    else:
                        # å¦‚æœæ²¡æœ‰chunksï¼Œä½¿ç”¨summaryä½œä¸ºfallback
                        self.doc_to_chunks_mapping[doc_idx] = [record.get('summary', '')]
                else:
                    # å¦‚æœæ²¡æœ‰original_contextï¼Œä½¿ç”¨summary
                    self.doc_to_chunks_mapping[doc_idx] = [record.get('summary', '')]
            else:
                # è‹±æ–‡æ•°æ®ï¼Œä½¿ç”¨contextæˆ–content
                context = record.get('context', '') or record.get('content', '')
                self.doc_to_chunks_mapping[doc_idx] = [context]
        
        print(f"å»ºç«‹äº† {len(self.doc_to_chunks_mapping)} ä¸ªdoc_idåˆ°chunksçš„æ˜ å°„")
        
        # ç»Ÿè®¡chunksæ€»æ•°
        total_chunks = sum(len(chunks) for chunks in self.doc_to_chunks_mapping.values())
        print(f"æ€»å…±ç”Ÿæˆäº† {total_chunks} ä¸ªchunksç”¨äºé‡æ’åº")
        
        # ä½¿ç”¨åŸå§‹æ•°æ®ä½œä¸ºä¸»è¦æ•°æ®
        self.data = self.original_data
        
        print(f"æ•°æ®é›†ç±»å‹: {self.dataset_type}")
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        if self.data and isinstance(self.data[0], dict):
            sample_record = self.data[0]
            print(f"æ•°æ®å­—æ®µ: {list(sample_record.keys())}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å…ƒæ•°æ®å­—æ®µ
            has_metadata = any(field in sample_record for field in ['company_name', 'stock_code', 'report_date'])
            print(f"åŒ…å«å…ƒæ•°æ®å­—æ®µ: {has_metadata}")
    
    def _build_metadata_index(self):
        """æ„å»ºå…ƒæ•°æ®ç´¢å¼•ç”¨äºpre-filteringï¼ˆä»…ä¸­æ–‡æ•°æ®ï¼‰"""
        if self.dataset_type != "chinese":
            print("éä¸­æ–‡æ•°æ®é›†ï¼Œè·³è¿‡å…ƒæ•°æ®ç´¢å¼•æ„å»º")
            return
            
        print("æ­£åœ¨æ„å»ºå…ƒæ•°æ®ç´¢å¼•...")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å…ƒæ•°æ®å­—æ®µ
        if not self.data:
            print("æ•°æ®æ ¼å¼ä¸æ”¯æŒå…ƒæ•°æ®ç´¢å¼•")
            return
            
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        if hasattr(self.data[0], 'content'):
            # DocumentWithMetadataæ ¼å¼
            print("ä½¿ç”¨DocumentWithMetadataæ ¼å¼ï¼Œè·³è¿‡å…ƒæ•°æ®ç´¢å¼•æ„å»º")
            print("æ³¨æ„ï¼šchunkçº§åˆ«çš„æ•°æ®ä¸æ”¯æŒå…ƒæ•°æ®é¢„è¿‡æ»¤")
            return
        elif isinstance(self.data[0], dict):
            # å­—å…¸æ ¼å¼
            sample_record = self.data[0]
            has_metadata = any(field in sample_record for field in ['company_name', 'stock_code', 'report_date'])
            
            if not has_metadata:
                print("æ•°æ®ä¸åŒ…å«å…ƒæ•°æ®å­—æ®µï¼Œè·³è¿‡å…ƒæ•°æ®ç´¢å¼•æ„å»º")
                return
            
            # æŒ‰å…¬å¸åç§°ç´¢å¼•
            self.metadata_index['company_name'] = defaultdict(list)
            # æŒ‰è‚¡ç¥¨ä»£ç ç´¢å¼•
            self.metadata_index['stock_code'] = defaultdict(list)
            # æŒ‰æŠ¥å‘Šæ—¥æœŸç´¢å¼•
            self.metadata_index['report_date'] = defaultdict(list)
            # æŒ‰å…¬å¸åç§°+è‚¡ç¥¨ä»£ç ç»„åˆç´¢å¼•
            self.metadata_index['company_stock'] = defaultdict(list)
            
            for idx, record in enumerate(self.data):
                # å…¬å¸åç§°ç´¢å¼•
                if record.get('company_name'):
                    company_name = record['company_name'].strip().lower()
                    self.metadata_index['company_name'][company_name].append(idx)
                
                # è‚¡ç¥¨ä»£ç ç´¢å¼•
                if record.get('stock_code'):
                    stock_code = str(record['stock_code']).strip().lower()
                    self.metadata_index['stock_code'][stock_code].append(idx)
                
                # æŠ¥å‘Šæ—¥æœŸç´¢å¼•
                if record.get('report_date'):
                    report_date = record['report_date'].strip()
                    self.metadata_index['report_date'][report_date].append(idx)
                
                # å…¬å¸åç§°+è‚¡ç¥¨ä»£ç ç»„åˆç´¢å¼•
                if record.get('company_name') and record.get('stock_code'):
                    company_name = record['company_name'].strip().lower()
                    stock_code = str(record['stock_code']).strip().lower()
                    key = f"{company_name}_{stock_code}"
                    self.metadata_index['company_stock'][key].append(idx)
            
            print(f"å…ƒæ•°æ®ç´¢å¼•æ„å»ºå®Œæˆ:")
            print(f"  - å…¬å¸åç§°: {len(self.metadata_index['company_name'])} ä¸ª")
            print(f"  - è‚¡ç¥¨ä»£ç : {len(self.metadata_index['stock_code'])} ä¸ª")
            print(f"  - æŠ¥å‘Šæ—¥æœŸ: {len(self.metadata_index['report_date'])} ä¸ª")
            print(f"  - å…¬å¸+è‚¡ç¥¨ç»„åˆ: {len(self.metadata_index['company_stock'])} ä¸ª")
    
    def _init_embedding_model(self):
        """åˆå§‹åŒ–å¥å­åµŒå…¥æ¨¡å‹"""
        print(f"æ­£åœ¨åŠ è½½åµŒå…¥æ¨¡å‹: {self.model_name}")
        print(f"æ¨¡å‹ç±»å‹: {'å¤šè¯­è¨€ç¼–ç å™¨' if self.dataset_type == 'chinese' else 'è‹±æ–‡ç¼–ç å™¨'}")
        
        # ä½¿ç”¨ç°æœ‰é…ç½®çš„ç¼“å­˜ç›®å½•
        cache_dir = None
        if self.config:
            cache_dir = self.config.encoder.cache_dir
            print(f"ä½¿ç”¨é…ç½®çš„ç¼“å­˜ç›®å½•: {cache_dir}")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¾®è°ƒæ¨¡å‹è·¯å¾„
            if "finetuned" in self.model_name or "models/" in self.model_name:
                print("æ£€æµ‹åˆ°å¾®è°ƒæ¨¡å‹ï¼Œä½¿ç”¨FinbertEncoder...")
                from xlm.components.encoder.finbert import FinbertEncoder
                self.embedding_model = FinbertEncoder(
                    model_name=self.model_name,
                    cache_dir=cache_dir,
                    device="cuda:0"  # ç¼–ç å™¨ä½¿ç”¨cuda:0
                )
                print("å¾®è°ƒæ¨¡å‹åŠ è½½å®Œæˆ (cuda:0)")
            else:
                # ä½¿ç”¨SentenceTransformeråŠ è½½HuggingFaceæ¨¡å‹
                from sentence_transformers import SentenceTransformer
                self.embedding_model = SentenceTransformer(self.model_name, cache_folder=cache_dir)
                # å°†æ¨¡å‹ç§»åŠ¨åˆ°cuda:0
                if hasattr(self.embedding_model, 'to'):
                    self.embedding_model.to('cuda:0')
                print("HuggingFaceæ¨¡å‹åŠ è½½å®Œæˆ (cuda:0)")
        except Exception as e:
            print(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("å°è¯•ä½¿ç”¨é»˜è®¤æ¨¡å‹...")
            try:
                # å›é€€åˆ°é»˜è®¤æ¨¡å‹
                if self.dataset_type == "chinese":
                    fallback_model = "distiluse-base-multilingual-cased-v2"
                else:
                    fallback_model = "all-MiniLM-L6-v2"
                print(f"ä½¿ç”¨å›é€€æ¨¡å‹: {fallback_model}")
                from sentence_transformers import SentenceTransformer
                self.embedding_model = SentenceTransformer(fallback_model)
                # å°†æ¨¡å‹ç§»åŠ¨åˆ°cuda:0
                if hasattr(self.embedding_model, 'to'):
                    self.embedding_model.to('cuda:0')
                print("å›é€€æ¨¡å‹åŠ è½½æˆåŠŸ (cuda:0)")
            except Exception as e2:
                print(f"å›é€€æ¨¡å‹ä¹ŸåŠ è½½å¤±è´¥: {e2}")
                self.embedding_model = None
    
    def _build_faiss_index(self):
        """æ„å»ºFAISSç´¢å¼•"""
        if self.embedding_model is None:
            print("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè·³è¿‡FAISSç´¢å¼•æ„å»º")
            return
            
        print("æ­£åœ¨æ„å»ºFAISSç´¢å¼•...")
        print("ä¸­æ–‡æ•°æ®ï¼šä½¿ç”¨summaryå­—æ®µè¿›è¡Œå‘é‡ç¼–ç ")
        print("è‹±æ–‡æ•°æ®ï¼šä½¿ç”¨context/contentå­—æ®µè¿›è¡Œå‘é‡ç¼–ç ")
        
        # å‡†å¤‡ç”¨äºåµŒå…¥çš„æ–‡æœ¬
        texts_for_embedding = []
        valid_indices = []
        
        for idx, record in enumerate(self.data):
            # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©ä¸åŒçš„æ–‡æœ¬ç»„åˆç­–ç•¥
            if self.dataset_type == "chinese":
                # ä¸­æ–‡æ•°æ®ï¼šåªä½¿ç”¨summary
                summary = record.get('summary', '')
                
                if summary:
                    texts_for_embedding.append(summary)
                    valid_indices.append(idx)
                else:
                    continue
            else:
                # è‹±æ–‡æ•°æ®ï¼šä½¿ç”¨contextæˆ–contentå­—æ®µ
                context = record.get('context', '') or record.get('content', '')
                
                if context:
                    texts_for_embedding.append(context)
                    valid_indices.append(idx)
                else:
                    continue
        
        if not texts_for_embedding:
            print("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬ç”¨äºåµŒå…¥")
            return
        
        # ç”ŸæˆåµŒå…¥
        print(f"æ­£åœ¨ç¼–ç  {len(texts_for_embedding)} ä¸ªæ–‡æœ¬...")
        embeddings = self.embedding_model.encode(texts_for_embedding, show_progress_bar=True)
        
        # æ„å»ºFAISSç´¢å¼•
        dimension = embeddings.shape[1]
        self.faiss_index = faiss.IndexFlatIP(dimension)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        self.faiss_index.add(embeddings.astype('float32'))
        
        # ä¿å­˜æœ‰æ•ˆç´¢å¼•çš„æ˜ å°„
        self.valid_indices = valid_indices
        
        print(f"FAISSç´¢å¼•æ„å»ºå®Œæˆï¼Œç»´åº¦: {dimension}")
        print(f"æœ‰æ•ˆç´¢å¼•æ•°é‡: {len(self.valid_indices)}")
        print(f"åŸºäºsummaryæ„å»ºç´¢å¼•ï¼Œç”¨äºç²—ç²’åº¦æ£€ç´¢")
    
    def _init_qwen_reranker(self):
        """åˆå§‹åŒ–Qwen reranker"""
        print("æ­£åœ¨åˆå§‹åŒ–Qwen reranker...")
        try:
            # ä½¿ç”¨ç°æœ‰é…ç½®
            model_name = "Qwen/Qwen3-Reranker-0.6B"
            cache_dir = DEFAULT_CACHE_DIR  # ä½¿ç”¨DEFAULT_CACHE_DIR
            use_quantization = True
            quantization_type = "8bit"
            
            if self.config:
                model_name = self.config.reranker.model_name
                cache_dir = self.config.reranker.cache_dir or DEFAULT_CACHE_DIR  # ç¡®ä¿ä¸ä¸ºNone
                use_quantization = self.config.reranker.use_quantization
                quantization_type = self.config.reranker.quantization_type
            
            print(f"ä½¿ç”¨é…ç½®çš„é‡æ’åºå™¨: {model_name}")
            print(f"ç¼“å­˜ç›®å½•: {cache_dir}")
            print(f"é‡åŒ–: {use_quantization} ({quantization_type})")
            
            # ä½¿ç”¨ç°æœ‰çš„QwenReranker
            self.qwen_reranker = QwenReranker(
                model_name=model_name,
                device="cuda:0",  # é‡æ’åºå™¨ä½¿ç”¨cuda:0
                cache_dir=cache_dir,
                use_quantization=use_quantization,
                quantization_type=quantization_type
            )
            print("Qwen rerankeråˆå§‹åŒ–å®Œæˆ (cuda:0)")
        except Exception as e:
            print(f"Qwen rerankeråˆå§‹åŒ–å¤±è´¥: {e}")
            self.qwen_reranker = None
    
    def _init_llm_generator(self):
        """åˆå§‹åŒ–LLMç”Ÿæˆå™¨"""
        print("æ­£åœ¨åˆå§‹åŒ–LLMç”Ÿæˆå™¨...")
        try:
            # é‡ç”¨ç°æœ‰çš„LocalLLMGenerator
            from xlm.components.generator.local_llm_generator import LocalLLMGenerator
            
            # ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°
            model_name = None  # è®©LocalLLMGeneratorä»configè¯»å–
            cache_dir = None   # è®©LocalLLMGeneratorä»configè¯»å–
            device = None      # è®©LocalLLMGeneratorä»configè¯»å–
            use_quantization = None  # è®©LocalLLMGeneratorä»configè¯»å–
            quantization_type = None  # è®©LocalLLMGeneratorä»configè¯»å–
            
            if self.config:
                # å¦‚æœconfigä¸­æœ‰generatoré…ç½®ï¼Œä½¿ç”¨å®ƒ
                if hasattr(self.config, 'generator'):
                    model_name = self.config.generator.model_name
                    cache_dir = self.config.generator.cache_dir
                    device = self.config.generator.device
                    use_quantization = self.config.generator.use_quantization
                    quantization_type = self.config.generator.quantization_type
            
            # é¦–å…ˆå°è¯•GPUæ¨¡å¼
            try:
                print(f"å°è¯•GPUæ¨¡å¼åŠ è½½LLMç”Ÿæˆå™¨: {device}")
                self.llm_generator = LocalLLMGenerator(
                    model_name=model_name,
                    cache_dir=cache_dir,
                    device=device,
                    use_quantization=use_quantization,
                    quantization_type=quantization_type
                )
                print("âœ… LLMç”Ÿæˆå™¨GPUæ¨¡å¼åˆå§‹åŒ–å®Œæˆ")
            except Exception as gpu_error:
                print(f"âŒ GPUæ¨¡å¼åŠ è½½å¤±è´¥: {gpu_error}")
                print("å›é€€åˆ°CPUæ¨¡å¼...")
                
                # å›é€€åˆ°CPUæ¨¡å¼
                try:
                    self.llm_generator = LocalLLMGenerator(
                        model_name=model_name,
                        cache_dir=cache_dir,
                        device="cpu",  # å¼ºåˆ¶ä½¿ç”¨CPU
                        use_quantization=False,  # CPUæ¨¡å¼ä¸ä½¿ç”¨é‡åŒ–
                        quantization_type=None
                    )
                    print("âœ… LLMç”Ÿæˆå™¨CPUæ¨¡å¼åˆå§‹åŒ–å®Œæˆ")
                except Exception as cpu_error:
                    print(f"âŒ CPUæ¨¡å¼ä¹Ÿå¤±è´¥: {cpu_error}")
                    self.llm_generator = None
                    
        except Exception as e:
            print(f"LLMç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.llm_generator = None
    
    def pre_filter(self, 
                   company_name: Optional[str] = None,
                   stock_code: Optional[str] = None,
                   report_date: Optional[str] = None,
                   max_candidates: int = 1000) -> List[int]:
        """
        åŸºäºå…ƒæ•°æ®è¿›è¡Œé¢„è¿‡æ»¤ï¼ˆä»…ä¸­æ–‡æ•°æ®æ”¯æŒï¼‰
        
        Args:
            company_name: å…¬å¸åç§°
            stock_code: è‚¡ç¥¨ä»£ç 
            report_date: æŠ¥å‘Šæ—¥æœŸ
            max_candidates: æœ€å¤§å€™é€‰æ•°é‡
            
        Returns:
            å€™é€‰è®°å½•ç´¢å¼•åˆ—è¡¨
        """
        if self.dataset_type != "chinese":
            print("éä¸­æ–‡æ•°æ®é›†ï¼Œè·³è¿‡é¢„è¿‡æ»¤")
            return list(range(len(self.data)))
        
        print("å¼€å§‹å…ƒæ•°æ®é¢„è¿‡æ»¤...")
        
        # å¦‚æœæ²¡æœ‰æä¾›ä»»ä½•è¿‡æ»¤æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰è®°å½•
        if not any([company_name, stock_code, report_date]):
            print("æ— è¿‡æ»¤æ¡ä»¶ï¼Œè¿”å›æ‰€æœ‰è®°å½•")
            return list(range(len(self.data)))
        
        # ä¼˜å…ˆä½¿ç”¨ç»„åˆç´¢å¼•ï¼ˆå…¬å¸åç§°+è‚¡ç¥¨ä»£ç ï¼‰
        if company_name and stock_code:
            company_name_lower = company_name.strip().lower()
            stock_code_lower = str(stock_code).strip().lower()
            key = f"{company_name_lower}_{stock_code_lower}"
            
            if key in self.metadata_index['company_stock']:
                indices = self.metadata_index['company_stock'][key]
                print(f"ç»„åˆè¿‡æ»¤: å…¬å¸'{company_name}' + è‚¡ç¥¨'{stock_code}' åŒ¹é… {len(indices)} æ¡è®°å½•")
                return indices[:max_candidates]
            else:
                print(f"ç»„åˆè¿‡æ»¤: å…¬å¸'{company_name}' + è‚¡ç¥¨'{stock_code}' æ— åŒ¹é…è®°å½•")
                return []
        
        # å¦‚æœåªæä¾›äº†å…¬å¸åç§°
        elif company_name:
            company_name_lower = company_name.strip().lower()
            if company_name_lower in self.metadata_index['company_name']:
                indices = self.metadata_index['company_name'][company_name_lower]
                print(f"å…¬å¸åç§°è¿‡æ»¤: '{company_name}' åŒ¹é… {len(indices)} æ¡è®°å½•")
                return indices[:max_candidates]
            else:
                print(f"å…¬å¸åç§°è¿‡æ»¤: '{company_name}' æ— åŒ¹é…è®°å½•")
                return []
        
        # å¦‚æœåªæä¾›äº†è‚¡ç¥¨ä»£ç 
        elif stock_code:
            stock_code_lower = str(stock_code).strip().lower()
            if stock_code_lower in self.metadata_index['stock_code']:
                indices = self.metadata_index['stock_code'][stock_code_lower]
                print(f"è‚¡ç¥¨ä»£ç è¿‡æ»¤: '{stock_code}' åŒ¹é… {len(indices)} æ¡è®°å½•")
                return indices[:max_candidates]
            else:
                print(f"è‚¡ç¥¨ä»£ç è¿‡æ»¤: '{stock_code}' æ— åŒ¹é…è®°å½•")
                return []
        
        # å¦‚æœåªæä¾›äº†æŠ¥å‘Šæ—¥æœŸ
        elif report_date:
            report_date_str = report_date.strip()
            if report_date_str in self.metadata_index['report_date']:
                indices = self.metadata_index['report_date'][report_date_str]
                print(f"æŠ¥å‘Šæ—¥æœŸè¿‡æ»¤: '{report_date}' åŒ¹é… {len(indices)} æ¡è®°å½•")
                return indices[:max_candidates]
            else:
                print(f"æŠ¥å‘Šæ—¥æœŸè¿‡æ»¤: '{report_date}' æ— åŒ¹é…è®°å½•")
                return []
        
        print("é¢„è¿‡æ»¤å®Œæˆï¼Œå€™é€‰æ–‡æ¡£æ•°: 0")
        return []
    
    def faiss_search(self, query: str, candidate_indices: List[int], top_k: int = 100) -> List[Tuple[int, float]]:
        """
        ä½¿ç”¨FAISSè¿›è¡Œå‘é‡æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidate_indices: å€™é€‰è®°å½•ç´¢å¼•
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            (ç´¢å¼•, ç›¸ä¼¼åº¦åˆ†æ•°) çš„åˆ—è¡¨
        """
        if self.faiss_index is None:
            print("FAISSç´¢å¼•æœªåˆå§‹åŒ–")
            return []
        
        print(f"å¼€å§‹FAISSæ£€ç´¢ï¼Œå€™é€‰æ–‡æ¡£æ•°: {len(candidate_indices)}")
        
        # ç”ŸæˆæŸ¥è¯¢åµŒå…¥
        try:
            query_embedding = self.embedding_model.encode([query])
            print(f"æŸ¥è¯¢åµŒå…¥ç”Ÿæˆå®Œæˆï¼Œç»´åº¦: {query_embedding.shape}")
        except Exception as e:
            print(f"æŸ¥è¯¢åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}")
            return []
        
        # åœ¨FAISSä¸­æœç´¢
        try:
            scores, indices = self.faiss_index.search(query_embedding.astype('float32'), top_k)
            print(f"FAISSæœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(indices[0])} ä¸ªå€™é€‰")
        except Exception as e:
            print(f"FAISSæœç´¢å¤±è´¥: {e}")
            return []
        
        # å°†FAISSç´¢å¼•æ˜ å°„å›åŸå§‹æ•°æ®ç´¢å¼•
        results = []
        for faiss_idx, score in zip(indices[0], scores[0]):
            if faiss_idx < len(self.valid_indices):
                original_idx = self.valid_indices[faiss_idx]
                # æ£€æŸ¥æ˜¯å¦åœ¨å€™é€‰åˆ—è¡¨ä¸­
                if original_idx in candidate_indices:
                    results.append((original_idx, float(score)))
        
        print(f"FAISSæ£€ç´¢å®Œæˆï¼Œæœ‰æ•ˆç»“æœ: {len(results)} æ¡è®°å½•")
        return results
    
    def rerank(self, 
               query: str, 
               candidate_results: List[Tuple[int, float]], 
               top_k: int = 20) -> List[Tuple[int, float, float]]:
        """
        ä½¿ç”¨Qwené‡æ’åºå™¨å¯¹å€™é€‰ç»“æœè¿›è¡Œé‡æ’åº
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidate_results: å€™é€‰ç»“æœåˆ—è¡¨ [(doc_idx, faiss_score), ...]
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            é‡æ’åºåçš„ç»“æœåˆ—è¡¨ [(doc_idx, faiss_score, reranker_score), ...]
        """
        if not self.qwen_reranker or not candidate_results:
            print("é‡æ’åºå™¨ä¸å¯ç”¨æˆ–æ²¡æœ‰å€™é€‰ç»“æœ")
            return [(idx, score, 0.0) for idx, score in candidate_results[:top_k]]
        
        print(f"å¼€å§‹é‡æ’åº {len(candidate_results)} æ¡å€™é€‰ç»“æœ...")
        
        # å‡†å¤‡é‡æ’åºçš„æ–‡æ¡£ - ä½¿ç”¨doc_idåˆ°chunksçš„æ˜ å°„
        docs_for_rerank = []
        doc_to_rerank_mapping = []
        
        for doc_idx, faiss_score in candidate_results:
            if doc_idx in self.doc_to_chunks_mapping:
                chunks = self.doc_to_chunks_mapping[doc_idx]
                for chunk in chunks:
                    if chunk.strip():  # è·³è¿‡ç©ºchunk
                        docs_for_rerank.append(chunk)
                        doc_to_rerank_mapping.append((doc_idx, faiss_score))
            else:
                # å¦‚æœæ‰¾ä¸åˆ°æ˜ å°„ï¼Œä½¿ç”¨åŸå§‹æ•°æ®
                if doc_idx < len(self.data):
                    record = self.data[doc_idx]
                    if self.dataset_type == "chinese":
                        content = record.get('summary', '')
                    else:
                        content = record.get('context', '')
                    if content.strip():
                        docs_for_rerank.append(content)
                        doc_to_rerank_mapping.append((doc_idx, faiss_score))
        
        print(f"å‡†å¤‡é‡æ’åº {len(docs_for_rerank)} ä¸ªchunks...")
        
        if not docs_for_rerank:
            print("æ²¡æœ‰å¯é‡æ’åºçš„æ–‡æ¡£")
            return [(idx, score, 0.0) for idx, score in candidate_results[:top_k]]
        
        # ä½¿ç”¨Qwené‡æ’åºå™¨è¿›è¡Œé‡æ’åº
        try:
            reranked_results = self.qwen_reranker.rerank(query, docs_for_rerank, batch_size=4)
            print(f"é‡æ’åºå™¨å¤„ç†å®Œæˆï¼Œè¿”å› {len(reranked_results)} ä¸ªç»“æœ")
        except Exception as e:
            print(f"é‡æ’åºå¤±è´¥: {e}")
            # å›é€€åˆ°åŸå§‹ç»“æœ
            return [(idx, score, 0.0) for idx, score in candidate_results[:top_k]]
        
        # å°†é‡æ’åºç»“æœæ˜ å°„å›åŸå§‹æ–‡æ¡£ç´¢å¼•
        final_results = []
        for doc_text, reranker_score in reranked_results:
            # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ–‡æ¡£ç´¢å¼•
            for i, (doc_idx, faiss_score) in enumerate(doc_to_rerank_mapping):
                if i < len(docs_for_rerank) and docs_for_rerank[i] == doc_text:
                    # ç»„åˆåˆ†æ•°ï¼šFAISSåˆ†æ•° + é‡æ’åºåˆ†æ•°
                    combined_score = faiss_score + reranker_score
                    final_results.append((doc_idx, faiss_score, combined_score))
                    break
        
        # æŒ‰ç»„åˆåˆ†æ•°æ’åº
        final_results.sort(key=lambda x: x[2], reverse=True)
        
        print(f"é‡æ’åºå®Œæˆï¼Œè¿”å› {len(final_results)} ä¸ªç»“æœ")
        return final_results[:top_k]
    
    def generate_answer(self, query: str, candidate_results: List[Tuple[int, float, float]], top_k_for_context: int = 5) -> str:
        """
        ç”ŸæˆLLMç­”æ¡ˆ - ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–ï¼Œå¤§å¹…ç¼©çŸ­ä¼ é€’ç»™LLMçš„ä¸Šä¸‹æ–‡
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidate_results: å€™é€‰ç»“æœåˆ—è¡¨ [(doc_idx, faiss_score, reranker_score), ...]
            top_k_for_context: ç”¨äºç”Ÿæˆä¸Šä¸‹æ–‡çš„å€™é€‰æ•°é‡
            
        Returns:
            ç”Ÿæˆçš„LLMç­”æ¡ˆ
        """
        if not candidate_results:
            print("æ²¡æœ‰å€™é€‰ç»“æœï¼Œæ— æ³•ç”Ÿæˆç­”æ¡ˆ")
            return ""
        
        print(f"å¼€å§‹ç”ŸæˆLLMç­”æ¡ˆ...")
        print(f"åŸå§‹æŸ¥è¯¢: '{query}'")
        print(f"æŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")
        
        # ä½¿ç”¨æ™ºèƒ½ä¸Šä¸‹æ–‡æå–ï¼Œé™åˆ¶åœ¨2000å­—ç¬¦ä»¥å†…
        context = self.extract_relevant_context(query, candidate_results, max_chars=2000)
        
        print(f"æ™ºèƒ½æå–çš„ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} ä¸ªå­—ç¬¦")
        
        # ä½¿ç”¨LLMç”Ÿæˆå™¨ç”Ÿæˆç­”æ¡ˆ
        if self.llm_generator:
            try:
                # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©promptæ¨¡æ¿
                if self.dataset_type == "chinese":
                    # ä¸­æ–‡promptæ¨¡æ¿
                    # è·å–Top1æ–‡æ¡£çš„summary
                    if candidate_results and candidate_results[0][0] < len(self.data):
                        top1_record = self.data[candidate_results[0][0]]
                        summary = top1_record.get('summary', '')
                        if not summary:
                            # å¦‚æœæ²¡æœ‰summaryå­—æ®µï¼Œä½¿ç”¨contextå‰200å­—ç¬¦
                            summary = context[:200] + "..." if len(context) > 200 else context
                    else:
                        summary = context[:200] + "..." if len(context) > 200 else context
                    
                    prompt = template_loader.format_template(
                        "multi_stage_chinese_template",
                        context=context, 
                        query=query,
                        summary=summary
                    )
                else:
                    # è‹±æ–‡promptæ¨¡æ¿
                    prompt = template_loader.format_template(
                        "multi_stage_english_template",
                        context=context, 
                        query=query
                    )
                
                if prompt is None:
                    # å›é€€åˆ°ç®€å•prompt
                    if self.dataset_type == "chinese":
                        prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\n{context}\n\né—®é¢˜ï¼š{query}\n\nå›ç­”ï¼š"
                    else:
                        prompt = f"Context: {context}\nQuestion: {query}\nAnswer:"
                
                # ===== è¯¦ç»†çš„Promptè°ƒè¯•ä¿¡æ¯ =====
                print("\n" + "="*80)
                print("ğŸ” PROMPTè°ƒè¯•ä¿¡æ¯")
                print("="*80)
                print(f"ğŸ“ æ¨¡æ¿åç§°: {'multi_stage_chinese_template' if self.dataset_type == 'chinese' else 'multi_stage_english_template'}")
                print(f"ğŸ“ å®Œæ•´Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
                print(f"ğŸ“‹ åŸå§‹æŸ¥è¯¢: '{query}'")
                print(f"ğŸ“‹ æŸ¥è¯¢é•¿åº¦: {len(query)} å­—ç¬¦")
                print(f"ğŸ“„ ä¸Šä¸‹æ–‡é•¿åº¦: {len(context)} å­—ç¬¦")
                print(f"ğŸ“„ ä¸Šä¸‹æ–‡å‰200å­—ç¬¦: '{context[:200]}...'")
                print(f"ğŸ“„ ä¸Šä¸‹æ–‡å200å­—ç¬¦: '...{context[-200:]}'")
                
                # æ£€æŸ¥Promptæ˜¯å¦è¢«æˆªæ–­
                if len(prompt) > 10000:
                    print("âš ï¸  WARNING: Prompté•¿åº¦è¶…è¿‡10000å­—ç¬¦ï¼Œå¯èƒ½è¢«æˆªæ–­")
                else:
                    print("âœ… Prompté•¿åº¦æ­£å¸¸")
                
                # æ£€æŸ¥æŸ¥è¯¢æ˜¯å¦åœ¨Promptä¸­
                if query in prompt:
                    print("âœ… æŸ¥è¯¢æ­£ç¡®åŒ…å«åœ¨Promptä¸­")
                else:
                    print("âŒ æŸ¥è¯¢æœªåœ¨Promptä¸­æ‰¾åˆ°ï¼")
                    print(f"   æœŸæœ›çš„æŸ¥è¯¢: '{query}'")
                    print(f"   Promptä¸­çš„æŸ¥è¯¢éƒ¨åˆ†: '{prompt.split('é—®é¢˜ï¼š')[-1].split('å›ç­”ï¼š')[0] if 'é—®é¢˜ï¼š' in prompt else 'NOT_FOUND'}'")
                
                # æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦åœ¨Promptä¸­
                if context[:100] in prompt:
                    print("âœ… ä¸Šä¸‹æ–‡æ­£ç¡®åŒ…å«åœ¨Promptä¸­")
                else:
                    print("âŒ ä¸Šä¸‹æ–‡æœªåœ¨Promptä¸­æ‰¾åˆ°ï¼")
                
                print("\n" + "="*80)
                print("ğŸ“¤ å‘é€ç»™LLMçš„å®Œæ•´Prompt:")
                print("="*80)
                print(prompt)
                print("="*80)
                print("ğŸ“¤ Promptç»“æŸ")
                print("="*80 + "\n")
                
                # ç”Ÿæˆç­”æ¡ˆ
                answer = self.llm_generator.generate(texts=[prompt])[0]
                
                # ===== ç­”æ¡ˆè°ƒè¯•ä¿¡æ¯ =====
                print("\n" + "="*80)
                print("ğŸ“¥ LLMç”Ÿæˆçš„ç­”æ¡ˆ:")
                print("="*80)
                print(answer)
                print("="*80)
                print("ğŸ“¥ ç­”æ¡ˆç»“æŸ")
                print("="*80 + "\n")
                
                return answer
            except Exception as e:
                print(f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {e}")
                return "ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ã€‚"
        else:
            return "æœªé…ç½®LLMç”Ÿæˆå™¨ã€‚"
    
    def search(self, 
               query: str,
               company_name: Optional[str] = None,
               stock_code: Optional[str] = None,
               report_date: Optional[str] = None,
               top_k: int = 20) -> Dict:
        """
        å®Œæ•´çš„å¤šé˜¶æ®µæ£€ç´¢æµç¨‹
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            company_name: å…¬å¸åç§°ï¼ˆå¯é€‰ï¼Œä»…ä¸­æ–‡æ•°æ®ï¼‰
            stock_code: è‚¡ç¥¨ä»£ç ï¼ˆå¯é€‰ï¼Œä»…ä¸­æ–‡æ•°æ®ï¼‰
            report_date: æŠ¥å‘Šæ—¥æœŸï¼ˆå¯é€‰ï¼Œä»…ä¸­æ–‡æ•°æ®ï¼‰
            top_k: è¿”å›å‰kä¸ªç»“æœ
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        print(f"\nå¼€å§‹å¤šé˜¶æ®µæ£€ç´¢...")
        print(f"æŸ¥è¯¢: {query}")
        print(f"æ•°æ®é›†ç±»å‹: {self.dataset_type}")
        
        if self.dataset_type == "chinese":
            if company_name:
                print(f"å…¬å¸åç§°: {company_name}")
            if stock_code:
                print(f"è‚¡ç¥¨ä»£ç : {stock_code}")
            if report_date:
                print(f"æŠ¥å‘Šæ—¥æœŸ: {report_date}")
        else:
            print("è‹±æ–‡æ•°æ®é›†ï¼Œä¸æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤")
        
        # ä½¿ç”¨é…ç½®çš„æ£€ç´¢å‚æ•°
        retrieval_top_k = 100
        rerank_top_k = top_k
        
        if self.config:
            retrieval_top_k = self.config.retriever.retrieval_top_k
            rerank_top_k = self.config.retriever.rerank_top_k
        
        # 1. Pre-filteringï¼ˆä»…ä¸­æ–‡æ•°æ®æ”¯æŒï¼‰
        candidate_indices = self.pre_filter(company_name, stock_code, report_date)
        print(f"é¢„è¿‡æ»¤ç»“æœ: {len(candidate_indices)} ä¸ªå€™é€‰æ–‡æ¡£")
        
        # å¦‚æœé¢„è¿‡æ»¤æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ–‡æ¡£ï¼Œå›é€€åˆ°å…¨é‡FAISSæ£€ç´¢
        if len(candidate_indices) == 0:
            print("é¢„è¿‡æ»¤æ— ç»“æœï¼Œå›é€€åˆ°å…¨é‡FAISSæ£€ç´¢...")
            candidate_indices = list(range(len(self.data)))
            print(f"å›é€€åˆ°å…¨é‡æ£€ç´¢ï¼Œå€™é€‰æ–‡æ¡£æ•°: {len(candidate_indices)}")
        
        # 2. FAISSæ£€ç´¢
        faiss_results = self.faiss_search(query, candidate_indices, top_k=min(retrieval_top_k, len(candidate_indices)))
        print(f"FAISSæ£€ç´¢ç»“æœ: {len(faiss_results)} ä¸ªæ–‡æ¡£")
        
        # 3. Qwen Reranker
        print("å¼€å§‹é‡æ’åº...")
        final_results = self.rerank(query, faiss_results, top_k=rerank_top_k)
        print(f"é‡æ’åºå®Œæˆ: {len(final_results)} ä¸ªchunks")
        print("é‡æ’åºå™¨å¤„ç†å®Œæˆ")
        
        # 4. LLMç­”æ¡ˆç”Ÿæˆ - å°†é‡æ’åºåçš„Top-K1ä¸ªchunksæ‹¼æ¥ä½œä¸ºä¸Šä¸‹æ–‡
        llm_answer = self.generate_answer(query, final_results, top_k_for_context=5)
        
        # 5. æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for idx, faiss_score, combined_score in final_results:
            record = self.data[idx]
            
            # æ ¹æ®æ•°æ®é›†ç±»å‹é€‰æ‹©ä¸åŒçš„å­—æ®µ
            if hasattr(record, 'content'):
                # DocumentWithMetadataæ ¼å¼
                result = {
                    'index': idx,
                    'faiss_score': faiss_score,
                    'combined_score': combined_score,
                    'content': record.content[:200] + '...' if len(record.content) > 200 else record.content,
                    'source': record.metadata.source if hasattr(record.metadata, 'source') else 'unknown',
                    'language': record.metadata.language if hasattr(record.metadata, 'language') else 'unknown'
                }
            else:
                # å­—å…¸æ ¼å¼
                if self.dataset_type == "chinese":
                    # ä¸­æ–‡æ•°æ®ï¼šä½¿ç”¨original_context
                    context = record.get('original_context', '')
                    result = {
                        'index': idx,
                        'faiss_score': faiss_score,
                        'combined_score': combined_score,
                        'context': context[:200] + '...' if len(context) > 200 else context,
                        'company_name': record.get('company_name', ''),
                        'stock_code': record.get('stock_code', ''),
                        'report_date': record.get('report_date', ''),
                        'summary': record.get('summary', '')[:200] + '...' if len(record.get('summary', '')) > 200 else record.get('summary', ''),
                        'generated_question': record.get('generated_question', ''),
                        'original_question': record.get('original_question', ''),
                        'original_answer': record.get('original_answer', '')
                    }
                else:
                    # è‹±æ–‡æ•°æ®ï¼šä½¿ç”¨context
                    context = record.get('context', '') or record.get('content', '')
                    result = {
                        'index': idx,
                        'faiss_score': faiss_score,
                        'combined_score': combined_score,
                        'context': context[:200] + '...' if len(context) > 200 else context,
                        'question': record.get('question', ''),
                        'answer': record.get('answer', '')
                    }
            
            formatted_results.append(result)
        
        # æ·»åŠ LLMç”Ÿæˆçš„ç­”æ¡ˆåˆ°ç»“æœä¸­
        final_output = {
            'retrieved_documents': formatted_results,
            'llm_answer': llm_answer,
            'query': query,
            'total_documents': len(formatted_results)
        }
        
        print(f"æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(formatted_results)} æ¡ç»“æœ")
        print(f"LLMç­”æ¡ˆç”Ÿæˆå®Œæˆ")
        return final_output
    
    def save_index(self, output_dir: Path):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜FAISSç´¢å¼•
        if self.faiss_index:
            faiss.write_index(self.faiss_index, str(output_dir / "faiss_index.bin"))
        
        # ä¿å­˜å…ƒæ•°æ®ç´¢å¼•ï¼ˆä»…ä¸­æ–‡æ•°æ®ï¼‰
        if self.dataset_type == "chinese":
            with open(output_dir / "metadata_index.pkl", 'wb') as f:
                pickle.dump(self.metadata_index, f)
        
        # ä¿å­˜æœ‰æ•ˆç´¢å¼•æ˜ å°„
        with open(output_dir / "valid_indices.pkl", 'wb') as f:
            pickle.dump(self.valid_indices, f)
        
        # ä¿å­˜æ•°æ®é›†ç±»å‹ä¿¡æ¯
        with open(output_dir / "dataset_info.json", 'w') as f:
            json.dump({
                'dataset_type': self.dataset_type,
                'model_name': self.model_name
            }, f, indent=2)
        
        print(f"ç´¢å¼•å·²ä¿å­˜åˆ°: {output_dir}")
    
    def load_index(self, index_dir: Path):
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•"""
        # åŠ è½½æ•°æ®é›†ä¿¡æ¯
        info_path = index_dir / "dataset_info.json"
        if info_path.exists():
            with open(info_path, 'r') as f:
                info = json.load(f)
                self.dataset_type = info.get('dataset_type', 'chinese')
                self.model_name = info.get('model_name', 'all-MiniLM-L6-v2')
        
        # åŠ è½½FAISSç´¢å¼•
        faiss_path = index_dir / "faiss_index.bin"
        if faiss_path.exists():
            self.faiss_index = faiss.read_index(str(faiss_path))
        
        # åŠ è½½å…ƒæ•°æ®ç´¢å¼•ï¼ˆä»…ä¸­æ–‡æ•°æ®ï¼‰
        if self.dataset_type == "chinese":
            metadata_path = index_dir / "metadata_index.pkl"
            if metadata_path.exists():
                with open(metadata_path, 'rb') as f:
                    self.metadata_index = pickle.load(f)
        
        # åŠ è½½æœ‰æ•ˆç´¢å¼•æ˜ å°„
        valid_indices_path = index_dir / "valid_indices.pkl"
        if valid_indices_path.exists():
            with open(valid_indices_path, 'rb') as f:
                self.valid_indices = pickle.load(f)
        
        print(f"ç´¢å¼•å·²ä» {index_dir} åŠ è½½")
        print(f"æ•°æ®é›†ç±»å‹: {self.dataset_type}")

    def extract_relevant_context(self, query: str, candidate_results: List[Tuple[int, float, float]], max_chars: int = 2000) -> str:
        """
        å¯¹Top1æ–‡æ¡£æ™ºèƒ½æå–ç›¸å…³ä¸Šä¸‹æ–‡
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            candidate_results: å€™é€‰ç»“æœåˆ—è¡¨
            max_chars: æœ€å¤§å­—ç¬¦æ•°é™åˆ¶
            
        Returns:
            Top1æ–‡æ¡£æ™ºèƒ½æå–çš„ç›¸å…³ä¸Šä¸‹æ–‡
        """
        print(f"ğŸ” å¼€å§‹å¯¹Top1æ–‡æ¡£æ™ºèƒ½æå–ç›¸å…³ä¸Šä¸‹æ–‡...")
        print(f"ğŸ“‹ æŸ¥è¯¢: {query}")
        print(f"ğŸ“Š å€™é€‰æ–‡æ¡£æ•°: {len(candidate_results)}")
        
        if not candidate_results:
            print("âŒ æ²¡æœ‰å€™é€‰ç»“æœ")
            return ""
        
        # è·å–Top1æ–‡æ¡£
        top1_idx, top1_faiss_score, top1_reranker_score = candidate_results[0]
        
        if top1_idx >= len(self.data):
            print(f"âŒ Top1æ–‡æ¡£ç´¢å¼•è¶…å‡ºèŒƒå›´: {top1_idx}")
            return ""
        
        record = self.data[top1_idx]
        print(f"âœ… ä½¿ç”¨Top1æ–‡æ¡£ (ç´¢å¼•: {top1_idx}, FAISSåˆ†æ•°: {top1_faiss_score:.4f}, é‡æ’åºåˆ†æ•°: {top1_reranker_score:.4f})")
        
        # è·å–Top1æ–‡æ¡£çš„å®Œæ•´context
        if self.dataset_type == "chinese":
            full_context = record.get('original_context', '')
            if not full_context:
                full_context = record.get('summary', '')
        else:
            full_context = record.get('context', '') or record.get('content', '')
        
        if not full_context:
            print("âŒ Top1æ–‡æ¡£æ²¡æœ‰contextå†…å®¹")
            return ""
        
        print(f"ğŸ“„ Top1æ–‡æ¡£å®Œæ•´contexté•¿åº¦: {len(full_context)} å­—ç¬¦")
        
        # å¯¹Top1æ–‡æ¡£è¿›è¡Œæ™ºèƒ½æå–
        # æå–æŸ¥è¯¢å…³é”®è¯
        query_keywords = self._extract_keywords(query)
        print(f"ğŸ”‘ æŸ¥è¯¢å…³é”®è¯: {query_keywords}")
        
        # æ™ºèƒ½æå–ç›¸å…³å¥å­
        relevant_sentences = self._extract_relevant_sentences(full_context, query_keywords, max_chars_per_doc=max_chars)
        
        # æ‹¼æ¥ä¸Šä¸‹æ–‡
        context = "\n\n".join(relevant_sentences)
        
        print(f"âœ… Top1æ–‡æ¡£æ™ºèƒ½æå–å®Œæˆ:")
        print(f"   ğŸ“ åŸå§‹é•¿åº¦: {len(full_context)} å­—ç¬¦")
        print(f"   ğŸ“ æå–åé•¿åº¦: {len(context)} å­—ç¬¦")
        print(f"   ğŸ“„ å¥å­æ•°: {len(relevant_sentences)}")
        print(f"   ğŸ“ å‰100å­—ç¬¦: {context[:100]}...")
        
        return context
    
    def _extract_keywords(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        keywords = []
        
        # æå–è‚¡ç¥¨ä»£ç 
        import re
        stock_pattern = r'[A-Z]{2}\d{4}|[A-Z]{2}\d{6}|\d{6}'
        stock_matches = re.findall(stock_pattern, query)
        keywords.extend(stock_matches)
        
        # æå–å…¬å¸åç§°
        company_pattern = r'([A-Za-z\u4e00-\u9fff]+)(?:å…¬å¸|é›†å›¢|è‚¡ä»½|æœ‰é™)'
        company_matches = re.findall(company_pattern, query)
        keywords.extend(company_matches)
        
        # æå–å¹´ä»½
        year_pattern = r'20\d{2}å¹´'
        year_matches = re.findall(year_pattern, query)
        keywords.extend(year_matches)
        
        # æå–å…³é”®æ¦‚å¿µ
        key_concepts = ['åˆ©æ¶¦', 'è¥æ”¶', 'å¢é•¿', 'ä¸šç»©', 'é¢„æµ‹', 'åŸå› ', 'ä¸»è¦', 'æŒç»­']
        for concept in key_concepts:
            if concept in query:
                keywords.append(concept)
        
        return list(set(keywords))
    
    def _extract_relevant_sentences(self, content: str, keywords: List[str], max_chars_per_doc: int = 800) -> List[str]:
        """ä»æ–‡æ¡£ä¸­æå–ä¸å…³é”®è¯æœ€ç›¸å…³çš„å¥å­"""
        if not content or not keywords:
            return []
        
        # æŒ‰å¥å­åˆ†å‰²
        import re
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', content)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # è®¡ç®—æ¯ä¸ªå¥å­çš„ç›¸å…³æ€§åˆ†æ•°
        sentence_scores = []
        for sentence in sentences:
            score = 0
            for keyword in keywords:
                if keyword in sentence:
                    score += 1
            # è€ƒè™‘å¥å­é•¿åº¦ï¼Œé¿å…è¿‡é•¿çš„å¥å­
            if len(sentence) > 200:
                score *= 0.5
            sentence_scores.append((sentence, score))
        
        # æŒ‰åˆ†æ•°æ’åº
        sentence_scores.sort(key=lambda x: x[1], reverse=True)
        
        # é€‰æ‹©æœ€ç›¸å…³çš„å¥å­
        selected_sentences = []
        total_chars = 0
        
        for sentence, score in sentence_scores:
            if score > 0 and total_chars + len(sentence) <= max_chars_per_doc:
                selected_sentences.append(sentence)
                total_chars += len(sentence)
        
        return selected_sentences

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºå¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿ"""
    # æ•°æ®æ–‡ä»¶è·¯å¾„
    data_path = Path("data/alphafin/alphafin_merged_generated_qa.json")
    index_dir = Path("data/alphafin/retrieval_index")
    
    # åˆå§‹åŒ–æ£€ç´¢ç³»ç»Ÿï¼ˆä¸­æ–‡æ•°æ®ï¼‰
    print("æ­£åœ¨åˆå§‹åŒ–å¤šé˜¶æ®µæ£€ç´¢ç³»ç»Ÿï¼ˆä¸­æ–‡æ•°æ®ï¼‰...")
    retrieval_system = MultiStageRetrievalSystem(data_path, dataset_type="chinese")
    
    # ä¿å­˜ç´¢å¼•ï¼ˆå¯é€‰ï¼‰
    retrieval_system.save_index(index_dir)
    
    # æ¼”ç¤ºæ£€ç´¢
    print("\n" + "="*50)
    print("æ£€ç´¢æ¼”ç¤º")
    print("="*50)
    
    # ç¤ºä¾‹æŸ¥è¯¢1ï¼šåŸºäºå…¬å¸åç§°çš„æ£€ç´¢ï¼ˆä»…ä¸­æ–‡æ•°æ®æ”¯æŒï¼‰
    print("\nç¤ºä¾‹1: åŸºäºå…¬å¸åç§°çš„æ£€ç´¢")
    results1 = retrieval_system.search(
        query="å…¬å¸ä¸šç»©è¡¨ç°å¦‚ä½•ï¼Ÿ",
        company_name="ä¸­å›½å®æ­¦",
        top_k=5
    )
    
    for i, result in enumerate(results1['retrieved_documents']):
        print(f"\nç»“æœ {i+1}:")
        print(f"  å…¬å¸: {result['company_name']}")
        print(f"  è‚¡ç¥¨ä»£ç : {result['stock_code']}")
        print(f"  æ‘˜è¦: {result['summary']}")
        print(f"  ç›¸ä¼¼åº¦åˆ†æ•°: {result['combined_score']:.4f}")
    
    # ç¤ºä¾‹æŸ¥è¯¢2ï¼šé€šç”¨æ£€ç´¢
    print("\nç¤ºä¾‹2: é€šç”¨æ£€ç´¢ï¼ˆæ— å…ƒæ•°æ®è¿‡æ»¤ï¼‰")
    results2 = retrieval_system.search(
        query="é’¢é“è¡Œä¸šå‘å±•è¶‹åŠ¿",
        top_k=5
    )
    
    for i, result in enumerate(results2['retrieved_documents']):
        print(f"\nç»“æœ {i+1}:")
        print(f"  å…¬å¸: {result['company_name']}")
        print(f"  è‚¡ç¥¨ä»£ç : {result['stock_code']}")
        print(f"  æ‘˜è¦: {result['summary']}")
        print(f"  ç›¸ä¼¼åº¦åˆ†æ•°: {result['combined_score']:.4f}")

if __name__ == '__main__':
    main() 