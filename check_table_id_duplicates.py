#!/usr/bin/env python3
"""
æ£€æŸ¥çŸ¥è¯†åº“ä¸­ç›¸åŒTable IDä½†ä¸åŒcontextçš„æƒ…å†µ
"""

import json
import re
from collections import defaultdict
from pathlib import Path

def extract_table_id(context: str) -> str:
    """ä»contextä¸­æå–Table ID"""
    if "Table ID:" in context:
        match = re.search(r'Table ID:\s*([a-f0-9-]+)', context)
        if match:
            return match.group(1)
    return ""

def extract_paragraph_id(context: str) -> str:
    """ä»contextä¸­æå–Paragraph ID"""
    if "Paragraph ID:" in context:
        match = re.search(r'Paragraph ID:\s*([a-f0-9-]+)', context)
        if match:
            return match.group(1)
    return ""

def check_table_id_duplicates():
    """æ£€æŸ¥çŸ¥è¯†åº“ä¸­ç›¸åŒTable IDä½†ä¸åŒcontextçš„æƒ…å†µ"""
    
    knowledge_base_file = "data/unified/tatqa_knowledge_base_combined.jsonl"
    
    if not Path(knowledge_base_file).exists():
        print(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {knowledge_base_file}")
        return
    
    print("ğŸ” æ£€æŸ¥çŸ¥è¯†åº“ä¸­ç›¸åŒTable IDä½†ä¸åŒcontextçš„æƒ…å†µ...")
    
    # è¯»å–çŸ¥è¯†åº“
    documents = []
    with open(knowledge_base_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            if line.strip():
                try:
                    doc = json.loads(line)
                    documents.append(doc)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
                    continue
    
    print(f"ğŸ“Š çŸ¥è¯†åº“æ€»æ–‡æ¡£æ•°: {len(documents)}")
    
    # æŒ‰Table IDåˆ†ç»„
    table_id_groups = defaultdict(list)
    paragraph_id_groups = defaultdict(list)
    no_id_docs = []
    
    for doc in documents:
        context = doc.get('context', '')
        if context:
            table_id = extract_table_id(context)
            paragraph_id = extract_paragraph_id(context)
            
            if table_id:
                table_id_groups[table_id].append(doc)
            elif paragraph_id:
                paragraph_id_groups[paragraph_id].append(doc)
            else:
                no_id_docs.append(doc)
    
    print(f"\nğŸ“‹ æ–‡æ¡£åˆ†ç±»ç»Ÿè®¡:")
    print(f"   - åŒ…å«Table IDçš„æ–‡æ¡£: {sum(len(docs) for docs in table_id_groups.values())}")
    print(f"   - åŒ…å«Paragraph IDçš„æ–‡æ¡£: {sum(len(docs) for docs in paragraph_id_groups.values())}")
    print(f"   - æ— IDçš„æ–‡æ¡£: {len(no_id_docs)}")
    
    # æ£€æŸ¥Table IDé‡å¤
    table_id_duplicates = {table_id: docs for table_id, docs in table_id_groups.items() if len(docs) > 1}
    
    print(f"\nğŸ” Table IDé‡å¤ç»Ÿè®¡:")
    print(f"   - å”¯ä¸€Table IDæ•°é‡: {len(table_id_groups)}")
    print(f"   - æœ‰é‡å¤çš„Table IDæ•°é‡: {len(table_id_duplicates)}")
    print(f"   - Table IDé‡å¤æ–‡æ¡£æ€»æ•°: {sum(len(docs) for docs in table_id_duplicates.values())}")
    
    if table_id_duplicates:
        print(f"\nğŸ” Table IDé‡å¤è¯¦æƒ…:")
        for i, (table_id, docs) in enumerate(list(table_id_duplicates.items())[:10]):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"\né‡å¤Table ID {i+1}: {table_id} (å…±{len(docs)}ä¸ªæ–‡æ¡£)")
            
            for j, doc in enumerate(docs):
                doc_id = doc.get('doc_id', '')
                source = doc.get('source', '')
                context = doc.get('context', '')
                
                print(f"  æ–‡æ¡£ {j+1}: {doc_id} ({source})")
                print(f"    Contexté¢„è§ˆ: {context[:100]}...")
                
                # æ£€æŸ¥contextæ˜¯å¦çœŸçš„ä¸åŒ
                if j > 0:
                    prev_context = docs[j-1].get('context', '')
                    if context == prev_context:
                        print(f"    âš ï¸ ä¸å‰ä¸€æ–‡æ¡£contextç›¸åŒ")
                    else:
                        print(f"    âœ… ä¸å‰ä¸€æ–‡æ¡£contextä¸åŒ")
    
    # æ£€æŸ¥Paragraph IDé‡å¤
    paragraph_id_duplicates = {paragraph_id: docs for paragraph_id, docs in paragraph_id_groups.items() if len(docs) > 1}
    
    print(f"\nğŸ” Paragraph IDé‡å¤ç»Ÿè®¡:")
    print(f"   - å”¯ä¸€Paragraph IDæ•°é‡: {len(paragraph_id_groups)}")
    print(f"   - æœ‰é‡å¤çš„Paragraph IDæ•°é‡: {len(paragraph_id_duplicates)}")
    print(f"   - Paragraph IDé‡å¤æ–‡æ¡£æ€»æ•°: {sum(len(docs) for docs in paragraph_id_duplicates.values())}")
    
    if paragraph_id_duplicates:
        print(f"\nğŸ” Paragraph IDé‡å¤è¯¦æƒ…:")
        for i, (paragraph_id, docs) in enumerate(list(paragraph_id_duplicates.items())[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"\né‡å¤Paragraph ID {i+1}: {paragraph_id} (å…±{len(docs)}ä¸ªæ–‡æ¡£)")
            
            for j, doc in enumerate(docs):
                doc_id = doc.get('doc_id', '')
                source = doc.get('source', '')
                context = doc.get('context', '')
                
                print(f"  æ–‡æ¡£ {j+1}: {doc_id} ({source})")
                print(f"    Contexté¢„è§ˆ: {context[:100]}...")
    
    # æ€»ç»“
    print(f"\nğŸ“Š æ€»ç»“:")
    total_duplicates = len(table_id_duplicates) + len(paragraph_id_duplicates)
    if total_duplicates > 0:
        print(f"   âŒ å‘ç° {total_duplicates} ä¸ªé‡å¤IDç»„")
        print(f"   - Table IDé‡å¤: {len(table_id_duplicates)} ç»„")
        print(f"   - Paragraph IDé‡å¤: {len(paragraph_id_duplicates)} ç»„")
        print(f"   è¿™å¯èƒ½æ˜¯UIä¸­æ˜¾ç¤ºé‡å¤å†…å®¹çš„åŸå› ")
    else:
        print(f"   âœ… æ²¡æœ‰å‘ç°é‡å¤ID")
    
    return table_id_duplicates, paragraph_id_duplicates

if __name__ == "__main__":
    check_table_id_duplicates() 