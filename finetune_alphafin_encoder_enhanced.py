#!/usr/bin/env python3
"""
AlphaFinä¸­æ–‡ç¼–ç å™¨å¾®è°ƒè„šæœ¬ (å¢å¼ºç‰ˆ)
ä½¿ç”¨generated_questionä½œä¸ºqueryï¼Œsummaryä½œä¸ºcontextè¿›è¡Œå¾®è°ƒ
"""

import os
import json
import argparse
import torch
import numpy as np
from sentence_transformers import SentenceTransformer, InputExample, losses
from sentence_transformers.evaluation import SentenceEvaluator
from torch.utils.data import DataLoader
from tqdm import tqdm
import csv

class MRREvaluator(SentenceEvaluator):
    """
    å¯¹ç»™å®šçš„ (generated_question, summary) æ•°æ®é›†è®¡ç®— Mean Reciprocal Rank (MRR)
    ä½¿ç”¨doc_idè¿›è¡Œæ­£ç¡®çš„åŒ¹é…
    """
    def __init__(self, dataset, name='', show_progress_bar=False, write_csv=True):
        self.dataset = dataset
        self.name = name
        self.show_progress_bar = show_progress_bar
        self.write_csv = write_csv

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        if dataset and len(dataset) > 0:
            print(f"è°ƒè¯•ï¼šç¬¬ä¸€ä¸ªæ•°æ®é¡¹å­—æ®µ: {list(dataset[0].keys())}")
            if 'query' not in dataset[0]:
                print(f"é”™è¯¯ï¼šæ•°æ®é¡¹ç¼ºå°‘'query'å­—æ®µï¼Œå¯ç”¨å­—æ®µ: {list(dataset[0].keys())}")
                raise KeyError("æ•°æ®é¡¹ç¼ºå°‘'query'å­—æ®µ")

        # ç¡®ä¿æ•°æ®å­—æ®µå­˜åœ¨
        self.queries = [item['query'] for item in dataset]
        self.contexts = [item['context'] for item in dataset]
        self.answers = [item['answer'] for item in dataset] 

        self.csv_file: str = ""
        self.csv_headers = ["epoch", "steps", "MRR"]

    def __call__(self, model, output_path: str = None, epoch: int = -1, steps: int = -1) -> float:
        if epoch != -1:
            if self.write_csv:
                self.csv_file = os.path.join(output_path, self.name + "_mrr_evaluation_results.csv")
                if not os.path.isfile(self.csv_file) or epoch == 0:
                    with open(self.csv_file, newline="", mode="w", encoding="utf-8") as f:
                        writer = csv.writer(f)
                        writer.writerow(self.csv_headers)
                        
        print(f"\n--- å¼€å§‹ MRR è¯„ä¼° (Epoch: {epoch}, Steps: {steps}) ---")

        if not self.dataset:
            print("è­¦å‘Šï¼šè¯„ä¼°æ•°æ®é›†ä¸ºç©ºï¼ŒMRRä¸º0ã€‚")
            mrr = 0.0
        else:
            print(f"ç¼–ç  {len(self.contexts)} ä¸ªè¯„ä¼°ä¸Šä¸‹æ–‡...")
            # ç¼–ç æ‰€æœ‰ä¸Šä¸‹æ–‡
            context_embeddings = model.encode(self.contexts, batch_size=64, convert_to_tensor=True,
                                              show_progress_bar=self.show_progress_bar)

            mrrs = []
            iterator = tqdm(self.dataset, desc='è¯„ä¼° MRR', disable=not self.show_progress_bar)
            
            # åˆ›å»ºdoc_idåˆ°ç´¢å¼•çš„æ˜ å°„
            doc_id_to_idx = {}
            for idx, item in enumerate(self.dataset):
                doc_id = item.get('doc_id') or str(idx)
                doc_id_to_idx[doc_id] = idx
            
            for i, item in enumerate(iterator):
                query_emb = model.encode(item['query'], convert_to_tensor=True)
                scores = torch.cosine_similarity(query_emb.unsqueeze(0), context_embeddings)[0].cpu().numpy()

                # ä½¿ç”¨doc_idæ‰¾åˆ°ç›®æ ‡ä¸Šä¸‹æ–‡çš„ç´¢å¼•
                target_doc_id = item.get('doc_id') or str(i)
                target_context_idx = doc_id_to_idx.get(target_doc_id, i)

                sorted_indices = np.argsort(scores)[::-1]
                
                rank = -1
                for r, idx in enumerate(sorted_indices):
                    if idx == target_context_idx:
                        rank = r + 1
                        break
                
                if rank != -1:
                    mrr_score = 1.0 / rank
                    mrrs.append(mrr_score)
                else:
                    mrrs.append(0.0) 
            
            mrr = np.mean(mrrs) if mrrs else 0.0 

        print(f"MRR (Epoch: {epoch}, Steps: {steps}): {mrr:.4f}")
        print(f"--- MRR è¯„ä¼°ç»“æŸ ---")

        if output_path is not None and self.write_csv:
            with open(self.csv_file, newline="", mode="a", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow([epoch, steps, round(mrr, 4)])

        return mrr

def load_training_data(jsonl_path, max_samples=None):
    """åŠ è½½è®­ç»ƒæ•°æ®ï¼Œä½¿ç”¨generated_questionä½œä¸ºqueryï¼Œsummaryä½œä¸ºcontext"""
    examples = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                item = json.loads(line)
                # ä½¿ç”¨generated_questionä½œä¸ºqueryï¼Œsummaryä½œä¸ºcontext
                query = item.get('generated_question', item.get('query', ''))
                context = item.get('summary', item.get('context', ''))
                
                if query and context:
                    examples.append(InputExample(texts=[query, context]))
                    
                    if max_samples and len(examples) >= max_samples:
                        break
    
    print(f"åŠ è½½äº† {len(examples)} ä¸ªæœ‰æ•ˆè®­ç»ƒæ ·æœ¬ã€‚")
    return examples

def load_eval_data(jsonl_path, max_samples=None):
    """åŠ è½½è¯„ä¼°æ•°æ®ï¼Œä½¿ç”¨generated_questionä½œä¸ºqueryï¼Œsummaryä½œä¸ºcontext"""
    data = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                item = json.loads(line)
                # ä½¿ç”¨generated_questionä½œä¸ºqueryï¼Œsummaryä½œä¸ºcontext
                query = item.get('generated_question', item.get('query', ''))
                context = item.get('summary', item.get('context', ''))
                answer = item.get('answer', '')
                doc_id = item.get('doc_id', '')
                
                if query and context:
                    data.append({
                        'query': query,
                        'context': context,
                        'answer': answer,
                        'doc_id': doc_id
                    })
                    
                    if max_samples and len(data) >= max_samples:
                        break
    
    print(f"åŠ è½½äº† {len(data)} ä¸ªæœ‰æ•ˆè¯„ä¼°æ ·æœ¬ã€‚")
    return data

def main():
    parser = argparse.ArgumentParser(description="AlphaFinä¸­æ–‡ç¼–ç å™¨å¾®è°ƒ (å¢å¼ºç‰ˆ)")
    parser.add_argument("--model_name", type=str, default="Langboat/mengzi-bert-base-fin",
                       help="åŸºç¡€æ¨¡å‹åç§°")
    parser.add_argument("--train_jsonl", type=str, default="evaluate_mrr/alphafin_train_qc.jsonl",
                       help="è®­ç»ƒæ•°æ®æ–‡ä»¶")
    parser.add_argument("--eval_jsonl", type=str, default="evaluate_mrr/alphafin_eval.jsonl",
                       help="è¯„ä¼°æ•°æ®æ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default="models/alphafin_encoder_finetuned",
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--batch_size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--epochs", type=int, default=5, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--max_samples", type=int, default=None, help="æœ€å¤§æ ·æœ¬æ•°")
    parser.add_argument("--eval_steps", type=int, default=500, help="è¯„ä¼°æ­¥æ•°")
    
    args = parser.parse_args()
    
    print("ğŸš€ AlphaFinä¸­æ–‡ç¼–ç å™¨å¾®è°ƒ (å¢å¼ºç‰ˆ)")
    print(f"ğŸ“Š é…ç½®:")
    print(f"  - åŸºç¡€æ¨¡å‹: {args.model_name}")
    print(f"  - è®­ç»ƒæ•°æ®: {args.train_jsonl}")
    print(f"  - è¯„ä¼°æ•°æ®: {args.eval_jsonl}")
    print(f"  - è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  - æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  - è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  - æœ€å¤§æ ·æœ¬æ•°: {args.max_samples}")
    print(f"  - è¯„ä¼°æ­¥æ•°: {args.eval_steps}")
    print(f"  - ä½¿ç”¨å­—æ®µ: generated_question -> summary")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    print(f"\nğŸ“– åŠ è½½è®­ç»ƒæ•°æ®ï¼š{args.train_jsonl}")
    train_examples = load_training_data(args.train_jsonl, args.max_samples)
    if not train_examples:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬")
        return

    # åŠ è½½è¯„ä¼°æ•°æ®
    print(f"ğŸ“– åŠ è½½è¯„ä¼°æ•°æ®ï¼š{args.eval_jsonl}")
    eval_data = load_eval_data(args.eval_jsonl, args.max_samples)
    if not eval_data:
        print("âŒ æ²¡æœ‰åŠ è½½åˆ°æœ‰æ•ˆçš„è¯„ä¼°æ ·æœ¬")
        evaluator = None
    else:
        evaluator = MRREvaluator(dataset=eval_data, name='mrr_eval', show_progress_bar=True)

    # åŠ è½½æ¨¡å‹
    print(f"\nğŸ¤– åŠ è½½æ¨¡å‹ï¼š{args.model_name}")
    try:
        model = SentenceTransformer(args.model_name)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # å‡†å¤‡è®­ç»ƒ
    print(f"\nğŸ¯ å‡†å¤‡è®­ç»ƒ:")
    print(f"  - è®­ç»ƒæ ·æœ¬æ•°: {len(train_examples)}")
    train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=args.batch_size)
    train_loss = losses.MultipleNegativesRankingLoss(model)
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒ...")
    model.fit(
        train_objectives=[(train_dataloader, train_loss)],
        epochs=args.epochs,
        evaluator=evaluator,
        evaluation_steps=args.eval_steps,
        output_path=args.output_dir,
        show_progress_bar=True,
        optimizer_params={'lr': 2e-5, 'weight_decay': 0.01},
        scheduler='WarmupCosine',
        warmup_steps=100
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    model.save(args.output_dir)
    print(f"\nâœ… å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°ï¼š{args.output_dir}")

if __name__ == "__main__":
    main() 