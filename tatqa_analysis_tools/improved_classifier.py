#!/usr/bin/env python3
"""
æ”¹è¿›çš„TAT-QAé—®é¢˜ç±»å‹åˆ†ç±»å™¨
æ›´ç²¾ç¡®åœ°åˆ¤æ–­tableã€textå’Œtable+textç±»å‹
"""

import re
import json
from typing import Dict, List, Tuple

def improved_classify_question_type(context: str) -> str:
    """
    æ”¹è¿›çš„é—®é¢˜ç±»å‹åˆ†ç±»æ–¹æ³•
    """
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼
    has_table = "Table ID:" in context
    
    if not has_table:
        return "text"
    
    # å¦‚æœæœ‰è¡¨æ ¼ï¼Œè¿›ä¸€æ­¥åˆ†æ
    table_pattern = r'Table ID: [^\n]+\nHeaders:'
    table_matches = list(re.finditer(table_pattern, context))
    
    if not table_matches:
        return "table"  # æœ‰Table IDä½†æ²¡æœ‰æ ‡å‡†æ ¼å¼ï¼Œä»ç®—ä½œtable
    
    # åˆ†æè¡¨æ ¼å‰åçš„æ–‡æœ¬
    table_start = table_matches[0].start()
    table_end = context.rfind('\n')  # å‡è®¾è¡¨æ ¼åˆ°æ–‡ä»¶æœ«å°¾
    
    # æ£€æŸ¥è¡¨æ ¼å‰çš„æ–‡æœ¬
    text_before = context[:table_start].strip()
    
    # æ£€æŸ¥è¡¨æ ¼åçš„æ–‡æœ¬ï¼ˆå¦‚æœæœ‰å¤šä¸ªè¡¨æ ¼ï¼Œæ£€æŸ¥æœ€åä¸€ä¸ªè¡¨æ ¼åçš„æ–‡æœ¬ï¼‰
    if len(table_matches) > 1:
        last_table_end = table_matches[-1].end()
        text_after = context[last_table_end:].strip()
    else:
        text_after = context[table_end:].strip()
    
    # åˆ¤æ–­æ˜¯å¦æœ‰æœ‰æ„ä¹‰çš„æ–‡æœ¬æ®µè½
    has_meaningful_text = False
    
    # æ£€æŸ¥è¡¨æ ¼å‰çš„æ–‡æœ¬
    if len(text_before) > 30:  # é™ä½é˜ˆå€¼ï¼Œ30å­—ç¬¦ä»¥ä¸Šç®—æœ‰æ„ä¹‰
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å®Œæ•´çš„å¥å­
        sentences = re.split(r'[.!?]+', text_before)
        if any(len(s.strip()) > 20 for s in sentences):
            has_meaningful_text = True
    
    # æ£€æŸ¥è¡¨æ ¼åçš„æ–‡æœ¬
    if len(text_after) > 30:
        sentences = re.split(r'[.!?]+', text_after)
        if any(len(s.strip()) > 20 for s in sentences):
            has_meaningful_text = True
    
    # æ£€æŸ¥è¡¨æ ¼ä¸­é—´æ˜¯å¦æœ‰æ–‡æœ¬ï¼ˆå¤šä¸ªè¡¨æ ¼ä¹‹é—´ï¼‰
    if len(table_matches) > 1:
        for i in range(len(table_matches) - 1):
            between_text = context[table_matches[i].end():table_matches[i+1].start()].strip()
            if len(between_text) > 30:
                sentences = re.split(r'[.!?]+', between_text)
                if any(len(s.strip()) > 20 for s in sentences):
                    has_meaningful_text = True
                    break
    
    if has_meaningful_text:
        return "table+text"
    else:
        return "table"

def analyze_sample_distribution(data_file: str) -> Dict[str, any]:
    """
    åˆ†ææ ·æœ¬åˆ†å¸ƒï¼Œä½¿ç”¨æ”¹è¿›çš„åˆ†ç±»å™¨
    """
    print(f"ğŸ” ä½¿ç”¨æ”¹è¿›åˆ†ç±»å™¨åˆ†æ: {data_file}")
    
    type_counts = {"table": 0, "text": 0, "table+text": 0}
    samples_by_type = {"table": [], "text": [], "table+text": []}
    
    with open(data_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                sample = json.loads(line.strip())
                context = sample["context"]
                question_type = improved_classify_question_type(context)
                
                type_counts[question_type] += 1
                samples_by_type[question_type].append({
                    "line": line_num,
                    "query": sample["query"][:100] + "...",
                    "context_preview": context[:200] + "...",
                    "doc_id": sample.get("doc_id", "unknown")
                })
                
            except Exception as e:
                print(f"âš ï¸ ç¬¬{line_num}è¡Œå¤„ç†é”™è¯¯: {e}")
                continue
    
    total = sum(type_counts.values())
    
    print(f"ğŸ“Š æ”¹è¿›åˆ†ç±»ç»“æœ:")
    for qtype, count in type_counts.items():
        percentage = (count / total * 100) if total > 0 else 0
        print(f"  {qtype}: {count} ({percentage:.1f}%)")
    
    # æ˜¾ç¤ºä¸€äº›table+textçš„ç¤ºä¾‹
    if samples_by_type["table+text"]:
        print(f"\nğŸ“‹ table+textç¤ºä¾‹ (å‰3ä¸ª):")
        for i, sample in enumerate(samples_by_type["table+text"][:3]):
            print(f"  ç¤ºä¾‹{i+1}:")
            print(f"    é—®é¢˜: {sample['query']}")
            print(f"    æ–‡æ¡£ID: {sample['doc_id']}")
            print(f"    Contexté¢„è§ˆ: {sample['context_preview']}")
            print()
    
    return {
        "type_counts": type_counts,
        "samples_by_type": samples_by_type,
        "total_samples": total
    }

def main():
    """ä¸»å‡½æ•°"""
    data_file = '../evaluate_mrr/tatqa_eval_enhanced.jsonl'
    
    print("ğŸš€ æ”¹è¿›çš„TAT-QAé—®é¢˜ç±»å‹åˆ†ç±»å™¨")
    print("="*50)
    
    # åˆ†æè¯„ä¼°é›†
    results = analyze_sample_distribution(data_file)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_file = "improved_classification_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main() 