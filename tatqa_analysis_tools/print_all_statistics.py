#!/usr/bin/env python3
"""
æ‰“å°æ‰€æœ‰æ•°æ®ç»Ÿè®¡ä¿¡æ¯çš„å®Œæ•´æŠ¥å‘Š
"""

import json
import statistics
from pathlib import Path

def analyze_alphafin_raw_data():
    """åˆ†æAlphaFinåŸå§‹æ•°æ®"""
    print("=== AlphaFinåŸå§‹æ•°æ®åˆ†æ ===\n")
    
    try:
        with open('data/alphafin/data.json', 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
        
        print(f"ğŸ“Š åŸå§‹æ ·æœ¬æ€»æ•°: {len(raw_data):,} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: 425M")
        print(f"ğŸ“‹ å­—æ®µåˆ—è¡¨: {list(raw_data[0].keys())}")
        
        # ç¤ºä¾‹è®°å½•
        print(f"\nğŸ“ ç¤ºä¾‹è®°å½•:")
        sample_record = raw_data[0]
        print(json.dumps(sample_record, ensure_ascii=False, indent=2)[:800] + "...")
        
        return len(raw_data)
        
    except Exception as e:
        print(f"âŒ è¯»å–AlphaFinåŸå§‹æ•°æ®å¤±è´¥: {e}")
        return 0

def analyze_alphafin_filtered_data():
    """åˆ†æAlphaFinè¿‡æ»¤åæ•°æ®"""
    print("\n=== AlphaFinè¿‡æ»¤åæ•°æ®åˆ†æ ===\n")
    
    try:
        with open('data/alphafin/alphafin_rag_ready_0627.json', 'r', encoding='utf-8') as f:
            filtered_data = json.load(f)
        
        print(f"ğŸ“Š è¿‡æ»¤åæ ·æœ¬æ•°: {len(filtered_data):,} ä¸ªæ ·æœ¬")
        
        # è®¡ç®—è¿‡æ»¤ç‡
        raw_count = 167362  # ä»åŸå§‹æ•°æ®è·å¾—
        filter_rate = (raw_count - len(filtered_data)) / raw_count * 100
        print(f"ğŸ—‘ï¸  è¿‡æ»¤ç‡: {filter_rate:.1f}% ({raw_count - len(filtered_data):,}/{raw_count:,})")
        
        return len(filtered_data)
        
    except Exception as e:
        print(f"âŒ è¯»å–AlphaFinè¿‡æ»¤åæ•°æ®å¤±è´¥: {e}")
        return 0

def analyze_alphafin_processed_data():
    """åˆ†æAlphaFin LLMå¤„ç†åæ•°æ®"""
    print("\n=== AlphaFin LLMå¤„ç†åæ•°æ®åˆ†æ ===\n")
    
    try:
        with open('data/alphafin/alphafin_final_clean.json', 'r', encoding='utf-8') as f:
            processed_data = json.load(f)
        
        print(f"ğŸ“Š LLMå¤„ç†åæ ·æœ¬æ•°: {len(processed_data):,} ä¸ªæ ·æœ¬")
        
        # å…ƒæ•°æ®ç»Ÿè®¡
        metadata_fields = ['company_name', 'stock_code', 'report_date']
        metadata_stats = {}
        total_with_metadata = 0
        
        for field in metadata_fields:
            count = sum(1 for item in processed_data 
                       if item.get(field) and str(item.get(field)).strip() 
                       and str(item.get(field)).lower() != 'none')
            percentage = count / len(processed_data) * 100
            metadata_stats[field] = (count, percentage)
            
            if count > 0:
                total_with_metadata += 1
        
        print(f"ğŸ“‹ å…ƒæ•°æ®è¦†ç›–ç‡:")
        for field, (count, percentage) in metadata_stats.items():
            print(f"   {field}: {count:,}/{len(processed_data):,} ({percentage:.1f}%)")
        
        overall_metadata_rate = total_with_metadata / len(metadata_fields) * 100
        print(f"ğŸ“Š æ€»ä½“å…ƒæ•°æ®è¦†ç›–ç‡: {overall_metadata_rate:.1f}%")
        
        # é•¿åº¦ç»Ÿè®¡
        context_lengths = [len(item.get('original_context', item.get('context', ''))) for item in processed_data]
        answer_lengths = [len(item.get('original_answer', item.get('answer', ''))) for item in processed_data]
        question_lengths = [len(item.get('original_question', item.get('query', ''))) for item in processed_data]
        
        print(f"\nğŸ“ é•¿åº¦ç»Ÿè®¡:")
        print(f"   Contextå¹³å‡é•¿åº¦: {statistics.mean(context_lengths):.1f} å­—ç¬¦")
        print(f"   Answerå¹³å‡é•¿åº¦: {statistics.mean(answer_lengths):.1f} å­—ç¬¦")
        print(f"   Questionå¹³å‡é•¿åº¦: {statistics.mean(question_lengths):.1f} å­—ç¬¦")
        print(f"   Contexté•¿åº¦èŒƒå›´: {min(context_lengths)} - {max(context_lengths)} å­—ç¬¦")
        print(f"   Answeré•¿åº¦èŒƒå›´: {min(answer_lengths)} - {max(answer_lengths)} å­—ç¬¦")
        print(f"   Questioné•¿åº¦èŒƒå›´: {min(question_lengths)} - {max(question_lengths)} å­—ç¬¦")
        
        return len(processed_data), metadata_stats
        
    except Exception as e:
        print(f"âŒ è¯»å–AlphaFinå¤„ç†åæ•°æ®å¤±è´¥: {e}")
        return 0, {}

def analyze_tatqa_raw_data():
    """åˆ†æTatQAåŸå§‹æ•°æ®"""
    print("\n=== TatQAåŸå§‹æ•°æ®åˆ†æ ===\n")
    
    try:
        # è¯»å–åŸå§‹æ•°æ®
        with open('data/tatqa_dataset_raw/tatqa_dataset_train.json', 'r') as f:
            train_data = json.load(f)
        with open('data/tatqa_dataset_raw/tatqa_dataset_dev.json', 'r') as f:
            dev_data = json.load(f)
        with open('data/tatqa_dataset_raw/tatqa_dataset_test.json', 'r') as f:
            test_data = json.load(f)
        
        total_original = len(train_data) + len(dev_data) + len(test_data)
        print(f"ğŸ“Š åŸå§‹æ ·æœ¬æ€»æ•°: {total_original:,} ä¸ªæ ·æœ¬")
        print(f"  è®­ç»ƒé›†: {len(train_data):,} ä¸ªæ ·æœ¬")
        print(f"  éªŒè¯é›†: {len(dev_data):,} ä¸ªæ ·æœ¬")
        print(f"  æµ‹è¯•é›†: {len(test_data):,} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: 18M")
        
        # ç»Ÿè®¡åŸå§‹é—®é¢˜æ•°é‡
        total_questions = 0
        for dataset in [train_data, dev_data, test_data]:
            for item in dataset:
                questions = item.get('questions', [])
                total_questions += len(questions)
        
        print(f"â“ åŸå§‹é—®é¢˜æ€»æ•°: {total_questions:,} ä¸ªé—®é¢˜")
        
        # ç¤ºä¾‹è®°å½•
        print(f"\nğŸ“ ç¤ºä¾‹è®°å½•:")
        sample_record = train_data[0]
        print(json.dumps(sample_record, ensure_ascii=False, indent=2)[:800] + "...")
        
        return total_original, total_questions
        
    except Exception as e:
        print(f"âŒ è¯»å–TatQAåŸå§‹æ•°æ®å¤±è´¥: {e}")
        return 0, 0

def analyze_tatqa_converted_data():
    """åˆ†æTatQAè½¬æ¢åæ•°æ®"""
    print("\n=== TatQAè½¬æ¢åæ•°æ®åˆ†æ ===\n")
    
    try:
        # è¯»å–è®­ç»ƒå’Œè¯„ä¼°æ•°æ®
        with open('evaluate_mrr/tatqa_train_qc_enhanced.jsonl', 'r') as f:
            train_converted = [json.loads(line) for line in f if line.strip()]
        with open('evaluate_mrr/tatqa_eval_enhanced.jsonl', 'r') as f:
            eval_converted = [json.loads(line) for line in f if line.strip()]
        
        total_converted = len(train_converted) + len(eval_converted)
        print(f"ğŸ“Š QCAè¯„ä¼°æ ·æœ¬æ€»æ•°: {total_converted:,} ä¸ªæ ·æœ¬")
        print(f"  è®­ç»ƒé›†: {len(train_converted):,} ä¸ªæ ·æœ¬")
        print(f"  è¯„ä¼°é›†: {len(eval_converted):,} ä¸ªæ ·æœ¬")
        
        # ç»Ÿè®¡answer_fromåˆ†å¸ƒï¼ˆä½¿ç”¨è¯„ä¼°é›†ï¼‰
        answer_from_stats = {}
        for item in eval_converted:
            answer_from = item.get('answer_from', 'unknown')
            answer_from_stats[answer_from] = answer_from_stats.get(answer_from, 0) + 1
        
        print(f"\nğŸ“‹ ç­”æ¡ˆæ¥æºåˆ†å¸ƒ (è¯„ä¼°é›†):")
        for source, count in answer_from_stats.items():
            percentage = count / len(eval_converted) * 100
            print(f"   {source}: {count:,} ({percentage:.1f}%)")
        
        # é•¿åº¦ç»Ÿè®¡ï¼ˆä½¿ç”¨è¯„ä¼°é›†ï¼‰
        context_lengths = [len(item.get('context', '')) for item in eval_converted]
        answer_lengths = [len(item.get('answer', '')) for item in eval_converted]
        question_lengths = [len(item.get('query', '')) for item in eval_converted]
        
        print(f"\nğŸ“ é•¿åº¦ç»Ÿè®¡ (è¯„ä¼°é›†):")
        print(f"   Contextå¹³å‡é•¿åº¦: {statistics.mean(context_lengths):.1f} å­—ç¬¦")
        print(f"   Answerå¹³å‡é•¿åº¦: {statistics.mean(answer_lengths):.1f} å­—ç¬¦")
        print(f"   Questionå¹³å‡é•¿åº¦: {statistics.mean(question_lengths):.1f} å­—ç¬¦")
        
        # å…ƒæ•°æ®è¦†ç›–ç‡
        doc_id_coverage = sum(1 for item in eval_converted if item.get('doc_id')) / len(eval_converted) * 100
        relevant_doc_coverage = sum(1 for item in eval_converted if item.get('relevant_doc_ids')) / len(eval_converted) * 100
        
        print(f"\nğŸ“Š å…ƒæ•°æ®è¦†ç›–ç‡:")
        print(f"   doc_idè¦†ç›–ç‡: {doc_id_coverage:.1f}%")
        print(f"   relevant_doc_idsè¦†ç›–ç‡: {relevant_doc_coverage:.1f}%")
        
        return total_converted, len(eval_converted)
        
    except Exception as e:
        print(f"âŒ è¯»å–TatQAè½¬æ¢åæ•°æ®å¤±è´¥: {e}")
        return 0, 0

def analyze_tatqa_knowledge_base():
    """åˆ†æTatQAçŸ¥è¯†åº“æ•°æ®"""
    print("\n=== TatQAçŸ¥è¯†åº“æ•°æ®åˆ†æ ===\n")
    
    try:
        # è¯»å–çŸ¥è¯†åº“æ•°æ®
        with open('data/unified/tatqa_knowledge_base_unified.jsonl', 'r') as f:
            kb_data = [json.loads(line) for line in f if line.strip()]
        
        print(f"ğŸ“Š çŸ¥è¯†åº“æ–‡æ¡£æ€»æ•°: {len(kb_data):,} ä¸ªæ–‡æ¡£")
        
        # ç»Ÿè®¡source_typeåˆ†å¸ƒ
        source_type_stats = {}
        for item in kb_data:
            source_type = item.get('source_type', 'unknown')
            source_type_stats[source_type] = source_type_stats.get(source_type, 0) + 1
        
        print(f"\nğŸ“‹ æ•°æ®æ¥æºåˆ†å¸ƒ:")
        for source, count in source_type_stats.items():
            percentage = count / len(kb_data) * 100
            print(f"   {source}: {count:,} ({percentage:.1f}%)")
        
        # ç»Ÿè®¡æ–‡æ¡£ç±»å‹ï¼ˆè¡¨æ ¼vsæ®µè½ï¼‰
        table_count = sum(1 for item in kb_data if 'Table ID:' in item.get('context', ''))
        paragraph_count = sum(1 for item in kb_data if 'Paragraph ID:' in item.get('context', ''))
        
        print(f"\nğŸ“Š æ–‡æ¡£ç±»å‹åˆ†å¸ƒ:")
        print(f"   è¡¨æ ¼æ–‡æ¡£: {table_count:,} ({table_count/len(kb_data)*100:.1f}%)")
        print(f"   æ®µè½æ–‡æ¡£: {paragraph_count:,} ({paragraph_count/len(kb_data)*100:.1f}%)")
        
        # é•¿åº¦ç»Ÿè®¡
        context_lengths = [len(item.get('context', '')) for item in kb_data]
        print(f"\nğŸ“ æ–‡æ¡£é•¿åº¦ç»Ÿè®¡:")
        print(f"   å¹³å‡é•¿åº¦: {statistics.mean(context_lengths):.1f} å­—ç¬¦")
        print(f"   é•¿åº¦èŒƒå›´: {min(context_lengths)} - {max(context_lengths)} å­—ç¬¦")
        
        return len(kb_data)
        
    except Exception as e:
        print(f"âŒ è¯»å–TatQAçŸ¥è¯†åº“æ•°æ®å¤±è´¥: {e}")
        return 0

def print_summary_report():
    """æ‰“å°æ€»ç»“æŠ¥å‘Š"""
    print("\n" + "="*80)
    print("ğŸ“Š å®Œæ•´æ•°æ®æ¦‚å†µæ€»ç»“æŠ¥å‘Š")
    print("="*80)
    
    # æ”¶é›†æ‰€æœ‰ç»Ÿè®¡æ•°æ®
    alphafin_raw = analyze_alphafin_raw_data()
    alphafin_filtered = analyze_alphafin_filtered_data()
    alphafin_processed, alphafin_metadata = analyze_alphafin_processed_data()
    tatqa_raw, tatqa_questions = analyze_tatqa_raw_data()
    tatqa_converted, tatqa_eval = analyze_tatqa_converted_data()
    tatqa_kb = analyze_tatqa_knowledge_base()
    
    # è®¡ç®—è½¬æ¢ç‡
    tatqa_conversion_rate = tatqa_converted / tatqa_questions * 100 if tatqa_questions > 0 else 0
    alphafin_filter_rate = (alphafin_raw - alphafin_filtered) / alphafin_raw * 100 if alphafin_raw > 0 else 0
    
    print("\n" + "="*80)
    print("ğŸ“‹ æœ€ç»ˆç»Ÿè®¡æ€»ç»“")
    print("="*80)
    
    print("\nâ— 1.1 åŸå§‹æ•°æ®æ¦‚å†µ (Raw Data Overview):")
    print(f"  ä¸­æ–‡æ•°æ® (AlphaFin): {alphafin_raw:,} ä¸ªæ ·æœ¬ï¼Œ425M")
    print(f"  è‹±æ–‡æ•°æ® (TatQA): {tatqa_raw:,} ä¸ªæ ·æœ¬ï¼Œ18M")
    print(f"  TatQAåŸå§‹é—®é¢˜æ€»æ•°: {tatqa_questions:,} ä¸ªé—®é¢˜")
    
    print("\nâ— 1.2 LLM (Qwen2-7B) è‡ªåŠ¨åŒ–æ•°æ®å¤„ç†:")
    print("  æ ¸å¿ƒåŠŸèƒ½:")
    print("    - å…ƒæ•°æ®æå–å™¨: è‡ªåŠ¨æå–company_name, stock_code, report_date")
    print("    - é—®é¢˜ç”Ÿæˆå™¨: åŸºäºContextå’ŒAnswerç”ŸæˆQuestion")
    print("    - æ‘˜è¦ç”Ÿæˆå™¨: åŸºäºContextç”ŸæˆSummary")
    print(f"  å…ƒæ•°æ®è¦†ç›–ç‡: {alphafin_metadata.get('company_name', [0, 0])[1]:.1f}% (company_name)")
    
    print("\nâ— 1.3 å¤„ç†åæ•°æ®ç»Ÿè®¡ (Processed Data Statistics):")
    print(f"  ä¸­æ–‡ (QCA): {alphafin_processed:,} ä¸ªæ ·æœ¬")
    print(f"  è‹±æ–‡ (QCA): {tatqa_converted:,} ä¸ªæ ·æœ¬ (è®­ç»ƒ: {tatqa_converted - tatqa_eval:,}, è¯„ä¼°: {tatqa_eval:,})")
    
    print("\nâ— 1.4 TatQA æ•°æ®è½¬æ¢è¿‡ç¨‹ä¸è´¨é‡:")
    print("  å…³é”®æ­¥éª¤: Table Textualizationå°†è¡¨æ ¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€")
    print(f"  é—®é¢˜åˆ°QCAè½¬æ¢ç‡: {tatqa_conversion_rate:.1f}% ({tatqa_converted:,}/{tatqa_questions:,})")
    print(f"  è¿‡æ»¤ç‡: {100 - tatqa_conversion_rate:.1f}%")
    print("  ä¸»è¦åŸå› : answer_type=tableä½†rel_paragraphsä¸ºç©ºï¼Œè¡¨æ ¼è½¬æ¢é€»è¾‘ç¼ºé™·")
    print(f"  çŸ¥è¯†åº“æ–‡æ¡£æ•°: {tatqa_kb:,} ä¸ªæ–‡æ¡£")
    if tatqa_kb > 0:
        print(f"  æ–‡æ¡£åˆ©ç”¨ç‡: å¹³å‡æ¯ä¸ªæ–‡æ¡£ç”¨äº {tatqa_converted/tatqa_kb:.1f} ä¸ªé—®é¢˜")
    
    print("\nâ— AlphaFin æ•°æ®å¤„ç†æµç¨‹:")
    print(f"  åŸå§‹æ•°æ®è¿‡æ»¤ç‡: {alphafin_filter_rate:.1f}%")
    print(f"  LLMå¤„ç†åæ ·æœ¬æ•°: {alphafin_processed:,}")
    print(f"  å…ƒæ•°æ®è¦†ç›–ç‡: company_name({alphafin_metadata.get('company_name', [0, 0])[1]:.1f}%), stock_code({alphafin_metadata.get('stock_code', [0, 0])[1]:.1f}%), report_date({alphafin_metadata.get('report_date', [0, 0])[1]:.1f}%)")
    
    print("\n" + "="*80)
    print("âœ… æ•°æ®æ¦‚å†µåˆ†æå®Œæˆ")
    print("="*80)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹ç”Ÿæˆå®Œæ•´æ•°æ®ç»Ÿè®¡æŠ¥å‘Š...")
    print_all_statistics()
    
    # å¯é€‰ï¼šä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
    save_report = input("\næ˜¯å¦ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶? (y/n): ").lower().strip()
    if save_report == 'y':
        import datetime
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"data_statistics_report_{timestamp}.txt"
        
        # é‡å®šå‘è¾“å‡ºåˆ°æ–‡ä»¶
        import sys
        original_stdout = sys.stdout
        with open(report_file, 'w', encoding='utf-8') as f:
            sys.stdout = f
            print_all_statistics()
            sys.stdout = original_stdout
        
        print(f"ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def print_all_statistics():
    """æ‰“å°æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯"""
    print_summary_report()

if __name__ == "__main__":
    main() 