#!/usr/bin/env python3
"""
åˆ†æå®Œæ•´TAT-QAæ•°æ®é›†ç»Ÿè®¡è„šæœ¬
åˆ†æè®­ç»ƒé›†å’Œè¯„ä¼°é›†çš„é—®é¢˜ç±»å‹åˆ†å¸ƒ
"""

import json
import re
from collections import Counter
from typing import List, Dict, Any
import os

def classify_question_type(context: str) -> str:
    """
    æ ¹æ®ä¸Šä¸‹æ–‡å†…å®¹åˆ†ç±»é—®é¢˜ç±»å‹
    """
    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼
    has_table = "Table ID:" in context
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡æœ¬æ®µè½ï¼ˆéè¡¨æ ¼å†…å®¹ï¼‰
    # ç§»é™¤è¡¨æ ¼å†…å®¹ï¼Œæ£€æŸ¥å‰©ä½™éƒ¨åˆ†æ˜¯å¦è¿˜æœ‰æ–‡æœ¬
    context_without_table = context
    if has_table:
        # æ‰¾åˆ°è¡¨æ ¼å¼€å§‹ä½ç½®
        table_start = context.find("Table ID:")
        if table_start > 0:
            # æ£€æŸ¥è¡¨æ ¼å‰æ˜¯å¦æœ‰æ–‡æœ¬
            text_before = context[:table_start].strip()
            if len(text_before) > 50:  # å¦‚æœæœ‰è¶³å¤Ÿé•¿çš„æ–‡æœ¬æ®µè½
                return "table+text"
    
    # æ£€æŸ¥è¡¨æ ¼åæ˜¯å¦æœ‰æ–‡æœ¬
    if has_table:
        # ç®€å•çš„å¯å‘å¼æ–¹æ³•ï¼šå¦‚æœä¸Šä¸‹æ–‡å¾ˆé•¿ä¸”åŒ…å«è¡¨æ ¼ï¼Œå¯èƒ½è¿˜æœ‰æ–‡æœ¬
        if len(context) > 2000:  # å¦‚æœä¸Šä¸‹æ–‡å¾ˆé•¿
            return "table+text"
    
    if has_table:
        return "table"
    elif len(context.strip()) > 0:
        return "text"
    else:
        return "unknown"

def analyze_dataset_file(data_file: str) -> Dict[str, Any]:
    """
    åˆ†æå•ä¸ªæ•°æ®æ–‡ä»¶çš„ç»Ÿè®¡ä¿¡æ¯
    """
    print(f"ğŸ“Š åˆ†ææ•°æ®é›†æ–‡ä»¶: {data_file}")
    
    if not os.path.exists(data_file):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return {
            "total_samples": 0,
            "type_distribution": {},
            "type_percentages": {},
            "file_exists": False
        }
    
    question_types = []
    total_samples = 0
    
    with open(data_file, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                sample = json.loads(line.strip())
                question_type = classify_question_type(sample["context"])
                question_types.append(question_type)
                total_samples += 1
            except json.JSONDecodeError as e:
                print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æé”™è¯¯: {e}")
                continue
            except KeyError as e:
                print(f"âš ï¸ ç¬¬{line_num}è¡Œç¼ºå°‘å­—æ®µ: {e}")
                continue
    
    # ç»Ÿè®¡å„ç±»å‹æ•°é‡
    type_counts = Counter(question_types)
    
    statistics = {
        "total_samples": total_samples,
        "type_distribution": dict(type_counts),
        "type_percentages": {
            qtype: (count / total_samples * 100) if total_samples > 0 else 0
            for qtype, count in type_counts.items()
        },
        "file_exists": True
    }
    
    return statistics

def analyze_full_tatqa_dataset():
    """
    åˆ†æå®Œæ•´TAT-QAæ•°æ®é›†
    """
    print("ğŸš€ TAT-QAå®Œæ•´æ•°æ®é›†åˆ†æå·¥å…·")
    print("="*60)
    
    # å®šä¹‰è¦åˆ†æçš„æ–‡ä»¶è·¯å¾„
    possible_files = [
        "evaluate_mrr/tatqa_eval_enhanced.jsonl",
        "evaluate_mrr/tatqa_train_qc_enhanced.jsonl"
    ]
    
    # æŸ¥æ‰¾å­˜åœ¨çš„æ–‡ä»¶
    existing_files = []
    for file_path in possible_files:
        if os.path.exists(file_path):
            existing_files.append(file_path)
    
    if not existing_files:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•TAT-QAæ•°æ®æ–‡ä»¶")
        print("è¯·æ£€æŸ¥ä»¥ä¸‹å¯èƒ½çš„è·¯å¾„:")
        for file_path in possible_files:
            print(f"  - {file_path}")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(existing_files)} ä¸ªæ•°æ®æ–‡ä»¶:")
    for file_path in existing_files:
        print(f"  - {file_path}")
    
    # åˆ†ææ¯ä¸ªæ–‡ä»¶
    all_statistics = {}
    total_combined = {
        "total_samples": 0,
        "table_samples": 0,
        "text_samples": 0,
        "unknown_samples": 0
    }
    
    for file_path in existing_files:
        print(f"\nğŸ“Š åˆ†ææ–‡ä»¶: {file_path}")
        stats = analyze_dataset_file(file_path)
        all_statistics[file_path] = stats
        
        if stats["file_exists"]:
            print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
            for qtype, count in stats['type_distribution'].items():
                percentage = stats['type_percentages'][qtype]
                print(f"  {qtype}: {count} ({percentage:.1f}%)")
                
                # ç´¯è®¡åˆ°æ€»è®¡
                total_combined["total_samples"] += count
                if qtype == "table":
                    total_combined["table_samples"] += count
                elif qtype == "text":
                    total_combined["text_samples"] += count
                else:
                    total_combined["unknown_samples"] += count
    
    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    print(f"\nğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"æ€»æ ·æœ¬æ•°: {total_combined['total_samples']}")
    if total_combined['total_samples'] > 0:
        print(f"è¡¨æ ¼é—®é¢˜: {total_combined['table_samples']} ({total_combined['table_samples']/total_combined['total_samples']*100:.1f}%)")
        print(f"æ–‡æœ¬é—®é¢˜: {total_combined['text_samples']} ({total_combined['text_samples']/total_combined['total_samples']*100:.1f}%)")
        if total_combined['unknown_samples'] > 0:
            print(f"è¡¨æ ¼+æ–‡æœ¬: {total_combined['unknown_samples']} ({total_combined['unknown_samples']/total_combined['total_samples']*100:.1f}%)")
    
    # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„ç»Ÿè®¡
    print(f"\nğŸ“‹ æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç»„:")
    train_files = [f for f in existing_files if "train" in f.lower()]
    eval_files = [f for f in existing_files if "eval" in f.lower()]
    
    if train_files:
        train_total = sum(all_statistics[f]["total_samples"] for f in train_files if all_statistics[f]["file_exists"])
        print(f"è®­ç»ƒé›†æ–‡ä»¶æ•°: {len(train_files)}, æ€»æ ·æœ¬æ•°: {train_total}")
    
    if eval_files:
        eval_total = sum(all_statistics[f]["total_samples"] for f in eval_files if all_statistics[f]["file_exists"])
        print(f"è¯„ä¼°é›†æ–‡ä»¶æ•°: {len(eval_files)}, æ€»æ ·æœ¬æ•°: {eval_total}")
    
    # ä¿å­˜è¯¦ç»†ç»Ÿè®¡ç»“æœ
    output_file = "tatqa_dataset_statistics.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump({
            "file_statistics": all_statistics,
            "total_combined": total_combined,
            "existing_files": existing_files
        }, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… è¯¦ç»†ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    analyze_full_tatqa_dataset() 