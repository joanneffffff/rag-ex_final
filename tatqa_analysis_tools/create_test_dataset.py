#!/usr/bin/env python3
"""
åˆ›å»ºTAT-QAæµ‹è¯•æ•°æ®é›†è„šæœ¬
åˆ†æé—®é¢˜ç±»å‹åˆ†å¸ƒå¹¶åˆ›å»ºå¹³è¡¡çš„æµ‹è¯•æ•°æ®é›†
"""

import json
import re
from collections import Counter
from typing import List, Dict, Any
import random

def classify_question_type(context: str) -> str:
    """
    æ ¹æ®ä¸Šä¸‹æ–‡å†…å®¹åˆ†ç±»é—®é¢˜ç±»å‹
    """
    if "Table ID:" in context:
        return "table"
    elif "Table ID:" not in context and len(context.strip()) > 0:
        return "text"
    else:
        return "unknown"

def analyze_dataset_statistics(data_file: str) -> Dict[str, Any]:
    """
    åˆ†ææ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯
    """
    print(f"ğŸ“Š åˆ†ææ•°æ®é›†: {data_file}")
    
    question_types = []
    total_samples = 0
    
    with open(data_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                sample = json.loads(line.strip())
                question_type = classify_question_type(sample["context"])
                question_types.append(question_type)
                total_samples += 1
            except json.JSONDecodeError:
                continue
    
    # ç»Ÿè®¡å„ç±»å‹æ•°é‡
    type_counts = Counter(question_types)
    
    statistics = {
        "total_samples": total_samples,
        "type_distribution": dict(type_counts),
        "type_percentages": {
            qtype: (count / total_samples * 100) 
            for qtype, count in type_counts.items()
        }
    }
    
    return statistics

def create_balanced_test_dataset(data_file: str, output_file: str, 
                                table_samples: int = 5, 
                                text_samples: int = 5, 
                                mixed_samples: int = 5) -> Dict[str, Any]:
    """
    åˆ›å»ºå¹³è¡¡çš„æµ‹è¯•æ•°æ®é›†
    """
    print(f"ğŸ”§ åˆ›å»ºæµ‹è¯•æ•°æ®é›†: {output_file}")
    
    # æŒ‰ç±»å‹åˆ†ç»„æ ·æœ¬
    table_samples_list = []
    text_samples_list = []
    
    with open(data_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                sample = json.loads(line.strip())
                question_type = classify_question_type(sample["context"])
                
                if question_type == "table":
                    table_samples_list.append(sample)
                elif question_type == "text":
                    text_samples_list.append(sample)
            except json.JSONDecodeError:
                continue
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(table_samples_list)} ä¸ªè¡¨æ ¼é—®é¢˜")
    print(f"ğŸ“ æ‰¾åˆ° {len(text_samples_list)} ä¸ªæ–‡æœ¬é—®é¢˜")
    
    # éšæœºé€‰æ‹©æ ·æœ¬
    random.seed(42)  # ç¡®ä¿å¯é‡ç°æ€§
    
    selected_samples = []
    
    # é€‰æ‹©è¡¨æ ¼æ ·æœ¬
    if len(table_samples_list) >= table_samples:
        selected_table = random.sample(table_samples_list, table_samples)
        selected_samples.extend(selected_table)
        print(f"âœ… é€‰æ‹©äº† {len(selected_table)} ä¸ªè¡¨æ ¼æ ·æœ¬")
    else:
        print(f"âš ï¸ è¡¨æ ¼æ ·æœ¬ä¸è¶³ï¼Œåªæœ‰ {len(table_samples_list)} ä¸ª")
        selected_samples.extend(table_samples_list)
    
    # é€‰æ‹©æ–‡æœ¬æ ·æœ¬
    if len(text_samples_list) >= text_samples:
        selected_text = random.sample(text_samples_list, text_samples)
        selected_samples.extend(selected_text)
        print(f"âœ… é€‰æ‹©äº† {len(selected_text)} ä¸ªæ–‡æœ¬æ ·æœ¬")
    else:
        print(f"âš ï¸ æ–‡æœ¬æ ·æœ¬ä¸è¶³ï¼Œåªæœ‰ {len(text_samples_list)} ä¸ª")
        selected_samples.extend(text_samples_list)
    
    # å¯¹äºæ··åˆæ ·æœ¬ï¼Œæˆ‘ä»¬ä»ä¸¤ç§ç±»å‹ä¸­å„é€‰ä¸€äº›
    if mixed_samples > 0:
        remaining_table = [s for s in table_samples_list if s not in selected_samples]
        remaining_text = [s for s in text_samples_list if s not in selected_samples]
        
        mixed_table_count = min(mixed_samples // 2, len(remaining_table))
        mixed_text_count = mixed_samples - mixed_table_count
        
        if mixed_table_count > 0:
            mixed_table = random.sample(remaining_table, mixed_table_count)
            selected_samples.extend(mixed_table)
        
        if mixed_text_count > 0 and len(remaining_text) >= mixed_text_count:
            mixed_text = random.sample(remaining_text, mixed_text_count)
            selected_samples.extend(mixed_text)
        
        print(f"âœ… é€‰æ‹©äº† {mixed_samples} ä¸ªæ··åˆæ ·æœ¬")
    
    # æ‰“ä¹±é¡ºåº
    random.shuffle(selected_samples)
    
    # ä¿å­˜æµ‹è¯•æ•°æ®é›†
    with open(output_file, 'w', encoding='utf-8') as f:
        for sample in selected_samples:
            f.write(json.dumps(sample, ensure_ascii=False) + '\n')
    
    # ç»Ÿè®¡æµ‹è¯•æ•°æ®é›†
    test_stats = {
        "total_test_samples": len(selected_samples),
        "table_samples": len([s for s in selected_samples if classify_question_type(s["context"]) == "table"]),
        "text_samples": len([s for s in selected_samples if classify_question_type(s["context"]) == "text"]),
        "sample_ids": [s.get("doc_id", "unknown") for s in selected_samples]
    }
    
    return test_stats

def main():
    """ä¸»å‡½æ•°"""
    data_file = 'evaluate_mrr/tatqa_eval_enhanced.jsonl'
    test_output_file = 'evaluate_mrr/tatqa_test_15_samples.jsonl'
    
    print("ğŸš€ TAT-QAæ•°æ®é›†åˆ†æå·¥å…·")
    print("="*50)
    
    # 1. åˆ†æå®Œæ•´æ•°æ®é›†ç»Ÿè®¡
    print("\nğŸ“Š æ­¥éª¤1: åˆ†æå®Œæ•´æ•°æ®é›†ç»Ÿè®¡")
    statistics = analyze_dataset_statistics(data_file)
    
    print(f"æ€»æ ·æœ¬æ•°: {statistics['total_samples']}")
    print("ç±»å‹åˆ†å¸ƒ:")
    for qtype, count in statistics['type_distribution'].items():
        percentage = statistics['type_percentages'][qtype]
        print(f"  {qtype}: {count} ({percentage:.1f}%)")
    
    # 2. åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    print("\nğŸ”§ æ­¥éª¤2: åˆ›å»ºå¹³è¡¡æµ‹è¯•æ•°æ®é›†")
    test_stats = create_balanced_test_dataset(
        data_file=data_file,
        output_file=test_output_file,
        table_samples=5,
        text_samples=5,
        mixed_samples=5
    )
    
    print(f"\nâœ… æµ‹è¯•æ•°æ®é›†åˆ›å»ºå®Œæˆ!")
    print(f"æµ‹è¯•æ ·æœ¬æ€»æ•°: {test_stats['total_test_samples']}")
    print(f"è¡¨æ ¼æ ·æœ¬: {test_stats['table_samples']}")
    print(f"æ–‡æœ¬æ ·æœ¬: {test_stats['text_samples']}")
    print(f"è¾“å‡ºæ–‡ä»¶: {test_output_file}")
    
    # 3. æ˜¾ç¤ºä¸€äº›æ ·æœ¬ç¤ºä¾‹
    print("\nğŸ“‹ æ ·æœ¬ç¤ºä¾‹:")
    with open(test_output_file, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬
                break
            sample = json.loads(line.strip())
            qtype = classify_question_type(sample["context"])
            print(f"æ ·æœ¬ {i+1} ({qtype}):")
            print(f"  é—®é¢˜: {sample['query'][:100]}...")
            print(f"  ç­”æ¡ˆ: {sample['answer']}")
            print(f"  æ–‡æ¡£ID: {sample.get('doc_id', 'unknown')}")
            print()

if __name__ == "__main__":
    main() 