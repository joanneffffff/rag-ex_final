#!/usr/bin/env python3
"""
æ£€æµ‹å’Œåˆ é™¤AlphaFinæ•°æ®é›†ä¸­çš„é‡å¤æ•°æ®
"""

import json
import hashlib
from collections import defaultdict
from pathlib import Path
import argparse

def calculate_content_hash(content: str) -> str:
    """è®¡ç®—å†…å®¹çš„å“ˆå¸Œå€¼ï¼Œç”¨äºæ£€æµ‹é‡å¤"""
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def load_data(file_path: str) -> list:
    """åŠ è½½æ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒjsonå’Œjsonlæ ¼å¼"""
    if file_path.endswith('.jsonl'):
        data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    data.append(json.loads(line))
    else:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    return data

def save_data(data: list, output_path: str):
    """ä¿å­˜æ•°æ®æ–‡ä»¶ï¼Œæ”¯æŒjsonå’Œjsonlæ ¼å¼"""
    if output_path.endswith('.jsonl'):
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
    else:
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

def analyze_duplicates(file_path: str) -> tuple[dict, list]:
    """åˆ†ææ–‡ä»¶ä¸­çš„é‡å¤æ•°æ®"""
    print(f"ğŸ” åˆ†ææ–‡ä»¶: {file_path}")
    
    duplicates = {
        'context_duplicates': defaultdict(list),
        'answer_duplicates': defaultdict(list),
        'full_duplicates': defaultdict(list),
        'total_records': 0,
        'unique_records': 0
    }
    
    data = load_data(file_path)
    
    duplicates['total_records'] = len(data)
    print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(data)}")
    
    # æ£€æµ‹é‡å¤
    for i, record in enumerate(data):
        # è·å–å­—æ®µå†…å®¹
        context = record.get('context', record.get('original_context', ''))
        answer = record.get('answer', record.get('original_answer', ''))
        question = record.get('query', record.get('original_question', ''))
        
        # è®¡ç®—å“ˆå¸Œå€¼
        context_hash = calculate_content_hash(context)
        answer_hash = calculate_content_hash(answer)
        full_content = f"{context}|{answer}|{question}"
        full_hash = calculate_content_hash(full_content)
        
        # è®°å½•é‡å¤
        duplicates['context_duplicates'][context_hash].append(i)
        duplicates['answer_duplicates'][answer_hash].append(i)
        duplicates['full_duplicates'][full_hash].append(i)
    
    # ç»Ÿè®¡é‡å¤æƒ…å†µ
    context_dups = sum(1 for indices in duplicates['context_duplicates'].values() if len(indices) > 1)
    answer_dups = sum(1 for indices in duplicates['answer_duplicates'].values() if len(indices) > 1)
    full_dups = sum(1 for indices in duplicates['full_duplicates'].values() if len(indices) > 1)
    
    print(f"ğŸ“‹ Contexté‡å¤ç»„æ•°: {context_dups}")
    print(f"ğŸ“‹ Answeré‡å¤ç»„æ•°: {answer_dups}")
    print(f"ğŸ“‹ å®Œå…¨é‡å¤ç»„æ•°: {full_dups}")
    
    return duplicates, data

def remove_duplicates(file_path: str, output_path: str, duplicate_type: str = 'full') -> dict:
    """åˆ é™¤é‡å¤æ•°æ®å¹¶ä¿å­˜æ¸…ç†åçš„æ–‡ä»¶"""
    print(f"ğŸ§¹ å¼€å§‹åˆ é™¤é‡å¤æ•°æ®...")
    print(f"ğŸ“ è¾“å…¥æ–‡ä»¶: {file_path}")
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶: {output_path}")
    print(f"ğŸ¯ é‡å¤ç±»å‹: {duplicate_type}")
    
    data = load_data(file_path)
    original_count = len(data)
    print(f"ğŸ“Š åŸå§‹è®°å½•æ•°: {original_count}")
    
    # æ£€æµ‹é‡å¤
    duplicates, _ = analyze_duplicates(file_path)
    
    # æ ¹æ®é‡å¤ç±»å‹é€‰æ‹©è¦ä¿ç•™çš„è®°å½•
    if duplicate_type == 'full':
        duplicate_groups = duplicates['full_duplicates']
    elif duplicate_type == 'context':
        duplicate_groups = duplicates['context_duplicates']
    elif duplicate_type == 'answer':
        duplicate_groups = duplicates['answer_duplicates']
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„é‡å¤ç±»å‹: {duplicate_type}")
    
    # æ‰¾å‡ºè¦ä¿ç•™çš„è®°å½•ç´¢å¼•ï¼ˆæ¯ç»„ä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
    keep_indices = set()
    removed_indices = set()
    
    for hash_val, indices in duplicate_groups.items():
        if len(indices) > 1:
            # ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåˆ é™¤å…¶ä½™çš„
            keep_indices.add(indices[0])
            removed_indices.update(indices[1:])
        else:
            # æ²¡æœ‰é‡å¤ï¼Œä¿ç•™
            keep_indices.add(indices[0])
    
    # åˆ›å»ºæ¸…ç†åçš„æ•°æ®
    cleaned_data = [data[i] for i in range(len(data)) if i in keep_indices]
    
    # ä¿å­˜æ¸…ç†åçš„æ–‡ä»¶
    save_data(cleaned_data, output_path)
    
    removed_count = len(removed_indices)
    cleaned_count = len(cleaned_data)
    
    print(f"âœ… æ¸…ç†å®Œæˆ!")
    print(f"ğŸ“Š åˆ é™¤è®°å½•æ•°: {removed_count}")
    print(f"ğŸ“Š ä¿ç•™è®°å½•æ•°: {cleaned_count}")
    print(f"ğŸ“Š é‡å¤ç‡: {removed_count/original_count*100:.2f}%")
    
    return {
        'original_count': original_count,
        'cleaned_count': cleaned_count,
        'removed_count': removed_count,
        'duplicate_rate': removed_count/original_count*100
    }

def main():
    parser = argparse.ArgumentParser(description="åˆ é™¤AlphaFinæ•°æ®é›†ä¸­çš„é‡å¤æ•°æ®")
    parser.add_argument("--input", type=str, required=True, help="è¾“å…¥æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output", type=str, help="è¾“å‡ºæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--type", type=str, default="full", 
                       choices=["full", "context", "answer"], 
                       help="é‡å¤æ£€æµ‹ç±»å‹: full(å®Œå…¨é‡å¤), context(contexté‡å¤), answer(answeré‡å¤)")
    parser.add_argument("--analyze-only", action="store_true", help="ä»…åˆ†æï¼Œä¸åˆ é™¤")
    
    args = parser.parse_args()
    
    if args.analyze_only:
        # ä»…åˆ†æé‡å¤æƒ…å†µ
        duplicates, data = analyze_duplicates(args.input)
        
        # æ˜¾ç¤ºè¯¦ç»†çš„é‡å¤ä¿¡æ¯
        print(f"\nğŸ“‹ è¯¦ç»†é‡å¤ä¿¡æ¯:")
        for dup_type, dup_data in duplicates.items():
            if dup_type.endswith('_duplicates'):
                dup_count = sum(1 for indices in dup_data.values() if len(indices) > 1)
                print(f"   {dup_type}: {dup_count} ç»„é‡å¤")
                
                # æ˜¾ç¤ºå‰å‡ ä¸ªé‡å¤ç»„çš„è¯¦ç»†ä¿¡æ¯
                count = 0
                for hash_val, indices in dup_data.items():
                    if len(indices) > 1 and count < 3:
                        print(f"     é‡å¤ç»„ {count+1}: {len(indices)} ä¸ªè®°å½• (ç´¢å¼•: {indices[:5]}{'...' if len(indices) > 5 else ''})")
                        # æ˜¾ç¤ºé‡å¤è®°å½•çš„å†…å®¹ç¤ºä¾‹
                        if count == 0:
                            print(f"       ç¤ºä¾‹è®°å½• {indices[0]}:")
                            record = data[indices[0]]
                            context_preview = record.get('context', record.get('original_context', ''))[:100]
                            answer_preview = record.get('answer', record.get('original_answer', ''))[:50]
                            print(f"         Context: {context_preview}...")
                            print(f"         Answer: {answer_preview}...")
                        count += 1
    else:
        # åˆ é™¤é‡å¤æ•°æ®
        if not args.output:
            print("âŒ é”™è¯¯: åˆ é™¤é‡å¤æ•°æ®æ—¶éœ€è¦æŒ‡å®š --output å‚æ•°")
            return
        result = remove_duplicates(args.input, args.output, args.type)
        
        # ä¿å­˜åˆ†ææŠ¥å‘Š
        report_path = args.output.replace('.json', '_cleaning_report.json').replace('.jsonl', '_cleaning_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“„ æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

if __name__ == "__main__":
    main() 