# AlphaFin vs TAT-QA ç¼–ç å™¨å¾®è°ƒå¯¹æ¯”

## ğŸ“Š æ ¸å¿ƒå·®å¼‚æ€»ç»“

| æ–¹é¢ | AlphaFin (ä¸­æ–‡) | TAT-QA (è‹±æ–‡) |
|------|----------------|---------------|
| **è¯­è¨€** | ä¸­æ–‡ | è‹±æ–‡ |
| **åŸºç¡€æ¨¡å‹** | `Langboat/mengzi-bert-base-fin` | `ProsusAI/finbert` |
| **å¾®è°ƒè„šæœ¬** | `finetune_encoder.py` | `finetune_encoder.py` |
| **æ•°æ®å¤„ç†** | ä½¿ç”¨generated_questionå’Œsummary | ç›´æ¥ä½¿ç”¨æ–‡æœ¬ |
| **æ‰¹æ¬¡å¤§å°** | 16 | 32 |
| **è®­ç»ƒè½®æ•°** | 3 | 2 |

## ğŸ”§ æŠ€æœ¯å®ç°å·®å¼‚

### 1. æ•°æ®å¤„ç†é€»è¾‘

#### AlphaFin (ä¸­æ–‡)
```python
# ç›´æ¥ä½¿ç”¨generated_questionå’Œsummary
query_text = data.get('generated_question', '').strip()
doc_text = data.get('summary', '').strip()
```

#### TAT-QA (è‹±æ–‡)
```python
# ç›´æ¥ä½¿ç”¨æ–‡æœ¬ï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†
query_text = data.get('query', '').strip()
doc_text = data.get('context', '').strip()
```

### 2. æ¨¡å‹é€‰æ‹©åŸå› 

#### AlphaFin: `Langboat/mengzi-bert-base-fin`
- **ä¸“é—¨é’ˆå¯¹ä¸­æ–‡é‡‘èé¢†åŸŸ**
- **é¢„è®­ç»ƒæ•°æ®åŒ…å«è´¢åŠ¡æŠ¥å‘Šã€æ–°é—»ç­‰**
- **æ”¯æŒä¸­æ–‡é‡‘èæœ¯è¯­ç†è§£**

#### TAT-QA: `ProsusAI/finbert`
- **ä¸“é—¨é’ˆå¯¹è‹±æ–‡é‡‘èé¢†åŸŸ**
- **åœ¨é‡‘èæ–‡æœ¬ä¸Šé¢„è®­ç»ƒ**
- **é€‚åˆå¤„ç†è‹±æ–‡è´¢åŠ¡æ•°æ®**

### 3. è®­ç»ƒå‚æ•°å·®å¼‚

| å‚æ•° | AlphaFin | TAT-QA | åŸå›  |
|------|----------|--------|------|
| **æ‰¹æ¬¡å¤§å°** | 16 | 32 | ä¸­æ–‡æ¨¡å‹æ›´å¤§ï¼Œéœ€è¦æ›´å°æ‰¹æ¬¡ |
| **è®­ç»ƒè½®æ•°** | 3 | 2 | ä¸­æ–‡æ•°æ®æ›´å¤æ‚ï¼Œéœ€è¦æ›´å¤šè½®æ•° |
| **å­¦ä¹ ç‡** | 2e-5 | 2e-5 | ç›¸åŒï¼Œéƒ½æ˜¯æ ‡å‡†å­¦ä¹ ç‡ |

## ğŸš€ ä½¿ç”¨æ–¹å¼

### é€šç”¨è„šæœ¬ä½¿ç”¨
```bash
# AlphaFinå¾®è°ƒ
./finetune_encoder_universal.sh alphafin

# TAT-QAå¾®è°ƒ
./finetune_encoder_universal.sh tatqa

# å¿«é€Ÿæµ‹è¯•æ¨¡å¼
./finetune_encoder_universal.sh alphafin quick
./finetune_encoder_universal.sh tatqa quick
```

### å•ç‹¬è„šæœ¬ä½¿ç”¨
```bash
# AlphaFinä¸“ç”¨
./finetune_alphafin_encoder.sh

# TAT-QAä½¿ç”¨åŸæœ‰è„šæœ¬
python encoder_finetune_evaluate/finetune_encoder.py \
    --model_name ProsusAI/finbert \
    --train_jsonl evaluate_mrr/tatqa_train_qc.jsonl \
    --output_dir models/finetuned_tatqa_encoder \
    --batch_size 32 \
    --epochs 2
```

## ğŸ“ æ•°æ®æ–‡ä»¶ç»“æ„

### AlphaFinæ•°æ®
```
evaluate_mrr/
â”œâ”€â”€ alphafin_train_qc.jsonl           # è®­ç»ƒæ•°æ® (generated_question + summary)
â””â”€â”€ alphafin_eval.jsonl               # è¯„ä¼°æ•°æ®
```

### TAT-QAæ•°æ®
```
evaluate_mrr/
â”œâ”€â”€ tatqa_train_qc.jsonl              # è®­ç»ƒæ•°æ®
â”œâ”€â”€ tatqa_eval.jsonl                  # è¯„ä¼°æ•°æ®
â””â”€â”€ tatqa_knowledge_base.jsonl        # åŸå§‹æ•°æ®
```

## ğŸ¯ è¯„ä¼°æ–¹å¼

### AlphaFinè¯„ä¼°
```bash
# æ£€ç´¢è¯„ä¼°
python alphafin_data_process/run_retrieval_evaluation_background.py \
    --eval_data_path data/alphafin/eval_data_100_from_corpus.jsonl \
    --modes baseline prefilter reranker \
    --encoder_model_path ./models/finetuned_alphafin_encoder

# é›†æˆè¯„ä¼°
python encoder_finetune_evaluate/evaluate_chinese_encoder_reranker_mrr.py \
    --encoder_model_name ./models/finetuned_alphafin_encoder
```

### TAT-QAè¯„ä¼°
```bash
# ç¼–ç å™¨è¯„ä¼°
python encoder_finetune_evaluate/run_encoder_eval.py \
    --model_name ./models/finetuned_tatqa_encoder \
    --eval_jsonl evaluate_mrr/tatqa_eval.jsonl

# TAT-QAä¸“ç”¨è¯„ä¼°
python alphafin_data_process/run_tatqa_retrieval_evaluation.py \
    --mode reranker \
    --encoder_model_path ./models/finetuned_tatqa_encoder
```

## ğŸ”„ æ ¸å¿ƒé€»è¾‘ç›¸åŒç‚¹

1. **æŸå¤±å‡½æ•°**: éƒ½ä½¿ç”¨ `MultipleNegativesRankingLoss`
2. **è¯„ä¼°æŒ‡æ ‡**: éƒ½ä½¿ç”¨ MRR (Mean Reciprocal Rank)
3. **è®­ç»ƒç­–ç•¥**: éƒ½æ˜¯å¯¹æ¯”å­¦ä¹ ï¼Œquery-documenté…å¯¹
4. **è¯„ä¼°æ–¹å¼**: éƒ½ä½¿ç”¨ `InformationRetrievalEvaluator`
5. **ä¼˜åŒ–å™¨**: éƒ½ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨

## ğŸ’¡ å…³é”®æ´å¯Ÿ

### ä¸ºä»€ä¹ˆé€»è¾‘åŸºæœ¬ç›¸åŒï¼Ÿ
1. **ä»»åŠ¡ç›¸åŒ**: éƒ½æ˜¯ä¿¡æ¯æ£€ç´¢ä»»åŠ¡
2. **ç›®æ ‡ç›¸åŒ**: éƒ½æ˜¯å­¦ä¹ queryå’Œdocumentçš„è¯­ä¹‰ç›¸ä¼¼æ€§
3. **æ–¹æ³•ç›¸åŒ**: éƒ½ä½¿ç”¨å¯¹æ¯”å­¦ä¹ æ–¹æ³•
4. **è¯„ä¼°ç›¸åŒ**: éƒ½ä½¿ç”¨æ£€ç´¢è¯„ä¼°æŒ‡æ ‡

### ä¸»è¦å·®å¼‚æ¥æºï¼Ÿ
1. **è¯­è¨€å·®å¼‚**: ä¸­æ–‡vsè‹±æ–‡ï¼Œéœ€è¦ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹
2. **æ•°æ®æ ¼å¼**: AlphaFinä½¿ç”¨generated_question+summaryï¼ŒTAT-QAç›´æ¥ä½¿ç”¨æ–‡æœ¬
3. **é¢†åŸŸç‰¹ç‚¹**: ä¸­æ–‡è´¢åŠ¡æ•°æ®æ›´å¤æ‚ï¼Œéœ€è¦æ›´å¤šè®­ç»ƒè½®æ•°

## âœ… ç»“è®º

**æ˜¯çš„ï¼ŒAlphaFinå’ŒTAT-QAçš„å¾®è°ƒé€»è¾‘åŸºæœ¬ç›¸åŒï¼Œä¸»è¦åŒºåˆ«åœ¨äºï¼š**

1. **æ¨¡å‹é€‰æ‹©**: æ ¹æ®è¯­è¨€é€‰æ‹©åˆé€‚çš„åŸºç¡€æ¨¡å‹
2. **æ•°æ®è·¯å¾„**: æŒ‡å‘ä¸åŒçš„æ•°æ®é›†æ–‡ä»¶
3. **æ•°æ®å¤„ç†**: AlphaFinä½¿ç”¨generated_question+summaryï¼ŒTAT-QAç›´æ¥ä½¿ç”¨æ–‡æœ¬
4. **è®­ç»ƒå‚æ•°**: æ ¹æ®æ•°æ®ç‰¹ç‚¹è°ƒæ•´æ‰¹æ¬¡å¤§å°å’Œè½®æ•°

**æ ¸å¿ƒçš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ã€æŸå¤±å‡½æ•°ã€è¯„ä¼°æ–¹å¼éƒ½æ˜¯å®Œå…¨ç›¸åŒçš„ï¼** 