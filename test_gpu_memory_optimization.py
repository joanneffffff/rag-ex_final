#!/usr/bin/env python3
"""
GPU å†…å­˜ä¼˜åŒ–æµ‹è¯•è„šæœ¬
ä¸“é—¨é’ˆå¯¹ CUDA:1 å†…å­˜ä¸è¶³é—®é¢˜
"""

import torch
import gc
import os
import sys
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

def clear_gpu_memory():
    """æ¸…ç† GPU å†…å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
        print("âœ… GPU å†…å­˜æ¸…ç†å®Œæˆ")

def check_gpu_memory(device_id=1):
    """æ£€æŸ¥ GPU å†…å­˜çŠ¶æ€"""
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(device_id).total_memory
        allocated_memory = torch.cuda.memory_allocated(device_id)
        reserved_memory = torch.cuda.memory_reserved(device_id)
        free_memory = gpu_memory - allocated_memory
        
        print(f"ğŸ“Š GPU {device_id} å†…å­˜çŠ¶æ€:")
        print(f"   - æ€»å†…å­˜: {gpu_memory / 1024**3:.1f}GB")
        print(f"   - å·²åˆ†é…: {allocated_memory / 1024**3:.1f}GB")
        print(f"   - å·²ä¿ç•™: {reserved_memory / 1024**3:.1f}GB")
        print(f"   - å¯ç”¨å†…å­˜: {free_memory / 1024**3:.1f}GB")
        
        return free_memory
    return 0

def test_aggressive_memory_optimization():
    """æµ‹è¯•æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–ç­–ç•¥"""
    print("ğŸš€ å¼€å§‹æ¿€è¿›å†…å­˜ä¼˜åŒ–æµ‹è¯•...")
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True,max_split_size_mb:64'
    os.environ['TOKENIZERS_PARALLELISM'] = 'false'
    
    # æ¸…ç†å†…å­˜
    clear_gpu_memory()
    check_gpu_memory(1)
    
    model_name = "SUFE-AIFLM-Lab/Fin-R1"
    cache_dir = "/users/sgjfei3/data/huggingface"
    
    try:
        print("\nğŸ”§ æ­¥éª¤ 1: åŠ è½½ tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            use_fast=True,
            local_files_only=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ")
        clear_gpu_memory()
        
        print("\nğŸ”§ æ­¥éª¤ 2: é…ç½® 4bit é‡åŒ–...")
        # ä½¿ç”¨æ›´æ¿€è¿›çš„ 4bit é‡åŒ–é…ç½®
        quantization_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,  # å¯ç”¨åŒé‡é‡åŒ–
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
        )
        
        print("âœ… é‡åŒ–é…ç½®å®Œæˆ")
        
        print("\nğŸ”§ æ­¥éª¤ 3: åŠ è½½æ¨¡å‹ (4bit é‡åŒ–)...")
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            quantization_config=quantization_config,
            device_map="auto",  # è®© transformers è‡ªåŠ¨ç®¡ç†è®¾å¤‡åˆ†é…
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            use_cache=False,  # ç¦ç”¨ KV ç¼“å­˜ä»¥èŠ‚çœå†…å­˜
            local_files_only=True
        )
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        check_gpu_memory(1)
        
        # æµ‹è¯•ç”Ÿæˆ
        print("\nğŸ”§ æ­¥éª¤ 4: æµ‹è¯•ç”Ÿæˆ...")
        test_prompt = "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹é‡‘èåˆ†æçš„åŸºæœ¬æ–¹æ³•ã€‚"
        
        inputs = tokenizer(test_prompt, return_tensors="pt", truncation=True, max_length=512)
        
        # ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        if hasattr(model, 'device'):
            device = model.device
        else:
            device = "cuda:1"
        
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=False,
                temperature=0.1,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ… ç”ŸæˆæˆåŠŸ: {response}")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return False

def test_cpu_fallback():
    """æµ‹è¯• CPU å›é€€æ–¹æ¡ˆ"""
    print("\nğŸ”„ æµ‹è¯• CPU å›é€€æ–¹æ¡ˆ...")
    
    model_name = "SUFE-AIFLM-Lab/Fin-R1"
    cache_dir = "/users/sgjfei3/data/huggingface"
    
    try:
        print("ğŸ”§ åœ¨ CPU ä¸ŠåŠ è½½æ¨¡å‹...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            local_files_only=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            device_map="cpu",
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True,
            local_files_only=True
        )
        
        print("âœ… CPU æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç”Ÿæˆ
        test_prompt = "è¯·ç®€è¦ä»‹ç»ä¸€ä¸‹é‡‘èåˆ†æçš„åŸºæœ¬æ–¹æ³•ã€‚"
        inputs = tokenizer(test_prompt, return_tensors="pt", truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                do_sample=False,
                temperature=0.1,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ… CPU ç”ŸæˆæˆåŠŸ: {response}")
        
        return True
        
    except Exception as e:
        print(f"âŒ CPU åŠ è½½å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ GPU å†…å­˜ä¼˜åŒ–æµ‹è¯•")
    print("=" * 50)
    
    # æ£€æŸ¥å½“å‰ GPU çŠ¶æ€
    print("ğŸ“Š å½“å‰ GPU çŠ¶æ€:")
    check_gpu_memory(1)
    
    # æµ‹è¯•æ¿€è¿›å†…å­˜ä¼˜åŒ–
    success = test_aggressive_memory_optimization()
    
    if not success:
        print("\nâš ï¸ GPU åŠ è½½å¤±è´¥ï¼Œå°è¯• CPU å›é€€...")
        test_cpu_fallback()
    
    print("\nğŸ æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main() 