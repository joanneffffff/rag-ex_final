#!/usr/bin/env python3
"""
æ£€æŸ¥æ•°æ®é›†å…¼å®¹æ€§
éªŒè¯ä¿®å¤åçš„æ•°æ®æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
"""

import os
import json
from pathlib import Path

def check_dataset_compatibility():
    """æ£€æŸ¥æ•°æ®é›†å…¼å®¹æ€§"""
    
    print("ğŸ” æ£€æŸ¥æ•°æ®é›†å…¼å®¹æ€§")
    print("=" * 50)
    
    # 1. æ£€æŸ¥ä¿®å¤åçš„çŸ¥è¯†åº“æ–‡ä»¶
    print("1. æ£€æŸ¥ä¿®å¤åçš„çŸ¥è¯†åº“æ–‡ä»¶...")
    knowledge_base_path = "data/unified/tatqa_knowledge_base_combined.jsonl"
    
    if not os.path.exists(knowledge_base_path):
        print(f"âŒ çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {knowledge_base_path}")
        return False
    
    # ç»Ÿè®¡æ–‡æ¡£æ•°é‡
    doc_count = 0
    table_count = 0
    paragraph_count = 0
    
    with open(knowledge_base_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                doc_count += 1
                try:
                    item = json.loads(line)
                    context = item.get('context', '')
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«è¡¨æ ¼
                    if 'Table ID:' in context:
                        table_count += 1
                    
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ®µè½
                    if 'Paragraph ID:' in context:
                        paragraph_count += 1
                        
                except json.JSONDecodeError:
                    continue
    
    print(f"   æ€»æ–‡æ¡£æ•°: {doc_count}")
    print(f"   è¡¨æ ¼æ–‡æ¡£: {table_count}")
    print(f"   æ®µè½æ–‡æ¡£: {paragraph_count}")
    
    # 2. æ£€æŸ¥ç¼“å­˜ç›®å½•
    print("\n2. æ£€æŸ¥ç¼“å­˜ç›®å½•...")
    cache_dirs = ["cache", "data/faiss_indexes", "xlm/cache"]
    
    for cache_dir in cache_dirs:
        if os.path.exists(cache_dir):
            files = os.listdir(cache_dir)
            print(f"   {cache_dir}: {len(files)} ä¸ªæ–‡ä»¶")
        else:
            print(f"   {cache_dir}: ç›®å½•ä¸å­˜åœ¨")
    
    # 3. æ£€æŸ¥æ•°æ®è´¨é‡
    print("\n3. æ£€æŸ¥æ•°æ®è´¨é‡...")
    
    # æ£€æŸ¥å‰å‡ ä¸ªæ–‡æ¡£çš„å®Œæ•´æ€§
    with open(knowledge_base_path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= 5:  # åªæ£€æŸ¥å‰5ä¸ªæ–‡æ¡£
                break
            if line.strip():
                try:
                    item = json.loads(line)
                    context = item.get('context', '')
                    
                    # æ£€æŸ¥è¡¨æ ¼å®Œæ•´æ€§
                    if 'Table ID:' in context:
                        has_columns = 'Table columns:' in context
                        has_data = 'For ' in context
                        
                        print(f"   æ–‡æ¡£ {i+1}: è¡¨æ ¼{'å®Œæ•´' if has_columns and has_data else 'ä¸å®Œæ•´'}")
                        
                except json.JSONDecodeError:
                    continue
    
    # 4. ç”Ÿæˆå…¼å®¹æ€§æŠ¥å‘Š
    print("\n4. å…¼å®¹æ€§æŠ¥å‘Š:")
    print(f"   âœ… çŸ¥è¯†åº“æ–‡ä»¶å­˜åœ¨: {os.path.exists(knowledge_base_path)}")
    print(f"   âœ… æ–‡æ¡£æ•°é‡åˆç†: {doc_count} (é¢„æœŸ: 5398)")
    print(f"   âœ… è¡¨æ ¼æ–‡æ¡£å­˜åœ¨: {table_count > 0}")
    print(f"   âœ… æ®µè½æ–‡æ¡£å­˜åœ¨: {paragraph_count > 0}")
    
    # 5. å»ºè®®
    print("\n5. å»ºè®®:")
    if doc_count != 5398:
        print(f"   âš ï¸ æ–‡æ¡£æ•°é‡ä¸åŒ¹é…ï¼Œå¯èƒ½éœ€è¦é‡æ–°ç”ŸæˆçŸ¥è¯†åº“")
    else:
        print(f"   âœ… æ–‡æ¡£æ•°é‡æ­£ç¡®ï¼Œå¯ä»¥æ­£å¸¸ä½¿ç”¨")
    
    print(f"   ğŸ“‹ ä¸‹ä¸€æ­¥: é‡æ–°å¯åŠ¨RAGç³»ç»Ÿï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ£€æµ‹æ•°æ®å˜åŒ–å¹¶é‡æ–°ç”Ÿæˆç´¢å¼•")
    
    return True

if __name__ == "__main__":
    check_dataset_compatibility() 