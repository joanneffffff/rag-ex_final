#!/bin/bash

# å®Œæ•´CPUè®­ç»ƒè„šæœ¬ - æ— GPUç¯å¢ƒ

echo "ğŸš€ å®Œæ•´CPUè®­ç»ƒ - AlphaFinç¼–ç å™¨å¾®è°ƒ"
echo "æ—¶é—´: $(date)"
echo ""

# è®¾ç½®ç¯å¢ƒå˜é‡ - å¼ºåˆ¶ä½¿ç”¨CPU
export CUDA_VISIBLE_DEVICES=""
export TOKENIZERS_PARALLELISM=false
export CUDA_LAUNCH_BLOCKING=1

echo "ğŸ”§ ç¯å¢ƒé…ç½®:"
echo "  - CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES (å¼ºåˆ¶CPUæ¨¡å¼)"
echo "  - TOKENIZERS_PARALLELISM: $TOKENIZERS_PARALLELISM"
echo ""

# è®­ç»ƒé…ç½®ï¼ˆCPUä¼˜åŒ–ï¼‰
FULL_EPOCHS=5
FULL_EVAL_STEPS=200
FULL_BATCH_SIZE=8  # CPUæ‰¹æ¬¡å¤§å°

# æ•°æ®è·¯å¾„
TRAIN_DATA="evaluate_mrr/alphafin_train_qc.jsonl"
EVAL_DATA="evaluate_mrr/alphafin_eval.jsonl"
FULL_OUTPUT_DIR="./models/alphafin_encoder_finetuned_cpu"

echo "ğŸ“Š è®­ç»ƒé…ç½®:"
echo "  - è®­ç»ƒæ•°æ®: $TRAIN_DATA"
echo "  - è¯„ä¼°æ•°æ®: $EVAL_DATA"
echo "  - è¾“å‡ºç›®å½•: $FULL_OUTPUT_DIR"
echo "  - æ ·æœ¬æ•°: å…¨éƒ¨æ•°æ® (æ— é™åˆ¶)"
echo "  - è½®æ•°: $FULL_EPOCHS"
echo "  - æ‰¹æ¬¡å¤§å°: $FULL_BATCH_SIZE"
echo "  - è¯„ä¼°æ­¥æ•°: $FULL_EVAL_STEPS"
echo "  - è®¾å¤‡: CPU"
echo ""

# æ£€æŸ¥æ•°æ®æ–‡ä»¶
echo "ğŸ“‹ æ£€æŸ¥æ•°æ®æ–‡ä»¶..."
if [ ! -f "$TRAIN_DATA" ]; then
    echo "âŒ è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: $TRAIN_DATA"
    exit 1
fi

if [ ! -f "$EVAL_DATA" ]; then
    echo "âŒ è¯„ä¼°æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: $EVAL_DATA"
    exit 1
fi

echo "âœ… æ•°æ®æ–‡ä»¶æ£€æŸ¥é€šè¿‡"
echo ""

# æ£€æŸ¥Pythonç¯å¢ƒ
echo "ğŸ æ£€æŸ¥Pythonç¯å¢ƒ..."
python --version
echo "  PyTorchç‰ˆæœ¬: $(python -c "import torch; print(torch.__version__)")"
echo "  CUDAå¯ç”¨: $(python -c "import torch; print(torch.cuda.is_available())")"
echo "  SentenceTransformersç‰ˆæœ¬: $(python -c "import sentence_transformers; print(sentence_transformers.__version__)")"
echo ""

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p "$FULL_OUTPUT_DIR"

# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p logs

# å¼€å§‹è®­ç»ƒ
echo "ğŸš€ å¼€å§‹å®Œæ•´CPUè®­ç»ƒ..."
echo "æ—¶é—´: $(date)"
echo ""

# åå°è¿è¡Œè®­ç»ƒ
nohup python finetune_alphafin_encoder_enhanced.py \
    --model_name "Langboat/mengzi-bert-base-fin" \
    --train_jsonl "$TRAIN_DATA" \
    --eval_jsonl "$EVAL_DATA" \
    --output_dir "$FULL_OUTPUT_DIR" \
    --batch_size $FULL_BATCH_SIZE \
    --epochs $FULL_EPOCHS \
    --eval_steps $FULL_EVAL_STEPS \
    > logs/alphafin_finetune_cpu_$(date +%Y%m%d_%H%M%S).log 2>&1 &

# è·å–åå°è¿›ç¨‹ID
PID=$!
echo "ğŸ”„ å®Œæ•´è®­ç»ƒå·²åœ¨åå°å¯åŠ¨ (PID: $PID)"
echo "ğŸ“ æ—¥å¿—æ–‡ä»¶: logs/alphafin_finetune_cpu_$(date +%Y%m%d_%H%M%S).log"
echo ""
echo "ğŸ’¡ ç›‘æ§å‘½ä»¤:"
echo "  - æŸ¥çœ‹æ—¥å¿—: tail -f logs/alphafin_finetune_cpu_*.log"
echo "  - æŸ¥çœ‹è¿›ç¨‹: ps aux | grep $PID"
echo "  - åœæ­¢è®­ç»ƒ: kill $PID"
echo ""
echo "â° é¢„è®¡å®Œæˆæ—¶é—´: 3-5å°æ—¶ (CPUæ¨¡å¼)"
echo ""
echo "ğŸ“Š è®­ç»ƒè¿›åº¦ç›‘æ§:"
echo "  - å®æ—¶æ—¥å¿—: tail -f logs/alphafin_finetune_cpu_*.log | grep -E '(Epoch|MRR|loss)'"
echo "  - æ¨¡å‹æ–‡ä»¶: ls -la $FULL_OUTPUT_DIR/"
echo "  - è¯„ä¼°ç»“æœ: tail -f $FULL_OUTPUT_DIR/mrr_eval_mrr_evaluation_results.csv" 