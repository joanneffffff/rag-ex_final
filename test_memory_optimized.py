#!/usr/bin/env python3
"""
å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬çš„æµ‹è¯•è„šæœ¬ï¼Œä½¿ç”¨4bité‡åŒ–
"""

import sys
import os
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent))

def test_memory_optimized():
    """ä½¿ç”¨4bité‡åŒ–çš„å†…å­˜ä¼˜åŒ–æµ‹è¯•"""
    try:
        from config.parameters import Config
        from xlm.registry.generator import load_generator
        from xlm.components.rag_system.rag_system import PROMPT_TEMPLATE_ZH
        
        print("ğŸ§ª å†…å­˜ä¼˜åŒ–æµ‹è¯•ï¼ˆ4bité‡åŒ–ï¼‰...")
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä¼˜åŒ–å†…å­˜ä½¿ç”¨
        os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
        
        # åŠ è½½é…ç½®
        config = Config()
        print(f"ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å‹: {config.generator.model_name}")
        print(f"é‡åŒ–ç±»å‹: {config.generator.quantization_type}")
        print(f"max_new_tokens: {config.generator.max_new_tokens}")
        
        # åŠ è½½ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨4bité‡åŒ–ï¼‰
        print("\nğŸ”§ åŠ è½½ç”Ÿæˆå™¨ï¼ˆ4bité‡åŒ–ï¼‰...")
        generator = load_generator(
            generator_model_name=config.generator.model_name,
            use_local_llm=True,
            use_gpu=True,
            gpu_device="cuda:1",
            cache_dir=config.generator.cache_dir
        )
        
        # æµ‹è¯•é—®é¢˜
        test_question = "é¦–é’¢è‚¡ä»½çš„ä¸šç»©è¡¨ç°å¦‚ä½•ï¼Ÿ"
        
        # æ¨¡æ‹Ÿæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        test_context = """
        é¦–é’¢è‚¡ä»½2023å¹´ä¸ŠåŠå¹´ä¸šç»©æŠ¥å‘Šæ˜¾ç¤ºï¼Œå…¬å¸å®ç°è¥ä¸šæ”¶å…¥1,234.56äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™21.7%ï¼›
        å‡€åˆ©æ¶¦ä¸º-3.17äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™77.14%ï¼Œæ¯è‚¡äºæŸ0.06å…ƒã€‚
        å…¬å¸è¡¨ç¤ºå°†é€šè¿‡æ³¨å…¥ä¼˜è´¨èµ„äº§æ¥æå‡é•¿æœŸç›ˆåˆ©èƒ½åŠ›ï¼Œå›é¦ˆè‚¡ä¸œã€‚
        """
        
        # æ„å»ºprompt
        prompt = PROMPT_TEMPLATE_ZH.format(context=test_context, question=test_question)
        
        print(f"\nğŸ“ æµ‹è¯•é—®é¢˜: {test_question}")
        print(f"ğŸ“„ ä¸Šä¸‹æ–‡é•¿åº¦: {len(test_context)} å­—ç¬¦")
        print(f"ğŸ”§ Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
        
        # ç”Ÿæˆå›ç­”
        print(f"\nğŸ¤– ç”Ÿæˆå›ç­”ä¸­ï¼ˆ4bité‡åŒ–ï¼‰...")
        generated_responses = generator.generate(texts=[prompt])
        answer = generated_responses[0] if generated_responses else "ç”Ÿæˆå¤±è´¥"
        
        print(f"\nâœ… ç”Ÿæˆçš„å›ç­”:")
        print("=" * 50)
        print(answer)
        print("=" * 50)
        
        # åˆ†æå›ç­”
        answer_length = len(answer)
        word_count = len(answer.split())
        
        print(f"\nğŸ“Š å›ç­”åˆ†æ:")
        print(f"å­—ç¬¦æ•°: {answer_length}")
        print(f"è¯æ•°: {word_count}")
        
        # è´¨é‡è¯„ä¼°
        quality_score = 0
        issues = []
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
        if "é¦–é’¢" in answer and ("ä¸šç»©" in answer or "åˆ©æ¶¦" in answer or "æ”¶å…¥" in answer):
            quality_score += 2
            print("âœ… åŒ…å«å…³é”®ä¿¡æ¯")
        else:
            issues.append("ç¼ºå°‘å…³é”®ä¿¡æ¯")
            print("âŒ ç¼ºå°‘å…³é”®ä¿¡æ¯")
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æ•°æ®
        if any(char.isdigit() for char in answer):
            quality_score += 1
            print("âœ… åŒ…å«å…·ä½“æ•°æ®")
        else:
            issues.append("ç¼ºå°‘å…·ä½“æ•°æ®")
            print("âŒ ç¼ºå°‘å…·ä½“æ•°æ®")
        
        # æ£€æŸ¥æ˜¯å¦å®Œæ•´
        if answer.endswith(('.', 'ã€‚', 'ï¼', 'ï¼Ÿ')):
            quality_score += 1
            print("âœ… å›ç­”å®Œæ•´")
        else:
            issues.append("å›ç­”ä¸å®Œæ•´")
            print("âŒ å›ç­”ä¸å®Œæ•´")
        
        # æ£€æŸ¥é•¿åº¦æ˜¯å¦åˆé€‚
        if 50 <= answer_length <= 500:  # æ”¾å®½é•¿åº¦é™åˆ¶ï¼Œå› ä¸ºå¢åŠ äº†tokenæ•°
            quality_score += 1
            print("âœ… é•¿åº¦é€‚ä¸­")
        elif answer_length < 50:
            issues.append("å›ç­”è¿‡çŸ­")
            print("âŒ å›ç­”è¿‡çŸ­")
        else:
            issues.append("å›ç­”è¿‡é•¿")
            print("âŒ å›ç­”è¿‡é•¿")
        
        print(f"\nğŸ† è´¨é‡è¯„åˆ†: {quality_score}/5")
        if issues:
            print(f"é—®é¢˜: {', '.join(issues)}")
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        try:
            import torch
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.memory_allocated(1) / 1024**3
                gpu_memory_reserved = torch.cuda.memory_reserved(1) / 1024**3
                print(f"\nğŸ’¾ GPUå†…å­˜ä½¿ç”¨æƒ…å†µ:")
                print(f"å·²åˆ†é…: {gpu_memory:.2f} GB")
                print(f"å·²ä¿ç•™: {gpu_memory_reserved:.2f} GB")
        except Exception as e:
            print(f"æ— æ³•è·å–GPUå†…å­˜ä¿¡æ¯: {e}")
        
        return {
            "answer": answer,
            "length": answer_length,
            "word_count": word_count,
            "quality_score": quality_score,
            "issues": issues
        }
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    test_memory_optimized() 