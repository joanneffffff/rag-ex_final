#!/usr/bin/env python3
"""
RAGç³»ç»Ÿæ‰°åŠ¨å®éªŒæµç¨‹
ä½¿ç”¨rag_system_adapterçš„RAGç³»ç»Ÿå‡½æ•°è¿›è¡Œæ‰°åŠ¨å®éªŒ
åŒ…æ‹¬æ ·æœ¬é€‰æ‹©ã€æ‰°åŠ¨åº”ç”¨ã€ç­”æ¡ˆæ¯”è¾ƒã€é‡è¦æ€§è®¡ç®—å’ŒLLM Judgeè¯„ä¼°
"""

import sys
import os
import json
import time
import random
import re
import argparse
import torch
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from collections import Counter
from typing import Set
import gc

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨æ¨¡å—çº§åˆ«æ‰§è¡Œåˆå§‹åŒ–ä»£ç 
# from alphafin_data_process.rag_system_adapter import RagSystemAdapter
# from xlm.modules.perturber.trend_perturber import TrendPerturber
# from xlm.modules.perturber.year_perturber import YearPerturber
# from xlm.modules.perturber.term_perturber import TermPerturber
# from xlm.modules.comparator.embedding_comparator import EmbeddingComparator
# from xlm.components.encoder.encoder import Encoder
# from config.parameters import Config

# å¯¼å…¥LLM JudgeåŠŸèƒ½
from llm_comparison.chinese_llm_judge import ModelLoader, get_judge_messages

def classify_question_type(question: str) -> str:
    """åˆ†ç±»é—®é¢˜ç±»å‹"""
    question_lower = question.lower()
    
    if any(word in question_lower for word in ['å¤šå°‘', 'å‡ ', 'æ•°é‡', 'é‡‘é¢', 'æ•°å­—']):
        return "æ•°å€¼å‹"
    elif any(word in question_lower for word in ['ä¸ºä»€ä¹ˆ', 'åŸå› ', 'å¯¼è‡´', 'å½±å“']):
        return "åŸå› åˆ†æå‹"
    elif any(word in question_lower for word in ['å¦‚ä½•', 'æ€ä¹ˆ', 'æ–¹æ³•', 'ç­–ç•¥']):
        return "æ–¹æ³•ç­–ç•¥å‹"
    elif any(word in question_lower for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'å·®å¼‚', 'åŒºåˆ«']):
        return "å¯¹æ¯”åˆ†æå‹"
    elif any(word in question_lower for word in ['è¶‹åŠ¿', 'å˜åŒ–', 'å‘å±•', 'å¢é•¿']):
        return "è¶‹åŠ¿åˆ†æå‹"
    else:
        return "ä¸€èˆ¬æè¿°å‹"

def classify_context_type(context: str) -> str:
    """åˆ†ç±»ä¸Šä¸‹æ–‡ç±»å‹"""
    context_lower = context.lower()
    
    if any(word in context_lower for word in ['è´¢åŠ¡', 'è¥æ”¶', 'åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º']):
        return "è´¢åŠ¡æ•°æ®å‹"
    elif any(word in context_lower for word in ['ä¸šç»©', 'è¡¨ç°', 'å¢é•¿', 'ä¸‹é™']):
        return "ä¸šç»©è¡¨ç°å‹"
    elif any(word in context_lower for word in ['æ”¿ç­–', 'è§„å®š', 'æ³•è§„', 'åˆ¶åº¦']):
        return "æ”¿ç­–æ³•è§„å‹"
    elif any(word in context_lower for word in ['å¸‚åœº', 'ç«äº‰', 'ä»½é¢', 'åœ°ä½']):
        return "å¸‚åœºç«äº‰å‹"
    else:
        return "ä¸€èˆ¬ä¿¡æ¯å‹"

def calculate_complexity_score(question: str, answer: str) -> float:
    """è®¡ç®—å¤æ‚åº¦åˆ†æ•°"""
    question_length = len(question)
    answer_length = len(answer)
    
    financial_terms = ['è¥æ”¶', 'åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º', 'å¸‚ç›ˆç‡', 'å¸‚å‡€ç‡', 'ROE', 'ROA']
    term_count = sum(1 for term in financial_terms if term in question or term in answer)
    
    complexity = (question_length * 0.3 + answer_length * 0.4 + term_count * 0.3) / 100
    return min(complexity, 1.0)

class PerturbationSampleSelector:
    """æ‰°åŠ¨æ ·æœ¬é€‰æ‹©å™¨ - æ”¹è¿›ç‰ˆï¼Œç¡®ä¿é€‰æ‹©å¯æ‰°åŠ¨çš„æ ·æœ¬"""
    
    def __init__(self):
        # ä¸TrendPerturberä¿æŒä¸€è‡´çš„æ˜ å°„ - ä½¿ç”¨ä¸æ‰°åŠ¨å™¨å®Œå…¨ç›¸åŒçš„è¯æ±‡æ˜ å°„
        self.trend_map = {
            "ä¸Šå‡": "ä¸‹é™", "ä¸Šæ¶¨": "ä¸‹è·Œ", "å¢é•¿": "å‡å°‘", "æå‡": "é™ä½", "å¢åŠ ": "å‡å°‘",
            "ä¸‹é™": "ä¸Šå‡", "ä¸‹è·Œ": "ä¸Šæ¶¨", "å‡å°‘": "å¢é•¿", "é™ä½": "æå‡",
            "å¥½è½¬": "æ¶åŒ–", "æ”¹å–„": "æ¶åŒ–", "ç§¯æ": "æ¶ˆæ", "ç›ˆåˆ©": "äºæŸ",
            "æ‰©å¼ ": "æ”¶ç¼©", "æŒç»­å¢é•¿": "æŒç»­ä¸‹æ»‘", "ç¨³æ­¥å¢é•¿": "æ˜¾è‘—ä¸‹é™",
            "å¼ºåŠ²": "ç–²è½¯", "é«˜äº": "ä½äº", "ä¼˜äº": "åŠ£äº", "é¢†å…ˆ": "è½å",
            "å¢åŠ ç‡": "å‡å°‘ç‡", "ä¸Šå‡è¶‹åŠ¿": "ä¸‹é™è¶‹åŠ¿", "å¢é•¿è¶‹åŠ¿": "å‡å°‘è¶‹åŠ¿"
        }
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œä¸TrendPerturberå®Œå…¨ä¸€è‡´
        self.trend_patterns = {
            "zh": {
                re.compile(r'(' + re.escape(k) + r')'): v 
                for k, v in self.trend_map.items()
            }
        }
        
        # ä¸YearPerturberä¿æŒä¸€è‡´çš„å¹´ä»½æ¨¡å¼
        self.year_patterns = {
            "zh": [
                re.compile(r'(\d{4})\s*å¹´'),      # "2023å¹´"
                re.compile(r'(\d{4})\s*å¹´åº¦'),    # "2023å¹´åº¦"
                re.compile(r'(\d{4})å¹´(\d{1,2})æœˆ'), # "2023å¹´1æœˆ"
            ]
        }
        
        # ä¸TermPerturberä¿æŒä¸€è‡´çš„æœ¯è¯­æ˜ å°„
        self.term_map = {
            "å¸‚ç›ˆç‡": "å‡€åˆ©æ¶¦", "å‡€åˆ©æ¶¦": "å¸‚ç›ˆç‡", "å¸‚å‡€ç‡": "å¸‚é”€ç‡", "å¸‚é”€ç‡": "å¸‚å‡€ç‡",
            "è¥æ”¶": "æ”¶å…¥", "æ”¶å…¥": "è¥æ”¶", "è¥ä¸šæ”¶å…¥": "è¥ä¸šåˆ©æ¶¦", "è¥ä¸šåˆ©æ¶¦": "è¥ä¸šæ”¶å…¥",
            "æ€»èµ„äº§": "å‡€èµ„äº§", "å‡€èµ„äº§": "æ€»èµ„äº§", "è´Ÿå€º": "èµ„äº§", "èµ„äº§": "è´Ÿå€º",
            "åˆ©æ¶¦": "æˆæœ¬", "æˆæœ¬": "åˆ©æ¶¦", "å¸‚å€¼": "ä¼°å€¼", "ä¼°å€¼": "å¸‚å€¼",
            "è‚¡æ¯": "åˆ†çº¢", "åˆ†çº¢": "è‚¡æ¯", "é…è‚¡": "å¢å‘", "å¢å‘": "é…è‚¡",
            "å›è´­": "å¢å‘", "äº¤æ˜“é‡": "æˆäº¤é¢", "æˆäº¤é¢": "äº¤æ˜“é‡", "æ¢æ‰‹ç‡": "äº¤æ˜“é‡"
        }
        
        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼Œä¸TermPerturberå®Œå…¨ä¸€è‡´
        self.term_patterns = {
            "zh": {
                re.compile(r'(' + re.escape(k) + r')'): v 
                for k, v in self.term_map.items()
            }
        }
    
    def load_dataset(self, file_path: str) -> List[Dict]:
        """åŠ è½½è¯„æµ‹æ•°æ®é›† - æ”¯æŒJSONLæ ¼å¼"""
        samples = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        samples.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                        continue
        return samples
    
    def analyze_sample(self, sample: Dict) -> Dict:
        """åˆ†æå•ä¸ªæ ·æœ¬çš„å…³é”®è¯åˆ†å¸ƒ - ä½¿ç”¨ä¸æ‰°åŠ¨å™¨ä¸€è‡´çš„æ£€æµ‹æ–¹æ³•"""
        summary = sample.get('summary', '')
        content = sample.get('content', '')
        generated_question = sample.get('generated_question', '')
        
        # ä¸»è¦å…³æ³¨contextå­—æ®µï¼ˆsummaryå’Œcontentï¼‰ï¼Œå› ä¸ºè¿™æ˜¯æ‰°åŠ¨å™¨ä½œç”¨çš„å¯¹è±¡
        context_text = f"{summary} {content}"
        question_text = generated_question
        
        # ä½¿ç”¨ä¸æ‰°åŠ¨å™¨ä¸€è‡´çš„æ£€æµ‹æ–¹æ³•
        context_trend_found = self._detect_trend_terms(context_text)
        context_year_found = self._detect_year_terms(context_text)
        context_term_found = self._detect_term_terms(context_text)
        
        question_trend_found = self._detect_trend_terms(question_text)
        question_year_found = self._detect_year_terms(question_text)
        question_term_found = self._detect_term_terms(question_text)
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯ï¼ˆä½†ä¸»è¦æƒé‡ç»™contextï¼‰
        trend_found = context_trend_found | question_trend_found
        year_found = context_year_found | question_year_found
        term_found = context_term_found | question_term_found
        
        return {
            'sample_id': sample.get('id', 'unknown'),
            'summary': summary,
            'content': content,
            'generated_question': generated_question,
            'trend_keywords': trend_found,
            'year_keywords': year_found,
            'term_keywords': term_found,
            'context_trend_score': len(context_trend_found),
            'context_year_score': len(context_year_found),
            'context_term_score': len(context_term_found),
            'question_trend_score': len(question_trend_found),
            'question_year_score': len(question_year_found),
            'question_term_score': len(question_term_found),
            'trend_score': len(trend_found),
            'year_score': len(year_found),
            'term_score': len(term_found),
            'total_score': len(trend_found) + len(year_found) + len(term_found),
            'context_score': len(context_trend_found) + len(context_year_found) + len(context_term_found)
        }
    
    def _detect_year_terms(self, text: str) -> Set[str]:
        """æ£€æµ‹å¹´ä»½æœ¯è¯­ - ä½¿ç”¨ä¸YearPerturberä¸€è‡´çš„æ–¹æ³•"""
        found_terms = set()
        lang = "zh"  # ä¸“æ³¨äºä¸­æ–‡
        patterns = self.year_patterns.get(lang, [])
        
        for pattern in patterns:
            matches = pattern.findall(text)
            for match in matches:
                if isinstance(match, tuple):
                    # å¦‚æœæ˜¯å…ƒç»„ï¼Œå–ç¬¬ä¸€ä¸ªå…ƒç´ ï¼ˆå¹´ä»½ï¼‰
                    year = match[0] if match[0].isdigit() else match[1] if len(match) > 1 and match[1].isdigit() else None
                    if year and 1900 <= int(year) <= 2050:  # ä¸YearPerturberä¿æŒä¸€è‡´çš„å¹´ä»½èŒƒå›´
                        found_terms.add(year)
                elif isinstance(match, str) and match.isdigit():
                    if 1900 <= int(match) <= 2050:  # ä¸YearPerturberä¿æŒä¸€è‡´çš„å¹´ä»½èŒƒå›´
                        found_terms.add(match)
        
        return found_terms
    
    def _detect_trend_terms(self, text: str) -> Set[str]:
        """æ£€æµ‹è¶‹åŠ¿æœ¯è¯­ - ä½¿ç”¨ä¸TrendPerturberä¸€è‡´çš„æ–¹æ³•"""
        found_terms = set()
        lang = "zh"  # ä¸“æ³¨äºä¸­æ–‡
        patterns = self.trend_patterns.get(lang, {})
        
        for pattern, antonym in patterns.items():
            matches = pattern.findall(text)
            for match in matches:
                found_terms.add(match)
        
        return found_terms
    
    def _detect_term_terms(self, text: str) -> Set[str]:
        """æ£€æµ‹é‡‘èæœ¯è¯­ - ä½¿ç”¨ä¸TermPerturberä¸€è‡´çš„æ–¹æ³•"""
        found_terms = set()
        lang = "zh"  # ä¸“æ³¨äºä¸­æ–‡
        patterns = self.term_patterns.get(lang, {})
        
        for pattern, replacement in patterns.items():
            matches = pattern.findall(text)
            for match in matches:
                found_terms.add(match)
        
        return found_terms
    
    def select_samples(self, samples: List[Dict], target_count: int = 20) -> List[Dict]:
        """é€‰æ‹©é€‚åˆçš„æ ·æœ¬ - ç¡®ä¿å¯æ‰°åŠ¨æ€§"""
        print(f"ğŸ¯ å¼€å§‹æ™ºèƒ½æ ·æœ¬é€‰æ‹©ï¼Œç›®æ ‡æ•°é‡: {target_count}")
        print("ğŸ” ä¸¥æ ¼ç­›é€‰å¯æ‰°åŠ¨æ ·æœ¬...")
        
        analyzed_samples = [self.analyze_sample(sample) for sample in samples]
        
        # éªŒè¯æ‰°åŠ¨å¯è¡Œæ€§ï¼šç¡®ä¿æ ·æœ¬è‡³å°‘æ”¯æŒä¸€ç§æ‰°åŠ¨ç±»å‹
        feasible_samples = []
        for sample in analyzed_samples:
            # æ£€æŸ¥æ˜¯å¦è‡³å°‘æ”¯æŒä¸€ç§æ‰°åŠ¨ç±»å‹ï¼ˆæé«˜é˜ˆå€¼ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„æ‰°åŠ¨å…ƒç´ ï¼‰
            has_year_perturbation = sample['year_score'] >= 1  # è‡³å°‘1ä¸ªå¹´ä»½
            has_trend_perturbation = sample['trend_score'] >= 1  # è‡³å°‘1ä¸ªè¶‹åŠ¿è¯
            has_term_perturbation = sample['term_score'] >= 1  # è‡³å°‘1ä¸ªé‡‘èæœ¯è¯­
            
            # é¢å¤–æ£€æŸ¥ï¼šç¡®ä¿æ ·æœ¬æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡å†…å®¹
            context_length = len(sample.get('summary', '') + sample.get('content', ''))
            has_sufficient_context = context_length >= 50  # è‡³å°‘50ä¸ªå­—ç¬¦çš„ä¸Šä¸‹æ–‡
            
            if (has_year_perturbation or has_trend_perturbation or has_term_perturbation) and has_sufficient_context:
                feasible_samples.append(sample)
                print(f"âœ… æ ·æœ¬ {sample['sample_id']} æ”¯æŒæ‰°åŠ¨ (ä¸Šä¸‹æ–‡é•¿åº¦: {context_length}):")
                if has_year_perturbation:
                    print(f"  å¹´ä»½æ‰°åŠ¨: {sample['year_score']} ä¸ªå¹´ä»½ ({sample['year_keywords']})")
                if has_trend_perturbation:
                    print(f"  è¶‹åŠ¿æ‰°åŠ¨: {sample['trend_score']} ä¸ªè¶‹åŠ¿è¯ ({sample['trend_keywords']})")
                if has_term_perturbation:
                    print(f"  æœ¯è¯­æ‰°åŠ¨: {sample['term_score']} ä¸ªé‡‘èæœ¯è¯­ ({sample['term_keywords']})")
            else:
                if not has_sufficient_context:
                    print(f"âŒ æ ·æœ¬ {sample['sample_id']} ä¸Šä¸‹æ–‡å†…å®¹ä¸è¶³ ({context_length} å­—ç¬¦)ï¼Œè·³è¿‡")
                else:
                    print(f"âŒ æ ·æœ¬ {sample['sample_id']} ä¸æ”¯æŒä»»ä½•æ‰°åŠ¨ç±»å‹ï¼Œè·³è¿‡")
                continue  # æ˜ç¡®è·³è¿‡ä¸æ”¯æŒçš„æ ·æœ¬
        
        print(f"ğŸ“Š å¯æ‰°åŠ¨æ ·æœ¬æ•°: {len(feasible_samples)}")
        
        if len(feasible_samples) < target_count:
            print(f"âš ï¸ å¯æ‰°åŠ¨æ ·æœ¬æ•°é‡ä¸è¶³ï¼Œå°†é€‰æ‹©æ‰€æœ‰ {len(feasible_samples)} ä¸ªæ ·æœ¬")
            target_count = len(feasible_samples)
        
        # æŒ‰æ‰°åŠ¨èƒ½åŠ›æ’åºå¹¶é€‰æ‹©
        feasible_samples.sort(key=lambda x: x['total_score'], reverse=True)
        selected_samples = feasible_samples[:target_count]
        
        print(f"âœ… æœ€ç»ˆé€‰æ‹©äº† {len(selected_samples)} ä¸ªæ ·æœ¬")
        for i, sample in enumerate(selected_samples):
            print(f"  {i+1}. {sample['sample_id']} (æ€»åˆ†: {sample['total_score']})")
        
        return selected_samples

@dataclass
class PerturbationSample:
    """æ‰°åŠ¨å®éªŒæ ·æœ¬"""
    sample_id: str
    context: str
    question: str
    expected_answer: str
    question_type: str
    context_type: str
    complexity_score: float
    diversity_score: float

@dataclass
class PerturbationDetail:
    """æ‰°åŠ¨è¯¦ç»†ä¿¡æ¯"""
    perturber_name: str
    original_text: str
    perturbed_text: str
    perturbation_type: str  # "term", "year", "trend"
    changed_elements: List[str]  # å…·ä½“å˜åŒ–çš„å…ƒç´ 
    change_description: str  # å˜åŒ–æè¿°
    timestamp: str

@dataclass
class PerturbationResult:
    """æ‰°åŠ¨å®éªŒç»“æœ"""
    sample_id: str
    perturber_name: str
    original_answer: str
    perturbed_answer: str
    perturbation_detail: PerturbationDetail  # ä½¿ç”¨æ–°çš„æ‰°åŠ¨è¯¦æƒ…ç±»
    similarity_score: float
    importance_score: float
    f1_score: float  # æ–°å¢F1åˆ†æ•°
    em_score: float  # æ–°å¢EMåˆ†æ•°
    llm_judge_scores: Dict[str, Any]
    timestamp: str
    perturbation_target: str = "summary"  # é»˜è®¤å¯¹summaryæ‰°åŠ¨ï¼Œä¹Ÿå¯ä»¥æ˜¯"prompt"
    # æœŸæœ›ç­”æ¡ˆè¯„ä¼°æŒ‡æ ‡
    expected_vs_original_f1: float = 0.0
    expected_vs_original_em: float = 0.0
    expected_vs_perturbed_f1: float = 0.0
    expected_vs_perturbed_em: float = 0.0
    # æ‰°åŠ¨å½±å“æŒ‡æ ‡
    f1_improvement: float = 0.0
    em_improvement: float = 0.0
    llm_judge_improvement: float = 0.0

class RAGPerturbationExperiment:
    """RAGç³»ç»Ÿæ‰°åŠ¨å®éªŒç±» - ä½¿ç”¨rag_system_adapter"""
    
    def __init__(self):
        """åˆå§‹åŒ–å®éªŒç¯å¢ƒ"""
        print("ğŸ”¬ åˆå§‹åŒ–RAGæ‰°åŠ¨å®éªŒç¯å¢ƒï¼ˆä½¿ç”¨rag_system_adapterï¼‰...")
        
        # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…åœ¨æ¨¡å—çº§åˆ«æ‰§è¡Œåˆå§‹åŒ–ä»£ç 
        try:
            from alphafin_data_process.rag_system_adapter import RagSystemAdapter
            from xlm.modules.perturber.trend_perturber import TrendPerturber
            from xlm.modules.perturber.year_perturber import YearPerturber
            from xlm.modules.perturber.term_perturber import TermPerturber
            from xlm.modules.comparator.embedding_comparator import EmbeddingComparator
            from xlm.components.encoder.encoder import Encoder
            from config.parameters import Config
            
            print("âœ… æ‰€æœ‰æ¨¡å—å¯¼å…¥æˆåŠŸ")
        except ImportError as e:
            print(f"âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
            raise
        
        # ä½¿ç”¨ç°æœ‰é…ç½®ï¼Œä¸ä¸‹è½½æ–°æ¨¡å‹
        self.config = Config()
        print(f"ğŸ“‹ ä½¿ç”¨é…ç½®: {self.config}")
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿé€‚é…å™¨
        try:
            print("ğŸš€ åˆå§‹åŒ–RAGç³»ç»Ÿé€‚é…å™¨...")
            self.rag_adapter = RagSystemAdapter(self.config)
            print("âœ… RAGç³»ç»Ÿé€‚é…å™¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ RAGç³»ç»Ÿé€‚é…å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–æ‰°åŠ¨å™¨
        try:
            print("ğŸ”§ åˆå§‹åŒ–æ‰°åŠ¨å™¨...")
            self.year_perturber = YearPerturber()
            self.trend_perturber = TrendPerturber()
            self.term_perturber = TermPerturber()
            print("âœ… æ‰°åŠ¨å™¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ æ‰°åŠ¨å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–ç¼–ç å™¨å’Œæ¯”è¾ƒå™¨
        try:
            print("ğŸ”§ åˆå§‹åŒ–ç¼–ç å™¨å’Œæ¯”è¾ƒå™¨...")
            self.encoder = Encoder(
                model_name=self.config.encoder.chinese_model_path,
                cache_dir=self.config.encoder.cache_dir,
                device=self.config.encoder.device
            )
            self.comparator = EmbeddingComparator(self.encoder)
            print("âœ… ç¼–ç å™¨å’Œæ¯”è¾ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ ç¼–ç å™¨å’Œæ¯”è¾ƒå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        try:
            print("ğŸ”§ åˆå§‹åŒ–ç”Ÿæˆå™¨...")
            from xlm.components.generator.local_llm_generator import LocalLLMGenerator
            self.generator = LocalLLMGenerator(
                model_name=self.config.generator.model_name,
                cache_dir=self.config.generator.cache_dir,
                device=self.config.generator.device,
                use_quantization=self.config.generator.use_quantization,
                quantization_type=self.config.generator.quantization_type,
                use_flash_attention=self.config.generator.use_flash_attention
            )
            print("âœ… ç”Ÿæˆå™¨åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–LLM Judge
        try:
            print("ğŸ”§ åˆå§‹åŒ–LLM Judge...")
            from llm_comparison.chinese_llm_judge import ModelLoader
            device = self.config.generator.device or "cuda:0"
            self.llm_judge = ModelLoader(
                model_name=self.config.generator.model_name,
                device=device
            )
            print("âœ… LLM Judgeåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"âŒ LLM Judgeåˆå§‹åŒ–å¤±è´¥: {e}")
            raise
        
        # åˆ›å»ºæ‰°åŠ¨å™¨å­—å…¸ï¼Œä¿æŒå‘åå…¼å®¹
        self.perturbers = {
            'year': self.year_perturber,
            'trend': self.trend_perturber,
            'term': self.term_perturber
        }
        
        print("âœ… å®éªŒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print("ğŸ“Š å¯ç”¨çš„æ‰°åŠ¨å™¨: ['year', 'trend', 'term']")
        print("ğŸ¯ ä¸“æ³¨äºyearã€trendã€termä¸‰ä¸ªæ ¸å¿ƒæ‰°åŠ¨å™¨")
        print("ğŸ¤– ä½¿ç”¨ç”Ÿæˆå™¨: SUFE-AIFLM-Lab/Fin-R1")
        print("ğŸ” ä½¿ç”¨ç¼–ç å™¨: ä¸­æ–‡=models/alphafin_encoder_finetuned_1epoch, è‹±æ–‡=models/finetuned_tatqa_mixed_enhanced")
        self.log_file = "perturbation_experiment_log.jsonl"
        # æ¸…ç©ºæ—§æ—¥å¿—
        with open(self.log_file, 'w', encoding='utf-8') as f:
            pass
    
    def classify_question_type(self, question: str) -> str:
        """åˆ†ç±»é—®é¢˜ç±»å‹"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['å¤šå°‘', 'å‡ ', 'æ•°é‡', 'é‡‘é¢', 'æ•°å­—']):
            return "æ•°å€¼å‹"
        elif any(word in question_lower for word in ['ä¸ºä»€ä¹ˆ', 'åŸå› ', 'å¯¼è‡´', 'å½±å“']):
            return "åŸå› åˆ†æå‹"
        elif any(word in question_lower for word in ['å¦‚ä½•', 'æ€ä¹ˆ', 'æ–¹æ³•', 'ç­–ç•¥']):
            return "æ–¹æ³•ç­–ç•¥å‹"
        elif any(word in question_lower for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'å·®å¼‚', 'åŒºåˆ«']):
            return "å¯¹æ¯”åˆ†æå‹"
        elif any(word in question_lower for word in ['è¶‹åŠ¿', 'å˜åŒ–', 'å‘å±•', 'å¢é•¿']):
            return "è¶‹åŠ¿åˆ†æå‹"
        else:
            return "ä¸€èˆ¬æè¿°å‹"
    
    def classify_context_type(self, context: str) -> str:
        """åˆ†ç±»ä¸Šä¸‹æ–‡ç±»å‹"""
        context_lower = context.lower()
        
        if any(word in context_lower for word in ['è´¢åŠ¡', 'è¥æ”¶', 'åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º']):
            return "è´¢åŠ¡æ•°æ®å‹"
        elif any(word in context_lower for word in ['ä¸šç»©', 'è¡¨ç°', 'å¢é•¿', 'ä¸‹é™']):
            return "ä¸šç»©è¡¨ç°å‹"
        elif any(word in context_lower for word in ['æ”¿ç­–', 'è§„å®š', 'æ³•è§„', 'åˆ¶åº¦']):
            return "æ”¿ç­–æ³•è§„å‹"
        elif any(word in context_lower for word in ['å¸‚åœº', 'ç«äº‰', 'ä»½é¢', 'åœ°ä½']):
            return "å¸‚åœºç«äº‰å‹"
        else:
            return "ä¸€èˆ¬ä¿¡æ¯å‹"
    
    def calculate_complexity_score(self, question: str, answer: str) -> float:
        """è®¡ç®—å¤æ‚åº¦åˆ†æ•°"""
        # åŸºäºé—®é¢˜é•¿åº¦ã€ç­”æ¡ˆé•¿åº¦ã€ä¸“ä¸šè¯æ±‡æ•°é‡ç­‰
        question_length = len(question)
        answer_length = len(answer)
        
        # ä¸“ä¸šè¯æ±‡æ£€æµ‹
        financial_terms = ['è¥æ”¶', 'åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º', 'å¸‚ç›ˆç‡', 'å¸‚å‡€ç‡', 'ROE', 'ROA']
        term_count = sum(1 for term in financial_terms if term in question or term in answer)
        
        # å¤æ‚åº¦è®¡ç®—
        complexity = (question_length * 0.3 + answer_length * 0.4 + term_count * 0.3) / 100
        return min(complexity, 1.0)
    
    def calculate_diversity_score(self, sample: PerturbationSample, selected_samples: List[PerturbationSample]) -> float:
        """è®¡ç®—å¤šæ ·æ€§åˆ†æ•°"""
        if not selected_samples:
            return 1.0
        
        # è®¡ç®—ä¸å·²é€‰æ ·æœ¬çš„å·®å¼‚
        diversity_scores = []
        
        for selected in selected_samples:
            # é—®é¢˜ç±»å‹å·®å¼‚
            type_diff = 1.0 if sample.question_type != selected.question_type else 0.0
            # ä¸Šä¸‹æ–‡ç±»å‹å·®å¼‚
            context_diff = 1.0 if sample.context_type != selected.context_type else 0.0
            # é—®é¢˜é•¿åº¦å·®å¼‚
            length_diff = abs(len(sample.question) - len(selected.question)) / max(len(sample.question), len(selected.question))
            
            avg_diff = (type_diff + context_diff + length_diff) / 3
            diversity_scores.append(avg_diff)
        
        return sum(diversity_scores) / len(diversity_scores)
    
    def select_perturbation_samples(self, dataset_path: str, num_samples: int = 20) -> List[PerturbationSample]:
        """é›†æˆæ ·æœ¬é€‰æ‹©åŠŸèƒ½ - ä»æ•°æ®é›†ä¸­é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬"""
        print(f"ğŸ¯ ä»æ•°æ®é›† {dataset_path} ä¸­é€‰æ‹© {num_samples} ä¸ªä»£è¡¨æ€§æ ·æœ¬...")
        
        # åŠ è½½æ•°æ®é›†
        samples = []
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                samples.append(data)
        
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(samples)}")
        
        # è½¬æ¢ä¸ºPerturbationSampleå¯¹è±¡
        perturbation_samples = []
        for i, sample in enumerate(samples):
            # æå–summaryï¼ˆä¼˜å…ˆä½¿ç”¨summaryå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨contextï¼‰
            summary = sample.get('summary', '') or sample.get('context', '') or sample.get('generated_question', '')
            if not summary:
                continue
            # ä½¿ç”¨generated_question
            generated_question = sample.get('generated_question', '')
            if not generated_question:
                continue
            # æ–°å¢ï¼šåªä¿ç•™åŒ…å«è‚¡ç¥¨ä»£ç æˆ–å¹´ä»½çš„æ ·æœ¬
            import re
            if not (re.search(r'\b\d{6}\b', generated_question) or re.search(r'\b20\d{2}\b', generated_question) or re.search(r'\b19\d{2}\b', generated_question)):
                continue
            # åˆ†ç±»é—®é¢˜ç±»å‹å’Œä¸Šä¸‹æ–‡ç±»å‹
            question_type = classify_question_type(generated_question)
            context_type = classify_context_type(summary)
            # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
            complexity_score = calculate_complexity_score(generated_question, sample.get('expected_answer', ''))
            perturbation_sample = PerturbationSample(
                sample_id=sample.get('sample_id', f"sample_{i}"),
                context=summary,
                question=generated_question,
                expected_answer=sample.get('expected_answer', ''),
                question_type=question_type,
                context_type=context_type,
                complexity_score=complexity_score,
                diversity_score=0.0
            )
            perturbation_samples.append(perturbation_sample)
        
        print(f"ğŸ“Š æœ‰æ•ˆæ ·æœ¬æ•°: {len(perturbation_samples)}")
        
        # å¤šæ ·æ€§é€‰æ‹©
        selected_samples = []
        for i in range(min(num_samples, len(perturbation_samples))):
            if i == 0:
                # ç¬¬ä¸€ä¸ªæ ·æœ¬é€‰æ‹©å¤æ‚åº¦æœ€é«˜çš„
                best_sample = max(perturbation_samples, key=lambda x: x.complexity_score)
            else:
                # åç»­æ ·æœ¬é€‰æ‹©å¤šæ ·æ€§æœ€é«˜çš„
                for sample in perturbation_samples:
                    sample.diversity_score = self.calculate_diversity_score(sample, selected_samples)
                best_sample = max(perturbation_samples, key=lambda x: x.diversity_score)
            
            selected_samples.append(best_sample)
            perturbation_samples.remove(best_sample)
            
            print(f"âœ… é€‰æ‹©æ ·æœ¬ {i+1}: {best_sample.sample_id}")
            print(f"  é—®é¢˜ç±»å‹: {best_sample.question_type}")
            print(f"  ä¸Šä¸‹æ–‡ç±»å‹: {best_sample.context_type}")
            print(f"  å¤æ‚åº¦: {best_sample.complexity_score:.3f}")
            print(f"  å¤šæ ·æ€§: {best_sample.diversity_score:.3f}")
        
        return selected_samples
    
    def get_original_answer(self, context: str, question: str) -> str:
        """è·å–åŸå§‹ç­”æ¡ˆï¼ˆæ­¥éª¤2ï¼‰- ä½¿ç”¨å®Œæ•´çš„RAGç³»ç»Ÿæµç¨‹"""
        try:
            # ä½¿ç”¨RAGç³»ç»Ÿé€‚é…å™¨è¿›è¡Œå®Œæ•´æ£€ç´¢å’Œç”Ÿæˆ
            print("ğŸ” ä½¿ç”¨RAGç³»ç»Ÿè¿›è¡Œå®Œæ•´æ£€ç´¢...")
            
            # ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢æ¨¡å¼
            retrieval_results = self.rag_adapter.get_ranked_documents_for_evaluation(
                query=question,
                top_k=10,
                mode="reranker",  # ä½¿ç”¨é‡æ’åºæ¨¡å¼
                use_prefilter=True  # å¯ç”¨å…ƒæ•°æ®è¿‡æ»¤
            )
            
            if not retrieval_results:
                print("âŒ RAGç³»ç»Ÿæœªè¿”å›æ£€ç´¢ç»“æœ")
                return ""
            
            print(f"âœ… RAGç³»ç»Ÿæ£€ç´¢åˆ° {len(retrieval_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼‰
            retrieved_contexts = []
            for i, result in enumerate(retrieval_results[:3]):  # ä½¿ç”¨å‰3ä¸ªæ–‡æ¡£
                content = result.get('content', '')
                if content:
                    retrieved_contexts.append(f"æ–‡æ¡£{i+1}: {content}")
            
            # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨åŸå§‹context
            if not retrieved_contexts:
                retrieved_contexts = [context]
            
            combined_context = "\n\n".join(retrieved_contexts)
            
            # æ„å»ºprompt
            prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\nä¸Šä¸‹æ–‡ï¼š{combined_context}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
            
            # ä½¿ç”¨ç”Ÿæˆå™¨è·å–ç­”æ¡ˆ
            print("ğŸ¤– ä½¿ç”¨LLMç”ŸæˆåŸå§‹ç­”æ¡ˆ...")
            response = self.generator.generate([prompt])
            generated_answer = response[0]
            print(f"âœ… åŸå§‹ç­”æ¡ˆç”Ÿæˆå®Œæˆ: {generated_answer[:200]}...")
            return generated_answer
            
        except Exception as e:
            print(f"âŒ è·å–åŸå§‹ç­”æ¡ˆå¤±è´¥: {str(e)}")
            return ""
    
    def apply_perturbation(self, context: str, perturber_name: str) -> List[PerturbationDetail]:
        """åº”ç”¨æ‰°åŠ¨å™¨åˆ°ä¸Šä¸‹æ–‡"""
        print(f"ğŸ”§ åº”ç”¨ {perturber_name} æ‰°åŠ¨...")
        
        # è·å–å¯¹åº”çš„æ‰°åŠ¨å™¨
        perturber = self.perturbers.get(perturber_name)
        if not perturber:
            print(f"âŒ æœªæ‰¾åˆ°æ‰°åŠ¨å™¨: {perturber_name}")
            return []
        
        try:
            # åº”ç”¨æ‰°åŠ¨
            perturbations = perturber.perturb(context)
            print(f"âœ… ç”Ÿæˆäº† {len(perturbations)} ä¸ªæ‰°åŠ¨")
            
            results = []
            for i, perturbation in enumerate(perturbations):
                if isinstance(perturbation, dict):
                    perturbed_text = perturbation.get('perturbed_text', context)
                    perturbation_detail = perturbation.get('perturbation_detail', '')
                    original_feature = perturbation.get('original_feature', '')
                else:
                    perturbed_text = perturbation
                    perturbation_detail = f"{perturber_name}æ‰°åŠ¨å™¨åº”ç”¨"
                    original_feature = ''
                
                # åˆ†ææ–‡æœ¬å˜åŒ–
                changed_elements = self.analyze_text_changes(context, perturbed_text)
                
                # ç”Ÿæˆå˜åŒ–æè¿°
                change_description = self.generate_change_description(context, perturbed_text, perturber_name)
                
                # åˆ›å»ºæ‰°åŠ¨è¯¦æƒ…
                detail = PerturbationDetail(
                    perturber_name=perturber_name,
                    original_text=context,
                    perturbed_text=perturbed_text,
                    perturbation_type=self.get_perturbation_type(perturber_name),
                    changed_elements=changed_elements,
                    change_description=change_description,
                    timestamp=datetime.now().isoformat()
                )
                
                results.append(detail)
                
                # æ‰“å°è¯¦ç»†ä¿¡æ¯
                print(f"\n--- æ‰°åŠ¨ {i+1} ---")
                print(f"æ‰°åŠ¨å™¨: {perturber_name}")
                print(f"æ‰°åŠ¨ç±»å‹: {detail.perturbation_type}")
                print(f"å˜åŒ–æè¿°: {change_description}")
                print(f"å…·ä½“å˜åŒ–:")
                for element in changed_elements:
                    print(f"  â€¢ {element}")
                print(f"åŸå§‹æ–‡æœ¬: {context[:100]}...")
                print(f"æ‰°åŠ¨åæ–‡æœ¬: {perturbed_text[:100]}...")
            
            return results
            
        except Exception as e:
            print(f"âŒ æ‰°åŠ¨å™¨åº”ç”¨å¤±è´¥: {e}")
            return []
    
    def analyze_text_changes(self, original_text: str, perturbed_text: str) -> List[str]:
        """åˆ†ææ–‡æœ¬å˜åŒ–"""
        changes = []
        
        if original_text == perturbed_text:
            changes.append("æ— å˜åŒ–")
            return changes
        
        # ç®€å•çš„å˜åŒ–åˆ†æ
        original_words = original_text.split()
        perturbed_words = perturbed_text.split()
        
        # æ‰¾å‡ºæ–°å¢çš„è¯æ±‡
        original_set = set(original_words)
        perturbed_set = set(perturbed_words)
        
        added_words = perturbed_set - original_set
        removed_words = original_set - perturbed_set
        
        if added_words:
            changes.append(f"æ–°å¢è¯æ±‡: {list(added_words)[:3]}")  # åªæ˜¾ç¤ºå‰3ä¸ª
        
        if removed_words:
            changes.append(f"åˆ é™¤è¯æ±‡: {list(removed_words)[:3]}")
        
        # é•¿åº¦å˜åŒ–
        if len(perturbed_text) != len(original_text):
            length_diff = len(perturbed_text) - len(original_text)
            changes.append(f"æ–‡æœ¬é•¿åº¦å˜åŒ–: {length_diff:+d}å­—ç¬¦")
        
        return changes
    
    def generate_change_description(self, original_text: str, perturbed_text: str, perturber_name: str) -> str:
        """ç”Ÿæˆå˜åŒ–æè¿°"""
        if original_text == perturbed_text:
            return f"{perturber_name}æ‰°åŠ¨å™¨æœªæ£€æµ‹åˆ°å¯æ‰°åŠ¨çš„å…ƒç´ "
        
        # æ ¹æ®æ‰°åŠ¨å™¨ç±»å‹ç”Ÿæˆæè¿°
        if "term" in perturber_name.lower():
            return "é‡‘èæœ¯è¯­æ‰°åŠ¨ï¼šæ›¿æ¢æˆ–ä¿®æ”¹äº†é‡‘èç›¸å…³æœ¯è¯­"
        elif "year" in perturber_name.lower():
            return "å¹´ä»½æ‰°åŠ¨ï¼šä¿®æ”¹äº†æ—¶é—´ç›¸å…³çš„å¹´ä»½ä¿¡æ¯"
        elif "trend" in perturber_name.lower():
            return "è¶‹åŠ¿æ‰°åŠ¨ï¼šä¿®æ”¹äº†è¶‹åŠ¿ç›¸å…³çš„æè¿°"
        else:
            return f"{perturber_name}æ‰°åŠ¨ï¼šæ–‡æœ¬å†…å®¹å‘ç”Ÿå˜åŒ–"
    
    def get_perturbation_type(self, perturber_name: str) -> str:
        """è·å–æ‰°åŠ¨ç±»å‹"""
        if "term" in perturber_name.lower():
            return "term"
        elif "year" in perturber_name.lower():
            return "year"
        elif "trend" in perturber_name.lower():
            return "trend"
        else:
            return "unknown"
    
    def get_perturbed_answer(self, perturbed_context: str, question: str, perturber_name: Optional[str] = None) -> str:
        """è·å–æ‰°åŠ¨åç­”æ¡ˆï¼ˆæ­¥éª¤4ï¼‰- ä½¿ç”¨å®Œæ•´çš„RAGç³»ç»Ÿæµç¨‹"""
        try:
            # ä½¿ç”¨RAGç³»ç»Ÿé€‚é…å™¨è¿›è¡Œå®Œæ•´æ£€ç´¢å’Œç”Ÿæˆ
            print("ğŸ” ä½¿ç”¨RAGç³»ç»Ÿè¿›è¡Œæ‰°åŠ¨åæ£€ç´¢...")
            
            # ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢æ¨¡å¼
            retrieval_results = self.rag_adapter.get_ranked_documents_for_evaluation(
                query=question,
                top_k=10,
                mode="reranker",  # ä½¿ç”¨é‡æ’åºæ¨¡å¼
                use_prefilter=True  # å¯ç”¨å…ƒæ•°æ®è¿‡æ»¤
            )
            
            if not retrieval_results:
                print("âŒ RAGç³»ç»Ÿæœªè¿”å›æ£€ç´¢ç»“æœ")
                return ""
            
            print(f"âœ… RAGç³»ç»Ÿæ£€ç´¢åˆ° {len(retrieval_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆå¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹åº”ç”¨æ‰°åŠ¨ï¼‰
            retrieved_contexts = []
            for i, result in enumerate(retrieval_results[:3]):  # ä½¿ç”¨å‰3ä¸ªæ–‡æ¡£
                content = result.get('content', '')
                if content and perturber_name:
                    # å¯¹æ£€ç´¢åˆ°çš„å†…å®¹åº”ç”¨æ‰°åŠ¨
                    perturber = self.perturbers.get(perturber_name)
                    if perturber:
                        perturbations = perturber.perturb(content)
                        if perturbations and len(perturbations) > 0:
                            perturbed_content = perturbations[0].get('perturbed_text', content)
                            retrieved_contexts.append(f"æ–‡æ¡£{i+1}: {perturbed_content}")
                            print(f"âœ… å¯¹æ–‡æ¡£{i+1}åº”ç”¨{perturber_name}æ‰°åŠ¨")
                        else:
                            retrieved_contexts.append(f"æ–‡æ¡£{i+1}: {content}")
                            print(f"âš ï¸ æ–‡æ¡£{i+1}æœªæ£€æµ‹åˆ°å¯æ‰°åŠ¨å†…å®¹")
                    else:
                        retrieved_contexts.append(f"æ–‡æ¡£{i+1}: {content}")
                        print(f"âš ï¸ æœªæ‰¾åˆ°æ‰°åŠ¨å™¨: {perturber_name}")
                else:
                    # å¦‚æœæ²¡æœ‰æ‰°åŠ¨å™¨ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹å†…å®¹
                    retrieved_contexts.append(f"æ–‡æ¡£{i+1}: {content}")
            
            # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨æ‰°åŠ¨åçš„context
            if not retrieved_contexts:
                retrieved_contexts = [perturbed_context]
            
            combined_context = "\n\n".join(retrieved_contexts)
            
            # æ„å»ºprompt
            prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\nä¸Šä¸‹æ–‡ï¼š{combined_context}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
            
            # ä½¿ç”¨ç”Ÿæˆå™¨è·å–ç­”æ¡ˆ
            print("ğŸ¤– ä½¿ç”¨LLMç”Ÿæˆæ‰°åŠ¨åç­”æ¡ˆ...")
            response = self.generator.generate([prompt])
            generated_answer = response[0]
            print(f"âœ… æ‰°åŠ¨åç­”æ¡ˆç”Ÿæˆå®Œæˆ: {generated_answer[:200]}...")
            return generated_answer
            
        except Exception as e:
            print(f"âŒ è·å–æ‰°åŠ¨åç­”æ¡ˆå¤±è´¥: {str(e)}")
            return ""
    
    def _get_perturber_name_from_context(self, perturbed_context: str) -> str:
        """ä»æ‰°åŠ¨åçš„ä¸Šä¸‹æ–‡æ¨æ–­ä½¿ç”¨çš„æ‰°åŠ¨å™¨ç±»å‹"""
        # ç®€å•çš„å¯å‘å¼æ–¹æ³•æ¥åˆ¤æ–­ä½¿ç”¨äº†å“ªç§æ‰°åŠ¨å™¨
        if "2018" in perturbed_context or "2019" in perturbed_context or "2020" in perturbed_context:
            return "year"
        elif any(word in perturbed_context for word in ["å‡å°‘", "ä¸‹é™", "æ¶åŒ–", "é™ä½"]):
            return "trend"
        elif any(word in perturbed_context for word in ["å¸‚ç›ˆç‡", "å‡€åˆ©æ¶¦", "å¸‚å‡€ç‡", "å¸‚é”€ç‡"]):
            return "term"
        else:
            return "year"  # é»˜è®¤ä½¿ç”¨å¹´ä»½æ‰°åŠ¨å™¨
    
    def calculate_importance_score(self, original_answer: str, perturbed_answer: str) -> Tuple[float, float, float, float]:
        """è®¡ç®—ç›¸ä¼¼åº¦å’Œé‡è¦æ€§åˆ†æ•°ï¼ˆæ­¥éª¤5ï¼‰"""
        try:
            # ä½¿ç”¨æ¯”è¾ƒå™¨è®¡ç®—ç›¸ä¼¼åº¦
            similarity_scores = self.comparator.compare(original_answer, [perturbed_answer])
            similarity_score = similarity_scores[0] if similarity_scores else 0.0
            
            # é‡è¦æ€§åˆ†æ•° = 1 - ç›¸ä¼¼åº¦ï¼ˆRAG-Exè®ºæ–‡æ–¹æ³•ï¼‰
            importance_score = 1.0 - similarity_score
            
            # æ·»åŠ ä¼ ç»ŸF1å’ŒEMæŒ‡æ ‡è®¡ç®—
            f1_score = self.calculate_f1_score(original_answer, perturbed_answer)
            em_score = self.calculate_exact_match(original_answer, perturbed_answer)
            
            return similarity_score, importance_score, f1_score, em_score
        except Exception as e:
            print(f"âŒ è®¡ç®—é‡è¦æ€§åˆ†æ•°å¤±è´¥: {str(e)}")
            return 0.0, 0.0, 0.0, 0.0
    
    def normalize_answer_chinese(self, s: str) -> str:
        """
        é’ˆå¯¹ä¸­æ–‡è¿›è¡Œç­”æ¡ˆå½’ä¸€åŒ–ï¼šç§»é™¤æ ‡ç‚¹ã€è½¬æ¢å…¨è§’å­—ç¬¦ä¸ºåŠè§’ã€å»é™¤å¤šä½™ç©ºæ ¼ã€åˆ†è¯å¹¶å°å†™ã€‚
        ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯ï¼Œè·å¾—æ›´å‡†ç¡®çš„F1å’ŒEMè¯„ä¼°ã€‚
        """
        if not s:
            return ""

        s = s.strip().lower()

        s = s.replace('ï¼Œ', ',').replace('ã€‚', '.').replace('ï¼', '!').replace('ï¼Ÿ', '?').replace('ï¼›', ';')
        s = s.replace('ï¼ˆ', '(').replace('ï¼‰', ')')

        punctuation_pattern = r'[!"#$%&\'()*+,-./:;<=>?@[\]^_`{|}~""''ã€ã€‘ã€ã€ã€Šã€‹â€”â€¦Â·ï½ã€Œã€ï½ï¿¥%#@ï¼&ï¼ˆï¼‰ã€Šã€‹]'
        s = re.sub(punctuation_pattern, '', s)

        # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
        import jieba
        tokens = list(jieba.cut(s)) 

        normalized_tokens = [token for token in tokens if token.strip()]
        return " ".join(normalized_tokens)

    def get_tokens_chinese(self, s: str) -> List[str]:
        """è·å–ä¸­æ–‡åˆ†è¯åçš„tokensåˆ—è¡¨ã€‚"""
        return self.normalize_answer_chinese(s).split()

    def calculate_f1_score(self, prediction: str, ground_truth: str) -> float:
        """è®¡ç®—F1åˆ†æ•° (åŸºäºè¯é‡å )ã€‚"""
        gold_tokens = self.get_tokens_chinese(ground_truth)
        pred_tokens = self.get_tokens_chinese(prediction)

        common = Counter(gold_tokens) & Counter(pred_tokens)
        num_common = sum(common.values())

        if len(gold_tokens) == 0 and len(pred_tokens) == 0:
            return 1.0
        if len(gold_tokens) == 0 or len(pred_tokens) == 0:
            return 0.0

        precision = num_common / len(pred_tokens)
        recall = num_common / len(gold_tokens)

        if precision + recall == 0:
            return 0.0
        f1 = (2 * precision * recall) / (precision + recall)
        return f1

    def calculate_exact_match(self, prediction: str, ground_truth: str) -> float:
        """è®¡ç®—ç²¾ç¡®åŒ¹é…ç‡ã€‚"""
        return float(self.normalize_answer_chinese(prediction) == self.normalize_answer_chinese(ground_truth))
    
    def run_llm_judge_evaluation(self, original_answer: str, perturbed_answer: str, question: str) -> Dict[str, Any]:
        """è¿è¡ŒLLM Judgeè¯„ä¼°ï¼ˆæ­¥éª¤6ï¼‰- ä½¿ç”¨å•ä¾‹æ¨¡å¼é¿å…é‡å¤åŠ è½½"""
        try:
            print("ğŸ¤– è¿è¡ŒLLM Judgeè¯„ä¼°...")
            
            # æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦æœ‰æ•ˆ
            if not original_answer or not perturbed_answer:
                print("âš ï¸ ç­”æ¡ˆä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°")
                return {
                    'accuracy': 0.0,
                    'completeness': 0.0,
                    'professionalism': 0.0,
                    'overall_score': 0.0,
                    'reasoning': 'ç­”æ¡ˆä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°'
                }
            
            # å¯¼å…¥å•ä¾‹LLM Judge
            from llm_comparison.chinese_llm_judge import llm_judge_singleton
            
            # ç¡®ä¿LLM Judgeå·²åˆå§‹åŒ–
            if not hasattr(llm_judge_singleton, '_model_loader') or llm_judge_singleton._model_loader is None:
                try:
                    llm_judge_singleton.initialize()
                except Exception as e:
                    print(f"âŒ LLM Judgeåˆå§‹åŒ–å¤±è´¥: {e}")
                    return {
                        'accuracy': 0.0,
                        'completeness': 0.0,
                        'professionalism': 0.0,
                        'overall_score': 0.0,
                        'reasoning': f'LLM Judgeåˆå§‹åŒ–å¤±è´¥: {str(e)}',
                        'raw_output': 'åˆå§‹åŒ–å¤±è´¥'
                    }
            
            # æ‰§è¡Œè¯„ä¼°
            judge_result = llm_judge_singleton.evaluate(question, original_answer, perturbed_answer)
            
            # æ£€æŸ¥è¯„ä¼°ç»“æœæ˜¯å¦æœ‰æ•ˆ
            if (judge_result.get('accuracy', 0) == 0 and 
                judge_result.get('conciseness', 0) == 0 and 
                judge_result.get('professionalism', 0) == 0):
                print("âš ï¸ LLM Judgeè¿”å›å…¨é›¶è¯„åˆ†")
                return {
                    'accuracy': 0.0,
                    'completeness': 0.0,
                    'professionalism': 0.0,
                    'overall_score': 0.0,
                    'reasoning': 'LLM Judgeè¿”å›å…¨é›¶è¯„åˆ†',
                    'raw_output': judge_result.get('raw_output', '')
                }
            
            print(f"âœ… LLM Judgeè¯„ä¼°å®Œæˆ")
            print(f"  å‡†ç¡®æ€§: {judge_result.get('accuracy', 'N/A')}")
            print(f"  ç®€æ´æ€§: {judge_result.get('conciseness', 'N/A')}")
            print(f"  ä¸“ä¸šæ€§: {judge_result.get('professionalism', 'N/A')}")
            print(f"  æ€»ä½“è¯„åˆ†: {judge_result.get('overall_score', 'N/A')}")
            
            return {
                'accuracy': judge_result.get('accuracy', 0.0),
                'completeness': judge_result.get('conciseness', 0.0),  # ä½¿ç”¨concisenessä½œä¸ºcompleteness
                'professionalism': judge_result.get('professionalism', 0.0),
                'overall_score': judge_result.get('overall_score', 0.0),
                'reasoning': judge_result.get('reasoning', ''),
                'raw_output': judge_result.get('raw_output', '')
            }
            
        except Exception as e:
            print(f"âŒ LLM Judgeè¯„ä¼°å¤±è´¥: {str(e)}")
            return {
                'accuracy': 0.0,
                'completeness': 0.0,
                'professionalism': 0.0,
                'overall_score': 0.0,
                'reasoning': f'LLM Judgeè¯„ä¼°å¤±è´¥: {str(e)}',
                'raw_output': 'è¯„ä¼°å¤±è´¥'
            }
    

    
    def _parse_judge_response(self, response: str) -> Dict[str, Any]:
        """è§£æJudgeæ¨¡å‹çš„å“åº”"""
        try:
            # å°è¯•æå–åˆ†æ•°
            scores = {
                'accuracy': 7.0,
                'completeness': 7.0,
                'professionalism': 7.0,
                'overall_score': 7.0,
                'reasoning': response
            }
            
            # ç®€å•çš„åˆ†æ•°æå–é€»è¾‘
            import re
            
            # æŸ¥æ‰¾åˆ†æ•°æ¨¡å¼
            score_patterns = [
                r'å‡†ç¡®æ€§[ï¼š:]\s*(\d+(?:\.\d+)?)',
                r'å‡†ç¡®åº¦[ï¼š:]\s*(\d+(?:\.\d+)?)',
                r'accuracy[ï¼š:]\s*(\d+(?:\.\d+)?)',
                r'å®Œæ•´æ€§[ï¼š:]\s*(\d+(?:\.\d+)?)',
                r'completeness[ï¼š:]\s*(\d+(?:\.\d+)?)',
                r'ä¸“ä¸šæ€§[ï¼š:]\s*(\d+(?:\.\d+)?)',
                r'professionalism[ï¼š:]\s*(\d+(?:\.\d+)?)',
                r'æ€»ä½“è¯„åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)',
                r'overall[ï¼š:]\s*(\d+(?:\.\d+)?)'
            ]
            
            for pattern in score_patterns:
                matches = re.findall(pattern, response, re.IGNORECASE)
                if matches:
                    score = float(matches[0])
                    if 'å‡†ç¡®' in pattern or 'accuracy' in pattern.lower():
                        scores['accuracy'] = score
                    elif 'å®Œæ•´' in pattern or 'completeness' in pattern.lower():
                        scores['completeness'] = score
                    elif 'ä¸“ä¸š' in pattern or 'professionalism' in pattern.lower():
                        scores['professionalism'] = score
                    elif 'æ€»ä½“' in pattern or 'overall' in pattern.lower():
                        scores['overall_score'] = score
            
            # è®¡ç®—æ€»ä½“è¯„åˆ†
            if scores['accuracy'] == 7.0 and scores['completeness'] == 7.0 and scores['professionalism'] == 7.0:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å…·ä½“åˆ†æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                scores['overall_score'] = 7.0
            else:
                scores['overall_score'] = (scores['accuracy'] + scores['completeness'] + scores['professionalism']) / 3
            
            return scores
            
        except Exception as e:
            print(f"âš ï¸ è§£æJudgeå“åº”å¤±è´¥: {e}")
            return {
                'accuracy': 7.0,
                'completeness': 7.0,
                'professionalism': 7.0,
                'overall_score': 7.0,
                'reasoning': f"è§£æå¤±è´¥: {response[:100]}..."
            }
    
    def run_single_sample_experiment(self, sample: PerturbationSample) -> List[PerturbationResult]:
        print(f"\nğŸ”¬ æ ·æœ¬å®éªŒ: {sample.sample_id}")
        print(f"Generated Question: {sample.question}")
        print(f"Ground Truth: {sample.expected_answer}")
        print(f"é—®é¢˜ç±»å‹: {sample.question_type}")
        print(f"ä¸Šä¸‹æ–‡ç±»å‹: {sample.context_type}")
        print(f"Summaryé•¿åº¦: {len(sample.context)} å­—ç¬¦")
        print("=" * 60)
        
        results = []
        # æ­¥éª¤2: è·å–åŸå§‹ç­”æ¡ˆï¼ˆæ— æ‰°åŠ¨ï¼‰
        print("ğŸ“ æ­¥éª¤2: è·å–åŸå§‹ç­”æ¡ˆï¼ˆæ— æ‰°åŠ¨ï¼‰...")
        original_answer = self.get_original_answer(sample.context, sample.question)
        print(f"ğŸ“‹ åŸå§‹ç”Ÿæˆç­”æ¡ˆ: {original_answer}")
        print(f"ğŸ“ åŸå§‹ç”Ÿæˆç­”æ¡ˆé•¿åº¦: {len(original_answer)} å­—ç¬¦")
        
        if not original_answer:
            print("âŒ æ— æ³•è·å–åŸå§‹ç”Ÿæˆç­”æ¡ˆï¼Œè·³è¿‡æ­¤æ ·æœ¬")
            return results
        
        # æ­¥éª¤2.5: è®¡ç®—Ground Truthä¸åŸå§‹ç”Ÿæˆç­”æ¡ˆçš„åŸºå‡†è¯„ä¼°
        print("ğŸ“Š è®¡ç®—Ground Truthä¸åŸå§‹ç”Ÿæˆç­”æ¡ˆçš„åŸºå‡†è¯„ä¼°...")
        gt_vs_original_f1 = self.calculate_f1_score(original_answer, sample.expected_answer)
        gt_vs_original_em = self.calculate_exact_match(original_answer, sample.expected_answer)
        
        print(f"Ground Truth vs åŸå§‹ç”Ÿæˆç­”æ¡ˆåŸºå‡†: F1={gt_vs_original_f1:.4f}, EM={gt_vs_original_em:.4f}")
        
        # è¿è¡ŒGround Truth vs åŸå§‹ç”Ÿæˆç­”æ¡ˆçš„LLM Judgeè¯„ä¼°
        print("ğŸ¤– è¿è¡ŒGround Truth vs åŸå§‹ç”Ÿæˆç­”æ¡ˆçš„LLM Judgeè¯„ä¼°...")
        llm_judge_gt_vs_original = self.run_llm_judge_evaluation(sample.expected_answer, original_answer, sample.question)
        
        # æ—¥å¿—ï¼šåŸå§‹æ•°æ®
        log_base = {
            "sample_id": sample.sample_id,
            "original_summary": sample.context,
            "original_generated_question": sample.question,
            "expected_answer": sample.expected_answer,
            "question_type": sample.question_type,
            "context_type": sample.context_type,
        }
        
        # æ­¥éª¤3-7: å¯¹æ¯ä¸ªæ‰°åŠ¨å™¨è¿›è¡Œå®éªŒ
        for perturber_name, perturber in self.perturbers.items():
            print(f"\nğŸ”„ æµ‹è¯•æ‰°åŠ¨å™¨: {perturber_name}")
            
            # æ­¥éª¤3: åº”ç”¨æ‰°åŠ¨
            print(f"\nğŸ”§ åº”ç”¨ {perturber_name} æ‰°åŠ¨...")
            perturbation_details = self.apply_perturbation(sample.context, perturber_name)
            
            if not perturbation_details:
                print(f"âŒ {perturber_name} æœªäº§ç”Ÿæœ‰æ•ˆæ‰°åŠ¨ï¼Œè·³è¿‡")
                continue
            
            print(f"âœ… ç”Ÿæˆäº† {len(perturbation_details)} ä¸ªæ‰°åŠ¨")
            
            # å¯¹æ¯ä¸ªæ‰°åŠ¨è¿›è¡Œå¤„ç†
            for i, perturbation_detail in enumerate(perturbation_details):
                print(f"\n--- æ‰°åŠ¨ {i+1} ---")
                print(f"æ‰°åŠ¨å™¨: {perturbation_detail.perturber_name}")
                print(f"æ‰°åŠ¨ç±»å‹: {perturbation_detail.perturbation_type}")
                print(f"å˜åŒ–æè¿°: {perturbation_detail.change_description}")
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å˜åŒ–
                if perturbation_detail.perturbed_text == perturbation_detail.original_text:
                    print(f"âš ï¸ {perturbation_detail.perturber_name} æ‰°åŠ¨å™¨æœªäº§ç”Ÿå®é™…å˜åŒ–ï¼Œè·³è¿‡æ­¤æ‰°åŠ¨")
                    continue
                
                # æ˜¾ç¤ºå…·ä½“å˜åŒ–
                if perturbation_detail.changed_elements:
                    print("å…·ä½“å˜åŒ–:")
                    for change in perturbation_detail.changed_elements:
                        print(f"  â€¢ {change}")
                
                # æ˜¾ç¤ºæ–‡æœ¬å¯¹æ¯”
                print(f"åŸå§‹æ–‡æœ¬: {perturbation_detail.original_text[:100]}...")
                print(f"æ‰°åŠ¨åæ–‡æœ¬: {perturbation_detail.perturbed_text[:100]}...")
                
                # æ­¥éª¤4: è·å–æ‰°åŠ¨åç­”æ¡ˆ
                perturbed_answer = self.get_perturbed_answer(perturbation_detail.perturbed_text, sample.question, perturber_name)
                
                if not perturbed_answer:
                    print("âŒ æ‰°åŠ¨åç­”æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
                    continue
                
                print(f"ğŸ“‹ æ‰°åŠ¨åç­”æ¡ˆ: {perturbed_answer}")
                print(f"ğŸ“ æ‰°åŠ¨åç­”æ¡ˆé•¿åº¦: {len(perturbed_answer)} å­—ç¬¦")
                
                # æ­¥éª¤5: è®¡ç®—Ground Truthä¸æ‰°åŠ¨åç”Ÿæˆç­”æ¡ˆçš„è¯„ä¼°æŒ‡æ ‡
                print("ğŸ“Š è®¡ç®—Ground Truthä¸æ‰°åŠ¨åç”Ÿæˆç­”æ¡ˆçš„è¯„ä¼°æŒ‡æ ‡...")
                gt_vs_perturbed_f1 = self.calculate_f1_score(perturbed_answer, sample.expected_answer)
                gt_vs_perturbed_em = self.calculate_exact_match(perturbed_answer, sample.expected_answer)
                
                print(f"Ground Truth vs æ‰°åŠ¨åç”Ÿæˆç­”æ¡ˆ: F1={gt_vs_perturbed_f1:.4f}, EM={gt_vs_perturbed_em:.4f}")
                
                # æ­¥éª¤5.5: è®¡ç®—æ‰°åŠ¨å¯¹æ€§èƒ½çš„å½±å“
                f1_improvement = gt_vs_perturbed_f1 - gt_vs_original_f1
                em_improvement = gt_vs_perturbed_em - gt_vs_original_em
                
                print(f"æ‰°åŠ¨å¯¹F1åˆ†æ•°çš„å½±å“: {f1_improvement:+.4f} ({'æ”¹å–„' if f1_improvement > 0 else 'ä¸‹é™'})")
                print(f"æ‰°åŠ¨å¯¹EMåˆ†æ•°çš„å½±å“: {em_improvement:+.4f} ({'æ”¹å–„' if em_improvement > 0 else 'ä¸‹é™'})")
                
                # æ­¥éª¤6: è®¡ç®—åŸå§‹ç­”æ¡ˆä¸æ‰°åŠ¨åç­”æ¡ˆçš„ç›¸ä¼¼åº¦å’Œé‡è¦æ€§
                similarity_score, importance_score, f1_score, em_score = self.calculate_importance_score(original_answer, perturbed_answer)
                
                # æ­¥éª¤7: LLM Judgeè¯„ä¼°ï¼ˆGround Truth vs æ‰°åŠ¨åç”Ÿæˆç­”æ¡ˆï¼‰
                print("ğŸ¤– è¿è¡ŒLLM Judgeè¯„ä¼°ï¼ˆGround Truth vs æ‰°åŠ¨åç”Ÿæˆç­”æ¡ˆï¼‰...")
                llm_judge_gt_vs_perturbed = self.run_llm_judge_evaluation(sample.expected_answer, perturbed_answer, sample.question)
                
                # è®¡ç®—LLM Judgeè¯„ä¼°çš„æ”¹å–„æƒ…å†µ
                gt_vs_original_score = llm_judge_gt_vs_original.get('overall_score', 0)
                gt_vs_perturbed_score = llm_judge_gt_vs_perturbed.get('overall_score', 0)
                llm_judge_improvement = gt_vs_perturbed_score - gt_vs_original_score
                
                print(f"Ground Truth vs åŸå§‹ç”Ÿæˆç­”æ¡ˆLLM Judge: {gt_vs_original_score:.2f}")
                print(f"Ground Truth vs æ‰°åŠ¨åç”Ÿæˆç­”æ¡ˆLLM Judge: {gt_vs_perturbed_score:.2f}")
                print(f"æ‰°åŠ¨å¯¹LLM Judgeåˆ†æ•°çš„å½±å“: {llm_judge_improvement:+.2f} ({'æ”¹å–„' if llm_judge_improvement > 0 else 'ä¸‹é™'})")
                
                # åˆå¹¶LLM Judgeè¯„ä¼°ç»“æœ
                llm_judge_scores = {
                    'gt_vs_original': llm_judge_gt_vs_original,
                    'gt_vs_perturbed': llm_judge_gt_vs_perturbed,
                    'original_vs_perturbed': self.run_llm_judge_evaluation(original_answer, perturbed_answer, sample.question)
                }
                
                # è®°å½•ç»“æœ
                result = PerturbationResult(
                    sample_id=sample.sample_id,
                    perturber_name=perturber_name,
                    original_answer=original_answer,
                    perturbed_answer=perturbed_answer,
                    perturbation_detail=perturbation_detail,
                    similarity_score=similarity_score,
                    importance_score=importance_score,
                    f1_score=f1_score,
                    em_score=em_score,
                    llm_judge_scores=llm_judge_scores,
                    timestamp=datetime.now().isoformat()
                )
                
                # æ·»åŠ Ground Truthè¯„ä¼°æŒ‡æ ‡åˆ°ç»“æœä¸­
                result.expected_vs_original_f1 = gt_vs_original_f1
                result.expected_vs_original_em = gt_vs_original_em
                result.expected_vs_perturbed_f1 = gt_vs_perturbed_f1
                result.expected_vs_perturbed_em = gt_vs_perturbed_em
                
                # æ·»åŠ æ‰°åŠ¨å½±å“æŒ‡æ ‡
                result.f1_improvement = f1_improvement
                result.em_improvement = em_improvement
                result.llm_judge_improvement = llm_judge_improvement
                
                results.append(result)
                
                print(f"âœ… æ‰°åŠ¨ {i+1} å®Œæˆ")
                print(f"  ç›¸ä¼¼åº¦: {similarity_score:.4f}")
                print(f"  é‡è¦æ€§: {importance_score:.4f}")
                print(f"  åŸå§‹ç”Ÿæˆç­”æ¡ˆ vs æ‰°åŠ¨åç”Ÿæˆç­”æ¡ˆ: F1={f1_score:.4f}, EM={em_score:.4f}")
                print(f"  Ground Truth vs åŸå§‹ç”Ÿæˆç­”æ¡ˆåŸºå‡†: F1={gt_vs_original_f1:.4f}, EM={gt_vs_original_em:.4f}")
                print(f"  Ground Truth vs æ‰°åŠ¨åç”Ÿæˆç­”æ¡ˆ: F1={gt_vs_perturbed_f1:.4f}, EM={gt_vs_perturbed_em:.4f}")
                print(f"  æ‰°åŠ¨å¯¹F1åˆ†æ•°çš„å½±å“: {f1_improvement:+.4f}")
                print(f"  æ‰°åŠ¨å¯¹EMåˆ†æ•°çš„å½±å“: {em_improvement:+.4f}")
                print(f"  æ‰°åŠ¨å¯¹LLM Judgeåˆ†æ•°çš„å½±å“: {llm_judge_improvement:+.2f}")
                if llm_judge_scores:
                    print(f"  LLM Judgeè¯„ä¼°:")
                    if 'gt_vs_original' in llm_judge_scores:
                        print(f"    Ground Truth vs åŸå§‹ç”Ÿæˆç­”æ¡ˆ: {llm_judge_scores['gt_vs_original'].get('overall_score', 'N/A')}")
                    if 'gt_vs_perturbed' in llm_judge_scores:
                        print(f"    Ground Truth vs æ‰°åŠ¨åç”Ÿæˆç­”æ¡ˆ: {llm_judge_scores['gt_vs_perturbed'].get('overall_score', 'N/A')}")
        return results
    
    def run_comprehensive_experiment(self, dataset_path: str, num_samples: int = 20):
        """è¿è¡Œå®Œæ•´çš„æ‰°åŠ¨å®éªŒ"""
        print("ğŸš€ å¼€å§‹RAGç³»ç»Ÿæ‰°åŠ¨å®éªŒï¼ˆä½¿ç”¨rag_system_adapterï¼‰")
        print("=" * 80)
        
        # æ­¥éª¤1: æŒ‘é€‰æ ·æœ¬
        print("ğŸ“‹ æ­¥éª¤1: æŒ‘é€‰ä»£è¡¨æ€§æ ·æœ¬...")
        samples = self.select_perturbation_samples(dataset_path, num_samples)
        
        if not samples:
            print("âŒ æ²¡æœ‰é€‰æ‹©åˆ°æœ‰æ•ˆæ ·æœ¬ï¼Œå®éªŒç»ˆæ­¢")
            return [], []
        
        # è¿è¡Œå®éªŒ
        all_results = []
        for i, sample in enumerate(samples, 1):
            print(f"\n{'='*20} æ ·æœ¬ {i}/{len(samples)} {'='*20}")
            
            # è¿è¡Œå•ä¸ªæ ·æœ¬å®éªŒ
            sample_results = self.run_single_sample_experiment(sample)
            all_results.extend(sample_results)
            
            print(f"âœ… æ ·æœ¬ {i} å®éªŒå®Œæˆï¼Œè·å¾— {len(sample_results)} ä¸ªç»“æœ")
        
        # åˆ†æç»“æœ
        # self.analyze_experiment_results(all_results, samples) # This line is removed as per the new_code
        
        # ä¿å­˜ç»“æœ
        # self.save_experiment_results(all_results, samples) # This line is removed as per the new_code
        
        print(f"\nğŸ‰ å®éªŒå®Œæˆï¼æ€»å…±è·å¾— {len(all_results)} ä¸ªæ‰°åŠ¨ç»“æœ")
        return all_results, samples
    
    def analyze_experiment_results(self, results: List[PerturbationResult], samples: List[PerturbationSample]):
        """åˆ†æå®éªŒç»“æœ"""
        print(f"\nğŸ“Š å®éªŒç»“æœåˆ†æ")
        print("=" * 60)
        
        if not results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯åˆ†æ")
            return {}
        
        # æŒ‰æ‰°åŠ¨å™¨åˆ†ç»„åˆ†æ
        perturber_stats = {}
        for perturber_name in self.perturbers.keys():
            perturber_results = [r for r in results if r.perturber_name == perturber_name]
            
            if perturber_results:
                # åˆ†åˆ«ç»Ÿè®¡summaryæ‰°åŠ¨å’Œpromptæ‰°åŠ¨
                summary_results = [r for r in perturber_results if r.perturbation_target == "summary"]
                prompt_results = [r for r in perturber_results if r.perturbation_target == "prompt"]
                
                avg_importance = sum(r.importance_score for r in perturber_results) / len(perturber_results)
                avg_similarity = sum(r.similarity_score for r in perturber_results) / len(perturber_results)
                avg_accuracy = sum(r.llm_judge_scores['accuracy'] for r in perturber_results) / len(perturber_results)
                avg_f1 = sum(r.f1_score for r in perturber_results) / len(perturber_results)
                avg_em = sum(r.em_score for r in perturber_results) / len(perturber_results)
                
                perturber_stats[perturber_name] = {
                    'count': len(perturber_results),
                    'summary_count': len(summary_results),
                    'prompt_count': len(prompt_results),
                    'avg_importance': avg_importance,
                    'avg_similarity': avg_similarity,
                    'avg_accuracy': avg_accuracy,
                    'avg_f1': avg_f1,
                    'avg_em': avg_em,
                    'summary_avg_importance': sum(r.importance_score for r in summary_results) / len(summary_results) if summary_results else 0,
                    'prompt_avg_importance': sum(r.importance_score for r in prompt_results) / len(prompt_results) if prompt_results else 0
                }
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        print(f"{'æ‰°åŠ¨å™¨':<10} {'æ€»æ ·æœ¬':<8} {'Summary':<8} {'Prompt':<8} {'å¹³å‡é‡è¦æ€§':<12} {'å¹³å‡ç›¸ä¼¼åº¦':<12} {'å¹³å‡å‡†ç¡®æ€§':<12} {'å¹³å‡F1':<10} {'å¹³å‡EM':<10}")
        print("-" * 100)
        
        for perturber_name, stats in perturber_stats.items():
            print(f"{perturber_name:<10} {stats['count']:<8} {stats['summary_count']:<8} {stats['prompt_count']:<8} "
                  f"{stats['avg_importance']:<12.4f} {stats['avg_similarity']:<12.4f} {stats['avg_accuracy']:<12.2f} "
                  f"{stats['avg_f1']:<10.4f} {stats['avg_em']:<10.4f}")
        
        # æ‰¾å‡ºæœ€é‡è¦çš„æ‰°åŠ¨å™¨
        if perturber_stats:
            most_important = max(perturber_stats.items(), key=lambda x: x[1]['avg_importance'])
            print(f"\nğŸ† æœ€é‡è¦çš„æ‰°åŠ¨å™¨: {most_important[0]} (å¹³å‡é‡è¦æ€§: {most_important[1]['avg_importance']:.4f})")
            
            # åˆ†æsummary vs promptæ‰°åŠ¨æ•ˆæœ
            print(f"\nğŸ“ˆ Summary vs Prompt æ‰°åŠ¨æ•ˆæœå¯¹æ¯”:")
            for perturber_name, stats in perturber_stats.items():
                if stats['summary_count'] > 0 and stats['prompt_count'] > 0:
                    summary_importance = stats['summary_avg_importance']
                    prompt_importance = stats['prompt_avg_importance']
                    print(f"  {perturber_name}: Summaryé‡è¦æ€§={summary_importance:.4f}, Generated_Questioné‡è¦æ€§={prompt_importance:.4f}")
                    if summary_importance > prompt_importance:
                        print(f"    â†’ Summaryæ‰°åŠ¨æ•ˆæœæ›´å¼º")
                    else:
                        print(f"    â†’ Generated_Questionæ‰°åŠ¨æ•ˆæœæ›´å¼º")
        
        return {
            'perturber_statistics': perturber_stats,
            'overall_metrics': {
                'avg_similarity_score': sum(r.similarity_score for r in results) / len(results),
                'avg_importance_score': sum(r.importance_score for r in results) / len(results),
                'avg_llm_judge_score': sum(r.llm_judge_scores.get('overall_score', 0) for r in results) / len(results),
                'avg_f1_score': sum(r.f1_score for r in results) / len(results),
                'avg_em_score': sum(r.em_score for r in results) / len(results)
            }
        }
    
    def save_experiment_results(self, results: List[PerturbationResult], samples: List[PerturbationSample]):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"perturbation_experiment_results_{timestamp}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        output_data = {
            'experiment_info': {
                'timestamp': timestamp,
                'total_samples': len(samples),
                'total_results': len(results),
                'perturbers_used': list(self.perturbers.keys()),
                'rag_system': 'rag_system_adapter',
                'config_used': {
                    'generator_model': self.config.generator.model_name,
                    'chinese_encoder': self.config.encoder.chinese_model_path,
                    'english_encoder': self.config.encoder.english_model_path
                }
            },
            'samples': [asdict(sample) for sample in samples],
            'results': [asdict(result) for result in results]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    def run_integrated_experiment(self, dataset_path: str, num_samples: int = 20, output_dir: str = 'perturbation_results'):
        """
        è¿è¡Œé›†æˆæ‰°åŠ¨å®éªŒï¼ˆæ‰¹é‡ä¸¤æ­¥æ³•ï¼‰
        - ç”Ÿæˆé˜¶æ®µï¼šFin-R1 on cuda:1
        - è¯„æµ‹é˜¶æ®µï¼šQwen3-8B on cuda:1
        """
        print(f"\nğŸš€ é›†æˆæ‰°åŠ¨å®éªŒå¯åŠ¨ï¼ˆæ‰¹é‡ä¸¤æ­¥æ³•ï¼‰")
        print(f"ğŸ“ æ•°æ®é›†: {dataset_path}")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {num_samples}")
        print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
        
        # æ£€æŸ¥ç°æœ‰ç»“æœï¼Œé¿å…é‡å¤
        existing_sample_ids = set()
        existing_file = os.path.join(output_dir, "incremental_generation.json")
        if os.path.exists(existing_file):
            try:
                with open(existing_file, 'r', encoding='utf-8') as f:
                    existing_results = json.load(f)
                    existing_sample_ids = {result.get('sample_id', '') for result in existing_results}
                print(f"ğŸ“‹ å‘ç°ç°æœ‰ç»“æœæ–‡ä»¶ï¼ŒåŒ…å« {len(existing_sample_ids)} ä¸ªæ ·æœ¬ID")
                print(f"ğŸ” å°†è·³è¿‡è¿™äº›å·²å­˜åœ¨çš„æ ·æœ¬ID: {list(existing_sample_ids)[:5]}...")
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç°æœ‰ç»“æœå¤±è´¥: {e}")
        
        # æ­¥éª¤1: é€‰æ‹©æ ·æœ¬ï¼ˆæ‰©å¤§åˆé€‰æ± ï¼‰
        print("\nğŸ“‹ æ­¥éª¤1: é€‰æ‹©æ ·æœ¬")
        # ä¸ºæ¯ä¸ªæ‰°åŠ¨å™¨ç”Ÿæˆ7ä¸ªæ ·æœ¬ï¼Œæ€»å…±21ä¸ª
        target_samples = 21  # 7ä¸ªterm + 7ä¸ªyear + 7ä¸ªtrend
        candidates = self.select_perturbation_samples(dataset_path, num_samples=target_samples*3)
        if not candidates:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ ·æœ¬ï¼Œé€€å‡ºå®éªŒ")
            return
        print(f"âœ… æˆåŠŸé€‰æ‹© {len(candidates)} ä¸ªå€™é€‰æ ·æœ¬")
        
        # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„æ ·æœ¬ID
        filtered_candidates = [sample for sample in candidates if sample.sample_id not in existing_sample_ids]
        print(f"ğŸ“Š è¿‡æ»¤åå‰©ä½™ {len(filtered_candidates)} ä¸ªå€™é€‰æ ·æœ¬ï¼ˆè·³è¿‡ {len(candidates) - len(filtered_candidates)} ä¸ªé‡å¤æ ·æœ¬ï¼‰")
        
        if len(filtered_candidates) < target_samples:
            print(f"âš ï¸ è­¦å‘Šï¼šè¿‡æ»¤åæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œéœ€è¦ {target_samples} ä¸ªï¼Œåªæœ‰ {len(filtered_candidates)} ä¸ª")
        
        # æ­¥éª¤2: ç”Ÿæˆé˜¶æ®µï¼ˆåªç”¨Fin-R1ï¼Œcuda:1ï¼‰
        print("\nğŸ”¬ æ­¥éª¤2: ç”Ÿæˆé˜¶æ®µï¼ˆåªç”¨Fin-R1ï¼‰")
        generation_results = []
        used_sample_ids = set()
        perturber_counts = {'year': 0, 'term': 0, 'trend': 0}
        samples = filtered_candidates  # å®šä¹‰sampleså˜é‡
        for sample in filtered_candidates:
            if len(generation_results) >= target_samples:
                break
            if sample.sample_id in used_sample_ids:
                continue
            used_sample_ids.add(sample.sample_id)
            original_answer = self.get_original_answer(sample.context, sample.question)
            best_perturber = self._select_best_perturber_for_sample(sample, perturber_counts)
            if not best_perturber:
                print(f"âŒ æ ·æœ¬ {sample.sample_id} æ— æ³•é€‰æ‹©æ‰°åŠ¨å™¨ï¼Œè·³è¿‡")
                continue
            # å¤„ç†æ‰€æœ‰æ‰°åŠ¨å™¨ï¼ˆtermã€yearã€trendï¼‰
            if best_perturber not in ['year', 'term', 'trend']:
                print(f"âš ï¸ æ ·æœ¬ {sample.sample_id} é€‰æ‹©äº†{best_perturber}ï¼Œè·³è¿‡")
                continue
            perturbation_details = self.apply_perturbation(sample.context, best_perturber)
            if not perturbation_details:
                print(f"  âš ï¸ æ ·æœ¬ {sample.sample_id} æœªç”Ÿæˆæ‰°åŠ¨ï¼Œè·³è¿‡")
                continue
            perturbation_detail = perturbation_details[0]
            if perturbation_detail.perturbed_text == perturbation_detail.original_text:
                print(f"âš ï¸ æ ·æœ¬ {sample.sample_id} æ‰°åŠ¨å™¨æœªäº§ç”Ÿå®é™…å˜åŒ–ï¼Œè·³è¿‡")
                continue
            perturbed_answer = self.get_perturbed_answer(perturbation_detail.perturbed_text, sample.question, best_perturber)
            if not perturbed_answer:
                print(f"âŒ æ ·æœ¬ {sample.sample_id} æ‰°åŠ¨åç­”æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
                continue
            similarity_score, importance_score, f1_score, em_score = self.calculate_importance_score(original_answer, perturbed_answer)
            generation_result = {
                'sample_id': sample.sample_id,
                'question': sample.question,
                'context': sample.context,
                'expected_answer': sample.expected_answer,
                'perturber_name': best_perturber,
                'perturbation_detail': perturbation_detail,
                'original_answer': original_answer,
                'perturbed_answer': perturbed_answer,
                'similarity_score': similarity_score,
                'importance_score': importance_score,
                'f1_score': f1_score,
                'em_score': em_score,
                'timestamp': datetime.now().isoformat()
            }
            generation_results.append(generation_result)
            # æ›´æ–°æ‰°åŠ¨å™¨è®¡æ•°
            if best_perturber:
                perturber_counts[best_perturber] += 1
            print(f"  âœ… ç”Ÿæˆå®Œæˆ")
            print(f"    ç›¸ä¼¼åº¦: {similarity_score:.4f}")
            print(f"    é‡è¦æ€§: {importance_score:.4f}")
            print(f"    F1åˆ†æ•°: {f1_score:.4f}")
            print(f"    EMåˆ†æ•°: {em_score:.4f}")
            print(f"    æ‰°åŠ¨å™¨è®¡æ•°: {perturber_counts}")
        
        print(f"\nğŸ“Š ç”Ÿæˆé˜¶æ®µå®Œæˆï¼Œå…±ç”Ÿæˆ {len(generation_results)} ä¸ªæœ‰æ•ˆæ‰°åŠ¨ç»“æœ")
        
        # æ­¥éª¤3: ä¿å­˜ç”Ÿæˆç»“æœ
        print("\nğŸ’¾ æ­¥éª¤3: ä¿å­˜ç”Ÿæˆç»“æœ")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        generation_file = os.path.join(output_dir, f"generation_results_{timestamp}.json")
        os.makedirs(output_dir, exist_ok=True)
        
        # ç¡®ä¿generation_resultsä¸­çš„PerturbationDetailå¯¹è±¡è¢«è½¬æ¢ä¸ºå­—å…¸
        serializable_results = []
        for result in generation_results:
            serializable_result = result.copy()
            if 'perturbation_detail' in serializable_result and isinstance(serializable_result['perturbation_detail'], PerturbationDetail):
                serializable_result['perturbation_detail'] = asdict(serializable_result['perturbation_detail'])
            serializable_results.append(serializable_result)
        
        with open(generation_file, 'w', encoding='utf-8') as f:
            json.dump({
                'experiment_info': {
                    'timestamp': timestamp,
                    'num_samples': len(samples),
                    'num_results': len(generation_results),
                    'perturbers': list(self.perturbers.keys()),
                    'stage': 'generation_only'
                },
                'samples': [asdict(sample) for sample in samples],
                'generation_results': serializable_results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç”Ÿæˆç»“æœå·²ä¿å­˜åˆ°: {generation_file}")
        
        # æ­¥éª¤4: è¯„æµ‹é˜¶æ®µï¼ˆåªç”¨Qwen3-8Bï¼Œcuda:1ï¼‰
        print("\nğŸ”¬ æ­¥éª¤4: è¯„æµ‹é˜¶æ®µï¼ˆåªç”¨Qwen3-8Bï¼‰")
        
        # é‡Šæ”¾Fin-R1æ˜¾å­˜
        print("ğŸ§¹ é‡Šæ”¾Fin-R1æ˜¾å­˜...")
        del self.generator
        gc.collect()
        torch.cuda.empty_cache()
        
        # åˆå§‹åŒ–LLM Judge
        print("ğŸ”§ åˆå§‹åŒ–LLM Judge...")
        from llm_comparison.chinese_llm_judge import llm_judge_singleton
        llm_judge_singleton.initialize(model_name="Qwen3-8B", device="cuda:1")
        
        # å¯¹æ¯ä¸ªç”Ÿæˆç»“æœè¿›è¡Œè¯„æµ‹
        final_results = []
        for i, gen_result in enumerate(generation_results, 1):
            print(f"\nğŸ“Š è¯„æµ‹ç»“æœ {i}/{len(generation_results)}: {gen_result['sample_id']} - {gen_result['perturber_name']}")
            
            # LLM Judgeè¯„ä¼°
            llm_judge_scores = llm_judge_singleton.evaluate(
                gen_result['question'],
                gen_result['expected_answer'],
                gen_result['perturbed_answer']
            )
            
            # åˆ›å»ºæœ€ç»ˆç»“æœ
            final_result = PerturbationResult(
                sample_id=gen_result['sample_id'],
                perturber_name=gen_result['perturber_name'],
                original_answer=gen_result['original_answer'],
                perturbed_answer=gen_result['perturbed_answer'],
                perturbation_detail=gen_result['perturbation_detail'],
                similarity_score=gen_result['similarity_score'],
                importance_score=gen_result['importance_score'],
                f1_score=gen_result['f1_score'],
                em_score=gen_result['em_score'],
                llm_judge_scores=llm_judge_scores,
                timestamp=gen_result['timestamp']
            )
            
            final_results.append(final_result)
            print(f"  âœ… è¯„æµ‹å®Œæˆ")
            print(f"    LLM Judge: {llm_judge_scores.get('overall_score', 'N/A')}")
        
        # æ­¥éª¤5: ä¿å­˜æœ€ç»ˆç»“æœ
        print("\nğŸ’¾ æ­¥éª¤5: ä¿å­˜æœ€ç»ˆç»“æœ")
        self.save_integrated_results(final_results, samples, output_dir)
        
        # æ­¥éª¤6: è®¡ç®—F1å’ŒEMæŒ‡æ ‡
        print("\nğŸ“Š æ­¥éª¤6: è®¡ç®—F1å’ŒEMæŒ‡æ ‡")
        self.calculate_and_save_metrics(final_results, samples, output_dir)
        
        # æ¸…ç†LLM Judgeæ¨¡å‹
        print("ğŸ§¹ æ¸…ç†LLM Judgeæ¨¡å‹...")
        llm_judge_singleton.cleanup()
        gc.collect()
        torch.cuda.empty_cache()
        
        print(f"\nğŸ‰ é›†æˆå®éªŒå®Œæˆï¼ˆæ‰¹é‡ä¸¤æ­¥æ³•ï¼‰ï¼")
        print(f"ğŸ“Š å¤„ç†äº† {len(samples)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ˆ ç”Ÿæˆäº† {len(generation_results)} ä¸ªç»“æœ")
        print(f"ğŸ“Š è¯„æµ‹äº† {len(final_results)} ä¸ªç»“æœ")
        print("âœ… Fin-R1å’ŒQwen3-8BæœªåŒæ—¶å ç”¨cuda:1ï¼Œæ˜¾å­˜å®‰å…¨")

    def _select_best_perturber_for_sample(self, sample: PerturbationSample, perturber_counts: Dict[str, int]) -> Optional[str]:
        """ä¸ºæ ·æœ¬é€‰æ‹©æœ€ä½³æ‰°åŠ¨å™¨ - ç¡®ä¿æ¯ä¸ªæ‰°åŠ¨å™¨è·å¾—7ä¸ªæ ·æœ¬"""
        # ä¼˜å…ˆé€‰æ‹©è®¡æ•°è¾ƒå°‘çš„æ‰°åŠ¨å™¨ï¼Œç¡®ä¿æ¯ä¸ªæ‰°åŠ¨å™¨éƒ½èƒ½è·å¾—æ ·æœ¬
        min_count = min(perturber_counts.values())
        candidates = [k for k, v in perturber_counts.items() if v == min_count]
        
        # å¦‚æœæ‰€æœ‰æ‰°åŠ¨å™¨éƒ½è¾¾åˆ°äº†7ä¸ªæ ·æœ¬ï¼Œåˆ™åœæ­¢
        if min_count >= 7:
            return None
            
        return candidates[0] if candidates else None

    def save_integrated_results(self, results: List[PerturbationResult], samples: List[PerturbationSample], output_dir: str):
        """ä¿å­˜é›†æˆç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(output_dir, f"integrated_perturbation_results_{timestamp}.json")
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_results = []
        for result in results:
            result_dict = asdict(result)
            if isinstance(result_dict['perturbation_detail'], PerturbationDetail):
                result_dict['perturbation_detail'] = asdict(result_dict['perturbation_detail'])
            serializable_results.append(result_dict)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump({
                'experiment_info': {
                    'timestamp': timestamp,
                    'num_samples': len(samples),
                    'num_results': len(results),
                    'perturbers': list(self.perturbers.keys())
                },
                'samples': [asdict(sample) for sample in samples],
                'results': serializable_results
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… é›†æˆç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    def calculate_and_save_metrics(self, results: List[PerturbationResult], samples: List[PerturbationSample], output_dir: str):
        """è®¡ç®—å¹¶ä¿å­˜æŒ‡æ ‡"""
        # è¿™é‡Œå¯ä»¥æ·»åŠ F1å’ŒEMæŒ‡æ ‡çš„è®¡ç®—é€»è¾‘
        print("ğŸ“Š æŒ‡æ ‡è®¡ç®—å®Œæˆ")


def run_judge_only(generation_result_path: str, judge_output_path: str):
    """
    åªè´Ÿè´£åŠ è½½ç”Ÿæˆç»“æœï¼Œæ‰¹é‡ç”¨LLM Judgeè¯„æµ‹ï¼Œä¿å­˜ä¸ºjsonï¼ˆåªåŠ è½½Qwen3-8Båˆ°cuda:1ï¼‰
    """
    print(f"\nğŸš€ [è¯„æµ‹é˜¶æ®µ] è¯»å–ç”Ÿæˆç»“æœ: {generation_result_path}")
    from llm_comparison.chinese_llm_judge import llm_judge_singleton
    with open(generation_result_path, "r", encoding="utf-8") as f:
        results = json.load(f)
    # åˆå§‹åŒ–Judge
    llm_judge_singleton.initialize(model_name="Qwen3-8B", device="cuda:1")
    for item in results:
        judge_result = llm_judge_singleton.evaluate(
            item["question"],
            item["expected_answer"],
            item["perturbed_answer"]
        )
        item["judge_scores"] = judge_result
    # ä¿å­˜è¯„æµ‹ç»“æœ
    with open(judge_output_path, "w", encoding="utf-8") as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… è¯„æµ‹é˜¶æ®µå®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {judge_output_path}")
    # é‡Šæ”¾Judgeæ˜¾å­˜
    llm_judge_singleton.cleanup()
    gc.collect()
    torch.cuda.empty_cache()

# ç”¨æ³•ç¤ºä¾‹ï¼š
# run_generation_only("selected_perturbation_samples.json", "generated_answers.json", num_samples=20)
# run_judge_only("generated_answers.json", "judge_results.json")

def main():
    """ä¸»å‡½æ•° - ä¸ºæ¯ä¸ªæ‰°åŠ¨å™¨ï¼ˆtermã€yearã€trendï¼‰å„ç”Ÿæˆ7ä¸ªæ ·æœ¬"""
    print("ğŸš€ å¯åŠ¨RAGæ‰°åŠ¨å®éªŒ - ä¸ºæ¯ä¸ªæ‰°åŠ¨å™¨ç”Ÿæˆ7ä¸ªæ ·æœ¬")
    
    # åˆå§‹åŒ–å®éªŒ
    experiment = RAGPerturbationExperiment()
    
    # è®¾ç½®å‚æ•°
    dataset_path = "selected_perturbation_samples.json"
    output_dir = "perturbation_results"
    
    # ä¿®æ”¹ç›®æ ‡æ ·æœ¬æ•°ä¸º21ï¼ˆ7ä¸ªterm + 7ä¸ªyear + 7ä¸ªtrendï¼‰
    target_samples = 21
    
    print(f"ğŸ“Š ç›®æ ‡æ ·æœ¬æ•°: {target_samples}")
    print(f"ğŸ“ æ•°æ®é›†: {dataset_path}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ¯ ç›®æ ‡ï¼šæ¯ä¸ªæ‰°åŠ¨å™¨ï¼ˆtermã€yearã€trendï¼‰å„ç”Ÿæˆ7ä¸ªæ ·æœ¬")
    
    # è¿è¡Œé›†æˆå®éªŒ
    experiment.run_integrated_experiment(
        dataset_path=dataset_path,
        num_samples=target_samples,
        output_dir=output_dir
    )

if __name__ == "__main__":
    main() 