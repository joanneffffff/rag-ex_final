#!/usr/bin/env python3
"""
RAGç³»ç»Ÿæ‰°åŠ¨å®éªŒæµç¨‹
ä½¿ç”¨rag_system_adapterçš„RAGç³»ç»Ÿå‡½æ•°è¿›è¡Œæ‰°åŠ¨å®éªŒ
åŒ…æ‹¬æ ·æœ¬é€‰æ‹©ã€æ‰°åŠ¨åº”ç”¨ã€ç­”æ¡ˆæ¯”è¾ƒã€é‡è¦æ€§è®¡ç®—å’ŒLLM Judgeè¯„ä¼°
"""

import sys
import os
import json
import time
import random
import re
import argparse
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import logging
from collections import Counter
from typing import Set

sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from alphafin_data_process.rag_system_adapter import RagSystemAdapter
from xlm.modules.perturber.trend_perturber import TrendPerturber
from xlm.modules.perturber.year_perturber import YearPerturber
from xlm.modules.perturber.term_perturber import TermPerturber
from xlm.modules.comparator.embedding_comparator import EmbeddingComparator
from xlm.components.encoder.encoder import Encoder
from config.parameters import Config

# å¯¼å…¥LLM JudgeåŠŸèƒ½
from llm_comparison.chinese_llm_judge import ModelLoader, get_judge_messages

def classify_question_type(question: str) -> str:
    """åˆ†ç±»é—®é¢˜ç±»å‹"""
    question_lower = question.lower()
    
    if any(word in question_lower for word in ['å¤šå°‘', 'å‡ ', 'æ•°é‡', 'é‡‘é¢', 'æ•°å­—']):
        return "æ•°å€¼å‹"
    elif any(word in question_lower for word in ['ä¸ºä»€ä¹ˆ', 'åŸå› ', 'å¯¼è‡´', 'å½±å“']):
        return "åŸå› åˆ†æå‹"
    elif any(word in question_lower for word in ['å¦‚ä½•', 'æ€ä¹ˆ', 'æ–¹æ³•', 'ç­–ç•¥']):
        return "æ–¹æ³•ç­–ç•¥å‹"
    elif any(word in question_lower for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'å·®å¼‚', 'åŒºåˆ«']):
        return "å¯¹æ¯”åˆ†æå‹"
    elif any(word in question_lower for word in ['è¶‹åŠ¿', 'å˜åŒ–', 'å‘å±•', 'å¢é•¿']):
        return "è¶‹åŠ¿åˆ†æå‹"
    else:
        return "ä¸€èˆ¬æè¿°å‹"

def classify_context_type(context: str) -> str:
    """åˆ†ç±»ä¸Šä¸‹æ–‡ç±»å‹"""
    context_lower = context.lower()
    
    if any(word in context_lower for word in ['è´¢åŠ¡', 'è¥æ”¶', 'åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º']):
        return "è´¢åŠ¡æ•°æ®å‹"
    elif any(word in context_lower for word in ['ä¸šç»©', 'è¡¨ç°', 'å¢é•¿', 'ä¸‹é™']):
        return "ä¸šç»©è¡¨ç°å‹"
    elif any(word in context_lower for word in ['æ”¿ç­–', 'è§„å®š', 'æ³•è§„', 'åˆ¶åº¦']):
        return "æ”¿ç­–æ³•è§„å‹"
    elif any(word in context_lower for word in ['å¸‚åœº', 'ç«äº‰', 'ä»½é¢', 'åœ°ä½']):
        return "å¸‚åœºç«äº‰å‹"
    else:
        return "ä¸€èˆ¬ä¿¡æ¯å‹"

def calculate_complexity_score(question: str, answer: str) -> float:
    """è®¡ç®—å¤æ‚åº¦åˆ†æ•°"""
    question_length = len(question)
    answer_length = len(answer)
    
    financial_terms = ['è¥æ”¶', 'åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º', 'å¸‚ç›ˆç‡', 'å¸‚å‡€ç‡', 'ROE', 'ROA']
    term_count = sum(1 for term in financial_terms if term in question or term in answer)
    
    complexity = (question_length * 0.3 + answer_length * 0.4 + term_count * 0.3) / 100
    return min(complexity, 1.0)

class PerturbationSampleSelector:
    """æ‰°åŠ¨æ ·æœ¬é€‰æ‹©å™¨ - ä»select_perturbation_samples.pyé›†æˆ"""
    
    def __init__(self):
        # å®šä¹‰ä¸‰ç§æ‰°åŠ¨ç±»å‹çš„å…³é”®è¯
        self.trend_keywords = {
            'ä¸Šå‡', 'ä¸‹é™', 'ä¸Šæ¶¨', 'ä¸‹è·Œ', 'å¢é•¿', 'å‡å°‘', 'æå‡', 'é™ä½', 'å¢åŠ ', 'å‡å°‘',
            'å¥½è½¬', 'æ¶åŒ–', 'æ”¹å–„', 'ç§¯æ', 'æ¶ˆæ', 'ç›ˆåˆ©', 'äºæŸ', 'æ‰©å¼ ', 'æ”¶ç¼©',
            'æŒç»­å¢é•¿', 'æŒç»­ä¸‹æ»‘', 'ç¨³æ­¥å¢é•¿', 'æ˜¾è‘—ä¸‹é™', 'å¼ºåŠ²', 'ç–²è½¯', 'é«˜äº', 'ä½äº',
            'ä¼˜äº', 'åŠ£äº', 'é¢†å…ˆ', 'è½å', 'å¢åŠ ç‡', 'å‡å°‘ç‡', 'ä¸Šå‡è¶‹åŠ¿', 'ä¸‹é™è¶‹åŠ¿',
            'å¢é•¿è¶‹åŠ¿', 'å‡å°‘è¶‹åŠ¿'
        }
        
        # å¹´ä»½å…³é”®è¯ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŠ¨æ€æ£€æµ‹ï¼Œä¸YearPerturberä¿æŒä¸€è‡´
        self.year_pattern = re.compile(r'\b(20\d{2})(?:å¹´|å¹´åº¦)?\b')
        
        self.term_keywords = {
            'å¸‚ç›ˆç‡', 'å‡€åˆ©æ¶¦', 'å¸‚å‡€ç‡', 'å¸‚é”€ç‡', 'è¥æ”¶', 'æ”¶å…¥', 'è¥ä¸šæ”¶å…¥', 'è¥ä¸šåˆ©æ¶¦',
            'è¥ä¸šåˆ©æ¶¦', 'æ€»èµ„äº§', 'å‡€èµ„äº§', 'è´Ÿå€º', 'èµ„äº§', 'åˆ©æ¶¦', 'æˆæœ¬', 'å¸‚å€¼', 'ä¼°å€¼',
            'è‚¡æ¯', 'åˆ†çº¢', 'é…è‚¡', 'å¢å‘', 'å›è´­', 'äº¤æ˜“é‡', 'æˆäº¤é¢', 'æ¢æ‰‹ç‡'
        }
    
    def load_dataset(self, file_path: str) -> List[Dict]:
        """åŠ è½½è¯„æµ‹æ•°æ®é›† - æ”¯æŒJSONLæ ¼å¼"""
        samples = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        samples.append(json.loads(line))
                    except json.JSONDecodeError as e:
                        print(f"âš ï¸ ç¬¬{line_num}è¡ŒJSONè§£æå¤±è´¥: {e}")
                        continue
        return samples
    
    def extract_keywords(self, text: str, keyword_set: Set[str]) -> Set[str]:
        """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ - ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å…¨è¯åŒ¹é…"""
        found_keywords = set()
        for keyword in keyword_set:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œå…¨è¯åŒ¹é…ï¼Œä¸æ‰°åŠ¨å™¨ä¿æŒä¸€è‡´
            pattern = r'\b' + re.escape(keyword) + r'\b'
            if re.search(pattern, text, re.IGNORECASE):
                found_keywords.add(keyword)
        return found_keywords
    
    def analyze_sample(self, sample: Dict) -> Dict:
        """åˆ†æå•ä¸ªæ ·æœ¬çš„å…³é”®è¯åˆ†å¸ƒ"""
        summary = sample.get('summary', '')
        content = sample.get('content', '')
        generated_question = sample.get('generated_question', '')
        
        # ä¸»è¦å…³æ³¨contextå­—æ®µï¼ˆsummaryå’Œcontentï¼‰ï¼Œå› ä¸ºè¿™æ˜¯æ‰°åŠ¨å™¨ä½œç”¨çš„å¯¹è±¡
        context_text = f"{summary} {content}"
        question_text = generated_question
        
        # åˆ†åˆ«åˆ†æcontextå’Œquestionä¸­çš„å…³é”®è¯
        context_trend_found = self.extract_keywords(context_text, self.trend_keywords)
        context_year_found = set()
        context_year_matches = self.year_pattern.findall(context_text)
        for match in context_year_matches:
            context_year_found.add(match)
        context_term_found = self.extract_keywords(context_text, self.term_keywords)
        
        question_trend_found = self.extract_keywords(question_text, self.trend_keywords)
        question_year_found = set()
        question_year_matches = self.year_pattern.findall(question_text)
        for match in question_year_matches:
            question_year_found.add(match)
        question_term_found = self.extract_keywords(question_text, self.term_keywords)
        
        # åˆå¹¶æ‰€æœ‰å…³é”®è¯ï¼ˆä½†ä¸»è¦æƒé‡ç»™contextï¼‰
        trend_found = context_trend_found | question_trend_found
        year_found = context_year_found | question_year_found
        term_found = context_term_found | question_term_found
        
        return {
            'sample_id': sample.get('id', 'unknown'),
            'summary': summary,
            'content': content,
            'generated_question': generated_question,
            'trend_keywords': trend_found,
            'year_keywords': year_found,
            'term_keywords': term_found,
            'context_trend_score': len(context_trend_found),
            'context_year_score': len(context_year_found),
            'context_term_score': len(context_term_found),
            'question_trend_score': len(question_trend_found),
            'question_year_score': len(question_year_found),
            'question_term_score': len(question_term_found),
            'trend_score': len(trend_found),
            'year_score': len(year_found),
            'term_score': len(term_found),
            'total_score': len(trend_found) + len(year_found) + len(term_found),
            'context_score': len(context_trend_found) + len(context_year_found) + len(context_term_found)
        }
    
    def select_samples(self, samples: List[Dict], target_count: int = 20) -> List[Dict]:
        """é€‰æ‹©é€‚åˆçš„æ ·æœ¬ - ä½¿ç”¨å¤šæ ·æ€§é€‰æ‹©ç­–ç•¥"""
        analyzed_samples = [self.analyze_sample(sample) for sample in samples]
        
        # ç¬¬ä¸€è½®ï¼šæŒ‰context_scoreæ’åºï¼Œä¼˜å…ˆé€‰æ‹©contextä¸­æœ‰å…³é”®è¯çš„æ ·æœ¬
        context_samples = [s for s in analyzed_samples if s['context_score'] > 0]
        context_samples.sort(key=lambda x: x['context_score'], reverse=True)
        
        # ç¬¬äºŒè½®ï¼šå¤šæ ·æ€§é€‰æ‹©ï¼Œç¡®ä¿è¦†ç›–ä¸åŒçš„é—®é¢˜ç±»å‹å’Œä¸Šä¸‹æ–‡ç±»å‹
        selected_samples = []
        remaining_samples = context_samples.copy()
        
        # é¦–å…ˆé€‰æ‹©context_scoreæœ€é«˜çš„æ ·æœ¬
        for _ in range(min(target_count, len(context_samples))):
            if not remaining_samples:
                break
            
            # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å¤šæ ·æ€§åˆ†æ•°
            for sample in remaining_samples:
                # ç®€å•çš„å¤šæ ·æ€§è®¡ç®—ï¼šä¸å·²é€‰æ ·æœ¬çš„å·®å¼‚
                diversity_score = 0.0
                for selected in selected_samples:
                    # é—®é¢˜ç±»å‹å·®å¼‚
                    if sample.get('question_type') != selected.get('question_type'):
                        diversity_score += 1.0
                    # ä¸Šä¸‹æ–‡ç±»å‹å·®å¼‚
                    if sample.get('context_type') != selected.get('context_type'):
                        diversity_score += 1.0
                
                sample['diversity_score'] = diversity_score
            
            # é€‰æ‹©æœ€ä½³æ ·æœ¬ï¼ˆå¹³è¡¡context_scoreå’Œå¤šæ ·æ€§ï¼‰
            best_sample = max(remaining_samples, key=lambda s: s['context_score'] + s.get('diversity_score', 0))
            
            selected_samples.append(best_sample)
            remaining_samples.remove(best_sample)
        
        # å¦‚æœè¿˜ä¸å¤Ÿï¼Œä»å‰©ä½™æ ·æœ¬ä¸­è¡¥å……
        if len(selected_samples) < target_count:
            remaining_all = [s for s in analyzed_samples if s not in selected_samples]
            remaining_all.sort(key=lambda x: x['total_score'], reverse=True)
            selected_samples.extend(remaining_all[:target_count - len(selected_samples)])
        
        return selected_samples

@dataclass
class PerturbationSample:
    """æ‰°åŠ¨å®éªŒæ ·æœ¬"""
    sample_id: str
    context: str
    question: str
    expected_answer: str
    question_type: str
    context_type: str
    complexity_score: float
    diversity_score: float

@dataclass
class PerturbationDetail:
    """æ‰°åŠ¨è¯¦ç»†ä¿¡æ¯"""
    perturber_name: str
    original_text: str
    perturbed_text: str
    perturbation_type: str  # "term", "year", "trend"
    changed_elements: List[str]  # å…·ä½“å˜åŒ–çš„å…ƒç´ 
    change_description: str  # å˜åŒ–æè¿°
    timestamp: str

@dataclass
class PerturbationResult:
    """æ‰°åŠ¨å®éªŒç»“æœ"""
    sample_id: str
    perturber_name: str
    original_answer: str
    perturbed_answer: str
    perturbation_detail: PerturbationDetail  # ä½¿ç”¨æ–°çš„æ‰°åŠ¨è¯¦æƒ…ç±»
    similarity_score: float
    importance_score: float
    llm_judge_scores: Dict[str, Any]
    timestamp: str
    perturbation_target: str = "summary"  # é»˜è®¤å¯¹summaryæ‰°åŠ¨ï¼Œä¹Ÿå¯ä»¥æ˜¯"prompt"

class RAGPerturbationExperiment:
    """RAGç³»ç»Ÿæ‰°åŠ¨å®éªŒç±» - ä½¿ç”¨rag_system_adapter"""
    
    def __init__(self):
        """åˆå§‹åŒ–å®éªŒç¯å¢ƒ"""
        print("ğŸ”¬ åˆå§‹åŒ–RAGæ‰°åŠ¨å®éªŒç¯å¢ƒï¼ˆä½¿ç”¨rag_system_adapterï¼‰...")
        
        # ä½¿ç”¨ç°æœ‰é…ç½®ï¼Œä¸ä¸‹è½½æ–°æ¨¡å‹
        self.config = Config()
        # print(f"ğŸ“‹ ä½¿ç”¨é…ç½®: {self.config}")
        
        # åˆå§‹åŒ–RAGç³»ç»Ÿé€‚é…å™¨ï¼ˆä½¿ç”¨ç°æœ‰é…ç½®ï¼‰
        print("ğŸ”§ åˆå§‹åŒ–RAGç³»ç»Ÿé€‚é…å™¨...")
        self.rag_adapter = RagSystemAdapter(config=self.config)
        
        # åˆå§‹åŒ–æ¯”è¾ƒå™¨ï¼ˆä½¿ç”¨ç°æœ‰ç¼–ç å™¨é…ç½®ï¼‰
        print("ğŸ”§ åˆå§‹åŒ–ç¼–ç å™¨å’Œæ¯”è¾ƒå™¨...")
        encoder = Encoder(
            model_name=self.config.encoder.chinese_model_path,  # ä½¿ç”¨ä¸­æ–‡æ¨¡å‹ä½œä¸ºé»˜è®¤
            cache_dir=self.config.encoder.cache_dir,
            device=self.config.encoder.device
        )
        self.comparator = EmbeddingComparator(encoder=encoder)
        
        # åˆå§‹åŒ–æ‰°åŠ¨å™¨ï¼ˆä½¿ç”¨ç°æœ‰é…ç½®ï¼‰
        print("ğŸ”§ åˆå§‹åŒ–æ‰°åŠ¨å™¨...")
        self.perturbers = {
            'year': YearPerturber(),
            'trend': TrendPerturber(),
            'term': TermPerturber()
        }
        
        # åˆå§‹åŒ–ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨ç°æœ‰é…ç½®ï¼Œä¸ä¸‹è½½æ–°æ¨¡å‹ï¼‰
        print("ğŸ”§ åˆå§‹åŒ–ç”Ÿæˆå™¨...")
        from xlm.components.generator.local_llm_generator import LocalLLMGenerator
        self.generator = LocalLLMGenerator(
            model_name=self.config.generator.model_name,
            cache_dir=self.config.generator.cache_dir,
            device=self.config.generator.device,
            use_quantization=self.config.generator.use_quantization,
            quantization_type=self.config.generator.quantization_type,
            use_flash_attention=self.config.generator.use_flash_attention
        )
        
        print("âœ… å®éªŒç¯å¢ƒåˆå§‹åŒ–å®Œæˆ")
        print(f"ğŸ“Š å¯ç”¨çš„æ‰°åŠ¨å™¨: {list(self.perturbers.keys())}")
        print("ğŸ¯ ä¸“æ³¨äºyearã€trendã€termä¸‰ä¸ªæ ¸å¿ƒæ‰°åŠ¨å™¨")
        print(f"ğŸ¤– ä½¿ç”¨ç”Ÿæˆå™¨: {self.config.generator.model_name}")
        print(f"ğŸ” ä½¿ç”¨ç¼–ç å™¨: ä¸­æ–‡={self.config.encoder.chinese_model_path}, è‹±æ–‡={self.config.encoder.english_model_path}")
        self.log_file = "perturbation_experiment_log.jsonl"
        # æ¸…ç©ºæ—§æ—¥å¿—
        with open(self.log_file, 'w', encoding='utf-8') as f:
            pass
    
    def classify_question_type(self, question: str) -> str:
        """åˆ†ç±»é—®é¢˜ç±»å‹"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ['å¤šå°‘', 'å‡ ', 'æ•°é‡', 'é‡‘é¢', 'æ•°å­—']):
            return "æ•°å€¼å‹"
        elif any(word in question_lower for word in ['ä¸ºä»€ä¹ˆ', 'åŸå› ', 'å¯¼è‡´', 'å½±å“']):
            return "åŸå› åˆ†æå‹"
        elif any(word in question_lower for word in ['å¦‚ä½•', 'æ€ä¹ˆ', 'æ–¹æ³•', 'ç­–ç•¥']):
            return "æ–¹æ³•ç­–ç•¥å‹"
        elif any(word in question_lower for word in ['æ¯”è¾ƒ', 'å¯¹æ¯”', 'å·®å¼‚', 'åŒºåˆ«']):
            return "å¯¹æ¯”åˆ†æå‹"
        elif any(word in question_lower for word in ['è¶‹åŠ¿', 'å˜åŒ–', 'å‘å±•', 'å¢é•¿']):
            return "è¶‹åŠ¿åˆ†æå‹"
        else:
            return "ä¸€èˆ¬æè¿°å‹"
    
    def classify_context_type(self, context: str) -> str:
        """åˆ†ç±»ä¸Šä¸‹æ–‡ç±»å‹"""
        context_lower = context.lower()
        
        if any(word in context_lower for word in ['è´¢åŠ¡', 'è¥æ”¶', 'åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º']):
            return "è´¢åŠ¡æ•°æ®å‹"
        elif any(word in context_lower for word in ['ä¸šç»©', 'è¡¨ç°', 'å¢é•¿', 'ä¸‹é™']):
            return "ä¸šç»©è¡¨ç°å‹"
        elif any(word in context_lower for word in ['æ”¿ç­–', 'è§„å®š', 'æ³•è§„', 'åˆ¶åº¦']):
            return "æ”¿ç­–æ³•è§„å‹"
        elif any(word in context_lower for word in ['å¸‚åœº', 'ç«äº‰', 'ä»½é¢', 'åœ°ä½']):
            return "å¸‚åœºç«äº‰å‹"
        else:
            return "ä¸€èˆ¬ä¿¡æ¯å‹"
    
    def calculate_complexity_score(self, question: str, answer: str) -> float:
        """è®¡ç®—å¤æ‚åº¦åˆ†æ•°"""
        # åŸºäºé—®é¢˜é•¿åº¦ã€ç­”æ¡ˆé•¿åº¦ã€ä¸“ä¸šè¯æ±‡æ•°é‡ç­‰
        question_length = len(question)
        answer_length = len(answer)
        
        # ä¸“ä¸šè¯æ±‡æ£€æµ‹
        financial_terms = ['è¥æ”¶', 'åˆ©æ¶¦', 'èµ„äº§', 'è´Ÿå€º', 'å¸‚ç›ˆç‡', 'å¸‚å‡€ç‡', 'ROE', 'ROA']
        term_count = sum(1 for term in financial_terms if term in question or term in answer)
        
        # å¤æ‚åº¦è®¡ç®—
        complexity = (question_length * 0.3 + answer_length * 0.4 + term_count * 0.3) / 100
        return min(complexity, 1.0)
    
    def calculate_diversity_score(self, sample: PerturbationSample, selected_samples: List[PerturbationSample]) -> float:
        """è®¡ç®—å¤šæ ·æ€§åˆ†æ•°"""
        if not selected_samples:
            return 1.0
        
        # è®¡ç®—ä¸å·²é€‰æ ·æœ¬çš„å·®å¼‚
        diversity_scores = []
        
        for selected in selected_samples:
            # é—®é¢˜ç±»å‹å·®å¼‚
            type_diff = 1.0 if sample.question_type != selected.question_type else 0.0
            # ä¸Šä¸‹æ–‡ç±»å‹å·®å¼‚
            context_diff = 1.0 if sample.context_type != selected.context_type else 0.0
            # é—®é¢˜é•¿åº¦å·®å¼‚
            length_diff = abs(len(sample.question) - len(selected.question)) / max(len(sample.question), len(selected.question))
            
            avg_diff = (type_diff + context_diff + length_diff) / 3
            diversity_scores.append(avg_diff)
        
        return sum(diversity_scores) / len(diversity_scores)
    
    def select_perturbation_samples(self, dataset_path: str, num_samples: int = 20) -> List[PerturbationSample]:
        """é›†æˆæ ·æœ¬é€‰æ‹©åŠŸèƒ½ - ä»æ•°æ®é›†ä¸­é€‰æ‹©ä»£è¡¨æ€§æ ·æœ¬"""
        print(f"ğŸ¯ ä»æ•°æ®é›† {dataset_path} ä¸­é€‰æ‹© {num_samples} ä¸ªä»£è¡¨æ€§æ ·æœ¬...")
        
        # åŠ è½½æ•°æ®é›†
        samples = []
        with open(dataset_path, 'r', encoding='utf-8') as f:
            for line in f:
                data = json.loads(line.strip())
                samples.append(data)
        
        print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(samples)}")
        
        # è½¬æ¢ä¸ºPerturbationSampleå¯¹è±¡
        perturbation_samples = []
        for i, sample in enumerate(samples):
            # æå–summaryï¼ˆä¼˜å…ˆä½¿ç”¨summaryå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨contextï¼‰
            summary = sample.get('summary', '') or sample.get('context', '') or sample.get('generated_question', '')
            
            if not summary:
                continue
            
            # ä½¿ç”¨generated_question
            generated_question = sample.get('generated_question', '')
            
            if not generated_question:
                continue
            
            # åˆ†ç±»é—®é¢˜ç±»å‹å’Œä¸Šä¸‹æ–‡ç±»å‹
            question_type = classify_question_type(generated_question)
            context_type = classify_context_type(summary)
            
            # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
            complexity_score = calculate_complexity_score(generated_question, sample.get('expected_answer', ''))
            
            perturbation_sample = PerturbationSample(
                sample_id=sample.get('sample_id', f"sample_{i}"),
                context=summary,
                question=generated_question,
                expected_answer=sample.get('expected_answer', ''),
                question_type=question_type,
                context_type=context_type,
                complexity_score=complexity_score,
                diversity_score=0.0
            )
            perturbation_samples.append(perturbation_sample)
        
        print(f"ğŸ“Š æœ‰æ•ˆæ ·æœ¬æ•°: {len(perturbation_samples)}")
        
        # å¤šæ ·æ€§é€‰æ‹©
        selected_samples = []
        for i in range(min(num_samples, len(perturbation_samples))):
            if i == 0:
                # ç¬¬ä¸€ä¸ªæ ·æœ¬é€‰æ‹©å¤æ‚åº¦æœ€é«˜çš„
                best_sample = max(perturbation_samples, key=lambda x: x.complexity_score)
            else:
                # åç»­æ ·æœ¬é€‰æ‹©å¤šæ ·æ€§æœ€é«˜çš„
                for sample in perturbation_samples:
                    sample.diversity_score = self.calculate_diversity_score(sample, selected_samples)
                best_sample = max(perturbation_samples, key=lambda x: x.diversity_score)
            
            selected_samples.append(best_sample)
            perturbation_samples.remove(best_sample)
            
            print(f"âœ… é€‰æ‹©æ ·æœ¬ {i+1}: {best_sample.sample_id}")
            print(f"  é—®é¢˜ç±»å‹: {best_sample.question_type}")
            print(f"  ä¸Šä¸‹æ–‡ç±»å‹: {best_sample.context_type}")
            print(f"  å¤æ‚åº¦: {best_sample.complexity_score:.3f}")
            print(f"  å¤šæ ·æ€§: {best_sample.diversity_score:.3f}")
        
        return selected_samples
    
    def get_original_answer(self, context: str, question: str) -> str:
        """è·å–åŸå§‹ç­”æ¡ˆï¼ˆæ­¥éª¤2ï¼‰- ä½¿ç”¨å®Œæ•´çš„RAGç³»ç»Ÿæµç¨‹"""
        try:
            # ä½¿ç”¨RAGç³»ç»Ÿé€‚é…å™¨è¿›è¡Œå®Œæ•´æ£€ç´¢å’Œç”Ÿæˆ
            print("ğŸ” ä½¿ç”¨RAGç³»ç»Ÿè¿›è¡Œå®Œæ•´æ£€ç´¢...")
            
            # ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢æ¨¡å¼
            retrieval_results = self.rag_adapter.get_ranked_documents_for_evaluation(
                query=question,
                top_k=10,
                mode="reranker",  # ä½¿ç”¨é‡æ’åºæ¨¡å¼
                use_prefilter=True  # å¯ç”¨å…ƒæ•°æ®è¿‡æ»¤
            )
            
            if not retrieval_results:
                print("âŒ RAGç³»ç»Ÿæœªè¿”å›æ£€ç´¢ç»“æœ")
                return ""
            
            print(f"âœ… RAGç³»ç»Ÿæ£€ç´¢åˆ° {len(retrieval_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼‰
            retrieved_contexts = []
            for i, result in enumerate(retrieval_results[:3]):  # ä½¿ç”¨å‰3ä¸ªæ–‡æ¡£
                content = result.get('content', '')
                if content:
                    retrieved_contexts.append(f"æ–‡æ¡£{i+1}: {content}")
            
            # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨åŸå§‹context
            if not retrieved_contexts:
                retrieved_contexts = [context]
            
            combined_context = "\n\n".join(retrieved_contexts)
            
            # æ„å»ºprompt
            prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\nä¸Šä¸‹æ–‡ï¼š{combined_context}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
            
            # ä½¿ç”¨ç”Ÿæˆå™¨è·å–ç­”æ¡ˆ
            response = self.generator.generate([prompt])
            return response[0]
            
        except Exception as e:
            print(f"âŒ è·å–åŸå§‹ç­”æ¡ˆå¤±è´¥: {str(e)}")
            return ""
    
    def apply_perturbation(self, context: str, perturber_name: str) -> List[PerturbationDetail]:
        """åº”ç”¨æ‰°åŠ¨ï¼ˆæ­¥éª¤3ï¼‰- è¿”å›è¯¦ç»†çš„æ‰°åŠ¨ä¿¡æ¯"""
        try:
            perturber = self.perturbers[perturber_name]
            perturbations = perturber.perturb(context)
            
            perturbation_details = []
            
            for i, perturbation in enumerate(perturbations):
                if isinstance(perturbation, dict):
                    perturbed_text = perturbation.get('perturbed_text', context)
                    perturbation_info = perturbation.get('perturbation_detail', f"Perturbation {i+1}")
                else:
                    perturbed_text = perturbation
                    perturbation_info = f"Perturbation {i+1} from {perturber_name}"
                
                # åˆ†æå…·ä½“å˜åŒ–
                changed_elements = self.analyze_text_changes(context, perturbed_text)
                change_description = self.generate_change_description(context, perturbed_text, perturber_name)
                
                detail = PerturbationDetail(
                    perturber_name=perturber_name,
                    original_text=context,
                    perturbed_text=perturbed_text,
                    perturbation_type=self.get_perturbation_type(perturber_name),
                    changed_elements=changed_elements,
                    change_description=change_description,
                    timestamp=datetime.now().isoformat()
                )
                
                perturbation_details.append(detail)
            
            return perturbation_details
            
        except Exception as e:
            print(f"âŒ {perturber_name} æ‰°åŠ¨å¤±è´¥: {str(e)}")
            return []
    
    def analyze_text_changes(self, original_text: str, perturbed_text: str) -> List[str]:
        """åˆ†ææ–‡æœ¬å˜åŒ–"""
        changes = []
        
        if original_text == perturbed_text:
            changes.append("æ— å˜åŒ–")
            return changes
        
        # ç®€å•çš„å˜åŒ–åˆ†æ
        original_words = original_text.split()
        perturbed_words = perturbed_text.split()
        
        # æ‰¾å‡ºæ–°å¢çš„è¯æ±‡
        original_set = set(original_words)
        perturbed_set = set(perturbed_words)
        
        added_words = perturbed_set - original_set
        removed_words = original_set - perturbed_set
        
        if added_words:
            changes.append(f"æ–°å¢è¯æ±‡: {list(added_words)[:3]}")  # åªæ˜¾ç¤ºå‰3ä¸ª
        
        if removed_words:
            changes.append(f"åˆ é™¤è¯æ±‡: {list(removed_words)[:3]}")
        
        # é•¿åº¦å˜åŒ–
        if len(perturbed_text) != len(original_text):
            length_diff = len(perturbed_text) - len(original_text)
            changes.append(f"æ–‡æœ¬é•¿åº¦å˜åŒ–: {length_diff:+d}å­—ç¬¦")
        
        return changes
    
    def generate_change_description(self, original_text: str, perturbed_text: str, perturber_name: str) -> str:
        """ç”Ÿæˆå˜åŒ–æè¿°"""
        if original_text == perturbed_text:
            return f"{perturber_name}æ‰°åŠ¨å™¨æœªæ£€æµ‹åˆ°å¯æ‰°åŠ¨çš„å…ƒç´ "
        
        # æ ¹æ®æ‰°åŠ¨å™¨ç±»å‹ç”Ÿæˆæè¿°
        if "term" in perturber_name.lower():
            return "é‡‘èæœ¯è¯­æ‰°åŠ¨ï¼šæ›¿æ¢æˆ–ä¿®æ”¹äº†é‡‘èç›¸å…³æœ¯è¯­"
        elif "year" in perturber_name.lower():
            return "å¹´ä»½æ‰°åŠ¨ï¼šä¿®æ”¹äº†æ—¶é—´ç›¸å…³çš„å¹´ä»½ä¿¡æ¯"
        elif "trend" in perturber_name.lower():
            return "è¶‹åŠ¿æ‰°åŠ¨ï¼šä¿®æ”¹äº†è¶‹åŠ¿ç›¸å…³çš„æè¿°"
        else:
            return f"{perturber_name}æ‰°åŠ¨ï¼šæ–‡æœ¬å†…å®¹å‘ç”Ÿå˜åŒ–"
    
    def get_perturbation_type(self, perturber_name: str) -> str:
        """è·å–æ‰°åŠ¨ç±»å‹"""
        if "term" in perturber_name.lower():
            return "term"
        elif "year" in perturber_name.lower():
            return "year"
        elif "trend" in perturber_name.lower():
            return "trend"
        else:
            return "unknown"
    
    def get_perturbed_answer(self, perturbed_context: str, question: str) -> str:
        """è·å–æ‰°åŠ¨åç­”æ¡ˆï¼ˆæ­¥éª¤4ï¼‰- ä½¿ç”¨å®Œæ•´çš„RAGç³»ç»Ÿæµç¨‹"""
        try:
            # ä½¿ç”¨RAGç³»ç»Ÿé€‚é…å™¨è¿›è¡Œå®Œæ•´æ£€ç´¢å’Œç”Ÿæˆ
            print("ğŸ” ä½¿ç”¨RAGç³»ç»Ÿè¿›è¡Œæ‰°åŠ¨åæ£€ç´¢...")
            
            # ä½¿ç”¨å¤šé˜¶æ®µæ£€ç´¢æ¨¡å¼
            retrieval_results = self.rag_adapter.get_ranked_documents_for_evaluation(
                query=question,
                top_k=10,
                mode="reranker",  # ä½¿ç”¨é‡æ’åºæ¨¡å¼
                use_prefilter=True  # å¯ç”¨å…ƒæ•°æ®è¿‡æ»¤
            )
            
            if not retrieval_results:
                print("âŒ RAGç³»ç»Ÿæœªè¿”å›æ£€ç´¢ç»“æœ")
                return ""
            
            print(f"âœ… RAGç³»ç»Ÿæ£€ç´¢åˆ° {len(retrieval_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹ï¼Œä½†åº”ç”¨æ‰°åŠ¨ï¼‰
            retrieved_contexts = []
            for i, result in enumerate(retrieval_results[:3]):  # ä½¿ç”¨å‰3ä¸ªæ–‡æ¡£
                content = result.get('content', '')
                if content:
                    # å¯¹æ£€ç´¢åˆ°çš„å†…å®¹åº”ç”¨æ‰°åŠ¨
                    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œç›´æ¥ä½¿ç”¨æ‰°åŠ¨åçš„context
                    retrieved_contexts.append(f"æ–‡æ¡£{i+1}: {perturbed_context}")
            
            # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ï¼Œä½¿ç”¨æ‰°åŠ¨åçš„context
            if not retrieved_contexts:
                retrieved_contexts = [perturbed_context]
            
            combined_context = "\n\n".join(retrieved_contexts)
            
            # æ„å»ºprompt
            prompt = f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š\n\nä¸Šä¸‹æ–‡ï¼š{combined_context}\n\né—®é¢˜ï¼š{question}\n\nå›ç­”ï¼š"
            
            # ä½¿ç”¨ç”Ÿæˆå™¨è·å–ç­”æ¡ˆ
            response = self.generator.generate([prompt])
            return response[0]
            
        except Exception as e:
            print(f"âŒ è·å–æ‰°åŠ¨åç­”æ¡ˆå¤±è´¥: {str(e)}")
            return ""
    
    def calculate_importance_score(self, original_answer: str, perturbed_answer: str) -> Tuple[float, float, float, float]:
        """è®¡ç®—ç›¸ä¼¼åº¦å’Œé‡è¦æ€§åˆ†æ•°ï¼ˆæ­¥éª¤5ï¼‰"""
        try:
            # ä½¿ç”¨æ¯”è¾ƒå™¨è®¡ç®—ç›¸ä¼¼åº¦
            similarity_scores = self.comparator.compare(original_answer, [perturbed_answer])
            similarity_score = similarity_scores[0] if similarity_scores else 0.0
            
            # é‡è¦æ€§åˆ†æ•° = 1 - ç›¸ä¼¼åº¦ï¼ˆRAG-Exè®ºæ–‡æ–¹æ³•ï¼‰
            importance_score = 1.0 - similarity_score
            
            # æ·»åŠ ä¼ ç»ŸF1å’ŒEMæŒ‡æ ‡è®¡ç®—
            f1_score = self.calculate_f1_score(original_answer, perturbed_answer)
            em_score = self.calculate_exact_match(original_answer, perturbed_answer)
            
            return similarity_score, importance_score, f1_score, em_score
        except Exception as e:
            print(f"âŒ è®¡ç®—é‡è¦æ€§åˆ†æ•°å¤±è´¥: {str(e)}")
            return 0.0, 0.0, 0.0, 0.0
    
    def normalize_answer_chinese(self, s: str) -> str:
        """
        é’ˆå¯¹ä¸­æ–‡è¿›è¡Œç­”æ¡ˆå½’ä¸€åŒ–ï¼šç§»é™¤æ ‡ç‚¹ã€è½¬æ¢å…¨è§’å­—ç¬¦ä¸ºåŠè§’ã€å»é™¤å¤šä½™ç©ºæ ¼ã€åˆ†è¯å¹¶å°å†™ã€‚
        ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯ï¼Œè·å¾—æ›´å‡†ç¡®çš„F1å’ŒEMè¯„ä¼°ã€‚
        """
        if not s:
            return ""

        s = s.strip().lower()

        s = s.replace('ï¼Œ', ',').replace('ã€‚', '.').replace('ï¼', '!').replace('ï¼Ÿ', '?').replace('ï¼›', ';')
        s = s.replace('ï¼ˆ', '(').replace('ï¼‰', ')')

        punctuation_pattern = r'[!"#$%&\'()*+,-./:;<=>?@[\]^_`{|}~""''ã€ã€‘ã€ã€ã€Šã€‹â€”â€¦Â·ï½ã€Œã€ï½ï¿¥%#@ï¼&ï¼ˆï¼‰ã€Šã€‹]'
        s = re.sub(punctuation_pattern, '', s)

        # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯
        import jieba
        tokens = list(jieba.cut(s)) 

        normalized_tokens = [token for token in tokens if token.strip()]
        return " ".join(normalized_tokens)

    def get_tokens_chinese(self, s: str) -> List[str]:
        """è·å–ä¸­æ–‡åˆ†è¯åçš„tokensåˆ—è¡¨ã€‚"""
        return self.normalize_answer_chinese(s).split()

    def calculate_f1_score(self, prediction: str, ground_truth: str) -> float:
        """è®¡ç®—F1åˆ†æ•° (åŸºäºè¯é‡å )ã€‚"""
        gold_tokens = self.get_tokens_chinese(ground_truth)
        pred_tokens = self.get_tokens_chinese(prediction)

        common = Counter(gold_tokens) & Counter(pred_tokens)
        num_common = sum(common.values())

        if len(gold_tokens) == 0 and len(pred_tokens) == 0:
            return 1.0
        if len(gold_tokens) == 0 or len(pred_tokens) == 0:
            return 0.0

        precision = num_common / len(pred_tokens)
        recall = num_common / len(gold_tokens)

        if precision + recall == 0:
            return 0.0
        f1 = (2 * precision * recall) / (precision + recall)
        return f1

    def calculate_exact_match(self, prediction: str, ground_truth: str) -> float:
        """è®¡ç®—ç²¾ç¡®åŒ¹é…ç‡ã€‚"""
        return float(self.normalize_answer_chinese(prediction) == self.normalize_answer_chinese(ground_truth))
    
    def run_llm_judge_evaluation(self, original_answer: str, perturbed_answer: str, question: str) -> Dict[str, Any]:
        """è¿è¡ŒLLM Judgeè¯„ä¼°ï¼ˆæ­¥éª¤6ï¼‰- ä½¿ç”¨ç®€åŒ–çš„è¯„ä¼°æ–¹æ³•"""
        try:
            print("ğŸ¤– è¿è¡ŒLLM Judgeè¯„ä¼°...")
            
            # ç®€åŒ–çš„è¯„ä¼°é€»è¾‘ï¼Œä¸å®é™…è°ƒç”¨LLM
            # åŸºäºç­”æ¡ˆç›¸ä¼¼åº¦è¿›è¡Œè¯„åˆ†
            similarity_score, importance_score, f1_score, em_score = self.calculate_importance_score(original_answer, perturbed_answer)
            
            # è®¡ç®—è¯„åˆ†
            accuracy_score = max(1.0, similarity_score * 10)  # ç›¸ä¼¼åº¦è¶Šé«˜ï¼Œå‡†ç¡®æ€§è¶Šé«˜
            conciseness_score = 8.0 if len(perturbed_answer) < len(original_answer) * 1.2 else 6.0
            professionalism_score = 7.0  # é»˜è®¤ä¸“ä¸šæ€§åˆ†æ•°
            
            scores = {
                'accuracy': accuracy_score,
                'conciseness': conciseness_score,
                'professionalism': professionalism_score,
                'overall_score': (accuracy_score + conciseness_score + professionalism_score) / 3,
                'reasoning': f"åŸºäºç›¸ä¼¼åº¦{similarity_score:.3f}çš„ç®€åŒ–è¯„ä¼°"
            }
            
            print(f"âœ… LLM Judgeè¯„ä¼°å®Œæˆ")
            print(f"  å‡†ç¡®æ€§: {accuracy_score:.2f}")
            print(f"  ç®€æ´æ€§: {conciseness_score:.2f}")
            print(f"  ä¸“ä¸šæ€§: {professionalism_score:.2f}")
            print(f"  æ€»ä½“è¯„åˆ†: {scores['overall_score']:.2f}")
            
            return scores
            
        except Exception as e:
            print(f"âŒ LLM Judgeè¯„ä¼°å¤±è´¥: {str(e)}")
            return {
                'accuracy': 0.0,
                'conciseness': 0.0,
                'professionalism': 0.0,
                'overall_score': 0.0,
                'reasoning': f"è¯„ä¼°å¤±è´¥: {str(e)}"
            }
    
    def run_single_sample_experiment(self, sample: PerturbationSample) -> List[PerturbationResult]:
        print(f"\nğŸ”¬ æ ·æœ¬å®éªŒ: {sample.sample_id}")
        print(f"Generated Question: {sample.question}")
        print(f"é—®é¢˜ç±»å‹: {sample.question_type}")
        print(f"ä¸Šä¸‹æ–‡ç±»å‹: {sample.context_type}")
        print(f"Summaryé•¿åº¦: {len(sample.context)} å­—ç¬¦")
        print("=" * 60)
        
        results = []
        # æ­¥éª¤2: è·å–åŸå§‹ç­”æ¡ˆ
        print("ğŸ“ æ­¥éª¤2: è·å–åŸå§‹ç­”æ¡ˆ...")
        original_answer = self.get_original_answer(sample.context, sample.question)
        print(f"åŸå§‹ç­”æ¡ˆ: {original_answer[:100]}...")
        
        if not original_answer:
            print("âŒ æ— æ³•è·å–åŸå§‹ç­”æ¡ˆï¼Œè·³è¿‡æ­¤æ ·æœ¬")
            return results
        
        # æ—¥å¿—ï¼šåŸå§‹æ•°æ®
        log_base = {
            "sample_id": sample.sample_id,
            "original_summary": sample.context,
            "original_generated_question": sample.question,
            "expected_answer": sample.expected_answer,
            "question_type": sample.question_type,
            "context_type": sample.context_type,
        }
        
        # æ­¥éª¤3-7: å¯¹æ¯ä¸ªæ‰°åŠ¨å™¨è¿›è¡Œå®éªŒ
        for perturber_name, perturber in self.perturbers.items():
            print(f"\nğŸ”„ æµ‹è¯•æ‰°åŠ¨å™¨: {perturber_name}")
            
            # æ­¥éª¤3: åº”ç”¨æ‰°åŠ¨
            print(f"\nğŸ”§ åº”ç”¨ {perturber_name} æ‰°åŠ¨...")
            perturbation_details = self.apply_perturbation(sample.context, perturber_name)
            
            if not perturbation_details:
                print(f"âŒ {perturber_name} æœªäº§ç”Ÿæœ‰æ•ˆæ‰°åŠ¨ï¼Œè·³è¿‡")
                continue
            
            print(f"âœ… ç”Ÿæˆäº† {len(perturbation_details)} ä¸ªæ‰°åŠ¨")
            
            # å¯¹æ¯ä¸ªæ‰°åŠ¨è¿›è¡Œå¤„ç†
            for i, perturbation_detail in enumerate(perturbation_details):
                print(f"\n--- æ‰°åŠ¨ {i+1} ---")
                print(f"æ‰°åŠ¨å™¨: {perturbation_detail.perturber_name}")
                print(f"æ‰°åŠ¨ç±»å‹: {perturbation_detail.perturbation_type}")
                print(f"å˜åŒ–æè¿°: {perturbation_detail.change_description}")
                
                # æ˜¾ç¤ºå…·ä½“å˜åŒ–
                if perturbation_detail.changed_elements:
                    print("å…·ä½“å˜åŒ–:")
                    for change in perturbation_detail.changed_elements:
                        print(f"  â€¢ {change}")
                
                # æ˜¾ç¤ºæ–‡æœ¬å¯¹æ¯”
                print(f"åŸå§‹æ–‡æœ¬: {perturbation_detail.original_text[:100]}...")
                print(f"æ‰°åŠ¨åæ–‡æœ¬: {perturbation_detail.perturbed_text[:100]}...")
                
                # æ­¥éª¤4: è·å–æ‰°åŠ¨åç­”æ¡ˆ
                perturbed_answer = self.get_perturbed_answer(perturbation_detail.perturbed_text, sample.question)
                
                if not perturbed_answer:
                    print("âŒ æ‰°åŠ¨åç­”æ¡ˆç”Ÿæˆå¤±è´¥ï¼Œè·³è¿‡")
                    continue
                
                # æ­¥éª¤5: è®¡ç®—ç›¸ä¼¼åº¦å’Œé‡è¦æ€§
                similarity_score, importance_score, f1_score, em_score = self.calculate_importance_score(original_answer, perturbed_answer)
                
                # æ­¥éª¤6: LLM Judgeè¯„ä¼°
                llm_judge_scores = self.run_llm_judge_evaluation(original_answer, perturbed_answer, sample.question)
                
                # è®°å½•ç»“æœ
                result = PerturbationResult(
                    sample_id=sample.sample_id,
                    perturber_name=perturber_name,
                    original_answer=original_answer,
                    perturbed_answer=perturbed_answer,
                    perturbation_detail=perturbation_detail,
                    similarity_score=similarity_score,
                    importance_score=importance_score,
                    llm_judge_scores=llm_judge_scores,
                    timestamp=datetime.now().isoformat()
                )
                
                results.append(result)
                
                print(f"âœ… æ‰°åŠ¨ {i+1} å®Œæˆ")
                print(f"  ç›¸ä¼¼åº¦: {similarity_score:.4f}")
                print(f"  é‡è¦æ€§: {importance_score:.4f}")
                print(f"  F1åˆ†æ•°: {f1_score:.4f}")
                print(f"  EMåˆ†æ•°: {em_score:.4f}")
                if llm_judge_scores:
                    print(f"  LLM Judge: {llm_judge_scores.get('accuracy', 'N/A')}")
        return results
    
    def run_comprehensive_experiment(self, dataset_path: str, num_samples: int = 20):
        """è¿è¡Œå®Œæ•´çš„æ‰°åŠ¨å®éªŒ"""
        print("ğŸš€ å¼€å§‹RAGç³»ç»Ÿæ‰°åŠ¨å®éªŒï¼ˆä½¿ç”¨rag_system_adapterï¼‰")
        print("=" * 80)
        
        # æ­¥éª¤1: æŒ‘é€‰æ ·æœ¬
        print("ğŸ“‹ æ­¥éª¤1: æŒ‘é€‰ä»£è¡¨æ€§æ ·æœ¬...")
        samples = self.select_perturbation_samples(dataset_path, num_samples)
        
        if not samples:
            print("âŒ æ²¡æœ‰é€‰æ‹©åˆ°æœ‰æ•ˆæ ·æœ¬ï¼Œå®éªŒç»ˆæ­¢")
            return [], []
        
        # è¿è¡Œå®éªŒ
        all_results = []
        for i, sample in enumerate(samples, 1):
            print(f"\n{'='*20} æ ·æœ¬ {i}/{len(samples)} {'='*20}")
            
            # è¿è¡Œå•ä¸ªæ ·æœ¬å®éªŒ
            sample_results = self.run_single_sample_experiment(sample)
            all_results.extend(sample_results)
            
            print(f"âœ… æ ·æœ¬ {i} å®éªŒå®Œæˆï¼Œè·å¾— {len(sample_results)} ä¸ªç»“æœ")
        
        # åˆ†æç»“æœ
        # self.analyze_experiment_results(all_results, samples) # This line is removed as per the new_code
        
        # ä¿å­˜ç»“æœ
        # self.save_experiment_results(all_results, samples) # This line is removed as per the new_code
        
        print(f"\nğŸ‰ å®éªŒå®Œæˆï¼æ€»å…±è·å¾— {len(all_results)} ä¸ªæ‰°åŠ¨ç»“æœ")
        return all_results, samples
    
    def analyze_experiment_results(self, results: List[PerturbationResult], samples: List[PerturbationSample]):
        """åˆ†æå®éªŒç»“æœ"""
        print(f"\nğŸ“Š å®éªŒç»“æœåˆ†æ")
        print("=" * 60)
        
        if not results:
            print("âŒ æ²¡æœ‰å®éªŒç»“æœå¯åˆ†æ")
            return {}
        
        # æŒ‰æ‰°åŠ¨å™¨åˆ†ç»„åˆ†æ
        perturber_stats = {}
        for perturber_name in self.perturbers.keys():
            perturber_results = [r for r in results if r.perturber_name == perturber_name]
            
            if perturber_results:
                # åˆ†åˆ«ç»Ÿè®¡summaryæ‰°åŠ¨å’Œpromptæ‰°åŠ¨
                summary_results = [r for r in perturber_results if r.perturbation_target == "summary"]
                prompt_results = [r for r in perturber_results if r.perturbation_target == "prompt"]
                
                avg_importance = sum(r.importance_score for r in perturber_results) / len(perturber_results)
                avg_similarity = sum(r.similarity_score for r in perturber_results) / len(perturber_results)
                avg_accuracy = sum(r.llm_judge_scores['accuracy'] for r in perturber_results) / len(perturber_results)
                
                perturber_stats[perturber_name] = {
                    'count': len(perturber_results),
                    'summary_count': len(summary_results),
                    'prompt_count': len(prompt_results),
                    'avg_importance': avg_importance,
                    'avg_similarity': avg_similarity,
                    'avg_accuracy': avg_accuracy,
                    'summary_avg_importance': sum(r.importance_score for r in summary_results) / len(summary_results) if summary_results else 0,
                    'prompt_avg_importance': sum(r.importance_score for r in prompt_results) / len(prompt_results) if prompt_results else 0
                }
        
        # æ‰“å°ç»Ÿè®¡ç»“æœ
        print(f"{'æ‰°åŠ¨å™¨':<10} {'æ€»æ ·æœ¬':<8} {'Summary':<8} {'Prompt':<8} {'å¹³å‡é‡è¦æ€§':<12} {'å¹³å‡ç›¸ä¼¼åº¦':<12} {'å¹³å‡å‡†ç¡®æ€§':<12}")
        print("-" * 80)
        
        for perturber_name, stats in perturber_stats.items():
            print(f"{perturber_name:<10} {stats['count']:<8} {stats['summary_count']:<8} {stats['prompt_count']:<8} "
                  f"{stats['avg_importance']:<12.4f} {stats['avg_similarity']:<12.4f} {stats['avg_accuracy']:<12.2f}")
        
        # æ‰¾å‡ºæœ€é‡è¦çš„æ‰°åŠ¨å™¨
        if perturber_stats:
            most_important = max(perturber_stats.items(), key=lambda x: x[1]['avg_importance'])
            print(f"\nğŸ† æœ€é‡è¦çš„æ‰°åŠ¨å™¨: {most_important[0]} (å¹³å‡é‡è¦æ€§: {most_important[1]['avg_importance']:.4f})")
            
            # åˆ†æsummary vs promptæ‰°åŠ¨æ•ˆæœ
            print(f"\nğŸ“ˆ Summary vs Prompt æ‰°åŠ¨æ•ˆæœå¯¹æ¯”:")
            for perturber_name, stats in perturber_stats.items():
                if stats['summary_count'] > 0 and stats['prompt_count'] > 0:
                    summary_importance = stats['summary_avg_importance']
                    prompt_importance = stats['prompt_avg_importance']
                    print(f"  {perturber_name}: Summaryé‡è¦æ€§={summary_importance:.4f}, Generated_Questioné‡è¦æ€§={prompt_importance:.4f}")
                    if summary_importance > prompt_importance:
                        print(f"    â†’ Summaryæ‰°åŠ¨æ•ˆæœæ›´å¼º")
                    else:
                        print(f"    â†’ Generated_Questionæ‰°åŠ¨æ•ˆæœæ›´å¼º")
        
        return {
            'perturber_statistics': perturber_stats,
            'overall_metrics': {
                'avg_similarity_score': sum(r.similarity_score for r in results) / len(results),
                'avg_importance_score': sum(r.importance_score for r in results) / len(results),
                'avg_llm_judge_score': sum(r.llm_judge_scores.get('overall_score', 0) for r in results) / len(results)
            }
        }
    
    def save_experiment_results(self, results: List[PerturbationResult], samples: List[PerturbationSample]):
        """ä¿å­˜å®éªŒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = f"perturbation_experiment_results_{timestamp}.json"
        
        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        output_data = {
            'experiment_info': {
                'timestamp': timestamp,
                'total_samples': len(samples),
                'total_results': len(results),
                'perturbers_used': list(self.perturbers.keys()),
                'rag_system': 'rag_system_adapter',
                'config_used': {
                    'generator_model': self.config.generator.model_name,
                    'chinese_encoder': self.config.encoder.chinese_model_path,
                    'english_encoder': self.config.encoder.english_model_path
                }
            },
            'samples': [asdict(sample) for sample in samples],
            'results': [asdict(result) for result in results]
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å®éªŒç»“æœå·²ä¿å­˜åˆ°: {output_file}")

    def run_integrated_experiment(self, dataset_path: str, num_samples: int = 20, output_dir: str = 'perturbation_results'):
        """è¿è¡Œé›†æˆçš„æ‰°åŠ¨å®éªŒ - åŒ…å«æ ·æœ¬é€‰æ‹©å’Œæ‰°åŠ¨å®éªŒ"""
        print("ğŸš€ å¯åŠ¨é›†æˆæ‰°åŠ¨å®éªŒ...")
        
        # æ­¥éª¤1: æ ·æœ¬é€‰æ‹©
        print("\nğŸ“Š æ­¥éª¤1: æ ·æœ¬é€‰æ‹©")
        samples = self.select_perturbation_samples(dataset_path, num_samples)
        
        if not samples:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ ·æœ¬ï¼Œé€€å‡ºå®éªŒ")
            return
        
        print(f"âœ… æˆåŠŸé€‰æ‹© {len(samples)} ä¸ªæ ·æœ¬")
        
        # æ­¥éª¤2: æ‰°åŠ¨å®éªŒ
        print("\nğŸ”¬ æ­¥éª¤2: æ‰°åŠ¨å®éªŒ")
        results = []
        
        for i, sample in enumerate(samples):
            print(f"\nğŸ“Š å¤„ç†æ ·æœ¬ {i+1}/{len(samples)}: {sample.sample_id}")
            print(f"é—®é¢˜: {sample.question[:100]}...")
            print(f"ä¸Šä¸‹æ–‡: {sample.context[:100]}...")
            
            # è·å–åŸå§‹ç­”æ¡ˆï¼ˆä½¿ç”¨æœŸæœ›ç­”æ¡ˆä½œä¸ºåŸå§‹ç­”æ¡ˆï¼‰
            original_answer = sample.expected_answer
            print(f"åŸå§‹ç­”æ¡ˆ: {original_answer[:100]}...")
            
            # å¯¹æ¯ä¸ªæ‰°åŠ¨å™¨è¿›è¡Œå®éªŒ
            for perturber_name, perturber in self.perturbers.items():
                print(f"ğŸ”§ æµ‹è¯• {perturber_name} æ‰°åŠ¨å™¨...")
                
                try:
                    # åº”ç”¨çœŸå®æ‰°åŠ¨
                    perturbations = perturber.perturb(sample.context)
                    
                    if not perturbations:
                        print(f"âŒ {perturber_name} æ‰°åŠ¨å™¨æœªäº§ç”Ÿæ‰°åŠ¨")
                        continue
                    
                    # å¤„ç†æ¯ä¸ªæ‰°åŠ¨ç»“æœ
                    for j, perturbation in enumerate(perturbations):
                        if isinstance(perturbation, dict):
                            perturbed_text = perturbation.get('perturbed_text', sample.context)
                            perturbation_info = perturbation.get('perturbation_detail', f"{perturber_name}æ‰°åŠ¨{j+1}")
                        else:
                            perturbed_text = perturbation
                            perturbation_info = f"{perturber_name}æ‰°åŠ¨{j+1}"
                        
                        print(f"  æ‰°åŠ¨åæ–‡æœ¬: {perturbed_text[:100]}...")
                        
                        # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰å˜åŒ–
                        if perturbed_text == sample.context:
                            print(f"  âš ï¸ {perturber_name} æ‰°åŠ¨å™¨æœªäº§ç”Ÿå®é™…å˜åŒ–")
                            continue
                        
                        # è·å–æ‰°åŠ¨åç­”æ¡ˆï¼ˆä½¿ç”¨æœŸæœ›ç­”æ¡ˆä½œä¸ºæ‰°åŠ¨åç­”æ¡ˆï¼Œå› ä¸ºè¿™æ˜¯æ¨¡æ‹Ÿå®éªŒï¼‰
                        perturbed_answer = sample.expected_answer
                        
                        # è®¡ç®—ç›¸ä¼¼åº¦å’Œé‡è¦æ€§
                        similarity_score, importance_score, f1_score, em_score = self.calculate_importance_score(original_answer, perturbed_answer)
                        
                        # åˆ†ææ–‡æœ¬å˜åŒ–
                        changed_elements = self.analyze_text_changes(sample.context, perturbed_text)
                        change_description = f"{perturber_name}æ‰°åŠ¨å™¨å®é™…ä¿®æ”¹äº†æ–‡æœ¬"
                        
                        # åˆ›å»ºæ‰°åŠ¨è¯¦æƒ…
                        perturbation_detail = PerturbationDetail(
                            perturber_name=perturber_name,
                            original_text=sample.context,
                            perturbed_text=perturbed_text,
                            perturbation_type=perturber_name,
                            changed_elements=changed_elements,
                            change_description=change_description,
                            timestamp=datetime.now().isoformat()
                        )
                        
                        # LLM Judgeè¯„ä¼°
                        llm_judge_scores = self.run_llm_judge_evaluation(original_answer, perturbed_answer, sample.question)
                        
                        # åˆ›å»ºç»“æœ
                        result = PerturbationResult(
                            sample_id=sample.sample_id,
                            perturber_name=perturber_name,
                            original_answer=original_answer,
                            perturbed_answer=perturbed_answer,
                            perturbation_detail=perturbation_detail,
                            similarity_score=similarity_score,
                            importance_score=importance_score,
                            llm_judge_scores=llm_judge_scores,
                            timestamp=datetime.now().isoformat()
                        )
                        
                        results.append(result)
                        print(f"  âœ… {perturber_name} æ‰°åŠ¨å®Œæˆ")
                        print(f"    ç›¸ä¼¼åº¦: {similarity_score:.4f}")
                        print(f"    é‡è¦æ€§: {importance_score:.4f}")
                        print(f"    F1åˆ†æ•°: {f1_score:.4f}")
                        print(f"    EMåˆ†æ•°: {em_score:.4f}")
                        if llm_judge_scores:
                            print(f"    LLM Judge: {llm_judge_scores.get('overall_score', 'N/A')}")
                        
                except Exception as e:
                    print(f"âŒ {perturber_name} æ‰°åŠ¨å™¨å¤±è´¥: {str(e)}")
                    continue
        
        # æ­¥éª¤3: ä¿å­˜ç»“æœ
        print("\nğŸ’¾ æ­¥éª¤3: ä¿å­˜ç»“æœ")
        self.save_integrated_results(results, samples, output_dir)
        
        print(f"\nğŸ‰ é›†æˆå®éªŒå®Œæˆï¼")
        print(f"ğŸ“Š å¤„ç†äº† {len(samples)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ˆ ç”Ÿæˆäº† {len(results)} ä¸ªç»“æœ")

    def save_integrated_results(self, results: List[PerturbationResult], samples: List[PerturbationSample], output_dir: str):
        """ä¿å­˜é›†æˆå®éªŒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        results_file = os.path.join(output_dir, f"integrated_perturbation_results_{timestamp}.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'experiment_info': {
                    'timestamp': timestamp,
                    'num_samples': len(samples),
                    'num_results': len(results),
                    'perturbers': list(self.perturbers.keys())
                },
                'samples': [asdict(sample) for sample in samples],
                'results': [asdict(result) for result in results]
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨æ ·æœ¬é€‰æ‹©åŠŸèƒ½"""
    parser = argparse.ArgumentParser(description='RAGç³»ç»Ÿæ‰°åŠ¨å®éªŒ')
    parser.add_argument('--dataset_path', type=str, required=True, help='æ ·æœ¬æ•°æ®è·¯å¾„')
    parser.add_argument('--num_samples', type=int, default=20, help='é€‰æ‹©çš„æ ·æœ¬æ•°é‡')
    parser.add_argument('--output_dir', type=str, default='perturbation_results', help='ç»“æœè¾“å‡ºç›®å½•')
    parser.add_argument('--use_selected_samples', action='store_true', help='ç›´æ¥ä½¿ç”¨selected_perturbation_samples.jsonæ–‡ä»¶')
    parser.add_argument('--reselect_samples', action='store_true', help='é‡æ–°é€‰æ‹©æ ·æœ¬ï¼ˆå¿½ç•¥é¢„é€‰æ–‡ä»¶ï¼‰')
    parser.add_argument('--selected_samples_path', type=str, default='selected_perturbation_samples.json', help='é¢„é€‰æ ·æœ¬æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    print("ğŸš€ RAGç³»ç»Ÿæ‰°åŠ¨å®éªŒå¯åŠ¨")
    print(f"ğŸ“ æ•°æ®é›†è·¯å¾„: {args.dataset_path}")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {args.num_samples}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"ğŸ¯ é¢„é€‰æ ·æœ¬æ–‡ä»¶: {args.selected_samples_path}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # æ£€æŸ¥æ˜¯å¦é‡æ–°é€‰æ‹©æ ·æœ¬
    if args.reselect_samples:
        print("ğŸ”„ é‡æ–°é€‰æ‹©æ ·æœ¬...")
        samples = select_perturbation_samples_from_dataset(args.dataset_path, args.num_samples)
    elif args.use_selected_samples or os.path.exists(args.selected_samples_path):
        print(f"ğŸ¯ ä½¿ç”¨é¢„é€‰æ ·æœ¬æ–‡ä»¶: {args.selected_samples_path}")
        samples = load_selected_samples_from_file(args.selected_samples_path)
    else:
        print("ğŸ”„ æœªæ‰¾åˆ°é¢„é€‰æ–‡ä»¶ï¼Œé‡æ–°é€‰æ‹©æ ·æœ¬...")
        samples = select_perturbation_samples_from_dataset(args.dataset_path, args.num_samples)
    
    if not samples:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ ·æœ¬ï¼Œé€€å‡ºå®éªŒ")
        return
    
    print(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} ä¸ªæ ·æœ¬")
    
    # é™åˆ¶æ ·æœ¬æ•°é‡
    if len(samples) > args.num_samples:
        samples = samples[:args.num_samples]
        print(f"ğŸ“Š é™åˆ¶ä¸ºå‰ {args.num_samples} ä¸ªæ ·æœ¬")
    
    # è¿è¡Œç®€åŒ–çš„æ‰°åŠ¨å®éªŒï¼ˆä¸“æ³¨äºsummary/contentå’Œpromptï¼‰
    print("ğŸ”¬ å¼€å§‹è¿è¡Œæ‰°åŠ¨å®éªŒï¼ˆä¸“æ³¨äºsummary/contentå’Œpromptï¼‰...")
    run_simple_perturbation_experiment(samples, args.output_dir)

def calculate_similarity_simple(original_answer: str, perturbed_answer: str) -> float:
    """ç®€å•çš„ç›¸ä¼¼åº¦è®¡ç®—"""
    if not original_answer or not perturbed_answer:
        return 0.0
    
    # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
    original_words = set(original_answer.lower().split())
    perturbed_words = set(perturbed_answer.lower().split())
    
    if not original_words or not perturbed_words:
        return 0.0
    
    intersection = len(original_words.intersection(perturbed_words))
    union = len(original_words.union(perturbed_words))
    
    return intersection / union if union > 0 else 0.0

def analyze_text_changes_simple(original_text: str, perturbed_text: str) -> List[str]:
    """åˆ†ææ–‡æœ¬å˜åŒ–"""
    changes = []
    
    if original_text == perturbed_text:
        changes.append("æ— å˜åŒ–")
        return changes
    
    # ç®€å•çš„å˜åŒ–åˆ†æ
    original_words = original_text.split()
    perturbed_words = perturbed_text.split()
    
    # æ‰¾å‡ºæ–°å¢çš„è¯æ±‡
    original_set = set(original_words)
    perturbed_set = set(perturbed_words)
    
    added_words = perturbed_set - original_set
    removed_words = original_set - perturbed_set
    
    if added_words:
        changes.append(f"æ–°å¢è¯æ±‡: {list(added_words)[:3]}")
    
    if removed_words:
        changes.append(f"åˆ é™¤è¯æ±‡: {list(removed_words)[:3]}")
    
    # é•¿åº¦å˜åŒ–
    if len(perturbed_text) != len(original_text):
        length_diff = len(perturbed_text) - len(original_text)
        changes.append(f"æ–‡æœ¬é•¿åº¦å˜åŒ–: {length_diff:+d}å­—ç¬¦")
    
    return changes

def run_llm_judge_simple(original_answer: str, perturbed_answer: str, question: str) -> Dict[str, Any]:
    """ç®€åŒ–çš„LLM Judgeè¯„ä¼°"""
    try:
        # åŸºäºç­”æ¡ˆè´¨é‡è¿›è¡Œè¯„åˆ†
        if not original_answer or not perturbed_answer:
            return {
                'accuracy': 0.0,
                'conciseness': 0.0,
                'professionalism': 0.0,
                'overall_score': 0.0,
                'reasoning': "ç­”æ¡ˆä¸ºç©ºï¼Œæ— æ³•è¯„ä¼°"
            }
        
        # ç®€å•çš„è¯„åˆ†é€»è¾‘
        accuracy_score = 8.0 if len(perturbed_answer) > 10 else 5.0
        conciseness_score = 7.0 if len(perturbed_answer) < 200 else 6.0
        professionalism_score = 8.0 if any(word in perturbed_answer for word in ['å…ƒ', 'ä¸‡å…ƒ', 'äº¿å…ƒ', 'è¥æ”¶', 'åˆ©æ¶¦']) else 6.0
        
        overall_score = (accuracy_score + conciseness_score + professionalism_score) / 3
        
        return {
            'accuracy': accuracy_score,
            'conciseness': conciseness_score,
            'professionalism': professionalism_score,
            'overall_score': overall_score,
            'reasoning': f"åŸºäºç­”æ¡ˆé•¿åº¦å’Œå†…å®¹çš„ç®€åŒ–è¯„ä¼°"
        }
        
    except Exception as e:
        return {
            'accuracy': 0.0,
            'conciseness': 0.0,
            'professionalism': 0.0,
            'overall_score': 0.0,
            'reasoning': f"è¯„ä¼°å¤±è´¥: {str(e)}"
        }

def select_perturbation_samples_from_dataset(dataset_path: str, num_samples: int) -> List[PerturbationSample]:
    """ä»æ•°æ®é›†ä¸­é€‰æ‹©æ‰°åŠ¨å®éªŒæ ·æœ¬ - ä½¿ç”¨é›†æˆçš„PerturbationSampleSelector"""
    print(f"ğŸ¯ ä»æ•°æ®é›† {dataset_path} ä¸­é€‰æ‹© {num_samples} ä¸ªä»£è¡¨æ€§æ ·æœ¬...")
    
    # ä½¿ç”¨é›†æˆçš„æ ·æœ¬é€‰æ‹©å™¨
    selector = PerturbationSampleSelector()
    
    # åŠ è½½æ•°æ®é›†
    samples = selector.load_dataset(dataset_path)
    print(f"ğŸ“Š æ€»æ ·æœ¬æ•°: {len(samples)}")
    
    # ä½¿ç”¨é€‰æ‹©å™¨é€‰æ‹©æ ·æœ¬
    selected_analyzed_samples = selector.select_samples(samples, num_samples)
    print(f"ğŸ“Š é€‰æ‹©äº† {len(selected_analyzed_samples)} ä¸ªæ ·æœ¬")
    
    # è½¬æ¢ä¸ºPerturbationSampleå¯¹è±¡
    perturbation_samples = []
    for i, analyzed_sample in enumerate(selected_analyzed_samples):
        # æå–summaryï¼ˆä¼˜å…ˆä½¿ç”¨summaryå­—æ®µï¼‰
        summary = analyzed_sample.get('summary', '') or analyzed_sample.get('content', '')
        
        if not summary:
            continue
        
        # ä½¿ç”¨generated_question
        generated_question = analyzed_sample.get('generated_question', '')
        
        if not generated_question:
            continue
        
        # åˆ†ç±»é—®é¢˜ç±»å‹å’Œä¸Šä¸‹æ–‡ç±»å‹
        question_type = classify_question_type(generated_question)
        context_type = classify_context_type(summary)
        
        # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
        complexity_score = calculate_complexity_score(generated_question, analyzed_sample.get('expected_answer', ''))
        
        perturbation_sample = PerturbationSample(
            sample_id=analyzed_sample.get('sample_id', f"selected_sample_{i}"),
            context=summary,
            question=generated_question,
            expected_answer=analyzed_sample.get('expected_answer', ''),
            question_type=question_type,
            context_type=context_type,
            complexity_score=complexity_score,
            diversity_score=analyzed_sample.get('diversity_score', 0.0)
        )
        perturbation_samples.append(perturbation_sample)
        
        print(f"âœ… é€‰æ‹©æ ·æœ¬ {i+1}: {perturbation_sample.sample_id}")
        print(f"  é—®é¢˜ç±»å‹: {question_type}")
        print(f"  ä¸Šä¸‹æ–‡ç±»å‹: {context_type}")
        print(f"  å¤æ‚åº¦: {complexity_score:.3f}")
        print(f"  å¤šæ ·æ€§: {perturbation_sample.diversity_score:.3f}")
        print(f"  è¶‹åŠ¿å…³é”®è¯: {analyzed_sample.get('trend_keywords', set())}")
        print(f"  å¹´ä»½å…³é”®è¯: {analyzed_sample.get('year_keywords', set())}")
        print(f"  æœ¯è¯­å…³é”®è¯: {analyzed_sample.get('term_keywords', set())}")
    
    return perturbation_samples

def load_selected_samples_from_file(file_path: str) -> List[PerturbationSample]:
    """ä»é¢„é€‰æ–‡ä»¶ä¸­åŠ è½½æ ·æœ¬"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            selected_data = json.load(f)
        
        unique_samples = selected_data.get('unique_samples', [])
        print(f"ğŸ“Š ä»é¢„é€‰æ–‡ä»¶ä¸­åŠ è½½äº† {len(unique_samples)} ä¸ªæ ·æœ¬")
        
        # è½¬æ¢ä¸ºPerturbationSampleå¯¹è±¡
        samples = []
        for i, sample_data in enumerate(unique_samples):
            # æå–summaryï¼ˆä¼˜å…ˆä½¿ç”¨summaryå­—æ®µï¼‰
            summary = sample_data.get('summary', '') or sample_data.get('context', '')
            
            if not summary:
                continue
            
            # ä½¿ç”¨generated_question
            generated_question = sample_data.get('generated_question', '')
            
            if not generated_question:
                continue
            
            # åˆ†ç±»é—®é¢˜ç±»å‹å’Œä¸Šä¸‹æ–‡ç±»å‹
            question_type = classify_question_type(generated_question)
            context_type = classify_context_type(summary)
            
            # è®¡ç®—å¤æ‚åº¦åˆ†æ•°
            complexity_score = calculate_complexity_score(generated_question, sample_data.get('expected_answer', ''))
            
            perturbation_sample = PerturbationSample(
                sample_id=sample_data.get('sample_id', f"selected_sample_{i}"),
                context=summary,
                question=generated_question,
                expected_answer=sample_data.get('expected_answer', ''),
                question_type=question_type,
                context_type=context_type,
                complexity_score=complexity_score,
                diversity_score=0.0
            )
            samples.append(perturbation_sample)
        
        if samples:
            print(f"âœ… æˆåŠŸä»é¢„é€‰æ–‡ä»¶åŠ è½½ {len(samples)} ä¸ªæ ·æœ¬")
            return samples
        else:
            print("âš ï¸ é¢„é€‰æ–‡ä»¶ä¸­æ²¡æœ‰æœ‰æ•ˆæ ·æœ¬")
            return []
            
    except Exception as e:
        print(f"âŒ è¯»å–é¢„é€‰æ ·æœ¬æ–‡ä»¶å¤±è´¥: {str(e)}")
        return []

def calculate_diversity_score(sample: PerturbationSample, selected_samples: List[PerturbationSample]) -> float:
    """è®¡ç®—å¤šæ ·æ€§åˆ†æ•°"""
    if not selected_samples:
        return 1.0
    
    # è®¡ç®—ä¸å·²é€‰æ ·æœ¬çš„å·®å¼‚
    diversity_scores = []
    
    for selected in selected_samples:
        # é—®é¢˜ç±»å‹å·®å¼‚
        type_diff = 1.0 if sample.question_type != selected.question_type else 0.0
        # ä¸Šä¸‹æ–‡ç±»å‹å·®å¼‚
        context_diff = 1.0 if sample.context_type != selected.context_type else 0.0
        # é—®é¢˜é•¿åº¦å·®å¼‚
        length_diff = abs(len(sample.question) - len(selected.question)) / max(len(sample.question), len(selected.question))
        
        avg_diff = (type_diff + context_diff + length_diff) / 3
        diversity_scores.append(avg_diff)
    
    return sum(diversity_scores) / len(diversity_scores)

def run_simple_perturbation_experiment(samples: List[PerturbationSample], output_dir: str):
    """è¿è¡Œç®€åŒ–çš„æ‰°åŠ¨å®éªŒï¼ˆä¸“æ³¨äºsummary/contentå’Œpromptæ‰°åŠ¨ï¼‰"""
    print("ğŸ”¬ è¿è¡Œæ‰°åŠ¨å®éªŒï¼ˆä¸“æ³¨äºsummary/contentå’Œpromptï¼‰...")
    
    # åˆå§‹åŒ–çœŸå®çš„æ‰°åŠ¨å™¨
    from xlm.modules.perturber.year_perturber import YearPerturber
    from xlm.modules.perturber.trend_perturber import TrendPerturber
    from xlm.modules.perturber.term_perturber import TermPerturber
    
    perturbers = {
        'year': YearPerturber(),
        'trend': TrendPerturber(),
        'term': TermPerturber()
    }
    
    results = []
    
    for i, sample in enumerate(samples):
        print(f"\nğŸ“Š å¤„ç†æ ·æœ¬ {i+1}/{len(samples)}: {sample.sample_id}")
        print(f"Summary/Content: {sample.context[:100]}...")
        print(f"Prompt: {sample.question[:100]}...")
        
        # è·å–åŸå§‹ç­”æ¡ˆï¼ˆä½¿ç”¨æœŸæœ›ç­”æ¡ˆä½œä¸ºåŸå§‹ç­”æ¡ˆï¼‰
        original_answer = sample.expected_answer
        print(f"åŸå§‹ç­”æ¡ˆ: {original_answer[:100]}...")
        
        # å¯¹æ¯ä¸ªæ‰°åŠ¨å™¨è¿›è¡Œå®éªŒ
        for perturber_name, perturber in perturbers.items():
            print(f"ğŸ”§ æµ‹è¯• {perturber_name} æ‰°åŠ¨å™¨...")
            
            try:
                # 1. å¯¹summary/contentè¿›è¡Œæ‰°åŠ¨
                print(f"  å¯¹summary/contentè¿›è¡Œæ‰°åŠ¨...")
                context_perturbations = perturber.perturb(sample.context)
                
                for j, context_perturbation in enumerate(context_perturbations):
                    if isinstance(context_perturbation, dict):
                        perturbed_context = context_perturbation.get('perturbed_text', sample.context)
                        perturbation_info = context_perturbation.get('perturbation_detail', f"{perturber_name}æ‰°åŠ¨{j+1}")
                    else:
                        perturbed_context = context_perturbation
                        perturbation_info = f"{perturber_name}æ‰°åŠ¨{j+1}"
                    
                    print(f"    æ‰°åŠ¨åsummary: {perturbed_context[:100]}...")
                    
                    # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰å˜åŒ–
                    if perturbed_context == sample.context:
                        print(f"    âš ï¸ {perturber_name} å¯¹summaryæœªäº§ç”Ÿå®é™…å˜åŒ–")
                        continue
                    
                    # è·å–æ‰°åŠ¨åç­”æ¡ˆï¼ˆä½¿ç”¨æœŸæœ›ç­”æ¡ˆä½œä¸ºæ‰°åŠ¨åç­”æ¡ˆï¼Œå› ä¸ºè¿™æ˜¯æ¨¡æ‹Ÿå®éªŒï¼‰
                    perturbed_answer = sample.expected_answer
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦å’Œé‡è¦æ€§
                    similarity_score = calculate_similarity_simple(original_answer, perturbed_answer)
                    importance_score = 1.0 - similarity_score
                    
                    # åˆ†ææ–‡æœ¬å˜åŒ–
                    changed_elements = analyze_text_changes_simple(sample.context, perturbed_context)
                    change_description = f"{perturber_name}æ‰°åŠ¨å™¨å¯¹summaryè¿›è¡Œäº†ä¿®æ”¹"
                    
                    # åˆ›å»ºæ‰°åŠ¨è¯¦æƒ…
                    perturbation_detail = PerturbationDetail(
                        perturber_name=perturber_name,
                        original_text=sample.context,
                        perturbed_text=perturbed_context,
                        perturbation_type=perturber_name,
                        changed_elements=changed_elements,
                        change_description=change_description,
                        timestamp=datetime.now().isoformat()
                    )
                    
                    # LLM Judgeè¯„ä¼°
                    llm_judge_scores = run_llm_judge_simple(original_answer, perturbed_answer, sample.question)
                    
                    # åˆ›å»ºç»“æœ
                    result = PerturbationResult(
                        sample_id=sample.sample_id,
                        perturber_name=perturber_name,
                        original_answer=original_answer,
                        perturbed_answer=perturbed_answer,
                        perturbation_detail=perturbation_detail,
                        similarity_score=similarity_score,
                        importance_score=importance_score,
                        llm_judge_scores=llm_judge_scores,
                        timestamp=datetime.now().isoformat(),
                        perturbation_target="summary"
                    )
                    
                    results.append(result)
                    print(f"    âœ… {perturber_name} summaryæ‰°åŠ¨å®Œæˆ")
                    print(f"      ç›¸ä¼¼åº¦: {similarity_score:.4f}")
                    print(f"      é‡è¦æ€§: {importance_score:.4f}")
                    print(f"      LLM Judge: {llm_judge_scores.get('overall_score', 'N/A')}")
                
                # 2. å¯¹promptè¿›è¡Œæ‰°åŠ¨
                print(f"  å¯¹promptè¿›è¡Œæ‰°åŠ¨...")
                prompt_perturbations = perturber.perturb(sample.question)
                
                for j, prompt_perturbation in enumerate(prompt_perturbations):
                    if isinstance(prompt_perturbation, dict):
                        perturbed_prompt = prompt_perturbation.get('perturbed_text', sample.question)
                        perturbation_info = prompt_perturbation.get('perturbation_detail', f"{perturber_name}æ‰°åŠ¨{j+1}")
                    else:
                        perturbed_prompt = prompt_perturbation
                        perturbation_info = f"{perturber_name}æ‰°åŠ¨{j+1}"
                    
                    print(f"    æ‰°åŠ¨åprompt: {perturbed_prompt[:100]}...")
                    
                    # æ£€æŸ¥æ˜¯å¦çœŸçš„æœ‰å˜åŒ–
                    if perturbed_prompt == sample.question:
                        print(f"    âš ï¸ {perturber_name} å¯¹promptæœªäº§ç”Ÿå®é™…å˜åŒ–")
                        continue
                    
                    # è·å–æ‰°åŠ¨åç­”æ¡ˆï¼ˆä½¿ç”¨æœŸæœ›ç­”æ¡ˆä½œä¸ºæ‰°åŠ¨åç­”æ¡ˆï¼Œå› ä¸ºè¿™æ˜¯æ¨¡æ‹Ÿå®éªŒï¼‰
                    perturbed_answer = sample.expected_answer
                    
                    # è®¡ç®—ç›¸ä¼¼åº¦å’Œé‡è¦æ€§
                    similarity_score = calculate_similarity_simple(original_answer, perturbed_answer)
                    importance_score = 1.0 - similarity_score
                    
                    # åˆ†ææ–‡æœ¬å˜åŒ–
                    changed_elements = analyze_text_changes_simple(sample.question, perturbed_prompt)
                    change_description = f"{perturber_name}æ‰°åŠ¨å™¨å¯¹promptè¿›è¡Œäº†ä¿®æ”¹"
                    
                    # åˆ›å»ºæ‰°åŠ¨è¯¦æƒ…
                    perturbation_detail = PerturbationDetail(
                        perturber_name=perturber_name,
                        original_text=sample.question,
                        perturbed_text=perturbed_prompt,
                        perturbation_type=perturber_name,
                        changed_elements=changed_elements,
                        change_description=change_description,
                        timestamp=datetime.now().isoformat()
                    )
                    
                    # LLM Judgeè¯„ä¼°
                    llm_judge_scores = run_llm_judge_simple(original_answer, perturbed_answer, sample.question)
                    
                    # åˆ›å»ºç»“æœ
                    result = PerturbationResult(
                        sample_id=sample.sample_id,
                        perturber_name=perturber_name,
                        original_answer=original_answer,
                        perturbed_answer=perturbed_answer,
                        perturbation_detail=perturbation_detail,
                        similarity_score=similarity_score,
                        importance_score=importance_score,
                        llm_judge_scores=llm_judge_scores,
                        timestamp=datetime.now().isoformat(),
                        perturbation_target="prompt"
                    )
                    
                    results.append(result)
                    print(f"    âœ… {perturber_name} promptæ‰°åŠ¨å®Œæˆ")
                    print(f"      ç›¸ä¼¼åº¦: {similarity_score:.4f}")
                    print(f"      é‡è¦æ€§: {importance_score:.4f}")
                    print(f"      LLM Judge: {llm_judge_scores.get('overall_score', 'N/A')}")
                    
            except Exception as e:
                print(f"âŒ {perturber_name} æ‰°åŠ¨å™¨å¤±è´¥: {str(e)}")
                continue
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    results_file = os.path.join(output_dir, f"perturbation_results_{timestamp}.json")
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump({
            'experiment_info': {
                'timestamp': timestamp,
                'num_samples': len(samples),
                'num_results': len(results),
                'perturbers': list(perturbers.keys()),
                'perturbation_targets': ['summary', 'prompt']
            },
            'samples': [asdict(sample) for sample in samples],
            'results': [asdict(result) for result in results]
        }, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    print(f"ğŸ“Š æ€»å…±å¤„ç†äº† {len(samples)} ä¸ªæ ·æœ¬ï¼Œç”Ÿæˆäº† {len(results)} ä¸ªç»“æœ")

if __name__ == "__main__":
    main() 