#!/usr/bin/env python3
"""
8bit é‡åŒ–å†…å­˜é—®é¢˜åˆ†æ
è¯¦ç»†åˆ†æä¸ºä»€ä¹ˆ 8bit é‡åŒ–åœ¨ CUDA:1 ä¸Šæ— æ³•ä½¿ç”¨
"""

import torch
import gc
import os
import sys
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

def analyze_gpu_memory_status():
    """åˆ†æ GPU å†…å­˜çŠ¶æ€"""
    print("ğŸ” GPU å†…å­˜çŠ¶æ€åˆ†æ")
    print("=" * 50)
    
    if torch.cuda.is_available():
        for device_id in [0, 1]:
            gpu_memory = torch.cuda.get_device_properties(device_id).total_memory
            allocated_memory = torch.cuda.memory_allocated(device_id)
            reserved_memory = torch.cuda.memory_reserved(device_id)
            free_memory = gpu_memory - allocated_memory
            
            print(f"ğŸ“Š GPU {device_id}:")
            print(f"   - æ€»å†…å­˜: {gpu_memory / 1024**3:.1f}GB")
            print(f"   - å·²åˆ†é…: {allocated_memory / 1024**3:.1f}GB")
            print(f"   - å·²ä¿ç•™: {reserved_memory / 1024**3:.1f}GB")
            print(f"   - å¯ç”¨å†…å­˜: {free_memory / 1024**3:.1f}GB")
            print(f"   - åˆ©ç”¨ç‡: {(allocated_memory / gpu_memory) * 100:.1f}%")
            print()

def test_memory_allocation_sizes():
    """æµ‹è¯•ä¸åŒå¤§å°çš„å†…å­˜åˆ†é…"""
    print("ğŸ” å†…å­˜åˆ†é…æµ‹è¯•")
    print("=" * 50)
    
    if torch.cuda.is_available():
        # æµ‹è¯•ä¸åŒå¤§å°çš„å†…å­˜å—
        sizes_mb = [100, 500, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000]
        
        for size_mb in sizes_mb:
            try:
                size_bytes = size_mb * 1024 * 1024
                # å°è¯•åˆ†é… float16 å¼ é‡ï¼ˆæ¨¡æ‹Ÿæ¨¡å‹æƒé‡ï¼‰
                tensor = torch.empty(size_bytes // 2, dtype=torch.float16, device='cuda:1')
                print(f"âœ… æˆåŠŸåˆ†é… {size_mb}MB å†…å­˜å—")
                del tensor
                torch.cuda.empty_cache()
            except RuntimeError as e:
                print(f"âŒ æ— æ³•åˆ†é… {size_mb}MB å†…å­˜å—: {e}")
                break

def estimate_model_memory_requirements():
    """ä¼°ç®—æ¨¡å‹å†…å­˜éœ€æ±‚"""
    print("ğŸ” æ¨¡å‹å†…å­˜éœ€æ±‚ä¼°ç®—")
    print("=" * 50)
    
    # Fin-R1 æ¨¡å‹å‚æ•°ä¼°ç®—
    model_name = "SUFE-AIFLM-Lab/Fin-R1"
    
    # å¸¸è§çš„æ¨¡å‹å¤§å°ä¼°ç®—
    model_sizes = {
        "7B": 7 * 1024**3,  # 7B å‚æ•°
        "13B": 13 * 1024**3,  # 13B å‚æ•°
        "30B": 30 * 1024**3,  # 30B å‚æ•°
    }
    
    print("ğŸ“Š ä¸åŒç²¾åº¦ä¸‹çš„å†…å­˜éœ€æ±‚ä¼°ç®—:")
    
    for model_size_name, param_count in model_sizes.items():
        print(f"\nğŸ”§ {model_size_name} æ¨¡å‹:")
        
        # FP32 (32ä½æµ®ç‚¹)
        fp32_memory = param_count * 4  # 4 bytes per parameter
        print(f"   - FP32: {fp32_memory / 1024**3:.1f}GB")
        
        # FP16 (16ä½æµ®ç‚¹)
        fp16_memory = param_count * 2  # 2 bytes per parameter
        print(f"   - FP16: {fp16_memory / 1024**3:.1f}GB")
        
        # INT8 (8ä½æ•´æ•°)
        int8_memory = param_count * 1  # 1 byte per parameter
        print(f"   - INT8: {int8_memory / 1024**3:.1f}GB")
        
        # INT4 (4ä½æ•´æ•°)
        int4_memory = param_count * 0.5  # 0.5 bytes per parameter
        print(f"   - INT4: {int4_memory / 1024**3:.1f}GB")
        
        # é¢å¤–å¼€é”€ï¼ˆæ¿€æ´»å€¼ã€æ¢¯åº¦ç­‰ï¼‰
        overhead_ratio = 0.3  # 30% é¢å¤–å¼€é”€
        total_8bit = int8_memory * (1 + overhead_ratio)
        total_4bit = int4_memory * (1 + overhead_ratio)
        
        print(f"   - INT8 (å«å¼€é”€): {total_8bit / 1024**3:.1f}GB")
        print(f"   - INT4 (å«å¼€é”€): {total_4bit / 1024**3:.1f}GB")

def test_8bit_loading_attempt():
    """å°è¯•åŠ è½½ 8bit æ¨¡å‹å¹¶åˆ†æå¤±è´¥åŸå› """
    print("ğŸ” 8bit æ¨¡å‹åŠ è½½æµ‹è¯•")
    print("=" * 50)
    
    model_name = "SUFE-AIFLM-Lab/Fin-R1"
    cache_dir = "/users/sgjfei3/data/huggingface"
    
    try:
        print("ğŸ”§ æ­¥éª¤ 1: åŠ è½½ tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            local_files_only=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print("âœ… Tokenizer åŠ è½½æˆåŠŸ")
        
        print("\nğŸ”§ æ­¥éª¤ 2: é…ç½® 8bit é‡åŒ–...")
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
            llm_int8_has_fp16_weight=False,
        )
        
        print("âœ… 8bit é‡åŒ–é…ç½®å®Œæˆ")
        
        print("\nğŸ”§ æ­¥éª¤ 3: å°è¯•åŠ è½½ 8bit æ¨¡å‹...")
        print("ğŸ“Š åŠ è½½å‰ GPU 1 å†…å­˜çŠ¶æ€:")
        gpu_memory = torch.cuda.get_device_properties(1).total_memory
        allocated_memory = torch.cuda.memory_allocated(1)
        free_memory = gpu_memory - allocated_memory
        print(f"   - å¯ç”¨å†…å­˜: {free_memory / 1024**3:.1f}GB")
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            cache_dir=cache_dir,
            quantization_config=quantization_config,
            device_map="auto",
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            use_cache=False,
            local_files_only=True
        )
        
        print("âœ… 8bit æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ£€æŸ¥åŠ è½½åçš„å†…å­˜ä½¿ç”¨
        allocated_memory_after = torch.cuda.memory_allocated(1)
        memory_used = allocated_memory_after - allocated_memory
        print(f"ğŸ“Š æ¨¡å‹å ç”¨å†…å­˜: {memory_used / 1024**3:.1f}GB")
        
        return True
        
    except Exception as e:
        print(f"âŒ 8bit åŠ è½½å¤±è´¥: {e}")
        
        # åˆ†æé”™è¯¯ä¿¡æ¯
        error_str = str(e)
        if "out of memory" in error_str.lower():
            print("\nğŸ” OOM é”™è¯¯åˆ†æ:")
            print("   - é”™è¯¯ç±»å‹: CUDA å†…å­˜ä¸è¶³")
            print("   - å¯èƒ½åŸå› :")
            print("     * 8bit é‡åŒ–ä»éœ€è¦å¤§é‡å†…å­˜")
            print("     * æ¨¡å‹å¤§å°è¶…å‡ºå¯ç”¨å†…å­˜")
            print("     * å†…å­˜ç¢ç‰‡åŒ–å¯¼è‡´æ— æ³•åˆ†é…è¿ç»­å¤§å—")
        
        elif "cuda" in error_str.lower():
            print("\nğŸ” CUDA é”™è¯¯åˆ†æ:")
            print("   - é”™è¯¯ç±»å‹: CUDA ç›¸å…³é”™è¯¯")
            print("   - å¯èƒ½åŸå› :")
            print("     * CUDA ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜")
            print("     * é©±åŠ¨ç¨‹åºé—®é¢˜")
            print("     * ç¡¬ä»¶é™åˆ¶")
        
        return False

def analyze_memory_fragmentation():
    """åˆ†æå†…å­˜ç¢ç‰‡åŒ–é—®é¢˜"""
    print("ğŸ” å†…å­˜ç¢ç‰‡åŒ–åˆ†æ")
    print("=" * 50)
    
    if torch.cuda.is_available():
        # æ¨¡æ‹Ÿå†…å­˜ç¢ç‰‡åŒ–æƒ…å†µ
        print("ğŸ”§ æ¨¡æ‹Ÿå†…å­˜åˆ†é…æ¨¡å¼...")
        
        # åˆ†é…ä¸€äº›å°çš„å†…å­˜å—
        small_tensors = []
        for i in range(10):
            try:
                tensor = torch.empty(100 * 1024 * 1024, dtype=torch.float16, device='cuda:1')  # 100MB
                small_tensors.append(tensor)
                print(f"âœ… åˆ†é…å°å— {i+1}: 100MB")
            except:
                break
        
        # å°è¯•åˆ†é…å¤§å—å†…å­˜
        try:
            large_tensor = torch.empty(8 * 1024 * 1024 * 1024 // 2, dtype=torch.float16, device='cuda:1')  # 8GB
            print("âœ… æˆåŠŸåˆ†é… 8GB å¤§å—å†…å­˜")
            del large_tensor
        except RuntimeError as e:
            print(f"âŒ æ— æ³•åˆ†é… 8GB å¤§å—å†…å­˜: {e}")
            print("ğŸ’¡ è¿™è¡¨æ˜å­˜åœ¨å†…å­˜ç¢ç‰‡åŒ–é—®é¢˜")
        
        # æ¸…ç†å°å†…å­˜å—
        for tensor in small_tensors:
            del tensor
        torch.cuda.empty_cache()
        
        # å†æ¬¡å°è¯•åˆ†é…å¤§å—å†…å­˜
        try:
            large_tensor = torch.empty(8 * 1024 * 1024 * 1024 // 2, dtype=torch.float16, device='cuda:1')  # 8GB
            print("âœ… æ¸…ç†åæˆåŠŸåˆ†é… 8GB å¤§å—å†…å­˜")
            del large_tensor
        except RuntimeError as e:
            print(f"âŒ æ¸…ç†åä»æ— æ³•åˆ†é… 8GB å¤§å—å†…å­˜: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ 8bit é‡åŒ–å†…å­˜é—®é¢˜åˆ†æ")
    print("=" * 60)
    
    # 1. åˆ†æå½“å‰ GPU å†…å­˜çŠ¶æ€
    analyze_gpu_memory_status()
    
    # 2. ä¼°ç®—æ¨¡å‹å†…å­˜éœ€æ±‚
    estimate_model_memory_requirements()
    
    # 3. æµ‹è¯•å†…å­˜åˆ†é…
    test_memory_allocation_sizes()
    
    # 4. åˆ†æå†…å­˜ç¢ç‰‡åŒ–
    analyze_memory_fragmentation()
    
    # 5. å°è¯•åŠ è½½ 8bit æ¨¡å‹
    print("\n" + "=" * 60)
    success = test_8bit_loading_attempt()
    
    # 6. æ€»ç»“åˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ“Š åˆ†ææ€»ç»“")
    print("=" * 60)
    
    if success:
        print("âœ… 8bit é‡åŒ–å¯ä»¥æ­£å¸¸å·¥ä½œ")
    else:
        print("âŒ 8bit é‡åŒ–æ— æ³•åœ¨ CUDA:1 ä¸Šä½¿ç”¨")
        print("\nğŸ” ä¸»è¦åŸå› :")
        print("1. **å†…å­˜ä¸è¶³**: 8bit é‡åŒ–ä»éœ€è¦å¤§é‡å†…å­˜")
        print("2. **å…¶ä»–è¿›ç¨‹å ç”¨**: PID 587879 æ­£åœ¨ä½¿ç”¨ 17GB+ å†…å­˜")
        print("3. **å†…å­˜ç¢ç‰‡åŒ–**: æ— æ³•åˆ†é…è¿ç»­çš„å¤§å—å†…å­˜")
        print("4. **æ¨¡å‹å¤§å°**: Fin-R1 æ¨¡å‹è¾ƒå¤§ï¼Œ8bit é‡åŒ–åä»è¶…å‡ºå¯ç”¨å†…å­˜")
        
        print("\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
        print("1. **ä½¿ç”¨ 4bit é‡åŒ–**: å†…å­˜éœ€æ±‚å‡åŠ")
        print("2. **ç­‰å¾…å…¶ä»–è¿›ç¨‹ç»“æŸ**: é‡Šæ”¾ GPU å†…å­˜")
        print("3. **ä½¿ç”¨ CPU å›é€€**: é¿å… GPU å†…å­˜é™åˆ¶")
        print("4. **ä¼˜åŒ–å†…å­˜åˆ†é…**: ä½¿ç”¨æ›´æ¿€è¿›çš„å†…å­˜ä¼˜åŒ–ç­–ç•¥")

if __name__ == "__main__":
    main() 