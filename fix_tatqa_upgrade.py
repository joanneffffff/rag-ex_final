#!/usr/bin/env python3
"""
ä¿®å¤TatQAå‡çº§è„šæœ¬
ç¡®ä¿relevant_doc_idsåŒ…å«æ‰€æœ‰èƒ½å›ç­”è¯¥é—®é¢˜çš„æ®µè½/è¡¨æ ¼
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Any, Set
from tqdm import tqdm

def extract_unit_from_paragraph(paragraphs):
    """ä»æ®µè½ä¸­æå–æ•°å€¼å•ä½"""
    for para in paragraphs:
        text = para.get("text", "") if isinstance(para, dict) else para
        match = re.search(r'dollars in (millions|billions)|in (millions|billions)', text, re.IGNORECASE)
        if match:
            unit = match.group(1) or match.group(2)
            if unit:
                return unit.lower().replace('s', '') + " USD"
    return ""

def table_to_natural_text(table_dict, caption="", unit_info=""):
    """å°†è¡¨æ ¼è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€æ–‡æœ¬"""
    rows = table_dict.get("table", [])
    lines = []

    if caption:
        lines.append(f"Table Topic: {caption}.")

    if not rows:
        return ""

    headers = rows[0]
    data_rows = rows[1:]

    for i, row in enumerate(data_rows):
        if not row or all(str(v).strip() == "" for v in row):
            continue

        if len(row) > 1 and str(row[0]).strip() != "" and all(str(v).strip() == "" for v in row[1:]):
            lines.append(f"Table Category: {str(row[0]).strip()}.")
            continue

        row_name = str(row[0]).strip().replace('.', '')

        data_descriptions = []
        for h_idx, v in enumerate(row):
            if h_idx == 0:
                continue
            
            header = headers[h_idx] if h_idx < len(headers) else f"Column {h_idx+1}"
            value = str(v).strip()

            if value:
                if re.match(r'^-?\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?$|^\(\$?\d{1,3}(?:,\d{3})*(?:\.\d+)?\)$', value): 
                    formatted_value = value.replace('$', '')
                    if unit_info:
                        if formatted_value.startswith('(') and formatted_value.endswith(')'):
                             formatted_value = f"(${formatted_value[1:-1]} {unit_info})"
                        else:
                             formatted_value = f"${formatted_value} {unit_info}"
                    else:
                        formatted_value = f"${formatted_value}"
                else:
                    formatted_value = value
                
                data_descriptions.append(f"{header} is {formatted_value}")

        if row_name and data_descriptions:
            lines.append(f"Details for item {row_name}: {'; '.join(data_descriptions)}.")
        elif data_descriptions:
            lines.append(f"Other data item: {'; '.join(data_descriptions)}.")
        elif row_name:
            lines.append(f"Data item: {row_name}.")

    return "\n".join(lines)

def process_tatqa_to_qca_fixed(input_paths, output_path):
    """ä¿®å¤ç‰ˆï¼šå¤„ç†TatQAæ•°æ®é›†ï¼Œç”ŸæˆQ-C-Aæ ¼å¼çš„è¯„ä¼°æ•°æ®"""
    all_data = []
    for input_path in input_paths:
        with open(input_path, "r", encoding="utf-8") as f:
            all_data.extend(json.load(f))

    processed_qa_chunks = []

    for item in tqdm(all_data, desc=f"Processing {Path(output_path).name}"):
        doc_paragraphs = item.get("paragraphs", [])
        doc_tables = item.get("tables", [])
        qa_pairs = item.get("qa_pairs", item.get("questions", []))

        doc_unit_info = extract_unit_from_paragraph(doc_paragraphs)
        doc_id = item.get("uid", f"doc_{len(processed_qa_chunks)}")
        
        for qa in qa_pairs:
            question = qa.get("question", "").strip()
            answer = qa.get("answer", "")
            
            if isinstance(answer, list):
                answer_str = "; ".join(str(a) for a in answer)
            elif not isinstance(answer, str):
                answer_str = str(answer)
            else:
                answer_str = answer.strip()

            if not question or not answer_str:
                continue

            # æ”¶é›†æ‰€æœ‰ç›¸å…³çš„doc_ids
            relevant_doc_ids = set()
            
            # æ ¹æ®rel_paragraphsæ”¶é›†æ‰€æœ‰ç›¸å…³æ®µè½
            rel_paragraphs = qa.get("rel_paragraphs", [])
            for para_idx in rel_paragraphs:
                try:
                    p_idx = int(para_idx) - 1  # TatQAçš„rel_paragraphsæ˜¯1-based
                    if p_idx < len(doc_paragraphs):
                        relevant_doc_ids.add(f"{doc_id}_para_{p_idx}")
                except (ValueError, IndexError):
                    pass
            
            # å¦‚æœç­”æ¡ˆæ¥è‡ªè¡¨æ ¼ï¼Œä¹Ÿæ·»åŠ è¡¨æ ¼çš„doc_id
            answer_from = qa.get("answer_from", "")
            if answer_from in ["table", "table-text"]:
                for t_idx in range(len(doc_tables)):
                    relevant_doc_ids.add(f"{doc_id}_table_{t_idx}")
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç›¸å…³æ®µè½ï¼Œä½†æœ‰æ®µè½å­˜åœ¨ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ®µè½
            if not relevant_doc_ids and doc_paragraphs:
                relevant_doc_ids.add(f"{doc_id}_para_0")
            
            # ç”Ÿæˆcontextï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªç›¸å…³æ®µè½æˆ–è¡¨æ ¼ï¼‰
            correct_chunk_content = ""
            if relevant_doc_ids:
                first_doc_id = list(relevant_doc_ids)[0]
                if "_para_" in first_doc_id:
                    # ä»æ®µè½è·å–å†…å®¹
                    try:
                        p_idx = int(first_doc_id.split("_para_")[1])
                        if p_idx < len(doc_paragraphs):
                            correct_chunk_content = doc_paragraphs[p_idx].get("text", "") if isinstance(doc_paragraphs[p_idx], dict) else doc_paragraphs[p_idx]
                    except:
                        pass
                elif "_table_" in first_doc_id:
                    # ä»è¡¨æ ¼è·å–å†…å®¹
                    try:
                        t_idx = int(first_doc_id.split("_table_")[1])
                        if t_idx < len(doc_tables):
                            correct_chunk_content = table_to_natural_text(doc_tables[t_idx], doc_tables[t_idx].get("caption", ""), doc_unit_info)
                    except:
                        pass
            
            if correct_chunk_content.strip() and relevant_doc_ids:
                processed_qa_chunks.append({
                    "query": question,
                    "context": correct_chunk_content.strip(),
                    "answer": answer_str,
                    "relevant_doc_ids": list(relevant_doc_ids)  # åŒ…å«æ‰€æœ‰ç›¸å…³æ®µè½/è¡¨æ ¼çš„ID
                })

    with open(output_path, "w", encoding="utf-8") as fout:
        for item in processed_qa_chunks:
            fout.write(json.dumps(item, ensure_ascii=False) + "\n")
    
    print(f"Generated Q-Chunk-A data with comprehensive relevant_doc_ids (total {len(processed_qa_chunks)} pairs): {output_path}")
    return processed_qa_chunks

def verify_multi_questions():
    """éªŒè¯æ˜¯å¦æ­£ç¡®å¤„ç†äº†å¤šé—®é¢˜æƒ…å†µ"""
    print("=== éªŒè¯å¤šé—®é¢˜å¤„ç† ===")
    
    # åŠ è½½ä¿®å¤åçš„æ•°æ®
    fixed_eval_path = "evaluate_mrr/tatqa_eval_fixed.jsonl"
    
    if not Path(fixed_eval_path).exists():
        print(f"âŒ ä¿®å¤åçš„è¯„ä¼°æ•°æ®ä¸å­˜åœ¨: {fixed_eval_path}")
        return
    
    # åŠ è½½æ•°æ®
    data = []
    with open(fixed_eval_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    
    print(f"âœ… åŠ è½½äº† {len(data)} ä¸ªä¿®å¤åçš„è¯„ä¼°æ ·æœ¬")
    
    # æŒ‰åŸºç¡€doc_idåˆ†ç»„
    doc_groups = defaultdict(list)
    for item in data:
        relevant_doc_ids = item.get('relevant_doc_ids', [])
        if relevant_doc_ids:
            # æå–åŸºç¡€doc_idï¼ˆå»æ‰chunk_idéƒ¨åˆ†ï¼‰
            base_doc_id = relevant_doc_ids[0].rsplit('_', 1)[0] if '_' in relevant_doc_ids[0] else relevant_doc_ids[0]
            doc_groups[base_doc_id].append(item)
    
    # æ‰¾å‡ºåŒ…å«å¤šä¸ªé—®é¢˜çš„æ–‡æ¡£
    multi_question_docs = {doc_id: items for doc_id, items in doc_groups.items() if len(items) > 1}
    
    print(f"ğŸ“Š ä¿®å¤åçš„ç»Ÿè®¡:")
    print(f"  æ€»æ–‡æ¡£æ•°: {len(doc_groups)}")
    print(f"  åŒ…å«å¤šä¸ªé—®é¢˜çš„æ–‡æ¡£æ•°: {len(multi_question_docs)}")
    
    # æ˜¾ç¤ºå‰3ä¸ªå¤šé—®é¢˜æ–‡æ¡£çš„ç¤ºä¾‹
    print(f"\n=== ä¿®å¤åçš„å¤šé—®é¢˜ç¤ºä¾‹ ===")
    
    for i, (doc_id, items) in enumerate(list(multi_question_docs.items())[:3]):
        print(f"\nğŸ“„ æ–‡æ¡£ {i+1}: {doc_id}")
        print(f"   åŒ…å« {len(items)} ä¸ªé—®é¢˜")
        
        # æŒ‰chunk_idåˆ†ç»„
        chunk_groups = defaultdict(list)
        for item in items:
            relevant_doc_ids = item.get('relevant_doc_ids', [])
            for doc_id_full in relevant_doc_ids:
                chunk_id = doc_id_full.rsplit('_', 1)[1] if '_' in doc_id_full else 'unknown'
                chunk_groups[chunk_id].append(item)
        
        # æ˜¾ç¤ºæ¯ä¸ªchunkçš„é—®é¢˜
        for chunk_id, chunk_items in chunk_groups.items():
            print(f"\n   ğŸ“ Chunk: {chunk_id}")
            print(f"   åŒ…å« {len(chunk_items)} ä¸ªé—®é¢˜:")
            
            for j, item in enumerate(chunk_items[:3]):  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜
                print(f"     {j+1}. é—®é¢˜: {item['query'][:80]}...")
                print(f"        ç­”æ¡ˆ: {item['answer'][:50]}...")
                print(f"        ç›¸å…³æ–‡æ¡£ID: {item['relevant_doc_ids']}")
                print()
            
            if len(chunk_items) > 3:
                print(f"     ... è¿˜æœ‰ {len(chunk_items) - 3} ä¸ªé—®é¢˜")

def main():
    """ä¸»å‡½æ•°"""
    print("=== ä¿®å¤TatQAå‡çº§è„šæœ¬ ===")
    print("ç¡®ä¿relevant_doc_idsåŒ…å«æ‰€æœ‰èƒ½å›ç­”è¯¥é—®é¢˜çš„æ®µè½/è¡¨æ ¼")
    print()
    
    # åŸå§‹TatQAæ•°æ®è·¯å¾„
    tatqa_data_paths = [
        "data/tatqa_dataset_raw/tatqa_dataset_train.json",
        "data/tatqa_dataset_raw/tatqa_dataset_dev.json", 
        "data/tatqa_dataset_raw/tatqa_dataset_test_gold.json"
    ]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    existing_paths = []
    for path in tatqa_data_paths:
        if Path(path).exists():
            existing_paths.append(path)
        else:
            print(f"è­¦å‘Šï¼šæ–‡ä»¶ä¸å­˜åœ¨ {path}")
    
    if not existing_paths:
        print("é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°ä»»ä½•TatQAåŸå§‹æ•°æ®æ–‡ä»¶")
        return
    
    # ç”Ÿæˆä¿®å¤åçš„è¯„ä¼°æ•°æ®
    output_path = "evaluate_mrr/tatqa_eval_fixed.jsonl"
    
    try:
        processed_data = process_tatqa_to_qca_fixed(existing_paths, output_path)
        
        # éªŒè¯æ•°æ®
        print(f"\n=== éªŒè¯ä¿®å¤åçš„æ•°æ® ===")
        print(f"æ€»æ ·æœ¬æ•°: {len(processed_data)}")
        
        # æ£€æŸ¥relevant_doc_idsçš„åˆ†å¸ƒ
        relevant_doc_ids_count = 0
        multi_doc_ids_count = 0
        for item in processed_data:
            if item.get("relevant_doc_ids"):
                relevant_doc_ids_count += 1
                if len(item['relevant_doc_ids']) > 1:
                    multi_doc_ids_count += 1
        
        print(f"åŒ…å«relevant_doc_idsçš„æ ·æœ¬æ•°: {relevant_doc_ids_count}")
        print(f"åŒ…å«å¤šä¸ªrelevant_doc_idsçš„æ ·æœ¬æ•°: {multi_doc_ids_count}")
        print(f"è¦†ç›–ç‡: {relevant_doc_ids_count/len(processed_data)*100:.2f}%")
        print(f"å¤šæ–‡æ¡£è¦†ç›–ç‡: {multi_doc_ids_count/len(processed_data)*100:.2f}%")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ ·æœ¬çš„ç¤ºä¾‹
        print(f"\n=== ä¿®å¤åçš„æ ·æœ¬ç¤ºä¾‹ ===")
        for i, item in enumerate(processed_data[:3]):
            print(f"æ ·æœ¬ {i+1}:")
            print(f"  æŸ¥è¯¢: {item['query'][:100]}...")
            print(f"  ç­”æ¡ˆ: {item['answer'][:50]}...")
            print(f"  ç›¸å…³æ–‡æ¡£ID: {item.get('relevant_doc_ids', [])}")
            print()
        
        print(f"âœ… æˆåŠŸä¿®å¤TatQAè¯„ä¼°æ•°æ®: {output_path}")
        
        # éªŒè¯å¤šé—®é¢˜å¤„ç†
        verify_multi_questions()
        
    except Exception as e:
        print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"\n" + "="*50)
    print("âœ… ä¿®å¤å®Œæˆï¼")
    print("\nä¿®å¤å†…å®¹ï¼š")
    print("1. æ­£ç¡®å¤„ç†äº†rel_paragraphså­—æ®µï¼ŒåŒ…å«æ‰€æœ‰ç›¸å…³æ®µè½")
    print("2. å¯¹äºè¡¨æ ¼é—®é¢˜ï¼Œæ·»åŠ äº†è¡¨æ ¼çš„doc_id")
    print("3. relevant_doc_idsç°åœ¨åŒ…å«æ‰€æœ‰èƒ½å›ç­”è¯¥é—®é¢˜çš„æ®µè½/è¡¨æ ¼")
    print("4. è¿™ç¡®ä¿äº†è¯„ä¼°æ—¶èƒ½å¤Ÿè¿›è¡Œä¸¥æ ¼çš„doc_idåŒ¹é…")
    print("5. æ¶ˆé™¤äº†å› æ¨¡ç³ŠåŒ¹é…å¯¼è‡´çš„é«˜ä¼°é—®é¢˜")

if __name__ == "__main__":
    main() 