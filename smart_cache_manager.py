#!/usr/bin/env python3
"""
æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ - è‡ªåŠ¨æ£€æµ‹å’Œä¿®å¤ç¼“å­˜é—®é¢˜
"""

import os
import sys
import shutil
import numpy as np
import faiss
from pathlib import Path
sys.path.append(str(Path(__file__).parent))

from xlm.utils.dual_language_loader import DualLanguageLoader
from config.parameters import Config

class SmartCacheManager:
    def __init__(self):
        self.config = Config()
        self.cache_dir = self.config.encoder.cache_dir
        self.data_loader = DualLanguageLoader()
        
    def check_cache_health(self):
        """æ£€æŸ¥ç¼“å­˜å¥åº·çŠ¶æ€"""
        print("=== ç¼“å­˜å¥åº·æ£€æŸ¥ ===")
        
        # æ£€æŸ¥ç¼“å­˜ç›®å½•
        if not os.path.exists(self.cache_dir):
            print(f"âŒ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {self.cache_dir}")
            return False
            
        print(f"âœ… ç¼“å­˜ç›®å½•å­˜åœ¨: {self.cache_dir}")
        
        # æ£€æŸ¥è‹±æ–‡æ•°æ®ç¼“å­˜
        english_cache_ok = self._check_english_cache()
        
        # æ£€æŸ¥ä¸­æ–‡æ•°æ®ç¼“å­˜
        chinese_cache_ok = self._check_chinese_cache()
        
        return english_cache_ok and chinese_cache_ok
    
    def _check_english_cache(self):
        """æ£€æŸ¥è‹±æ–‡ç¼“å­˜"""
        print(f"\n--- æ£€æŸ¥è‹±æ–‡ç¼“å­˜ ---")
        
        # åŠ è½½è‹±æ–‡æ•°æ®
        try:
            english_docs = self.data_loader.load_tatqa_context_only(self.config.data.english_data_path)
            print(f"è‹±æ–‡æ–‡æ¡£æ•°é‡: {len(english_docs)}")
            
            if not english_docs:
                print("âŒ æ²¡æœ‰è‹±æ–‡æ–‡æ¡£ï¼Œæ— æ³•æ£€æŸ¥ç¼“å­˜")
                return False
                
        except Exception as e:
            print(f"âŒ åŠ è½½è‹±æ–‡æ•°æ®å¤±è´¥: {e}")
            return False
        
        # ç”Ÿæˆç¼“å­˜é”®
        from xlm.components.encoder.finbert import FinbertEncoder
        encoder_en = FinbertEncoder(
            model_name=self.config.encoder.english_model_path,
            cache_dir=self.config.encoder.cache_dir,
            device=self.config.encoder.device
        )
        
        cache_key = self._get_cache_key(english_docs, str(encoder_en.model_name))
        embeddings_path = self._get_cache_path(cache_key, "npy")
        index_path = self._get_cache_path(cache_key, "faiss")
        
        print(f"ç¼“å­˜é”®: {cache_key}")
        print(f"åµŒå…¥å‘é‡è·¯å¾„: {embeddings_path}")
        print(f"FAISSç´¢å¼•è·¯å¾„: {index_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        embeddings_exist = os.path.exists(embeddings_path)
        index_exist = os.path.exists(index_path)
        
        print(f"åµŒå…¥å‘é‡æ–‡ä»¶å­˜åœ¨: {embeddings_exist}")
        print(f"FAISSç´¢å¼•æ–‡ä»¶å­˜åœ¨: {index_exist}")
        
        if not embeddings_exist or not index_exist:
            print("âŒ è‹±æ–‡ç¼“å­˜æ–‡ä»¶ä¸å®Œæ•´")
            return False
        
        # æ£€æŸ¥åµŒå…¥å‘é‡æœ‰æ•ˆæ€§
        try:
            embeddings = np.load(embeddings_path)
            print(f"åµŒå…¥å‘é‡å½¢çŠ¶: {embeddings.shape}")
            
            if embeddings.shape[0] != len(english_docs):
                print(f"âŒ æ–‡æ¡£æ•°é‡ä¸åŒ¹é…: ç¼“å­˜={embeddings.shape[0]}, å½“å‰={len(english_docs)}")
                return False
                
            if embeddings.size == 0:
                print("âŒ åµŒå…¥å‘é‡ä¸ºç©º")
                return False
                
        except Exception as e:
            print(f"âŒ åµŒå…¥å‘é‡è¯»å–å¤±è´¥: {e}")
            return False
        
        # æ£€æŸ¥FAISSç´¢å¼•æœ‰æ•ˆæ€§
        try:
            index = faiss.read_index(index_path)
            if hasattr(index, 'ntotal'):
                print(f"FAISSç´¢å¼•æ–‡æ¡£æ•°: {index.ntotal}")
                
                if index.ntotal != len(english_docs):
                    print(f"âŒ FAISSç´¢å¼•å¤§å°ä¸åŒ¹é…: ç¼“å­˜={index.ntotal}, å½“å‰={len(english_docs)}")
                    return False
                    
                if index.ntotal == 0:
                    print("âŒ FAISSç´¢å¼•ä¸ºç©º")
                    return False
                    
        except Exception as e:
            print(f"âŒ FAISSç´¢å¼•è¯»å–å¤±è´¥: {e}")
            return False
        
        print("âœ… è‹±æ–‡ç¼“å­˜å¥åº·")
        return True
    
    def _check_chinese_cache(self):
        """æ£€æŸ¥ä¸­æ–‡ç¼“å­˜"""
        print(f"\n--- æ£€æŸ¥ä¸­æ–‡ç¼“å­˜ ---")
        
        # åŠ è½½ä¸­æ–‡æ•°æ®
        try:
            chinese_docs = self.data_loader.load_alphafin_data(self.config.data.chinese_data_path)
            print(f"ä¸­æ–‡æ–‡æ¡£æ•°é‡: {len(chinese_docs)}")
            
            if not chinese_docs:
                print("âŒ æ²¡æœ‰ä¸­æ–‡æ–‡æ¡£ï¼Œæ— æ³•æ£€æŸ¥ç¼“å­˜")
                return False
                
        except Exception as e:
            print(f"âŒ åŠ è½½ä¸­æ–‡æ•°æ®å¤±è´¥: {e}")
            return False
        
        # ç”Ÿæˆç¼“å­˜é”®
        from xlm.components.encoder.finbert import FinbertEncoder
        encoder_ch = FinbertEncoder(
            model_name=self.config.encoder.chinese_model_path,
            cache_dir=self.config.encoder.cache_dir,
            device=self.config.encoder.device
        )
        
        cache_key = self._get_cache_key(chinese_docs, str(encoder_ch.model_name))
        embeddings_path = self._get_cache_path(cache_key, "npy")
        index_path = self._get_cache_path(cache_key, "faiss")
        
        print(f"ç¼“å­˜é”®: {cache_key}")
        print(f"åµŒå…¥å‘é‡è·¯å¾„: {embeddings_path}")
        print(f"FAISSç´¢å¼•è·¯å¾„: {index_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        embeddings_exist = os.path.exists(embeddings_path)
        index_exist = os.path.exists(index_path)
        
        print(f"åµŒå…¥å‘é‡æ–‡ä»¶å­˜åœ¨: {embeddings_exist}")
        print(f"FAISSç´¢å¼•æ–‡ä»¶å­˜åœ¨: {index_exist}")
        
        if not embeddings_exist or not index_exist:
            print("âŒ ä¸­æ–‡ç¼“å­˜æ–‡ä»¶ä¸å®Œæ•´")
            return False
        
        # æ£€æŸ¥åµŒå…¥å‘é‡æœ‰æ•ˆæ€§
        try:
            embeddings = np.load(embeddings_path)
            print(f"åµŒå…¥å‘é‡å½¢çŠ¶: {embeddings.shape}")
            
            if embeddings.shape[0] != len(chinese_docs):
                print(f"âŒ æ–‡æ¡£æ•°é‡ä¸åŒ¹é…: ç¼“å­˜={embeddings.shape[0]}, å½“å‰={len(chinese_docs)}")
                return False
                
            if embeddings.size == 0:
                print("âŒ åµŒå…¥å‘é‡ä¸ºç©º")
                return False
                
        except Exception as e:
            print(f"âŒ åµŒå…¥å‘é‡è¯»å–å¤±è´¥: {e}")
            return False
        
        # æ£€æŸ¥FAISSç´¢å¼•æœ‰æ•ˆæ€§
        try:
            index = faiss.read_index(index_path)
            if hasattr(index, 'ntotal'):
                print(f"FAISSç´¢å¼•æ–‡æ¡£æ•°: {index.ntotal}")
                
                if index.ntotal != len(chinese_docs):
                    print(f"âŒ FAISSç´¢å¼•å¤§å°ä¸åŒ¹é…: ç¼“å­˜={index.ntotal}, å½“å‰={len(chinese_docs)}")
                    return False
                    
                if index.ntotal == 0:
                    print("âŒ FAISSç´¢å¼•ä¸ºç©º")
                    return False
                    
        except Exception as e:
            print(f"âŒ FAISSç´¢å¼•è¯»å–å¤±è´¥: {e}")
            return False
        
        print("âœ… ä¸­æ–‡ç¼“å­˜å¥åº·")
        return True
    
    def _get_cache_key(self, documents, encoder_name):
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        
        # åˆ›å»ºæ–‡æ¡£å†…å®¹çš„å“ˆå¸Œ
        content_hash = hashlib.md5()
        for doc in documents:
            content_hash.update(doc.content.encode('utf-8'))
        
        # åªä½¿ç”¨ç¼–ç å™¨åç§°çš„æœ€åéƒ¨åˆ†ï¼Œé¿å…è·¯å¾„é—®é¢˜
        encoder_basename = os.path.basename(encoder_name)
        
        # ç»“åˆç¼–ç å™¨åç§°å’Œæ–‡æ¡£æ•°é‡
        cache_key = f"{encoder_basename}_{len(documents)}_{content_hash.hexdigest()[:16]}"
        return cache_key
    
    def _get_cache_path(self, cache_key, suffix):
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return os.path.join(self.cache_dir, f"{cache_key}.{suffix}")
    
    def clean_all_cache(self):
        """æ¸…ç†æ‰€æœ‰ç¼“å­˜"""
        print("=== æ¸…ç†æ‰€æœ‰ç¼“å­˜ ===")
        
        if not os.path.exists(self.cache_dir):
            print("ç¼“å­˜ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€æ¸…ç†")
            return
        
        try:
            # åˆ é™¤æ‰€æœ‰.npyå’Œ.faissæ–‡ä»¶
            removed_count = 0
            for file in os.listdir(self.cache_dir):
                if file.endswith(('.npy', '.faiss')):
                    file_path = os.path.join(self.cache_dir, file)
                    os.remove(file_path)
                    removed_count += 1
                    print(f"åˆ é™¤: {file}")
            
            print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {removed_count} ä¸ªç¼“å­˜æ–‡ä»¶")
            
        except Exception as e:
            print(f"âŒ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")
    
    def repair_cache(self):
        """ä¿®å¤ç¼“å­˜é—®é¢˜"""
        print("=== ä¿®å¤ç¼“å­˜é—®é¢˜ ===")
        
        # æ£€æŸ¥ç¼“å­˜å¥åº·çŠ¶æ€
        if self.check_cache_health():
            print("âœ… ç¼“å­˜å¥åº·ï¼Œæ— éœ€ä¿®å¤")
            return True
        
        print("ğŸ”„ æ£€æµ‹åˆ°ç¼“å­˜é—®é¢˜ï¼Œå¼€å§‹ä¿®å¤...")
        
        # æ¸…ç†æ‰€æœ‰ç¼“å­˜
        self.clean_all_cache()
        
        print("âœ… ç¼“å­˜ä¿®å¤å®Œæˆï¼Œä¸‹æ¬¡å¯åŠ¨RAGç³»ç»Ÿæ—¶å°†é‡æ–°ç”Ÿæˆç¼“å­˜")
        return True

def main():
    """ä¸»å‡½æ•°"""
    manager = SmartCacheManager()
    
    print("æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨")
    print("1. æ£€æŸ¥ç¼“å­˜å¥åº·çŠ¶æ€")
    print("2. æ¸…ç†æ‰€æœ‰ç¼“å­˜")
    print("3. ä¿®å¤ç¼“å­˜é—®é¢˜")
    
    choice = input("è¯·é€‰æ‹©æ“ä½œ (1-3): ").strip()
    
    if choice == "1":
        manager.check_cache_health()
    elif choice == "2":
        manager.clean_all_cache()
    elif choice == "3":
        manager.repair_cache()
    else:
        print("æ— æ•ˆé€‰æ‹©")

if __name__ == "__main__":
    main() 