#!/usr/bin/env python3
"""
å¤„ç†åŸå§‹æ•°æ®ï¼Œç§»é™¤[Reranker: Enabled]æ–‡æœ¬å¹¶é‡æ–°è®¡ç®—F1å’ŒEMåˆ†æ•°
"""

import json
import os
import re
from pathlib import Path
from typing import Dict, List, Any

def normalize_answer_chinese(s: str) -> str:
    """
    æ ‡å‡†åŒ–ä¸­æ–‡ç­”æ¡ˆ
    """
    if not s:
        return ""
    
    # ç§»é™¤"è§£æ"åŠå…¶åçš„å†…å®¹
    if "è§£æ" in s:
        s = s.split("è§£æ")[0]
    
    # ç§»é™¤"ã€è§£é‡Šã€‘"åŠå…¶åçš„å†…å®¹
    if "ã€è§£é‡Šã€‘" in s:
        s = s.split("ã€è§£é‡Šã€‘")[0]
    
    # ç§»é™¤"ã€è§£é‡Šã€‘"åŠå…¶åçš„å†…å®¹
    if "è§£é‡Š" in s:
        s = s.split("è§£é‡Š")[0]
    
    # æ¸…ç†ç©ºç™½å­—ç¬¦
    s = re.sub(r'\s+', ' ', s.strip())
    
    return s

def get_tokens_chinese(s: str) -> List[str]:
    """
    ä¸­æ–‡åˆ†è¯
    """
    if not s:
        return []
    return list(s)

def calculate_f1_score(prediction: str, ground_truth: str, language: str = "chinese") -> float:
    """
    è®¡ç®—F1åˆ†æ•°
    """
    if language == "chinese":
        pred_tokens = get_tokens_chinese(prediction)
        truth_tokens = get_tokens_chinese(ground_truth)
    else:
        pred_tokens = prediction.lower().split()
        truth_tokens = ground_truth.lower().split()
    
    if not pred_tokens or not truth_tokens:
        return 0.0
    
    # è®¡ç®—äº¤é›†
    common = set(pred_tokens) & set(truth_tokens)
    
    if not common:
        return 0.0
    
    precision = len(common) / len(pred_tokens)
    recall = len(common) / len(truth_tokens)
    
    if precision + recall == 0:
        return 0.0
    
    return 2 * precision * recall / (precision + recall)

def calculate_exact_match(prediction: str, ground_truth: str, language: str = "chinese") -> float:
    """
    è®¡ç®—ç²¾ç¡®åŒ¹é…åˆ†æ•°
    """
    if language == "chinese":
        pred_normalized = normalize_answer_chinese(prediction)
        truth_normalized = normalize_answer_chinese(ground_truth)
    else:
        pred_normalized = prediction.lower().strip()
        truth_normalized = ground_truth.lower().strip()
    
    return 1.0 if pred_normalized == truth_normalized else 0.0

def process_answer(answer: str) -> str:
    """
    ç§»é™¤ç­”æ¡ˆä¸­çš„[Reranker: Enabled]æ–‡æœ¬
    """
    if not answer:
        return answer
    
    # ç§»é™¤[Reranker: Enabled]å‰ç¼€
    answer = re.sub(r'^\[Reranker: Enabled\]\s*', '', answer)
    
    return answer.strip()

def process_batch_file(file_path: str) -> Dict[str, Any]:
    """
    å¤„ç†å•ä¸ªæ‰¹æ¬¡æ–‡ä»¶
    """
    print(f"ğŸ“ å¤„ç†æ–‡ä»¶: {file_path}")
    
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_samples = len(data["data"])
    processed_samples = 0
    total_f1 = 0.0
    total_em = 0.0
    
    # å¤„ç†æ¯ä¸ªæ ·æœ¬
    for sample in data["data"]:
        original_answer = sample.get("answer", "")
        expected_answer = sample.get("expected_answer", "")
        language = sample.get("language", "chinese")
        
        # ç§»é™¤[Reranker: Enabled]æ–‡æœ¬
        cleaned_answer = process_answer(original_answer)
        
        # é‡æ–°è®¡ç®—F1å’ŒEMåˆ†æ•°
        f1_score = calculate_f1_score(cleaned_answer, expected_answer, language)
        exact_match = calculate_exact_match(cleaned_answer, expected_answer, language)
        
        # æ›´æ–°æ ·æœ¬æ•°æ®
        sample["answer"] = cleaned_answer
        sample["f1"] = f1_score
        sample["em"] = exact_match
        
        # ç´¯è®¡ç»Ÿè®¡
        total_f1 += f1_score
        total_em += exact_match
        processed_samples += 1
    
    # è®¡ç®—å¹³å‡å€¼
    avg_f1 = total_f1 / processed_samples if processed_samples > 0 else 0.0
    avg_em = total_em / processed_samples if processed_samples > 0 else 0.0
    
    # æ›´æ–°æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
    data["avg_f1"] = avg_f1
    data["avg_em"] = avg_em
    data["processed_samples"] = processed_samples
    
    # ä¿å­˜å¤„ç†åçš„æ–‡ä»¶
    output_path = file_path.replace('.json', '_processed.json')
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å¤„ç†å®Œæˆ: {processed_samples} ä¸ªæ ·æœ¬")
    print(f"ğŸ“Š å¹³å‡F1: {avg_f1:.4f}")
    print(f"ğŸ“Š å¹³å‡EM: {avg_em:.4f}")
    print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_path}")
    
    return {
        "file": file_path,
        "processed_samples": processed_samples,
        "avg_f1": avg_f1,
        "avg_em": avg_em
    }

def process_all_batches(data_dir: str):
    """
    å¤„ç†æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶
    """
    print(f"ğŸš€ å¼€å§‹å¤„ç†ç›®å½•: {data_dir}")
    
    if not os.path.exists(data_dir):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        return
    
    # è·å–æ‰€æœ‰æ‰¹æ¬¡æ–‡ä»¶
    batch_files = []
    for file in os.listdir(data_dir):
        if file.startswith('batch_') and file.endswith('.json'):
            batch_files.append(os.path.join(data_dir, file))
    
    batch_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
    
    print(f"ğŸ“‹ æ‰¾åˆ° {len(batch_files)} ä¸ªæ‰¹æ¬¡æ–‡ä»¶")
    
    # å¤„ç†æ‰€æœ‰æ–‡ä»¶
    results = []
    total_samples = 0
    total_f1 = 0.0
    total_em = 0.0
    
    for file_path in batch_files:
        result = process_batch_file(file_path)
        results.append(result)
        
        total_samples += result["processed_samples"]
        total_f1 += result["avg_f1"] * result["processed_samples"]
        total_em += result["avg_em"] * result["processed_samples"]
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    overall_avg_f1 = total_f1 / total_samples if total_samples > 0 else 0.0
    overall_avg_em = total_em / total_samples if total_samples > 0 else 0.0
    
    print("\n" + "=" * 60)
    print("ğŸ“Š æ€»ä½“å¤„ç†ç»“æœ:")
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   å¹³å‡F1: {overall_avg_f1:.4f}")
    print(f"   å¹³å‡EM: {overall_avg_em:.4f}")
    print("=" * 60)
    
    # ä¿å­˜æ€»ä½“ç»“æœ
    summary = {
        "total_samples": total_samples,
        "overall_avg_f1": overall_avg_f1,
        "overall_avg_em": overall_avg_em,
        "batch_results": results
    }
    
    summary_file = os.path.join(data_dir, "processing_summary.json")
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ“ æ€»ä½“ç»“æœä¿å­˜åˆ°: {summary_file}")

def main():
    """
    ä¸»å‡½æ•°
    """
    data_dir = "raw_data_alphafin_eval_samples_updated"
    
    print("ğŸ”§ å¼€å§‹å¤„ç†åŸå§‹æ•°æ®...")
    print("ğŸ“ ä»»åŠ¡: ç§»é™¤[Reranker: Enabled]æ–‡æœ¬å¹¶é‡æ–°è®¡ç®—F1å’ŒEMåˆ†æ•°")
    print("=" * 60)
    
    process_all_batches(data_dir)
    
    print("\nğŸ‰ å¤„ç†å®Œæˆï¼")

if __name__ == "__main__":
    main() 