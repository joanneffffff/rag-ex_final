#!/usr/bin/env python3
"""
æµ‹è¯•å’Œæ¯”è¾ƒfew-shotä¸Chain-of-Thought (CoT) çš„æ•ˆæœ
"""

import sys
from pathlib import Path

# Add parent directory to path
sys.path.append(str(Path(__file__).parent))

def test_prompt_comparison():
    """æ¯”è¾ƒä¸åŒpromptæ¨¡æ¿çš„æ•ˆæœ"""
    try:
        from config.parameters import Config
        from xlm.registry.generator import load_generator
        from xlm.components.rag_system.rag_system import PROMPT_TEMPLATE_ZH, PROMPT_TEMPLATE_ZH_COT
        
        print("ğŸ§ª æµ‹è¯•Few-Shot vs Chain-of-Thoughtæ•ˆæœ...")
        
        # ä¸´æ—¶ä¿®æ”¹é…ç½® - ä½¿ç”¨æ›´å¤§çš„max_new_tokens
        config = Config()
        original_max_tokens = config.generator.max_new_tokens
        config.generator.max_new_tokens = 300  # å¤§å¹…å¢åŠ åˆ°300
        
        print(f"ä½¿ç”¨ç”Ÿæˆå™¨æ¨¡å‹: {config.generator.model_name}")
        print(f"ç”Ÿæˆå‚æ•°: max_tokens=300, temperature={config.generator.temperature}")
        
        # åŠ è½½ç”Ÿæˆå™¨
        generator = load_generator(
            generator_model_name=config.generator.model_name,
            use_local_llm=True,
            use_gpu=True,
            gpu_device="cuda:1",
            cache_dir=config.generator.cache_dir
        )
        
        # æµ‹è¯•é—®é¢˜
        test_question = "é¦–é’¢è‚¡ä»½çš„ä¸šç»©è¡¨ç°å¦‚ä½•ï¼Ÿ"
        
        # æ¨¡æ‹Ÿæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
        test_context = """
        é¦–é’¢è‚¡ä»½2023å¹´ä¸ŠåŠå¹´ä¸šç»©æŠ¥å‘Šæ˜¾ç¤ºï¼Œå…¬å¸å®ç°è¥ä¸šæ”¶å…¥1,234.56äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™21.7%ï¼›
        å‡€åˆ©æ¶¦ä¸º-3.17äº¿å…ƒï¼ŒåŒæ¯”ä¸‹é™77.14%ï¼Œæ¯è‚¡äºæŸ0.06å…ƒã€‚
        å…¬å¸è¡¨ç¤ºå°†é€šè¿‡æ³¨å…¥ä¼˜è´¨èµ„äº§æ¥æå‡é•¿æœŸç›ˆåˆ©èƒ½åŠ›ï¼Œå›é¦ˆè‚¡ä¸œã€‚
        """
        
        # æµ‹è¯•Few-Shotç‰ˆæœ¬
        print("\n" + "="*60)
        print("ğŸ“ æµ‹è¯•Few-Shotç‰ˆæœ¬")
        print("="*60)
        
        prompt_few_shot = PROMPT_TEMPLATE_ZH.format(context=test_context, question=test_question)
        print(f"Prompté•¿åº¦: {len(prompt_few_shot)} å­—ç¬¦")
        
        generated_responses = generator.generate(texts=[prompt_few_shot])
        answer_few_shot = generated_responses[0] if generated_responses else "ç”Ÿæˆå¤±è´¥"
        
        print(f"\nâœ… Few-Shotå›ç­”:")
        print("-" * 40)
        print(answer_few_shot)
        print("-" * 40)
        
        # æµ‹è¯•CoTç‰ˆæœ¬
        print("\n" + "="*60)
        print("ğŸ§  æµ‹è¯•Chain-of-Thoughtç‰ˆæœ¬")
        print("="*60)
        
        prompt_cot = PROMPT_TEMPLATE_ZH_COT.format(context=test_context, question=test_question)
        print(f"Prompté•¿åº¦: {len(prompt_cot)} å­—ç¬¦")
        
        generated_responses = generator.generate(texts=[prompt_cot])
        answer_cot = generated_responses[0] if generated_responses else "ç”Ÿæˆå¤±è´¥"
        
        print(f"\nâœ… CoTå›ç­”:")
        print("-" * 40)
        print(answer_cot)
        print("-" * 40)
        
        # æ¯”è¾ƒåˆ†æ
        print("\n" + "="*60)
        print("ğŸ“Š æ•ˆæœæ¯”è¾ƒåˆ†æ")
        print("="*60)
        
        # é•¿åº¦æ¯”è¾ƒ
        few_shot_length = len(answer_few_shot)
        cot_length = len(answer_cot)
        
        print(f"Few-Shotå›ç­”é•¿åº¦: {few_shot_length} å­—ç¬¦")
        print(f"CoTå›ç­”é•¿åº¦: {cot_length} å­—ç¬¦")
        
        if few_shot_length < cot_length:
            print("âœ… Few-Shotå›ç­”æ›´ç®€æ´")
        else:
            print("âœ… CoTå›ç­”æ›´ç®€æ´")
        
        # è´¨é‡åˆ†æ
        def analyze_quality(answer):
            quality_score = 0
            issues = []
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®ä¿¡æ¯
            if "é¦–é’¢" in answer and ("ä¸šç»©" in answer or "åˆ©æ¶¦" in answer or "æ”¶å…¥" in answer):
                quality_score += 2
            else:
                issues.append("ç¼ºå°‘å…³é”®ä¿¡æ¯")
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å…·ä½“æ•°æ®
            if any(char.isdigit() for char in answer):
                quality_score += 1
            else:
                issues.append("ç¼ºå°‘å…·ä½“æ•°æ®")
            
            # æ£€æŸ¥æ˜¯å¦é€»è¾‘æ¸…æ™°
            if "ä½†æ˜¯" in answer or "ç„¶è€Œ" in answer or "å°½ç®¡" in answer:
                quality_score += 1
            else:
                issues.append("é€»è¾‘å…³ç³»ä¸å¤Ÿæ¸…æ™°")
            
            # æ£€æŸ¥æ˜¯å¦ç®€æ´
            if len(answer) < 200:
                quality_score += 1
            else:
                issues.append("å›ç­”è¿‡é•¿")
            
            return quality_score, issues
        
        few_shot_score, few_shot_issues = analyze_quality(answer_few_shot)
        cot_score, cot_issues = analyze_quality(answer_cot)
        
        print(f"\nFew-Shotè´¨é‡è¯„åˆ†: {few_shot_score}/5")
        if few_shot_issues:
            print(f"Few-Shoté—®é¢˜: {', '.join(few_shot_issues)}")
        
        print(f"CoTè´¨é‡è¯„åˆ†: {cot_score}/5")
        if cot_issues:
            print(f"CoTé—®é¢˜: {', '.join(cot_issues)}")
        
        # æ¨è
        if few_shot_score > cot_score:
            print("\nğŸ† æ¨èä½¿ç”¨Few-Shotç‰ˆæœ¬")
        elif cot_score > few_shot_score:
            print("\nğŸ† æ¨èä½¿ç”¨Chain-of-Thoughtç‰ˆæœ¬")
        else:
            print("\nğŸ¤ ä¸¤ç§æ–¹æ³•æ•ˆæœç›¸å½“")
        
        return {
            "few_shot": answer_few_shot,
            "cot": answer_cot,
            "few_shot_score": few_shot_score,
            "cot_score": cot_score
        }
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    test_prompt_comparison() 